session_id,test_date,query_id,query_text,category,complexity,approach,response_time_ms,success,input_tokens_user,input_tokens_context,input_tokens_tools,input_tokens_files,input_tokens_total,output_tokens_reasoning,output_tokens_tools,output_tokens_code,output_tokens_explanations,output_tokens_total,cache_read_tokens,cache_creation_tokens,tokens_per_result,context_utilization,generation_efficiency,tool_call_count,tools_used,file_reads,file_writes,accuracy_score,result_quality
badee9bd-4114-482e-94e0-25c4201bb901,2025-06-26T07:33:48.096Z,transcript_badee9bd-4114-482e-94e0-25c4201bb901,Find all functions that use asyncio,transcript_query,unknown,mcp,0,True,4,0,0,0,4,0,0,0,1,1,13160,742,1,0.5,0.25,0,,,,,transcript_response
badee9bd-4114-482e-94e0-25c4201bb901,2025-06-26T07:33:52.115Z,transcript_badee9bd-4114-482e-94e0-25c4201bb901,"{'tool_use_id': 'toolu_01BxnWrP5deruNHCMqotmat2', 'type': 'tool_result', 'content': 'Found 72 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/conftest.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_developer_workflows.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_repository_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_semantic_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_plugins_direct.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/populate_bm25_quick.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_reranking_working.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/docs/development/TESTING-GUIDE.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/prometheus_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/watchdog.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/file_watcher.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/grpc_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/MCP.md'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,1,1,13902,5047,1,0.5,0.25,0,,,,,transcript_response
badee9bd-4114-482e-94e0-25c4201bb901,2025-06-26T07:33:57.310Z,transcript_badee9bd-4114-482e-94e0-25c4201bb901,"{'tool_use_id': 'toolu_011MwrmpEc2gEDUqiR1hvcxY', 'type': 'tool_result', 'content': 'Found 76 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_python_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_section_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/security/auth_manager.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/security/security_middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/api_gateway_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/cache_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/dispatcher_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/indexing_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/metrics_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/security_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/shared_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/storage_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_runner.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/benchmark_reranking_comparison.py'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,2,2,18949,2687,2,0.5,0.25,0,,,,,transcript_response
975ed141-04b0-4979-9ed2-38231876b9a9,2025-06-26T07:32:40.807Z,transcript_975ed141-04b0-4979-9ed2-38231876b9a9,Find the BM25Indexer class definition,transcript_query,unknown,mcp,0,True,4,0,0,0,4,0,0,0,58,58,13905,0,58,0.5,0.25,0,,,,,transcript_response
975ed141-04b0-4979-9ed2-38231876b9a9,2025-06-26T07:32:44.051Z,transcript_975ed141-04b0-4979-9ed2-38231876b9a9,"{'tool_use_id': 'toolu_018qBJwnF7aT19i4gE5eks5W', 'type': 'tool_result', 'content': 'Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/bm25_indexer.py'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,25,25,14133,0,25,0.5,0.25,0,,,,,transcript_response
975ed141-04b0-4979-9ed2-38231876b9a9,2025-06-26T07:32:48.004Z,transcript_975ed141-04b0-4979-9ed2-38231876b9a9,"{'tool_use_id': 'toolu_01RWxLRPbTPa7yLUcxF8Ahe3', 'type': 'tool_result', 'content': '     1→""""""\n     2→BM25 Indexer using SQLite FTS5 for full-text search.\n     3→\n     4→This module provides BM25-based full-text search indexing using SQLite\'s FTS5 \n     5→extension, which includes built-in BM25 ranking algorithms.\n     6→""""""\n     7→\n     8→import sqlite3\n     9→import logging\n    10→import hashlib\n    11→import json\n    12→from typing import List, Dict, Any, Optional, Tuple\n    13→from datetime import datetime\n    14→from pathlib import Path\n    15→from contextlib import contextmanager\n    16→\n    17→from ..storage.sqlite_store import SQLiteStore\n    18→# Interface definition inline for now\n    19→from abc import ABC, abstractmethod\n    20→\n    21→logger = logging.getLogger(__name__)\n    22→\n    23→# Define IIndexer interface inline\n    24→class IIndexer(ABC):\n    25→    @abstractmethod\n    26→    def index_documents(self, documents: List[Dict[str, Any]]) -> bool:\n    27→        pass\n    28→    \n    29→    @abstractmethod\n    30→    def search(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:\n    31→        pass\n    32→\n    33→\n    34→class BM25Indexer(IIndexer):\n    35→    """"""\n    36→    BM25-based full-text search indexer using SQLite FTS5.\n    37→    \n    38→    This indexer provides:\n    39→    - Efficient full-text search using inverted indexes\n    40→    - Built-in BM25 ranking algorithm\n    41→    - Support for phrase searches, prefix searches, and boolean operators\n    42→    - Integration with existing SQLite storage\n    43→    """"""\n    44→    \n    45→    def __init__(self, storage: SQLiteStore, table_name: str = ""bm25_content""):\n    46→        """"""\n    47→        Initialize the BM25 indexer.\n    48→        \n    49→        Args:\n    50→            storage: SQLite storage instance\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,1,1,14133,728,1,0.5,0.25,0,,,,,transcript_response
f7e3333c-51c3-40d3-a43a-b4b7a573f5aa,2025-06-26T07:36:17.714Z,transcript_f7e3333c-51c3-40d3-a43a-b4b7a573f5aa,Find the BM25Indexer class definition,transcript_query,unknown,mcp,0,True,4,0,0,0,4,0,0,0,58,58,13905,0,58,0.5,0.25,0,,,,,transcript_response
f7e3333c-51c3-40d3-a43a-b4b7a573f5aa,2025-06-26T07:36:21.835Z,transcript_f7e3333c-51c3-40d3-a43a-b4b7a573f5aa,"{'tool_use_id': 'toolu_01KLEcZQyms98jyEaZwfeqyQ', 'type': 'tool_result', 'content': 'Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/bm25_indexer.py'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,1,1,14133,0,1,0.5,0.25,0,,,,,transcript_response
0be9b10a-8143-4183-be2c-4956b3822221,2025-06-26T07:31:32.678Z,transcript_0be9b10a-8143-4183-be2c-4956b3822221,Locate the SQLiteStore class and explain its search method,transcript_query,unknown,mcp,0,True,4,0,0,0,4,0,0,0,1,1,13160,747,1,0.5,0.25,0,,,,,transcript_response
0be9b10a-8143-4183-be2c-4956b3822221,2025-06-26T07:31:36.781Z,transcript_0be9b10a-8143-4183-be2c-4956b3822221,"Search for the SQLiteStore class definition in the codebase. Look for files that contain ""class SQLiteStore"" or similar patterns. Return the file path and location where this class is defined.",transcript_query,unknown,mcp,0,True,4,0,0,0,4,0,0,0,81,81,0,10750,81,0.5,0.25,0,,,,,transcript_response
0be9b10a-8143-4183-be2c-4956b3822221,2025-06-26T07:31:41.222Z,transcript_0be9b10a-8143-4183-be2c-4956b3822221,"{'tool_use_id': 'toolu_01Cusb5WCkMw7KXUpdnkbXR2', 'type': 'tool_result', 'content': 'Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/storage/sqlite_store.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/path_management_architecture.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/indexer_actual.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/storage_actual.puml'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,105,105,10750,235,105,0.5,0.25,0,,,,,transcript_response
0be9b10a-8143-4183-be2c-4956b3822221,2025-06-26T07:31:44.671Z,transcript_0be9b10a-8143-4183-be2c-4956b3822221,"{'tool_use_id': 'toolu_01UosLuHspyRcapQjBtnW7ku', 'type': 'tool_result', 'content': '     1→""""""\n     2→SQLite-based persistence layer for the MCP Server.\n     3→\n     4→This module provides a local storage implementation using SQLite with FTS5\n     5→for efficient full-text search capabilities.\n     6→""""""\n     7→\n     8→import sqlite3\n     9→import json\n    10→import logging\n    11→from typing import List, Dict, Any, Optional, Tuple, Union\n    12→from datetime import datetime\n    13→from pathlib import Path\n    14→from contextlib import contextmanager\n    15→\n    16→from ..core.path_resolver import PathResolver\n    17→\n    18→logger = logging.getLogger(__name__)\n    19→\n    20→\n    21→class SQLiteStore:\n    22→    """"""SQLite-based storage implementation with FTS5 support.""""""\n    23→    \n    24→    def __init__(self, db_path: str = ""code_index.db"", path_resolver: Optional[PathResolver] = None):\n    25→        """"""\n    26→        Initialize the SQLite store.\n    27→        \n    28→        Args:\n    29→            db_path: Path to the SQLite database file\n    30→            path_resolver: PathResolver instance for path management\n    31→        """"""\n    32→        self.db_path = db_path\n    33→        self.path_resolver = path_resolver or PathResolver()\n    34→        self._init_database()\n    35→        self._run_migrations()\n    36→    \n    37→    def _init_database(self):\n    38→        """"""Initialize database and create schema if needed.""""""\n    39→        with self._get_connection() as conn:\n    40→            # Enable foreign keys\n    41→            conn.execute(""PRAGMA foreign_keys = ON"")\n    42→            \n    43→            # Enable FTS5 if available\n    44→            self._check_fts5_support(conn)\n    45→            \n    46→            # Check if schema exists\n    47→            cursor = conn.execute(\n    48→                ""SELECT name FROM sqlite_master WHERE type=\'table\' AND name=\'schema_version\'""\n    49→            )\n    50→            if not cursor.fetchone():\n    51→                self._init_schema(conn)\n    52→                logger.info(f""Initialized database schema at {self.db_path}"")\n    53→            else:\n    54→                logger.info(f""Using existing database at {self.db_path}"")\n    55→    \n    56→    def _run_migrations(self):\n    57→        """"""Run any pending database migrations.""""""\n    58→        migrations_dir = Path(__file__).parent / ""migrations""\n    59→        if not migrations_dir.exists():\n    60→            return\n    61→        \n    62→        with self._get_connection() as conn:\n    63→            # Get current schema version\n    64→            try:\n    65→                cursor = conn.execute(""SELECT MAX(version) FROM schema_version"")\n    66→                current_version = cursor.fetchone()[0] or 0\n    67→            except sqlite3.OperationalError:\n    68→                current_version = 0\n    69→            \n    70→            # Run migrations\n    71→            for migration_file in sorted(migrations_dir.glob(""*.sql"")):\n    72→                # Extract version from filename (e.g., ""002_relative_paths.sql"" -> 2)\n    73→                try:\n    74→                    version = int(migration_file.stem.split(\'_\')[0])\n    75→                except (ValueError, IndexError):\n    76→                    continue\n    77→                \n    78→                if version > current_version:\n    79→                    logger.info(f""Running migration {migration_file.name}"")\n    80→                    with open(migration_file, \'r\') as f:\n    81→                        conn.executescript(f.read())\n    82→                    logger.info(f""Completed migration to version {version}"")\n    83→    \n    84→    @contextmanager\n    85→    def _get_connection(self):\n    86→        """"""Get a database connection with proper error handling.""""""\n    87→        conn = sqlite3.connect(self.db_path)\n    88→        conn.row_factory = sqlite3.Row\n    89→        try:\n    90→            yield conn\n    91→            conn.commit()\n    92→        except Exception:\n    93→            conn.rollback()\n    94→            raise\n    95→        finally:\n    96→            conn.close()\n    97→    \n    98→    def _check_fts5_support(self, conn: sqlite3.Connection) -> bool:\n    99→        """"""Check if FTS5 is supported in this SQLite build.""""""\n   100→        try:\n   101→            cursor = conn.execute(""PRAGMA compile_options;"")\n   102→            options = [row[0] for row in cursor]\n   103→            fts5_enabled = any(\'ENABLE_FTS5\' in option for option in options)\n   104→            if fts5_enabled:\n   105→                logger.info(""FTS5 support confirmed"")\n   106→            else:\n   107→                logger.warning(""FTS5 may not be available in this SQLite build"")\n   108→            return fts5_enabled\n   109→        except Exception as e:\n   110→            logger.warning(f""Could not check FTS5 support: {e}"")\n   111→            return False\n   112→    \n   113→    def _init_schema(self, conn: sqlite3.Connection):\n   114→        """"""Initialize the database schema.""""""\n   115→        # Create core tables\n   116→        conn.executescript(""""""\n   117→            -- Schema Version\n   118→            CREATE TABLE IF NOT EXISTS schema_version (\n   119→                version INTEGER PRIMARY KEY,\n   120→                applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   121→                description TEXT\n   122→            );\n   123→            \n   124→            -- Repositories\n   125→            CREATE TABLE IF NOT EXISTS repositories (\n   126→                id INTEGER PRIMARY KEY,\n   127→                path TEXT UNIQUE NOT NULL,\n   128→                name TEXT NOT NULL,\n   129→                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   130→                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   131→                metadata JSON\n   132→            );\n   133→            \n   134→            -- Files\n   135→            CREATE TABLE IF NOT EXISTS files (\n   136→                id INTEGER PRIMARY KEY,\n   137→                repository_id INTEGER NOT NULL,\n   138→                path TEXT NOT NULL,\n   139→                relative_path TEXT NOT NULL,\n   140→                language TEXT,\n   141→                size INTEGER,\n   142→                hash TEXT,\n   143→                last_modified TIMESTAMP,\n   144→                indexed_at TIMESTAMP,\n   145→                metadata JSON,\n   146→                FOREIGN KEY (repository_id) REFERENCES repositories(id),\n   147→                UNIQUE(repository_id, path)\n   148→            );\n   149→            \n   150→            CREATE INDEX IF NOT EXISTS idx_files_language ON files(language);\n   151→            CREATE INDEX IF NOT EXISTS idx_files_hash ON files(hash);\n   152→            \n   153→            -- Symbols\n   154→            CREATE TABLE IF NOT EXISTS symbols (\n   155→                id INTEGER PRIMARY KEY,\n   156→                file_id INTEGER NOT NULL,\n   157→                name TEXT NOT NULL,\n   158→                kind TEXT NOT NULL,\n   159→                line_start INTEGER NOT NULL,\n   160→                line_end INTEGER NOT NULL,\n   161→                column_start INTEGER,\n   162→                column_end INTEGER,\n   163→                signature TEXT,\n   164→                documentation TEXT,\n   165→                metadata JSON,\n   166→                FOREIGN KEY (file_id) REFERENCES files(id)\n   167→            );\n   168→            \n   169→            CREATE INDEX IF NOT EXISTS idx_symbols_name ON symbols(name);\n   170→            CREATE INDEX IF NOT EXISTS idx_symbols_kind ON symbols(kind);\n   171→            CREATE INDEX IF NOT EXISTS idx_symbols_file ON symbols(file_id);\n   172→            \n   173→            -- Imports\n   174→            CREATE TABLE IF NOT EXISTS imports (\n   175→                id INTEGER PRIMARY KEY,\n   176→                file_id INTEGER NOT NULL,\n   177→                imported_path TEXT NOT NULL,\n   178→                imported_name TEXT,\n   179→                alias TEXT,\n   180→                line_number INTEGER,\n   181→                is_relative BOOLEAN,\n   182→                metadata JSON,\n   183→                FOREIGN KEY (file_id) REFERENCES files(id)\n   184→            );\n   185→            \n   186→            CREATE INDEX IF NOT EXISTS idx_imports_file ON imports(file_id);\n   187→            CREATE INDEX IF NOT EXISTS idx_imports_path ON imports(imported_path);\n   188→            \n   189→            -- References (using symbol_references to avoid keyword conflict)\n   190→            CREATE TABLE IF NOT EXISTS symbol_references (\n   191→                id INTEGER PRIMARY KEY,\n   192→                symbol_id INTEGER NOT NULL,\n   193→                file_id INTEGER NOT NULL,\n   194→                line_number INTEGER NOT NULL,\n   195→                column_number INTEGER,\n   196→                reference_kind TEXT,\n   197→                metadata JSON,\n   198→                FOREIGN KEY (symbol_id) REFERENCES symbols(id),\n   199→                FOREIGN KEY (file_id) REFERENCES files(id)\n   200→            );\n   201→            \n   202→            CREATE INDEX IF NOT EXISTS idx_references_symbol ON symbol_references(symbol_id);\n   203→            CREATE INDEX IF NOT EXISTS idx_references_file ON symbol_references(file_id);\n   204→            \n   205→            -- Full-Text Search tables\n   206→            CREATE VIRTUAL TABLE IF NOT EXISTS fts_symbols USING fts5(\n   207→                name,\n   208→                documentation,\n   209→                content=symbols,\n   210→                content_rowid=id\n   211→            );\n   212→            \n   213→            CREATE VIRTUAL TABLE IF NOT EXISTS fts_code USING fts5(\n   214→                content,\n   215→                file_id UNINDEXED\n   216→            );\n   217→            \n   218→            -- Fuzzy Search\n   219→            CREATE TABLE IF NOT EXISTS symbol_trigrams (\n   220→                symbol_id INTEGER NOT NULL,\n   221→                trigram TEXT NOT NULL,\n   222→                FOREIGN KEY (symbol_id) REFERENCES symbols(id)\n   223→            );\n   224→            \n   225→            CREATE INDEX IF NOT EXISTS idx_trigrams ON symbol_trigrams(trigram);\n   226→            \n   227→            -- Embeddings\n   228→            CREATE TABLE IF NOT EXISTS embeddings (\n   229→                id INTEGER PRIMARY KEY,\n   230→                file_id INTEGER,\n   231→                symbol_id INTEGER,\n   232→                chunk_start INTEGER,\n   233→                chunk_end INTEGER,\n   234→                embedding BLOB NOT NULL,\n   235→                model_version TEXT NOT NULL,\n   236→                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   237→                metadata JSON,\n   238→                FOREIGN KEY (file_id) REFERENCES files(id),\n   239→                FOREIGN KEY (symbol_id) REFERENCES symbols(id)\n   240→            );\n   241→            \n   242→            CREATE INDEX IF NOT EXISTS idx_embeddings_file ON embeddings(file_id);\n   243→            CREATE INDEX IF NOT EXISTS idx_embeddings_symbol ON embeddings(symbol_id);\n   244→            \n   245→            -- Cache Tables\n   246→            CREATE TABLE IF NOT EXISTS query_cache (\n   247→                query_hash TEXT PRIMARY KEY,\n   248→                query_text TEXT NOT NULL,\n   249→                result JSON NOT NULL,\n   250→                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   251→                expires_at TIMESTAMP NOT NULL,\n   252→                hit_count INTEGER DEFAULT 0\n   253→            );\n   254→            \n   255→            CREATE INDEX IF NOT EXISTS idx_cache_expires ON query_cache(expires_at);\n   256→            \n   257→            CREATE TABLE IF NOT EXISTS parse_cache (\n   258→                file_hash TEXT PRIMARY KEY,\n   259→                ast JSON NOT NULL,\n   260→                parser_version TEXT NOT NULL,\n   261→                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n   262→            );\n   263→            \n   264→            -- Migration Log\n   265→            CREATE TABLE IF NOT EXISTS migrations (\n   266→                id INTEGER PRIMARY KEY,\n   267→                version_from INTEGER,\n   268→                version_to INTEGER,\n   269→                executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   270→                duration_ms INTEGER,\n   271→                status TEXT\n   272→            );\n   273→            \n   274→            -- Index Configuration\n   275→            CREATE TABLE IF NOT EXISTS index_config (\n   276→                id INTEGER PRIMARY KEY,\n   277→                config_key TEXT UNIQUE NOT NULL,\n   278→                config_value TEXT NOT NULL,\n   279→                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   280→                description TEXT\n   281→            );\n   282→            \n   283→            -- Insert initial index configuration\n   284→            INSERT OR IGNORE INTO index_config (config_key, config_value, description) VALUES\n   285→                (\'embedding_model\', \'voyage-code-3\', \'Current embedding model used for vector search\'),\n   286→                (\'model_dimension\', \'1024\', \'Embedding vector dimension\'),\n   287→                (\'distance_metric\', \'cosine\', \'Distance metric for vector similarity\'),\n   288→                (\'index_version\', \'1.0\', \'Index schema version\');\n   289→            \n   290→            -- Insert initial schema version\n   291→            INSERT INTO schema_version (version, description) \n   292→            VALUES (1, \'Initial schema creation\');\n   293→        """""")\n   294→        \n   295→        # Create triggers for FTS\n   296→        conn.executescript(""""""\n   297→            -- Triggers to keep FTS in sync with symbols table\n   298→            CREATE TRIGGER IF NOT EXISTS symbols_ai AFTER INSERT ON symbols\n   299→            BEGIN\n   300→                INSERT INTO fts_symbols(rowid, name, documentation)\n   301→                VALUES (new.id, new.name, new.documentation);\n   302→            END;\n   303→            \n   304→            CREATE TRIGGER IF NOT EXISTS symbols_ad AFTER DELETE ON symbols\n   305→            BEGIN\n   306→                DELETE FROM fts_symbols WHERE rowid = old.id;\n   307→            END;\n   308→            \n   309→            CREATE TRIGGER IF NOT EXISTS symbols_au AFTER UPDATE ON symbols\n   310→            BEGIN\n   311→                UPDATE fts_symbols \n   312→                SET name = new.name, documentation = new.documentation\n   313→                WHERE rowid = new.id;\n   314→            END;\n   315→        """""")\n   316→    \n   317→    # Repository operations\n   318→    def create_repository(self, path: str, name: str, metadata: Optional[Dict] = None) -> int:\n   319→        """"""Create a new repository entry.""""""\n   320→        with self._get_connection() as conn:\n   321→            cursor = conn.execute(\n   322→                """"""INSERT INTO repositories (path, name, metadata) \n   323→                   VALUES (?, ?, ?)\n   324→                   ON CONFLICT(path) DO UPDATE SET \n   325→                   name=excluded.name, \n   326→                   metadata=excluded.metadata,\n   327→                   updated_at=CURRENT_TIMESTAMP"""""",\n   328→                (path, name, json.dumps(metadata or {}))\n   329→            )\n   330→            if cursor.lastrowid:\n   331→                return cursor.lastrowid\n   332→            else:\n   333→                # If lastrowid is None, it means we updated an existing row\n   334→                # Get the id of the existing repository\n   335→                cursor = conn.execute(""SELECT id FROM repositories WHERE path = ?"", (path,))\n   336→                return cursor.fetchone()[0]\n   337→    \n   338→    def get_repository(self, path: str) -> Optional[Dict]:\n   339→        """"""Get repository by path.""""""\n   340→        with self._get_connection() as conn:\n   341→            cursor = conn.execute(\n   342→                ""SELECT * FROM repositories WHERE path = ?"", (path,)\n   343→            )\n   344→            row = cursor.fetchone()\n   345→            return dict(row) if row else None\n   346→    \n   347→    # File operations\n   348→    def store_file(self, repository_id: int, file_path: Union[str, Path],\n   349→                   language: Optional[str] = None, size: Optional[int] = None,\n   350→                   metadata: Optional[Dict] = None) -> int:\n   351→        """"""Store file information using relative paths and content hashes.""""""\n   352→        # Convert to Path object and normalize\n   353→        path = Path(file_path)\n   354→        \n   355→        # Compute relative path\n   356→        relative_path = self.path_resolver.normalize_path(path)\n   357→        \n   358→        # Compute content hash\n   359→        content_hash = self.path_resolver.compute_content_hash(path)\n   360→        \n   361→        with self._get_connection() as conn:\n   362→            # Check if file already exists with same content hash\n   363→            existing = self.get_file_by_content_hash(content_hash, repository_id)\n   364→            if existing and existing[\'relative_path\'] != relative_path:\n   365→                # File moved - record the move\n   366→                self.move_file(existing[\'relative_path\'], relative_path, repository_id, content_hash)\n   367→                return existing[\'id\']\n   368→            \n   369→            # Store using relative path as primary identifier\n   370→            cursor = conn.execute(\n   371→                """"""INSERT INTO files \n   372→                   (repository_id, path, relative_path, language, size, hash, content_hash,\n   373→                    last_modified, indexed_at, metadata, is_deleted)\n   374→                   VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, ?, FALSE)\n   375→                   ON CONFLICT(repository_id, relative_path) DO UPDATE SET\n   376→                   path=excluded.path,\n   377→                   language=excluded.language,\n   378→                   size=excluded.size,\n   379→                   hash=excluded.hash,\n   380→                   content_hash=excluded.content_hash,\n   381→                   last_modified=CURRENT_TIMESTAMP,\n   382→                   indexed_at=CURRENT_TIMESTAMP,\n   383→                   metadata=excluded.metadata,\n   384→                   is_deleted=FALSE,\n   385→                   deleted_at=NULL"""""",\n   386→                (repository_id, str(path), relative_path, language, size, \n   387→                 self.path_resolver.compute_file_hash(path), content_hash,\n   388→                 json.dumps(metadata or {}))\n   389→            )\n   390→            if cursor.lastrowid:\n   391→                return cursor.lastrowid\n   392→            else:\n   393→                # Get the id of the existing file\n   394→                cursor = conn.execute(\n   395→                    ""SELECT id FROM files WHERE repository_id = ? AND relative_path = ?"", \n   396→                    (repository_id, relative_path)\n   397→                )\n   398→                return cursor.fetchone()[0]\n   399→    \n   400→    def get_file(self, file_path: Union[str, Path], repository_id: Optional[int] = None) -> Optional[Dict]:\n   401→        """"""Get file by path (relative or absolute).""""""\n   402→        # Try to normalize path if it\'s absolute\n   403→        try:\n   404→            relative_path = self.path_resolver.normalize_path(file_path)\n   405→        except ValueError:\n   406→            # Path might already be relative\n   407→            relative_path = str(file_path).replace(\'\\\\\', \'/\')\n   408→        \n   409→        with self._get_connection() as conn:\n   410→            if repository_id:\n   411→                cursor = conn.execute(\n   412→                    ""SELECT * FROM files WHERE relative_path = ? AND repository_id = ? AND is_deleted = FALSE"",\n   413→                    (relative_path, repository_id)\n   414→                )\n   415→            else:\n   416→                cursor = conn.execute(\n   417→                    ""SELECT * FROM files WHERE relative_path = ? AND is_deleted = FALSE"", \n   418→                    (relative_path,)\n   419→                )\n   420→            row = cursor.fetchone()\n   421→            return dict(row) if row else None\n   422→    \n   423→    # Symbol operations\n   424→    def store_symbol(self, file_id: int, name: str, kind: str,\n   425→                     line_start: int, line_end: int,\n   426→                     column_start: Optional[int] = None,\n   427→                     column_end: Optional[int] = None,\n   428→                     signature: Optional[str] = None,\n   429→                     documentation: Optional[str] = None,\n   430→                     metadata: Optional[Dict] = None) -> int:\n   431→        """"""Store a symbol definition.""""""\n   432→        with self._get_connection() as conn:\n   433→            cursor = conn.execute(\n   434→                """"""INSERT INTO symbols \n   435→                   (file_id, name, kind, line_start, line_end, column_start, \n   436→                    column_end, signature, documentation, metadata)\n   437→                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"""""",\n   438→                (file_id, name, kind, line_start, line_end, column_start,\n   439→                 column_end, signature, documentation, json.dumps(metadata or {}))\n   440→            )\n   441→            symbol_id = cursor.lastrowid\n   442→            \n   443→            # Generate and store trigrams for fuzzy search\n   444→            self._store_trigrams(conn, symbol_id, name)\n   445→            \n   446→            return symbol_id\n   447→    \n   448→    def _store_trigrams(self, conn: sqlite3.Connection, symbol_id: int, name: str):\n   449→        """"""Generate and store trigrams for a symbol name.""""""\n   450→        # Generate trigrams\n   451→        trigrams = set()\n   452→        padded_name = f""  {name}  ""  # Pad with spaces for edge trigrams\n   453→        for i in range(len(padded_name) - 2):\n   454→            trigram = padded_name[i:i+3].lower()\n   455→            trigrams.add(trigram)\n   456→        \n   457→        # Store trigrams\n   458→        for trigram in trigrams:\n   459→            conn.execute(\n   460→                ""INSERT INTO symbol_trigrams (symbol_id, trigram) VALUES (?, ?)"",\n   461→                (symbol_id, trigram)\n   462→            )\n   463→    \n   464→    def get_symbol(self, name: str, kind: Optional[str] = None) -> List[Dict]:\n   465→        """"""Get symbols by name and optionally kind.""""""\n   466→        with self._get_connection() as conn:\n   467→            if kind:\n   468→                cursor = conn.execute(\n   469→                    """"""SELECT s.*, f.path as file_path \n   470→                       FROM symbols s\n   471→                       JOIN files f ON s.file_id = f.id\n   472→                       WHERE s.name = ? AND s.kind = ?"""""",\n   473→                    (name, kind)\n   474→                )\n   475→            else:\n   476→                cursor = conn.execute(\n   477→                    """"""SELECT s.*, f.path as file_path \n   478→                       FROM symbols s\n   479→                       JOIN files f ON s.file_id = f.id\n   480→                       WHERE s.name = ?"""""",\n   481→                    (name,)\n   482→                )\n   483→            return [dict(row) for row in cursor.fetchall()]\n   484→    \n   485→    # Reference operations\n   486→    def store_reference(self, symbol_id: int, file_id: int,\n   487→                       line_number: int, column_number: Optional[int] = None,\n   488→                       reference_kind: Optional[str] = None,\n   489→                       metadata: Optional[Dict] = None) -> int:\n   490→        """"""Store a symbol reference.""""""\n   491→        with self._get_connection() as conn:\n   492→            cursor = conn.execute(\n   493→                """"""INSERT INTO symbol_references \n   494→                   (symbol_id, file_id, line_number, column_number, \n   495→                    reference_kind, metadata)\n   496→                   VALUES (?, ?, ?, ?, ?, ?)"""""",\n   497→                (symbol_id, file_id, line_number, column_number,\n   498→                 reference_kind, json.dumps(metadata or {}))\n   499→            )\n   500→            return cursor.lastrowid\n   501→    \n   502→    def get_references(self, symbol_id: int) -> List[Dict]:\n   503→        """"""Get all references to a symbol.""""""\n   504→        with self._get_connection() as conn:\n   505→            cursor = conn.execute(\n   506→                """"""SELECT r.*, f.path as file_path\n   507→                   FROM symbol_references r\n   508→                   JOIN files f ON r.file_id = f.id\n   509→                   WHERE r.symbol_id = ?"""""",\n   510→                (symbol_id,)\n   511→            )\n   512→            return [dict(row) for row in cursor.fetchall()]\n   513→    \n   514→    # Search operations\n   515→    def search_symbols_fuzzy(self, query: str, limit: int = 20) -> List[Dict]:\n   516→        """"""\n   517→        Fuzzy search for symbols using trigrams.\n   518→        \n   519→        Args:\n   520→            query: Search query\n   521→            limit: Maximum number of results\n   522→            \n   523→        Returns:\n   524→            List of matching symbols with relevance scores\n   525→        """"""\n   526→        with self._get_connection() as conn:\n   527→            # Generate trigrams for the query\n   528→            query_trigrams = set()\n   529→            padded_query = f""  {query}  ""\n   530→            for i in range(len(padded_query) - 2):\n   531→                trigram = padded_query[i:i+3].lower()\n   532→                query_trigrams.add(trigram)\n   533→            \n   534→            if not query_trigrams:\n   535→                return []\n   536→            \n   537→            # Build query with trigram matching\n   538→            placeholders = \',\'.join([\'?\'] * len(query_trigrams))\n   539→            \n   540→            cursor = conn.execute(f""""""\n   541→                SELECT s.*, f.path as file_path,\n   542→                       COUNT(DISTINCT st.trigram) as matches,\n   543→                       COUNT(DISTINCT st.trigram) * 1.0 / ? as score\n   544→                FROM symbols s\n   545→                JOIN files f ON s.file_id = f.id\n   546→                JOIN symbol_trigrams st ON s.id = st.symbol_id\n   547→                WHERE st.trigram IN ({placeholders})\n   548→                GROUP BY s.id\n   549→                ORDER BY score DESC, s.name\n   550→                LIMIT ?\n   551→            """""", [len(query_trigrams)] + list(query_trigrams) + [limit])\n   552→            \n   553→            return [dict(row) for row in cursor.fetchall()]\n   554→    \n   555→    def search_symbols_fts(self, query: str, limit: int = 20) -> List[Dict]:\n   556→        """"""\n   557→        Full-text search for symbols.\n   558→        \n   559→        Args:\n   560→            query: Search query\n   561→            limit: Maximum number of results\n   562→            \n   563→        Returns:\n   564→            List of matching symbols\n   565→        """"""\n   566→        with self._get_connection() as conn:\n   567→            cursor = conn.execute(\n   568→                """"""SELECT s.*, f.path as file_path\n   569→                   FROM symbols s\n   570→                   JOIN files f ON s.file_id = f.id\n   571→                   JOIN fts_symbols ON s.id = fts_symbols.rowid\n   572→                   WHERE fts_symbols MATCH ?\n   573→                   ORDER BY rank\n   574→                   LIMIT ?"""""",\n   575→                (query, limit)\n   576→            )\n   577→            return [dict(row) for row in cursor.fetchall()]\n   578→    \n   579→    def search_code_fts(self, query: str, limit: int = 20) -> List[Dict]:\n   580→        """"""\n   581→        Full-text search in code content.\n   582→        \n   583→        Args:\n   584→            query: Search query\n   585→            limit: Maximum number of results\n   586→            \n   587→        Returns:\n   588→            List of matching code snippets\n   589→        """"""\n   590→        with self._get_connection() as conn:\n   591→            cursor = conn.execute(\n   592→                """"""SELECT fts.*, f.path as file_path\n   593→                   FROM fts_code fts\n   594→                   JOIN files f ON fts.file_id = f.id\n   595→                   WHERE fts_code MATCH ?\n   596→                   ORDER BY rank\n   597→                   LIMIT ?"""""",\n   598→                (query, limit)\n   599→            )\n   600→            return [dict(row) for row in cursor.fetchall()]\n   601→    \n   602→    # Index operations for fuzzy_indexer.py integration\n   603→    def persist_fuzzy_index(self, index_data: Dict[str, List[Tuple[str, Any]]]):\n   604→        """"""\n   605→        Persist fuzzy index data to database.\n   606→        \n   607→        Args:\n   608→            index_data: Dictionary mapping trigrams to list of (item, metadata) tuples\n   609→        """"""\n   610→        with self._get_connection() as conn:\n   611→            # Clear existing fuzzy index (for simplicity; in production, use incremental updates)\n   612→            conn.execute(""DELETE FROM symbol_trigrams"")\n   613→            \n   614→            # Store all symbols and their trigrams\n   615→            for trigram, items in index_data.items():\n   616→                for item, metadata in items:\n   617→                    # For now, assume item is a symbol name and metadata contains file info\n   618→                    # This is a simplified implementation\n   619→                    if isinstance(metadata, dict) and \'file_id\' in metadata:\n   620→                        conn.execute(\n   621→                            ""INSERT INTO symbol_trigrams (symbol_id, trigram) VALUES (?, ?)"",\n   622→                            (metadata.get(\'symbol_id\', 0), trigram)\n   623→                        )\n   624→    \n   625→    def load_fuzzy_index(self) -> Dict[str, List[Tuple[str, Any]]]:\n   626→        """"""\n   627→        Load fuzzy index data from database.\n   628→        \n   629→        Returns:\n   630→            Dictionary mapping trigrams to list of (item, metadata) tuples\n   631→        """"""\n   632→        index_data = {}\n   633→        \n   634→        with self._get_connection() as conn:\n   635→            cursor = conn.execute(\n   636→                """"""SELECT st.trigram, s.name, s.id, s.file_id, f.path\n   637→                   FROM symbol_trigrams st\n   638→                   JOIN symbols s ON st.symbol_id = s.id\n   639→                   JOIN files f ON s.file_id = f.id""""""\n   640→            )\n   641→            \n   642→            for row in cursor:\n   643→                trigram = row[\'trigram\']\n   644→                if trigram not in index_data:\n   645→                    index_data[trigram] = []\n   646→                \n   647→                # Store symbol name with metadata\n   648→                metadata = {\n   649→                    \'symbol_id\': row[\'id\'],\n   650→                    \'file_id\': row[\'file_id\'],\n   651→                    \'file_path\': row[\'path\']\n   652→                }\n   653→                index_data[trigram].append((row[\'name\'], metadata))\n   654→        \n   655→        return index_data\n   656→    \n   657→    # Utility methods\n   658→    def clear_cache(self):\n   659→        """"""Clear expired cache entries.""""""\n   660→        with self._get_connection() as conn:\n   661→            conn.execute(\n   662→                ""DELETE FROM query_cache WHERE expires_at < CURRENT_TIMESTAMP""\n   663→            )\n   664→    \n   665→    def get_statistics(self) -> Dict[str, int]:\n   666→        """"""Get database statistics.""""""\n   667→        stats = {}\n   668→        with self._get_connection() as conn:\n   669→            tables = [\'repositories\', \'files\', \'symbols\', \'symbol_references\', \'imports\']\n   670→            for table in tables:\n   671→                cursor = conn.execute(f""SELECT COUNT(*) FROM {table}"")\n   672→                stats[table] = cursor.fetchone()[0]\n   673→        return stats\n   674→    \n   675→    # Enhanced FTS5 search methods for BM25\n   676→    \n   677→    def search_bm25(self, query: str, table: str = \'fts_code\', \n   678→                    limit: int = 20, offset: int = 0,\n   679→                    columns: Optional[List[str]] = None) -> List[Dict[str, Any]]:\n   680→        """"""\n   681→        Perform BM25 search using FTS5.\n   682→        \n   683→        Args:\n   684→            query: FTS5 query string\n   685→            table: FTS table to search\n   686→            limit: Maximum results\n   687→            offset: Result offset for pagination\n   688→            columns: Specific columns to return\n   689→            \n   690→        Returns:\n   691→            List of search results with BM25 scores\n   692→        """"""\n   693→        with self._get_connection() as conn:\n   694→            # Default columns if not specified\n   695→            if not columns:\n   696→                columns = [\'*\', f\'bm25({table}) as score\']\n   697→            else:\n   698→                columns = columns + [f\'bm25({table}) as score\']\n   699→            \n   700→            columns_str = \', \'.join(columns)\n   701→            \n   702→            cursor = conn.execute(f""""""\n   703→                SELECT {columns_str}\n   704→                FROM {table}\n   705→                WHERE {table} MATCH ?\n   706→                ORDER BY bm25({table})\n   707→                LIMIT ? OFFSET ?\n   708→            """""", (query, limit, offset))\n   709→            \n   710→            results = []\n   711→            for row in cursor:\n   712→                results.append(dict(row))\n   713→            \n   714→            return results\n   715→    \n   716→    def search_bm25_with_snippets(self, query: str, table: str = \'fts_code\',\n   717→                                  content_column: int = 0,\n   718→                                  limit: int = 20) -> List[Dict[str, Any]]:\n   719→        """"""\n   720→        Search with BM25 and return snippets.\n   721→        \n   722→        Args:\n   723→            query: FTS5 query string\n   724→            table: FTS table to search\n   725→            content_column: Column index for snippet extraction\n   726→            limit: Maximum results\n   727→            \n   728→        Returns:\n   729→            List of results with snippets\n   730→        """"""\n   731→        with self._get_connection() as conn:\n   732→            cursor = conn.execute(f""""""\n   733→                SELECT \n   734→                    *,\n   735→                    snippet({table}, {content_column}, \'<mark>\', \'</mark>\', \'...\', 32) as snippet,\n   736→                    bm25({table}) as score\n   737→                FROM {table}\n   738→                WHERE {table} MATCH ?\n   739→                ORDER BY bm25({table})\n   740→                LIMIT ?\n   741→            """""", (query, limit))\n   742→            \n   743→            results = []\n   744→            for row in cursor:\n   745→                results.append(dict(row))\n   746→            \n   747→            return results\n   748→    \n   749→    def search_bm25_with_highlight(self, query: str, table: str = \'fts_code\',\n   750→                                   highlight_column: int = 0,\n   751→                                   limit: int = 20) -> List[Dict[str, Any]]:\n   752→        """"""\n   753→        Search with BM25 and return highlighted content.\n   754→        \n   755→        Args:\n   756→            query: FTS5 query string\n   757→            table: FTS table to search\n   758→            highlight_column: Column index for highlighting\n   759→            limit: Maximum results\n   760→            \n   761→        Returns:\n   762→            List of results with highlighted content\n   763→        """"""\n   764→        with self._get_connection() as conn:\n   765→            cursor = conn.execute(f""""""\n   766→                SELECT \n   767→                    *,\n   768→                    highlight({table}, {highlight_column}, \'<b>\', \'</b>\') as highlighted,\n   769→                    bm25({table}) as score\n   770→                FROM {table}\n   771→                WHERE {table} MATCH ?\n   772→                ORDER BY bm25({table})\n   773→                LIMIT ?\n   774→            """""", (query, limit))\n   775→            \n   776→            results = []\n   777→            for row in cursor:\n   778→                results.append(dict(row))\n   779→            \n   780→            return results\n   781→    \n   782→    def get_bm25_term_statistics(self, term: str, table: str = \'fts_code\') -> Dict[str, Any]:\n   783→        """"""\n   784→        Get term statistics for BM25 tuning.\n   785→        \n   786→        Args:\n   787→            term: Term to analyze\n   788→            table: FTS table to search\n   789→            \n   790→        Returns:\n   791→            Dictionary with term statistics\n   792→        """"""\n   793→        with self._get_connection() as conn:\n   794→            # Get document frequency\n   795→            cursor = conn.execute(f""""""\n   796→                SELECT COUNT(*) FROM {table}\n   797→                WHERE {table} MATCH ?\n   798→            """""", (term,))\n   799→            doc_freq = cursor.fetchone()[0]\n   800→            \n   801→            # Get total documents\n   802→            cursor = conn.execute(f""SELECT COUNT(*) FROM {table}"")\n   803→            total_docs = cursor.fetchone()[0]\n   804→            \n   805→            # Get average document length (approximation)\n   806→            cursor = conn.execute(f""""""\n   807→                SELECT AVG(LENGTH(content)) FROM {table}\n   808→            """""")\n   809→            avg_doc_length = cursor.fetchone()[0] or 0\n   810→            \n   811→            return {\n   812→                \'term\': term,\n   813→                \'document_frequency\': doc_freq,\n   814→                \'total_documents\': total_docs,\n   815→                \'idf\': self._calculate_idf(doc_freq, total_docs),\n   816→                \'average_document_length\': avg_doc_length\n   817→            }\n   818→    \n   819→    def _calculate_idf(self, doc_freq: int, total_docs: int) -> float:\n   820→        """"""Calculate Inverse Document Frequency.""""""\n   821→        import math\n   822→        if doc_freq == 0:\n   823→            return 0\n   824→        return math.log((total_docs - doc_freq + 0.5) / (doc_freq + 0.5))\n   825→    \n   826→    def optimize_fts_tables(self):\n   827→        """"""Optimize all FTS5 tables for better performance.""""""\n   828→        with self._get_connection() as conn:\n   829→            # Get all FTS5 tables\n   830→            cursor = conn.execute(""""""\n   831→                SELECT name FROM sqlite_master \n   832→                WHERE type=\'table\' AND sql LIKE \'%USING fts5%\'\n   833→            """""")\n   834→            \n   835→            fts_tables = [row[0] for row in cursor]\n   836→            \n   837→            for table in fts_tables:\n   838→                try:\n   839→                    conn.execute(f""INSERT INTO {table}({table}) VALUES(\'optimize\')"")\n   840→                    logger.info(f""Optimized FTS5 table: {table}"")\n   841→                except Exception as e:\n   842→                    logger.warning(f""Could not optimize {table}: {e}"")\n   843→    \n   844→    def rebuild_fts_tables(self):\n   845→        """"""Rebuild all FTS5 tables.""""""\n   846→        with self._get_connection() as conn:\n   847→            # Get all FTS5 tables\n   848→            cursor = conn.execute(""""""\n   849→                SELECT name FROM sqlite_master \n   850→                WHERE type=\'table\' AND sql LIKE \'%USING fts5%\'\n   851→            """""")\n   852→            \n   853→            fts_tables = [row[0] for row in cursor]\n   854→            \n   855→            for table in fts_tables:\n   856→                try:\n   857→                    conn.execute(f""INSERT INTO {table}({table}) VALUES(\'rebuild\')"")\n   858→                    logger.info(f""Rebuilt FTS5 table: {table}"")\n   859→                except Exception as e:\n   860→                    logger.warning(f""Could not rebuild {table}: {e}"")\n   861→    \n   862→    # New file operation methods for path management\n   863→    def get_file_by_content_hash(self, content_hash: str, repository_id: int) -> Optional[Dict]:\n   864→        """"""Get file by content hash.""""""\n   865→        with self._get_connection() as conn:\n   866→            cursor = conn.execute(\n   867→                """"""SELECT * FROM files \n   868→                   WHERE content_hash = ? AND repository_id = ? AND is_deleted = FALSE\n   869→                   ORDER BY indexed_at DESC LIMIT 1"""""",\n   870→                (content_hash, repository_id)\n   871→            )\n   872→            row = cursor.fetchone()\n   873→            return dict(row) if row else None\n   874→    \n   875→    def mark_file_deleted(self, relative_path: str, repository_id: int):\n   876→        """"""Mark a file as deleted (soft delete).""""""\n   877→        with self._get_connection() as conn:\n   878→            conn.execute(\n   879→                """"""UPDATE files \n   880→                   SET is_deleted = TRUE, deleted_at = CURRENT_TIMESTAMP\n   881→                   WHERE relative_path = ? AND repository_id = ?"""""",\n   882→                (relative_path, repository_id)\n   883→            )\n   884→            logger.info(f""Marked file as deleted: {relative_path}"")\n   885→    \n   886→    def remove_file(self, relative_path: str, repository_id: int):\n   887→        """"""Remove file and all associated data (hard delete).""""""\n   888→        with self._get_connection() as conn:\n   889→            # Get file ID first\n   890→            cursor = conn.execute(\n   891→                ""SELECT id FROM files WHERE relative_path = ? AND repository_id = ?"",\n   892→                (relative_path, repository_id)\n   893→            )\n   894→            row = cursor.fetchone()\n   895→            if not row:\n   896→                logger.warning(f""File not found for removal: {relative_path}"")\n   897→                return\n   898→            \n   899→            file_id = row[0]\n   900→            \n   901→            # Delete all associated data (cascade should handle most)\n   902→            conn.execute(""DELETE FROM symbol_references WHERE file_id = ?"", (file_id,))\n   903→            conn.execute(""DELETE FROM imports WHERE file_id = ?"", (file_id,))\n   904→            conn.execute(""DELETE FROM embeddings WHERE file_id = ?"", (file_id,))\n   905→            conn.execute(""DELETE FROM fts_code WHERE file_id = ?"", (file_id,))\n   906→            conn.execute(""DELETE FROM symbols WHERE file_id = ?"", (file_id,))\n   907→            conn.execute(""DELETE FROM files WHERE id = ?"", (file_id,))\n   908→            \n   909→            logger.info(f""Removed file and all associated data: {relative_path}"")\n   910→    \n   911→    def move_file(self, old_path: str, new_path: str, repository_id: int, content_hash: str):\n   912→        """"""Record a file move operation.""""""\n   913→        with self._get_connection() as conn:\n   914→            # Update the file path\n   915→            conn.execute(\n   916→                """"""UPDATE files \n   917→                   SET relative_path = ?, path = ?\n   918→                   WHERE relative_path = ? AND repository_id = ?"""""",\n   919→                (new_path, new_path, old_path, repository_id)\n   920→            )\n   921→            \n   922→            # Record the move\n   923→            conn.execute(\n   924→                """"""INSERT INTO file_moves \n   925→                   (repository_id, old_relative_path, new_relative_path, content_hash, move_type)\n   926→                   VALUES (?, ?, ?, ?, ?)"""""",\n   927→                (repository_id, old_path, new_path, content_hash, \'rename\')\n   928→            )\n   929→            \n   930→            logger.info(f""Recorded file move: {old_path} -> {new_path}"")\n   931→    \n   932→    def cleanup_deleted_files(self, days_old: int = 30):\n   933→        """"""Clean up files marked as deleted for more than specified days.""""""\n   934→        with self._get_connection() as conn:\n   935→            cursor = conn.execute(\n   936→                """"""SELECT COUNT(*) FROM files \n   937→                   WHERE is_deleted = TRUE \n   938→                   AND deleted_at < datetime(\'now\', \'-\' || ? || \' days\')"""""",\n   939→                (days_old,)\n   940→            )\n   941→            count = cursor.fetchone()[0]\n   942→            \n   943→            if count > 0:\n   944→                # Get file IDs for cleanup\n   945→                cursor = conn.execute(\n   946→                    """"""SELECT id, relative_path FROM files \n   947→                       WHERE is_deleted = TRUE \n   948→                       AND deleted_at < datetime(\'now\', \'-\' || ? || \' days\')"""""",\n   949→                    (days_old,)\n   950→                )\n   951→                \n   952→                for row in cursor:\n   953→                    file_id, path = row\n   954→                    # Use remove_file for thorough cleanup\n   955→                    self.remove_file(path, repository_id=None)\n   956→                \n   957→                logger.info(f""Cleaned up {count} deleted files older than {days_old} days"")\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,366,366,10985,12413,366,0.5,0.25,0,,,,,transcript_response
0be9b10a-8143-4183-be2c-4956b3822221,2025-06-26T07:31:55.627Z,transcript_0be9b10a-8143-4183-be2c-4956b3822221,"{'tool_use_id': 'toolu_01JLJyGaGgmaMuwcEKSaza17', 'type': 'tool_result', 'content': [{'type': 'text', 'text': 'Based on my search of the codebase, I found the **SQLiteStore** class definition in the following location:\n\n**File Path:** `/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/storage/sqlite_store.py`\n\n**Class Definition Location:** Line 21\n\nThe class is defined as:\n```python\nclass SQLiteStore:\n    """"""SQLite-based storage implementation with FTS5 support.""""""\n```\n\n**Key Details:**\n- The SQLiteStore class begins at line 21 in the file\n- It\'s a comprehensive SQLite-based persistence layer for the MCP Server\n- The class includes full-text search capabilities using SQLite\'s FTS5 extension\n- It provides methods for managing repositories, files, symbols, references, and search operations\n- The class has 957 lines of code with extensive functionality for code indexing and storage\n\n**Additional Files Mentioning SQLiteStore:**\nThe search also found references to SQLiteStore in these documentation/architecture files:\n- `/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/path_management_architecture.md`\n- `/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/indexer_actual.puml`\n- `/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/storage_actual.puml`\n\nThe main implementation is in the `sqlite_store.py` file at the path specified above.'}]}",transcript_query,unknown,mcp,0,True,6,0,0,0,6,0,0,0,1,1,13907,1000,1,0.5,0.25,0,,,,,transcript_response
fc4165fc-a1eb-45da-99d7-83d1b18b5743,2025-06-26T07:39:47.003Z,transcript_fc4165fc-a1eb-45da-99d7-83d1b18b5743,Find all functions that use asyncio,transcript_query,unknown,mcp,0,True,4,0,0,0,4,0,0,0,1,1,13902,0,1,0.5,0.25,0,,,,,transcript_response
fc4165fc-a1eb-45da-99d7-83d1b18b5743,2025-06-26T07:39:50.112Z,transcript_fc4165fc-a1eb-45da-99d7-83d1b18b5743,"{'tool_use_id': 'toolu_01Tq8yDeNT3sdv3pFDCQ1Y23', 'type': 'tool_result', 'content': 'Found 78 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/conftest.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_developer_workflows.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_repository_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_semantic_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_plugins_direct.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/populate_bm25_quick.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/pytest.ini\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/requirements-production.txt\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/requirements.txt\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_reranking_working.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/docs/development/TESTING-GUIDE.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/codex-setup.sh\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/prometheus_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/watchdog.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/file_watcher.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/grpc_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/MCP.md'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,1,1,13902,5381,1,0.5,0.25,0,,,,,transcript_response
fc4165fc-a1eb-45da-99d7-83d1b18b5743,2025-06-26T07:39:53.803Z,transcript_fc4165fc-a1eb-45da-99d7-83d1b18b5743,"{'tool_use_id': 'toolu_01KstMUeVNDUTpQNu9C9UaWk', 'type': 'tool_result', 'content': 'Found 76 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_python_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_section_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/security/auth_manager.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/security/security_middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/api_gateway_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/cache_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/dispatcher_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/indexing_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/metrics_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/security_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/shared_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/storage_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_runner.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/benchmark_reranking_comparison.py'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,1,1,19283,2687,1,0.5,0.25,0,,,,,transcript_response
73b4d92d-1822-486e-a91b-1bead8ef1fed,2025-06-26T07:39:12.090Z,transcript_73b4d92d-1822-486e-a91b-1bead8ef1fed,Find the BM25Indexer class definition,transcript_query,unknown,mcp,0,True,4,0,0,0,4,0,0,0,58,58,13905,0,58,0.5,0.25,0,,,,,transcript_response
73b4d92d-1822-486e-a91b-1bead8ef1fed,2025-06-26T07:39:15.071Z,transcript_73b4d92d-1822-486e-a91b-1bead8ef1fed,"{'tool_use_id': 'toolu_01MvPjVajmr66G4Tb7Zs7iVN', 'type': 'tool_result', 'content': 'Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/bm25_indexer.py'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,54,54,14133,0,54,0.5,0.25,0,,,,,transcript_response
b3c51fa0-9c9d-4a32-8cbd-fa54e2104fdc,2025-06-26T07:32:52.428Z,transcript_b3c51fa0-9c9d-4a32-8cbd-fa54e2104fdc,Find the EnhancedDispatcher class and show its main methods,transcript_query,unknown,mcp,0,True,4,0,0,0,4,0,0,0,1,1,13909,0,1,0.5,0.25,0,,,,,transcript_response
b3c51fa0-9c9d-4a32-8cbd-fa54e2104fdc,2025-06-26T07:32:56.023Z,transcript_b3c51fa0-9c9d-4a32-8cbd-fa54e2104fdc,"{'tool_use_id': 'toolu_016exHaADieaLmnyDL7Yy2sa', 'type': 'tool_result', 'content': 'Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/path_management_architecture.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/path_management.puml'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,27,27,14364,0,27,0.5,0.25,0,,,,,transcript_response
b3c51fa0-9c9d-4a32-8cbd-fa54e2104fdc,2025-06-26T07:33:00.280Z,transcript_b3c51fa0-9c9d-4a32-8cbd-fa54e2104fdc,"{'tool_use_id': 'toolu_01YWtUxDN4QzWBEe9gnETM8r', 'type': 'tool_result', 'content': '     1→""""""Enhanced dispatcher with dynamic plugin loading via PluginFactory.""""""\n     2→from pathlib import Path\n     3→from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4→import logging\n     5→import hashlib\n     6→import time\n     7→import re\n     8→from datetime import datetime\n     9→import os\n    10→\n    11→from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12→from ..plugins.plugin_factory import PluginFactory\n    13→from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14→from ..storage.sqlite_store import SQLiteStore\n    15→from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16→from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17→# Note: We\'ve removed ignore pattern checks to allow indexing ALL files\n    18→# Filtering happens only during export via SecureIndexExporter\n    19→# from ..core.ignore_patterns import get_ignore_manager\n    20→\n    21→logger = logging.getLogger(__name__)\n    22→\n    23→\n    24→class EnhancedDispatcher:\n    25→    """"""Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.""""""\n    26→    \n    27→    # Document query patterns - common documentation search terms\n    28→    DOCUMENT_QUERY_PATTERNS = [\n    29→        r\'\\b(how\\s+to|howto)\\b\',\n    30→        r\'\\b(getting\\s+started|get\\s+started)\\b\',\n    31→        r\'\\b(installation|install|setup)\\b\',\n    32→        r\'\\b(configuration|configure|config)\\b\',\n    33→        r\'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b\',\n    34→        r\'\\b(tutorial|guide|walkthrough)\\b\',\n    35→        r\'\\b(example|sample|snippet)\\b\',\n    36→        r\'\\b(readme|documentation|docs)\\b\',\n    37→        r\'\\b(usage|use\\s+case|using)\\b\',\n    38→        r\'\\b(reference|manual)\\b\',\n    39→        r\'\\b(faq|frequently\\s+asked)\\b\',\n    40→        r\'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b\',\n    41→        r\'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b\',\n    42→        r\'\\b(architecture|design|overview)\\b\',\n    43→        r\'\\b(changelog|release\\s+notes|migration)\\b\'\n    44→    ]\n    45→    \n    46→    # Documentation file patterns\n    47→    DOCUMENTATION_FILE_PATTERNS = [\n    48→        r\'readme(\\.\\w+)?$\',\n    49→        r\'changelog(\\.\\w+)?$\',\n    50→        r\'contributing(\\.\\w+)?$\',\n    51→        r\'license(\\.\\w+)?$\',\n    52→        r\'install(\\.\\w+)?$\',\n    53→        r\'setup(\\.\\w+)?$\',\n    54→        r\'guide(\\.\\w+)?$\',\n    55→        r\'tutorial(\\.\\w+)?$\',\n    56→        r\'\\.md$\',\n    57→        r\'\\.rst$\',\n    58→        r\'\\.txt$\',\n    59→        r\'docs?/\',\n    60→        r\'documentation/\'\n    61→    ]\n    62→    \n    63→    def __init__(self, \n    64→                 plugins: Optional[List[IPlugin]] = None,\n    65→                 sqlite_store: Optional[SQLiteStore] = None,\n    66→                 enable_advanced_features: bool = True,\n    67→                 use_plugin_factory: bool = True,\n    68→                 lazy_load: bool = True,\n    69→                 semantic_search_enabled: bool = True):\n    70→        """"""Initialize the enhanced dispatcher.\n    71→        \n    72→        Args:\n    73→            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74→            sqlite_store: SQLite store for plugin persistence\n    75→            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76→            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77→            lazy_load: Whether to lazy-load plugins on demand\n    78→            semantic_search_enabled: Whether to enable semantic search in plugins\n    79→        """"""\n    80→        self._sqlite_store = sqlite_store\n    81→        self._enable_advanced = enable_advanced_features\n    82→        self._use_factory = use_plugin_factory\n    83→        self._lazy_load = lazy_load\n    84→        self._semantic_enabled = semantic_search_enabled\n    85→        \n    86→        # Plugin storage\n    87→        self._plugins: List[IPlugin] = []\n    88→        self._by_lang: Dict[str, IPlugin] = {}\n    89→        self._loaded_languages: set[str] = set()\n    90→        \n    91→        # Cache for file hashes to avoid re-indexing unchanged files\n    92→        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93→        \n    94→        # Advanced components\n    95→        if self._enable_advanced:\n    96→            self._file_matcher = FileTypeMatcher()\n    97→            self._router = PluginRouter(self._file_matcher)\n    98→            self._aggregator = ResultAggregator()\n    99→        \n   100→        # Performance tracking\n   101→        self._operation_stats = {\n   102→            \'searches\': 0,\n   103→            \'lookups\': 0,\n   104→            \'indexings\': 0,\n   105→            \'total_time\': 0.0,\n   106→            \'plugins_loaded\': 0\n   107→        }\n   108→        \n   109→        # Initialize plugins\n   110→        if plugins:\n   111→            # Use provided plugins (backward compatibility)\n   112→            self._plugins = plugins\n   113→            self._by_lang = {p.lang: p for p in plugins}\n   114→            for plugin in plugins:\n   115→                self._loaded_languages.add(getattr(plugin, \'lang\', \'unknown\'))\n   116→            if self._enable_advanced:\n   117→                self._register_plugins_with_router()\n   118→        elif use_plugin_factory and not lazy_load:\n   119→            # Load all plugins immediately\n   120→            self._load_all_plugins()\n   121→        # If lazy_load is True, plugins will be loaded on demand\n   122→        \n   123→        # Compile document query patterns for performance\n   124→        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125→        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126→        \n   127→        logger.info(f""Enhanced dispatcher initialized with {len(self._plugins)} plugins"")\n   128→    \n   129→    def _load_all_plugins(self):\n   130→        """"""Load all available plugins using PluginFactory.""""""\n   131→        logger.info(""Loading all available plugins..."")\n   132→        \n   133→        all_plugins = PluginFactory.create_all_plugins(\n   134→            sqlite_store=self._sqlite_store,\n   135→            enable_semantic=self._semantic_enabled\n   136→        )\n   137→        \n   138→        for lang, plugin in all_plugins.items():\n   139→            self._plugins.append(plugin)\n   140→            self._by_lang[lang] = plugin\n   141→            self._loaded_languages.add(lang)\n   142→            self._operation_stats[\'plugins_loaded\'] += 1\n   143→        \n   144→        if self._enable_advanced:\n   145→            self._register_plugins_with_router()\n   146→        \n   147→        logger.info(f""Loaded {len(all_plugins)} plugins: {\', \'.join(sorted(all_plugins.keys()))}"")\n   148→    \n   149→    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150→        """"""Ensure a plugin for the given language is loaded.\n   151→        \n   152→        Args:\n   153→            language: Language code (e.g., \'python\', \'go\')\n   154→            \n   155→        Returns:\n   156→            Plugin instance or None if not available\n   157→        """"""\n   158→        # Normalize language\n   159→        language = language.lower().replace(\'-\', \'_\')\n   160→        \n   161→        # Check if already loaded\n   162→        if language in self._by_lang:\n   163→            return self._by_lang[language]\n   164→        \n   165→        # If not using factory or already tried to load, return None\n   166→        if not self._use_factory or language in self._loaded_languages:\n   167→            return None\n   168→        \n   169→        # Try to load the plugin\n   170→        try:\n   171→            logger.info(f""Lazy loading plugin for {language}"")\n   172→            plugin = PluginFactory.create_plugin(\n   173→                language,\n   174→                sqlite_store=self._sqlite_store,\n   175→                enable_semantic=self._semantic_enabled\n   176→            )\n   177→            \n   178→            # Add to collections\n   179→            self._plugins.append(plugin)\n   180→            self._by_lang[language] = plugin\n   181→            self._loaded_languages.add(language)\n   182→            self._operation_stats[\'plugins_loaded\'] += 1\n   183→            \n   184→            # Register with router if needed\n   185→            if self._enable_advanced:\n   186→                capabilities = self._detect_plugin_capabilities(plugin)\n   187→                self._router.register_plugin(plugin, capabilities)\n   188→            \n   189→            logger.info(f""Successfully loaded {language} plugin"")\n   190→            return plugin\n   191→            \n   192→        except ValueError as e:\n   193→            logger.warning(f""No plugin available for {language}: {e}"")\n   194→            self._loaded_languages.add(language)  # Mark as attempted\n   195→            return None\n   196→        except Exception as e:\n   197→            logger.error(f""Error loading plugin for {language}: {e}"")\n   198→            self._loaded_languages.add(language)  # Mark as attempted\n   199→            return None\n   200→    \n   201→    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202→        """"""Ensure a plugin is loaded for the given file.\n   203→        \n   204→        Args:\n   205→            path: File path\n   206→            \n   207→        Returns:\n   208→            Plugin instance or None if not available\n   209→        """"""\n   210→        # Get language from file extension\n   211→        extension = path.suffix.lower()\n   212→        language = get_language_by_extension(extension)\n   213→        \n   214→        if language:\n   215→            return self._ensure_plugin_loaded(language)\n   216→        \n   217→        # Fallback: try all loaded plugins\n   218→        for plugin in self._plugins:\n   219→            if plugin.supports(path):\n   220→                return plugin\n   221→        \n   222→        return None\n   223→    \n   224→    def _register_plugins_with_router(self):\n   225→        """"""Register plugins with the router and assign capabilities.""""""\n   226→        for plugin in self._plugins:\n   227→            # Determine capabilities based on plugin type/language\n   228→            capabilities = self._detect_plugin_capabilities(plugin)\n   229→            self._router.register_plugin(plugin, capabilities)\n   230→    \n   231→    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232→        """"""Detect capabilities for a plugin based on its language and features.""""""\n   233→        capabilities = []\n   234→        lang = getattr(plugin, \'lang\', \'unknown\')\n   235→        \n   236→        # Base capabilities all plugins have\n   237→        capabilities.append(PluginCapability(\n   238→            \'syntax_analysis\', \'1.0\', f\'{lang} syntax analysis\', \n   239→            priority=70, metadata={\'language\': lang}\n   240→        ))\n   241→        \n   242→        capabilities.append(PluginCapability(\n   243→            \'code_search\', \'1.0\', f\'{lang} code search\', \n   244→            priority=80, metadata={\'language\': lang}\n   245→        ))\n   246→        \n   247→        # Check for semantic search capability\n   248→        if hasattr(plugin, \'_enable_semantic\') and plugin._enable_semantic:\n   249→            capabilities.append(PluginCapability(\n   250→                \'semantic_search\', \'1.0\', f\'{lang} semantic search\', \n   251→                priority=90, metadata={\'language\': lang}\n   252→            ))\n   253→        \n   254→        # Language-specific capabilities\n   255→        if lang == \'python\':\n   256→            capabilities.extend([\n   257→                PluginCapability(\'refactoring\', \'1.0\', \'Python refactoring support\', 75),\n   258→                PluginCapability(\'type_analysis\', \'1.0\', \'Python type analysis\', 85)\n   259→            ])\n   260→        elif lang in [\'javascript\', \'typescript\']:\n   261→            capabilities.extend([\n   262→                PluginCapability(\'linting\', \'1.0\', \'JavaScript/TypeScript linting\', 85),\n   263→                PluginCapability(\'bundling_analysis\', \'1.0\', \'Module bundling analysis\', 70),\n   264→                PluginCapability(\'framework_support\', \'1.0\', \'Framework-specific support\', 75)\n   265→            ])\n   266→        elif lang in [\'c\', \'cpp\']:\n   267→            capabilities.extend([\n   268→                PluginCapability(\'compilation_analysis\', \'1.0\', \'Compilation analysis\', 80),\n   269→                PluginCapability(\'memory_analysis\', \'1.0\', \'Memory usage analysis\', 70),\n   270→                PluginCapability(\'performance_profiling\', \'1.0\', \'Performance profiling\', 75)\n   271→            ])\n   272→        elif lang in [\'go\', \'rust\']:\n   273→            capabilities.extend([\n   274→                PluginCapability(\'package_analysis\', \'1.0\', f\'{lang} package analysis\', 80),\n   275→                PluginCapability(\'concurrency_analysis\', \'1.0\', f\'{lang} concurrency analysis\', 75)\n   276→            ])\n   277→        elif lang in [\'java\', \'kotlin\', \'scala\']:\n   278→            capabilities.extend([\n   279→                PluginCapability(\'jvm_analysis\', \'1.0\', \'JVM bytecode analysis\', 75),\n   280→                PluginCapability(\'build_tool_integration\', \'1.0\', \'Build tool integration\', 70)\n   281→            ])\n   282→        \n   283→        return capabilities\n   284→    \n   285→    @property\n   286→    def plugins(self):\n   287→        """"""Get the dictionary of loaded plugins by language.""""""\n   288→        return self._by_lang\n   289→    \n   290→    @property\n   291→    def supported_languages(self) -> List[str]:\n   292→        """"""Get list of all supported languages (loaded and available).""""""\n   293→        if self._use_factory:\n   294→            return PluginFactory.get_supported_languages()\n   295→        else:\n   296→            return list(self._by_lang.keys())\n   297→    \n   298→    def _match_plugin(self, path: Path) -> IPlugin:\n   299→        """"""Match a plugin for the given file path.""""""\n   300→        # Ensure plugin is loaded if using lazy loading\n   301→        if self._lazy_load and self._use_factory:\n   302→            plugin = self._ensure_plugin_for_file(path)\n   303→            if plugin:\n   304→                return plugin\n   305→        \n   306→        # Use advanced routing if available\n   307→        if self._enable_advanced and self._router:\n   308→            route_result = self._router.get_best_plugin(path)\n   309→            if route_result:\n   310→                return route_result.plugin\n   311→        \n   312→        # Fallback to basic matching\n   313→        for p in self._plugins:\n   314→            if p.supports(path):\n   315→                return p\n   316→        \n   317→        raise RuntimeError(f""No plugin found for {path}"")\n   318→    \n   319→    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320→        """"""Get all plugins that can handle a file with confidence scores.""""""\n   321→        # Ensure plugin is loaded if using lazy loading\n   322→        if self._lazy_load and self._use_factory:\n   323→            self._ensure_plugin_for_file(path)\n   324→        \n   325→        if self._enable_advanced and self._router:\n   326→            route_results = self._router.route_file(path)\n   327→            return [(result.plugin, result.confidence) for result in route_results]\n   328→        else:\n   329→            # Basic fallback\n   330→            matching_plugins = []\n   331→            for plugin in self._plugins:\n   332→                if plugin.supports(path):\n   333→                    matching_plugins.append((plugin, 1.0))\n   334→            return matching_plugins\n   335→    \n   336→    def lookup(self, symbol: str) -> SymbolDef | None:\n   337→        """"""Look up symbol definition across all plugins.""""""\n   338→        start_time = time.time()\n   339→        \n   340→        try:\n   341→            # For symbol lookup, we may need to search across all languages\n   342→            # Load all plugins if using lazy loading\n   343→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344→                self._load_all_plugins()\n   345→            \n   346→            if self._enable_advanced and self._aggregator:\n   347→                # Use advanced aggregation\n   348→                definitions_by_plugin = {}\n   349→                for plugin in self._plugins:\n   350→                    try:\n   351→                        definition = plugin.getDefinition(symbol)\n   352→                        definitions_by_plugin[plugin] = definition\n   353→                    except Exception as e:\n   354→                        logger.warning(f""Plugin {plugin.lang} failed to get definition for {symbol}: {e}"")\n   355→                        definitions_by_plugin[plugin] = None\n   356→                \n   357→                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358→                \n   359→                self._operation_stats[\'lookups\'] += 1\n   360→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   361→                \n   362→                return result\n   363→            else:\n   364→                # Fallback to basic lookup\n   365→                for p in self._plugins:\n   366→                    res = p.getDefinition(symbol)\n   367→                    if res:\n   368→                        self._operation_stats[\'lookups\'] += 1\n   369→                        self._operation_stats[\'total_time\'] += time.time() - start_time\n   370→                        return res\n   371→                return None\n   372→                \n   373→        except Exception as e:\n   374→            logger.error(f""Error in symbol lookup for {symbol}: {e}"", exc_info=True)\n   375→            return None\n   376→    \n   377→    def _is_document_query(self, query: str) -> bool:\n   378→        """"""Check if the query is looking for documentation.\n   379→        \n   380→        Args:\n   381→            query: Search query string\n   382→            \n   383→        Returns:\n   384→            True if this appears to be a documentation query\n   385→        """"""\n   386→        query_lower = query.lower()\n   387→        \n   388→        # Check against document query patterns\n   389→        for pattern in self._compiled_doc_patterns:\n   390→            if pattern.search(query_lower):\n   391→                return True\n   392→        \n   393→        # Check for question words at the beginning\n   394→        question_starters = [\'how\', \'what\', \'where\', \'when\', \'why\', \'can\', \'is\', \'does\', \'should\']\n   395→        first_word = query_lower.split()[0] if query_lower.split() else \'\'\n   396→        if first_word in question_starters:\n   397→            return True\n   398→        \n   399→        return False\n   400→    \n   401→    def _expand_document_query(self, query: str) -> List[str]:\n   402→        """"""Expand a document query with related terms for better search coverage.\n   403→        \n   404→        Args:\n   405→            query: Original search query\n   406→            \n   407→        Returns:\n   408→            List of expanded query variations\n   409→        """"""\n   410→        expanded_queries = [query]  # Always include original\n   411→        query_lower = query.lower()\n   412→        \n   413→        # Common expansions for documentation queries\n   414→        expansions = {\n   415→            \'install\': [\'installation\', \'setup\', \'getting started\', \'requirements\'],\n   416→            \'config\': [\'configuration\', \'configure\', \'settings\', \'options\', \'parameters\'],\n   417→            \'api\': [\'api documentation\', \'api reference\', \'endpoint\', \'method\'],\n   418→            \'how to\': [\'tutorial\', \'guide\', \'example\', \'usage\'],\n   419→            \'example\': [\'sample\', \'snippet\', \'demo\', \'code example\'],\n   420→            \'error\': [\'troubleshoot\', \'debug\', \'issue\', \'problem\', \'fix\'],\n   421→            \'getting started\': [\'quickstart\', \'tutorial\', \'introduction\', \'setup\'],\n   422→            \'guide\': [\'tutorial\', \'documentation\', \'walkthrough\', \'how to\'],\n   423→            \'usage\': [\'how to use\', \'example\', \'api\', \'reference\']\n   424→        }\n   425→        \n   426→        # Apply expansions\n   427→        for term, related_terms in expansions.items():\n   428→            if term in query_lower:\n   429→                for related in related_terms:\n   430→                    # Replace the term with related term\n   431→                    expanded = query_lower.replace(term, related)\n   432→                    if expanded != query_lower and expanded not in expanded_queries:\n   433→                        expanded_queries.append(expanded)\n   434→                \n   435→                # Also add queries with additional terms\n   436→                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437→                    expanded = f""{query} {related}""\n   438→                    if expanded not in expanded_queries:\n   439→                        expanded_queries.append(expanded)\n   440→        \n   441→        # Add file-specific searches for common documentation files\n   442→        if self._is_document_query(query):\n   443→            # Extract the main topic from the query\n   444→            topic_words = []\n   445→            for word in query.lower().split():\n   446→                if word not in [\'how\', \'to\', \'the\', \'a\', \'an\', \'is\', \'are\', \'what\', \'where\', \'when\']:\n   447→                    topic_words.append(word)\n   448→            \n   449→            if topic_words:\n   450→                topic = \' \'.join(topic_words[:2])  # Use first two topic words\n   451→                expanded_queries.extend([\n   452→                    f""README {topic}"",\n   453→                    f""{topic} documentation"",\n   454→                    f""{topic} docs"",\n   455→                    f""{topic} guide""\n   456→                ])\n   457→        \n   458→        # Remove duplicates while preserving order\n   459→        seen = set()\n   460→        unique_queries = []\n   461→        for q in expanded_queries:\n   462→            if q not in seen:\n   463→                seen.add(q)\n   464→                unique_queries.append(q)\n   465→        \n   466→        return unique_queries[:10]  # Limit to 10 queries max\n   467→    \n   468→    def _is_documentation_file(self, file_path: str) -> bool:\n   469→        """"""Check if a file path is likely a documentation file.\n   470→        \n   471→        Args:\n   472→            file_path: Path to check\n   473→            \n   474→        Returns:\n   475→            True if this appears to be a documentation file\n   476→        """"""\n   477→        path_lower = file_path.lower()\n   478→        \n   479→        for pattern in self._compiled_file_patterns:\n   480→            if pattern.search(path_lower):\n   481→                return True\n   482→        \n   483→        return False\n   484→    \n   485→    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486→        """"""Adjust ranking to prioritize documentation files for document queries.\n   487→        \n   488→        Args:\n   489→            query: Original search query\n   490→            results: List of aggregated results\n   491→            \n   492→        Returns:\n   493→            Re-ranked results with documentation prioritized\n   494→        """"""\n   495→        if not self._is_document_query(query):\n   496→            return results\n   497→        \n   498→        # Separate documentation and code results\n   499→        doc_results = []\n   500→        code_results = []\n   501→        \n   502→        for result in results:\n   503→            if self._is_documentation_file(result.primary_result.get(\'file\', \'\')):\n   504→                # Boost documentation files for document queries\n   505→                result.rank_score *= 1.5\n   506→                result.metadata[\'doc_boost\'] = True\n   507→                doc_results.append(result)\n   508→            else:\n   509→                code_results.append(result)\n   510→        \n   511→        # Sort each group by rank score\n   512→        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513→        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514→        \n   515→        # Combine with documentation files first\n   516→        return doc_results + code_results\n   517→    \n   518→    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519→        """"""Search for code and documentation across all plugins.""""""\n   520→        start_time = time.time()\n   521→        \n   522→        try:\n   523→            # For search, we may need to search across all languages\n   524→            # Load all plugins if using lazy loading\n   525→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526→                self._load_all_plugins()\n   527→            \n   528→            # Detect if this is a document query\n   529→            is_doc_query = self._is_document_query(query)\n   530→            \n   531→            # Expand query if it\'s a document query\n   532→            queries = [query]\n   533→            if is_doc_query:\n   534→                queries = self._expand_document_query(query)\n   535→                logger.info(f""Expanded document query \'{query}\' to {len(queries)} variations"")\n   536→                # Force semantic search for natural language queries\n   537→                semantic = True\n   538→            \n   539→            if self._enable_advanced and self._aggregator:\n   540→                # Use advanced aggregation\n   541→                all_results_by_plugin = {}\n   542→                opts = {""semantic"": semantic, ""limit"": limit * 2 if is_doc_query else limit}\n   543→                \n   544→                # Search with all query variations\n   545→                for search_query in queries:\n   546→                    for plugin in self._plugins:\n   547→                        try:\n   548→                            results = list(plugin.search(search_query, opts))\n   549→                            if results:\n   550→                                if plugin not in all_results_by_plugin:\n   551→                                    all_results_by_plugin[plugin] = []\n   552→                                all_results_by_plugin[plugin].extend(results)\n   553→                        except Exception as e:\n   554→                            logger.warning(f""Plugin {plugin.lang} failed to search for {search_query}: {e}"")\n   555→                \n   556→                # Deduplicate results per plugin\n   557→                for plugin, results in all_results_by_plugin.items():\n   558→                    seen = set()\n   559→                    unique_results = []\n   560→                    for result in results:\n   561→                        key = f""{result[\'file\']}:{result[\'line\']}""\n   562→                        if key not in seen:\n   563→                            seen.add(key)\n   564→                            unique_results.append(result)\n   565→                    all_results_by_plugin[plugin] = unique_results\n   566→                \n   567→                # Configure aggregator for document queries\n   568→                if is_doc_query and self._enable_advanced:\n   569→                    # Adjust ranking criteria for documentation\n   570→                    doc_criteria = RankingCriteria(\n   571→                        relevance_weight=0.5,      # Increase relevance weight\n   572→                        confidence_weight=0.2,     # Reduce confidence weight\n   573→                        frequency_weight=0.2,      # Keep frequency weight\n   574→                        recency_weight=0.1,        # Keep recency weight\n   575→                        prefer_exact_matches=False,  # Natural language doesn\'t need exact matches\n   576→                        boost_multiple_sources=True,\n   577→                        boost_common_extensions=True\n   578→                    )\n   579→                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580→                \n   581→                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582→                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583→                )\n   584→                \n   585→                # Adjust ranking for document queries\n   586→                if is_doc_query:\n   587→                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588→                \n   589→                # Apply final limit\n   590→                if limit and len(aggregated_results) > limit:\n   591→                    aggregated_results = aggregated_results[:limit]\n   592→                \n   593→                logger.debug(f""Search aggregation stats: {stats.total_results} total, ""\n   594→                           f""{stats.unique_results} unique, {stats.plugins_used} plugins used, ""\n   595→                           f""document_query={is_doc_query}"")\n   596→                \n   597→                self._operation_stats[\'searches\'] += 1\n   598→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   599→                \n   600→                # Yield primary results from aggregated results\n   601→                for aggregated in aggregated_results:\n   602→                    yield aggregated.primary_result\n   603→            else:\n   604→                # Fallback to basic search\n   605→                # Detect if this is a document query\n   606→                is_doc_query = self._is_document_query(query)\n   607→                \n   608→                # Expand query if it\'s a document query\n   609→                queries = [query]\n   610→                if is_doc_query:\n   611→                    queries = self._expand_document_query(query)\n   612→                    semantic = True  # Force semantic search for natural language\n   613→                \n   614→                opts = {""semantic"": semantic, ""limit"": limit}\n   615→                all_results = []\n   616→                \n   617→                # Search with all query variations\n   618→                for search_query in queries:\n   619→                    for p in self._plugins:\n   620→                        try:\n   621→                            for result in p.search(search_query, opts):\n   622→                                all_results.append(result)\n   623→                        except Exception as e:\n   624→                            logger.warning(f""Plugin {p.lang} failed to search for {search_query}: {e}"")\n   625→                \n   626→                # Deduplicate results\n   627→                seen = set()\n   628→                unique_results = []\n   629→                for result in all_results:\n   630→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   631→                    if key not in seen:\n   632→                        seen.add(key)\n   633→                        unique_results.append(result)\n   634→                \n   635→                # Sort by score if available\n   636→                unique_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   637→                \n   638→                # Prioritize documentation files for document queries\n   639→                if is_doc_query:\n   640→                    doc_results = []\n   641→                    code_results = []\n   642→                    for result in unique_results:\n   643→                        if self._is_documentation_file(result.get(\'file\', \'\')):\n   644→                            doc_results.append(result)\n   645→                        else:\n   646→                            code_results.append(result)\n   647→                    unique_results = doc_results + code_results\n   648→                \n   649→                # Apply limit\n   650→                count = 0\n   651→                for result in unique_results:\n   652→                    if limit and count >= limit:\n   653→                        break\n   654→                    yield result\n   655→                    count += 1\n   656→                \n   657→                self._operation_stats[\'searches\'] += 1\n   658→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   659→                \n   660→        except Exception as e:\n   661→            logger.error(f""Error in search for {query}: {e}"", exc_info=True)\n   662→    \n   663→    def index_file(self, path: Path) -> None:\n   664→        """"""Index a single file if it has changed.""""""\n   665→        try:\n   666→            # Ensure path is absolute to avoid relative/absolute path issues\n   667→            path = path.resolve()\n   668→            \n   669→            # Find the appropriate plugin\n   670→            plugin = self._match_plugin(path)\n   671→            \n   672→            # Read file content\n   673→            try:\n   674→                content = path.read_text(encoding=\'utf-8\')\n   675→            except UnicodeDecodeError:\n   676→                # Try with different encodings\n   677→                try:\n   678→                    content = path.read_text(encoding=\'latin-1\')\n   679→                except Exception as e:\n   680→                    logger.error(f""Failed to read {path}: {e}"")\n   681→                    return\n   682→            \n   683→            # Check if we need to re-index (simplified for now)\n   684→            # TODO: Implement proper caching logic\n   685→            \n   686→            # Index the file\n   687→            start_time = time.time()\n   688→            logger.info(f""Indexing {path} with {plugin.lang} plugin"")\n   689→            shard = plugin.indexFile(path, content)\n   690→            \n   691→            # Record performance if advanced features enabled\n   692→            if self._enable_advanced and self._router:\n   693→                execution_time = time.time() - start_time\n   694→                self._router.record_performance(plugin, execution_time)\n   695→            \n   696→            self._operation_stats[\'indexings\'] += 1\n   697→            self._operation_stats[\'total_time\'] += time.time() - start_time\n   698→            \n   699→            logger.info(f""Successfully indexed {path}: {len(shard.get(\'symbols\', []))} symbols found"")\n   700→            \n   701→        except RuntimeError as e:\n   702→            # No plugin found for this file type\n   703→            logger.debug(f""No plugin for {path}: {e}"")\n   704→        except Exception as e:\n   705→            logger.error(f""Error indexing {path}: {e}"", exc_info=True)\n   706→    \n   707→    def get_statistics(self) -> dict:\n   708→        """"""Get comprehensive statistics across all plugins and components.""""""\n   709→        stats = {\n   710→            ""total_plugins"": len(self._plugins),\n   711→            ""loaded_languages"": sorted(list(self._loaded_languages)),\n   712→            ""supported_languages"": len(self.supported_languages),\n   713→            ""operations"": self._operation_stats.copy()\n   714→        }\n   715→        \n   716→        # Add language breakdown\n   717→        stats[""by_language""] = {}\n   718→        for lang, plugin in self._by_lang.items():\n   719→            plugin_info = {\n   720→                ""loaded"": True,\n   721→                ""class"": plugin.__class__.__name__\n   722→            }\n   723→            if hasattr(plugin, \'get_indexed_count\'):\n   724→                plugin_info[""indexed_files""] = plugin.get_indexed_count()\n   725→            stats[""by_language""][lang] = plugin_info\n   726→        \n   727→        return stats\n   728→    \n   729→    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730→        """"""\n   731→        Index all files in a directory, respecting ignore patterns.\n   732→        \n   733→        Args:\n   734→            directory: Directory to index\n   735→            recursive: Whether to index subdirectories\n   736→            \n   737→        Returns:\n   738→            Statistics about indexed files\n   739→        """"""\n   740→        logger.info(f""Indexing directory: {directory} (recursive={recursive})"")\n   741→        \n   742→        # Note: We don\'t use ignore patterns during indexing\n   743→        # ALL files are indexed for local search capability\n   744→        # Filtering happens only during export/sharing\n   745→        \n   746→        # Get all supported extensions\n   747→        supported_extensions = get_all_extensions()\n   748→        \n   749→        stats = {\n   750→            ""total_files"": 0,\n   751→            ""indexed_files"": 0,\n   752→            ""ignored_files"": 0,\n   753→            ""failed_files"": 0,\n   754→            ""by_language"": {}\n   755→        }\n   756→        \n   757→        # Walk directory\n   758→        if recursive:\n   759→            file_iterator = directory.rglob(""*"")\n   760→        else:\n   761→            file_iterator = directory.glob(""*"")\n   762→            \n   763→        for path in file_iterator:\n   764→            if not path.is_file():\n   765→                continue\n   766→                \n   767→            stats[""total_files""] += 1\n   768→            \n   769→            # NOTE: We index ALL files locally, including gitignored ones\n   770→            # Filtering happens only during export/sharing\n   771→            # This allows local search of .env, secrets, etc.\n   772→                \n   773→            # Try to find a plugin that supports this file\n   774→            # This allows us to index ALL files, including .env, .key, etc.\n   775→            try:\n   776→                # First try to match by extension\n   777→                if path.suffix in supported_extensions:\n   778→                    self.index_file(path)\n   779→                    stats[""indexed_files""] += 1\n   780→                # For files without recognized extensions, try each plugin\'s supports() method\n   781→                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782→                else:\n   783→                    matched = False\n   784→                    for plugin in self._plugins:\n   785→                        if plugin.supports(path):\n   786→                            self.index_file(path)\n   787→                            stats[""indexed_files""] += 1\n   788→                            matched = True\n   789→                            break\n   790→                    \n   791→                    # If no plugin matched but we want to index everything,\n   792→                    # we could add a fallback here to index as plaintext\n   793→                    # For now, we\'ll skip unmatched files\n   794→                    if not matched:\n   795→                        logger.debug(f""No plugin found for {path}"")\n   796→                \n   797→                # Track by language\n   798→                language = get_language_by_extension(path.suffix)\n   799→                if language:\n   800→                    stats[""by_language""][language] = stats[""by_language""].get(language, 0) + 1\n   801→                    \n   802→            except Exception as e:\n   803→                logger.error(f""Failed to index {path}: {e}"")\n   804→                stats[""failed_files""] += 1\n   805→                \n   806→        logger.info(f""Directory indexing complete: {stats[\'indexed_files\']} indexed, ""\n   807→                   f""{stats[\'ignored_files\']} ignored, {stats[\'failed_files\']} failed"")\n   808→        \n   809→        return stats\n   810→    \n   811→    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812→        """"""Search specifically across documentation files.\n   813→        \n   814→        Args:\n   815→            topic: Topic to search for (e.g., ""installation"", ""configuration"")\n   816→            doc_types: Optional list of document types to search (e.g., [""readme"", ""guide"", ""api""])\n   817→            limit: Maximum number of results\n   818→            \n   819→        Returns:\n   820→            Search results from documentation files\n   821→        """"""\n   822→        # Default document types if not specified\n   823→        if doc_types is None:\n   824→            doc_types = [""readme"", ""documentation"", ""guide"", ""tutorial"", ""api"", ""changelog"", ""contributing""]\n   825→        \n   826→        # Build search queries for different document types\n   827→        queries = []\n   828→        for doc_type in doc_types:\n   829→            queries.extend([\n   830→                f""{doc_type} {topic}"",\n   831→                f""{topic} {doc_type}"",\n   832→                f""{topic} in {doc_type}""\n   833→            ])\n   834→        \n   835→        # Also search for the topic in common doc filenames\n   836→        queries.extend([\n   837→            f""README {topic}"",\n   838→            f""CONTRIBUTING {topic}"",\n   839→            f""docs {topic}"",\n   840→            f""documentation {topic}""\n   841→        ])\n   842→        \n   843→        # Deduplicate queries\n   844→        queries = list(dict.fromkeys(queries))\n   845→        \n   846→        logger.info(f""Cross-document search for \'{topic}\' with {len(queries)} query variations"")\n   847→        \n   848→        # Use the enhanced search with document-specific handling\n   849→        all_results = []\n   850→        seen = set()\n   851→        \n   852→        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853→            for result in self.search(query, semantic=True, limit=limit):\n   854→                # Only include documentation files\n   855→                if self._is_documentation_file(result.get(\'file\', \'\')):\n   856→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   857→                    if key not in seen:\n   858→                        seen.add(key)\n   859→                        all_results.append(result)\n   860→        \n   861→        # Sort by relevance (score) and return top results\n   862→        all_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   863→        \n   864→        count = 0\n   865→        for result in all_results:\n   866→            if count >= limit:\n   867→                break\n   868→            yield result\n   869→            count += 1\n   870→    \n   871→    def health_check(self) -> Dict[str, Any]:\n   872→        """"""Perform a health check on all components.""""""\n   873→        health = {\n   874→            \'status\': \'healthy\',\n   875→            \'components\': {\n   876→                \'dispatcher\': {\n   877→                    \'status\': \'healthy\',\n   878→                    \'plugins_loaded\': len(self._plugins),\n   879→                    \'languages_supported\': len(self.supported_languages),\n   880→                    \'factory_enabled\': self._use_factory,\n   881→                    \'lazy_loading\': self._lazy_load\n   882→                }\n   883→            },\n   884→            \'plugins\': {},\n   885→            \'errors\': []\n   886→        }\n   887→        \n   888→        # Check plugin health\n   889→        for lang, plugin in self._by_lang.items():\n   890→            try:\n   891→                plugin_health = {\n   892→                    \'status\': \'healthy\',\n   893→                    \'class\': plugin.__class__.__name__,\n   894→                    \'semantic_enabled\': getattr(plugin, \'_enable_semantic\', False)\n   895→                }\n   896→                if hasattr(plugin, \'get_indexed_count\'):\n   897→                    plugin_health[\'indexed_files\'] = plugin.get_indexed_count()\n   898→            except Exception as e:\n   899→                plugin_health = {\n   900→                    \'status\': \'error\',\n   901→                    \'error\': str(e)\n   902→                }\n   903→                health[\'errors\'].append(f""Plugin {lang}: {str(e)}"")\n   904→            \n   905→            health[\'plugins\'][lang] = plugin_health\n   906→        \n   907→        # Determine overall health\n   908→        if len(health[\'errors\']) > 0:\n   909→            health[\'status\'] = \'degraded\' if len(health[\'errors\']) < 3 else \'unhealthy\'\n   910→        \n   911→        return health\n   912→    \n   913→    def remove_file(self, path: Union[Path, str]) -> None:\n   914→        """"""Remove a file from all indexes.\n   915→        \n   916→        Args:\n   917→            path: File path to remove\n   918→        """"""\n   919→        path = Path(path).resolve()\n   920→        logger.info(f""Removing file from index: {path}"")\n   921→        \n   922→        try:\n   923→            # Remove from SQLite if available\n   924→            if self._sqlite_store:\n   925→                from ..core.path_resolver import PathResolver\n   926→                path_resolver = PathResolver()\n   927→                try:\n   928→                    relative_path = path_resolver.normalize_path(path)\n   929→                    # Get repository ID - for now assume 1\n   930→                    # TODO: Properly detect repository\n   931→                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932→                except Exception as e:\n   933→                    logger.error(f""Error removing from SQLite: {e}"")\n   934→            \n   935→            # Remove from semantic index if available\n   936→            try:\n   937→                plugin = self._match_plugin(path)\n   938→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   939→                    plugin._indexer.remove_file(path)\n   940→                    logger.info(f""Removed from semantic index: {path}"")\n   941→            except Exception as e:\n   942→                logger.warning(f""Error removing from semantic index: {e}"")\n   943→            \n   944→            # Update statistics\n   945→            self._operation_stats[\'deletions\'] = self._operation_stats.get(\'deletions\', 0) + 1\n   946→            \n   947→        except Exception as e:\n   948→            logger.error(f""Error removing file {path}: {e}"", exc_info=True)\n   949→    \n   950→    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951→        """"""Move a file in all indexes.\n   952→        \n   953→        Args:\n   954→            old_path: Original file path\n   955→            new_path: New file path\n   956→            content_hash: Optional content hash to verify unchanged content\n   957→        """"""\n   958→        old_path = Path(old_path).resolve()\n   959→        new_path = Path(new_path).resolve()\n   960→        logger.info(f""Moving file in index: {old_path} -> {new_path}"")\n   961→        \n   962→        try:\n   963→            # Move in SQLite if available\n   964→            if self._sqlite_store:\n   965→                from ..core.path_resolver import PathResolver\n   966→                path_resolver = PathResolver()\n   967→                try:\n   968→                    old_relative = path_resolver.normalize_path(old_path)\n   969→                    new_relative = path_resolver.normalize_path(new_path)\n   970→                    # Get repository ID - for now assume 1\n   971→                    # TODO: Properly detect repository\n   972→                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973→                except Exception as e:\n   974→                    logger.error(f""Error moving in SQLite: {e}"")\n   975→            \n   976→            # Move in semantic index if available\n   977→            try:\n   978→                plugin = self._match_plugin(new_path)\n   979→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   980→                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981→                    logger.info(f""Moved in semantic index: {old_path} -> {new_path}"")\n   982→            except Exception as e:\n   983→                logger.warning(f""Error moving in semantic index: {e}"")\n   984→            \n   985→            # Update statistics\n   986→            self._operation_stats[\'moves\'] = self._operation_stats.get(\'moves\', 0) + 1\n   987→            \n   988→        except Exception as e:\n   989→            logger.error(f""Error moving file {old_path} -> {new_path}: {e}"", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,1,1,27688,0,1,0.5,0.25,0,,,,,transcript_response
09502d6f-29cc-4822-b126-71a41e248439,2025-06-26T07:39:25.627Z,transcript_09502d6f-29cc-4822-b126-71a41e248439,Find the EnhancedDispatcher class and show its main methods,transcript_query,unknown,mcp,0,True,4,0,0,0,4,0,0,0,1,1,13909,0,1,0.5,0.25,0,,,,,transcript_response
09502d6f-29cc-4822-b126-71a41e248439,2025-06-26T07:39:28.899Z,transcript_09502d6f-29cc-4822-b126-71a41e248439,"{'tool_use_id': 'toolu_01Vo8bZb6youMntNGm5unGhc', 'type': 'tool_result', 'content': 'Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/path_management_architecture.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/path_management.puml'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,29,29,13909,457,29,0.5,0.25,0,,,,,transcript_response
09502d6f-29cc-4822-b126-71a41e248439,2025-06-26T07:39:32.568Z,transcript_09502d6f-29cc-4822-b126-71a41e248439,"{'tool_use_id': 'toolu_0119njWJTbS4dNHVKw9JiRgr', 'type': 'tool_result', 'content': '     1→""""""Enhanced dispatcher with dynamic plugin loading via PluginFactory.""""""\n     2→from pathlib import Path\n     3→from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4→import logging\n     5→import hashlib\n     6→import time\n     7→import re\n     8→from datetime import datetime\n     9→import os\n    10→\n    11→from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12→from ..plugins.plugin_factory import PluginFactory\n    13→from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14→from ..storage.sqlite_store import SQLiteStore\n    15→from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16→from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17→# Note: We\'ve removed ignore pattern checks to allow indexing ALL files\n    18→# Filtering happens only during export via SecureIndexExporter\n    19→# from ..core.ignore_patterns import get_ignore_manager\n    20→\n    21→logger = logging.getLogger(__name__)\n    22→\n    23→\n    24→class EnhancedDispatcher:\n    25→    """"""Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.""""""\n    26→    \n    27→    # Document query patterns - common documentation search terms\n    28→    DOCUMENT_QUERY_PATTERNS = [\n    29→        r\'\\b(how\\s+to|howto)\\b\',\n    30→        r\'\\b(getting\\s+started|get\\s+started)\\b\',\n    31→        r\'\\b(installation|install|setup)\\b\',\n    32→        r\'\\b(configuration|configure|config)\\b\',\n    33→        r\'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b\',\n    34→        r\'\\b(tutorial|guide|walkthrough)\\b\',\n    35→        r\'\\b(example|sample|snippet)\\b\',\n    36→        r\'\\b(readme|documentation|docs)\\b\',\n    37→        r\'\\b(usage|use\\s+case|using)\\b\',\n    38→        r\'\\b(reference|manual)\\b\',\n    39→        r\'\\b(faq|frequently\\s+asked)\\b\',\n    40→        r\'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b\',\n    41→        r\'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b\',\n    42→        r\'\\b(architecture|design|overview)\\b\',\n    43→        r\'\\b(changelog|release\\s+notes|migration)\\b\'\n    44→    ]\n    45→    \n    46→    # Documentation file patterns\n    47→    DOCUMENTATION_FILE_PATTERNS = [\n    48→        r\'readme(\\.\\w+)?$\',\n    49→        r\'changelog(\\.\\w+)?$\',\n    50→        r\'contributing(\\.\\w+)?$\',\n    51→        r\'license(\\.\\w+)?$\',\n    52→        r\'install(\\.\\w+)?$\',\n    53→        r\'setup(\\.\\w+)?$\',\n    54→        r\'guide(\\.\\w+)?$\',\n    55→        r\'tutorial(\\.\\w+)?$\',\n    56→        r\'\\.md$\',\n    57→        r\'\\.rst$\',\n    58→        r\'\\.txt$\',\n    59→        r\'docs?/\',\n    60→        r\'documentation/\'\n    61→    ]\n    62→    \n    63→    def __init__(self, \n    64→                 plugins: Optional[List[IPlugin]] = None,\n    65→                 sqlite_store: Optional[SQLiteStore] = None,\n    66→                 enable_advanced_features: bool = True,\n    67→                 use_plugin_factory: bool = True,\n    68→                 lazy_load: bool = True,\n    69→                 semantic_search_enabled: bool = True):\n    70→        """"""Initialize the enhanced dispatcher.\n    71→        \n    72→        Args:\n    73→            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74→            sqlite_store: SQLite store for plugin persistence\n    75→            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76→            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77→            lazy_load: Whether to lazy-load plugins on demand\n    78→            semantic_search_enabled: Whether to enable semantic search in plugins\n    79→        """"""\n    80→        self._sqlite_store = sqlite_store\n    81→        self._enable_advanced = enable_advanced_features\n    82→        self._use_factory = use_plugin_factory\n    83→        self._lazy_load = lazy_load\n    84→        self._semantic_enabled = semantic_search_enabled\n    85→        \n    86→        # Plugin storage\n    87→        self._plugins: List[IPlugin] = []\n    88→        self._by_lang: Dict[str, IPlugin] = {}\n    89→        self._loaded_languages: set[str] = set()\n    90→        \n    91→        # Cache for file hashes to avoid re-indexing unchanged files\n    92→        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93→        \n    94→        # Advanced components\n    95→        if self._enable_advanced:\n    96→            self._file_matcher = FileTypeMatcher()\n    97→            self._router = PluginRouter(self._file_matcher)\n    98→            self._aggregator = ResultAggregator()\n    99→        \n   100→        # Performance tracking\n   101→        self._operation_stats = {\n   102→            \'searches\': 0,\n   103→            \'lookups\': 0,\n   104→            \'indexings\': 0,\n   105→            \'total_time\': 0.0,\n   106→            \'plugins_loaded\': 0\n   107→        }\n   108→        \n   109→        # Initialize plugins\n   110→        if plugins:\n   111→            # Use provided plugins (backward compatibility)\n   112→            self._plugins = plugins\n   113→            self._by_lang = {p.lang: p for p in plugins}\n   114→            for plugin in plugins:\n   115→                self._loaded_languages.add(getattr(plugin, \'lang\', \'unknown\'))\n   116→            if self._enable_advanced:\n   117→                self._register_plugins_with_router()\n   118→        elif use_plugin_factory and not lazy_load:\n   119→            # Load all plugins immediately\n   120→            self._load_all_plugins()\n   121→        # If lazy_load is True, plugins will be loaded on demand\n   122→        \n   123→        # Compile document query patterns for performance\n   124→        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125→        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126→        \n   127→        logger.info(f""Enhanced dispatcher initialized with {len(self._plugins)} plugins"")\n   128→    \n   129→    def _load_all_plugins(self):\n   130→        """"""Load all available plugins using PluginFactory.""""""\n   131→        logger.info(""Loading all available plugins..."")\n   132→        \n   133→        all_plugins = PluginFactory.create_all_plugins(\n   134→            sqlite_store=self._sqlite_store,\n   135→            enable_semantic=self._semantic_enabled\n   136→        )\n   137→        \n   138→        for lang, plugin in all_plugins.items():\n   139→            self._plugins.append(plugin)\n   140→            self._by_lang[lang] = plugin\n   141→            self._loaded_languages.add(lang)\n   142→            self._operation_stats[\'plugins_loaded\'] += 1\n   143→        \n   144→        if self._enable_advanced:\n   145→            self._register_plugins_with_router()\n   146→        \n   147→        logger.info(f""Loaded {len(all_plugins)} plugins: {\', \'.join(sorted(all_plugins.keys()))}"")\n   148→    \n   149→    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150→        """"""Ensure a plugin for the given language is loaded.\n   151→        \n   152→        Args:\n   153→            language: Language code (e.g., \'python\', \'go\')\n   154→            \n   155→        Returns:\n   156→            Plugin instance or None if not available\n   157→        """"""\n   158→        # Normalize language\n   159→        language = language.lower().replace(\'-\', \'_\')\n   160→        \n   161→        # Check if already loaded\n   162→        if language in self._by_lang:\n   163→            return self._by_lang[language]\n   164→        \n   165→        # If not using factory or already tried to load, return None\n   166→        if not self._use_factory or language in self._loaded_languages:\n   167→            return None\n   168→        \n   169→        # Try to load the plugin\n   170→        try:\n   171→            logger.info(f""Lazy loading plugin for {language}"")\n   172→            plugin = PluginFactory.create_plugin(\n   173→                language,\n   174→                sqlite_store=self._sqlite_store,\n   175→                enable_semantic=self._semantic_enabled\n   176→            )\n   177→            \n   178→            # Add to collections\n   179→            self._plugins.append(plugin)\n   180→            self._by_lang[language] = plugin\n   181→            self._loaded_languages.add(language)\n   182→            self._operation_stats[\'plugins_loaded\'] += 1\n   183→            \n   184→            # Register with router if needed\n   185→            if self._enable_advanced:\n   186→                capabilities = self._detect_plugin_capabilities(plugin)\n   187→                self._router.register_plugin(plugin, capabilities)\n   188→            \n   189→            logger.info(f""Successfully loaded {language} plugin"")\n   190→            return plugin\n   191→            \n   192→        except ValueError as e:\n   193→            logger.warning(f""No plugin available for {language}: {e}"")\n   194→            self._loaded_languages.add(language)  # Mark as attempted\n   195→            return None\n   196→        except Exception as e:\n   197→            logger.error(f""Error loading plugin for {language}: {e}"")\n   198→            self._loaded_languages.add(language)  # Mark as attempted\n   199→            return None\n   200→    \n   201→    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202→        """"""Ensure a plugin is loaded for the given file.\n   203→        \n   204→        Args:\n   205→            path: File path\n   206→            \n   207→        Returns:\n   208→            Plugin instance or None if not available\n   209→        """"""\n   210→        # Get language from file extension\n   211→        extension = path.suffix.lower()\n   212→        language = get_language_by_extension(extension)\n   213→        \n   214→        if language:\n   215→            return self._ensure_plugin_loaded(language)\n   216→        \n   217→        # Fallback: try all loaded plugins\n   218→        for plugin in self._plugins:\n   219→            if plugin.supports(path):\n   220→                return plugin\n   221→        \n   222→        return None\n   223→    \n   224→    def _register_plugins_with_router(self):\n   225→        """"""Register plugins with the router and assign capabilities.""""""\n   226→        for plugin in self._plugins:\n   227→            # Determine capabilities based on plugin type/language\n   228→            capabilities = self._detect_plugin_capabilities(plugin)\n   229→            self._router.register_plugin(plugin, capabilities)\n   230→    \n   231→    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232→        """"""Detect capabilities for a plugin based on its language and features.""""""\n   233→        capabilities = []\n   234→        lang = getattr(plugin, \'lang\', \'unknown\')\n   235→        \n   236→        # Base capabilities all plugins have\n   237→        capabilities.append(PluginCapability(\n   238→            \'syntax_analysis\', \'1.0\', f\'{lang} syntax analysis\', \n   239→            priority=70, metadata={\'language\': lang}\n   240→        ))\n   241→        \n   242→        capabilities.append(PluginCapability(\n   243→            \'code_search\', \'1.0\', f\'{lang} code search\', \n   244→            priority=80, metadata={\'language\': lang}\n   245→        ))\n   246→        \n   247→        # Check for semantic search capability\n   248→        if hasattr(plugin, \'_enable_semantic\') and plugin._enable_semantic:\n   249→            capabilities.append(PluginCapability(\n   250→                \'semantic_search\', \'1.0\', f\'{lang} semantic search\', \n   251→                priority=90, metadata={\'language\': lang}\n   252→            ))\n   253→        \n   254→        # Language-specific capabilities\n   255→        if lang == \'python\':\n   256→            capabilities.extend([\n   257→                PluginCapability(\'refactoring\', \'1.0\', \'Python refactoring support\', 75),\n   258→                PluginCapability(\'type_analysis\', \'1.0\', \'Python type analysis\', 85)\n   259→            ])\n   260→        elif lang in [\'javascript\', \'typescript\']:\n   261→            capabilities.extend([\n   262→                PluginCapability(\'linting\', \'1.0\', \'JavaScript/TypeScript linting\', 85),\n   263→                PluginCapability(\'bundling_analysis\', \'1.0\', \'Module bundling analysis\', 70),\n   264→                PluginCapability(\'framework_support\', \'1.0\', \'Framework-specific support\', 75)\n   265→            ])\n   266→        elif lang in [\'c\', \'cpp\']:\n   267→            capabilities.extend([\n   268→                PluginCapability(\'compilation_analysis\', \'1.0\', \'Compilation analysis\', 80),\n   269→                PluginCapability(\'memory_analysis\', \'1.0\', \'Memory usage analysis\', 70),\n   270→                PluginCapability(\'performance_profiling\', \'1.0\', \'Performance profiling\', 75)\n   271→            ])\n   272→        elif lang in [\'go\', \'rust\']:\n   273→            capabilities.extend([\n   274→                PluginCapability(\'package_analysis\', \'1.0\', f\'{lang} package analysis\', 80),\n   275→                PluginCapability(\'concurrency_analysis\', \'1.0\', f\'{lang} concurrency analysis\', 75)\n   276→            ])\n   277→        elif lang in [\'java\', \'kotlin\', \'scala\']:\n   278→            capabilities.extend([\n   279→                PluginCapability(\'jvm_analysis\', \'1.0\', \'JVM bytecode analysis\', 75),\n   280→                PluginCapability(\'build_tool_integration\', \'1.0\', \'Build tool integration\', 70)\n   281→            ])\n   282→        \n   283→        return capabilities\n   284→    \n   285→    @property\n   286→    def plugins(self):\n   287→        """"""Get the dictionary of loaded plugins by language.""""""\n   288→        return self._by_lang\n   289→    \n   290→    @property\n   291→    def supported_languages(self) -> List[str]:\n   292→        """"""Get list of all supported languages (loaded and available).""""""\n   293→        if self._use_factory:\n   294→            return PluginFactory.get_supported_languages()\n   295→        else:\n   296→            return list(self._by_lang.keys())\n   297→    \n   298→    def _match_plugin(self, path: Path) -> IPlugin:\n   299→        """"""Match a plugin for the given file path.""""""\n   300→        # Ensure plugin is loaded if using lazy loading\n   301→        if self._lazy_load and self._use_factory:\n   302→            plugin = self._ensure_plugin_for_file(path)\n   303→            if plugin:\n   304→                return plugin\n   305→        \n   306→        # Use advanced routing if available\n   307→        if self._enable_advanced and self._router:\n   308→            route_result = self._router.get_best_plugin(path)\n   309→            if route_result:\n   310→                return route_result.plugin\n   311→        \n   312→        # Fallback to basic matching\n   313→        for p in self._plugins:\n   314→            if p.supports(path):\n   315→                return p\n   316→        \n   317→        raise RuntimeError(f""No plugin found for {path}"")\n   318→    \n   319→    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320→        """"""Get all plugins that can handle a file with confidence scores.""""""\n   321→        # Ensure plugin is loaded if using lazy loading\n   322→        if self._lazy_load and self._use_factory:\n   323→            self._ensure_plugin_for_file(path)\n   324→        \n   325→        if self._enable_advanced and self._router:\n   326→            route_results = self._router.route_file(path)\n   327→            return [(result.plugin, result.confidence) for result in route_results]\n   328→        else:\n   329→            # Basic fallback\n   330→            matching_plugins = []\n   331→            for plugin in self._plugins:\n   332→                if plugin.supports(path):\n   333→                    matching_plugins.append((plugin, 1.0))\n   334→            return matching_plugins\n   335→    \n   336→    def lookup(self, symbol: str) -> SymbolDef | None:\n   337→        """"""Look up symbol definition across all plugins.""""""\n   338→        start_time = time.time()\n   339→        \n   340→        try:\n   341→            # For symbol lookup, we may need to search across all languages\n   342→            # Load all plugins if using lazy loading\n   343→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344→                self._load_all_plugins()\n   345→            \n   346→            if self._enable_advanced and self._aggregator:\n   347→                # Use advanced aggregation\n   348→                definitions_by_plugin = {}\n   349→                for plugin in self._plugins:\n   350→                    try:\n   351→                        definition = plugin.getDefinition(symbol)\n   352→                        definitions_by_plugin[plugin] = definition\n   353→                    except Exception as e:\n   354→                        logger.warning(f""Plugin {plugin.lang} failed to get definition for {symbol}: {e}"")\n   355→                        definitions_by_plugin[plugin] = None\n   356→                \n   357→                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358→                \n   359→                self._operation_stats[\'lookups\'] += 1\n   360→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   361→                \n   362→                return result\n   363→            else:\n   364→                # Fallback to basic lookup\n   365→                for p in self._plugins:\n   366→                    res = p.getDefinition(symbol)\n   367→                    if res:\n   368→                        self._operation_stats[\'lookups\'] += 1\n   369→                        self._operation_stats[\'total_time\'] += time.time() - start_time\n   370→                        return res\n   371→                return None\n   372→                \n   373→        except Exception as e:\n   374→            logger.error(f""Error in symbol lookup for {symbol}: {e}"", exc_info=True)\n   375→            return None\n   376→    \n   377→    def _is_document_query(self, query: str) -> bool:\n   378→        """"""Check if the query is looking for documentation.\n   379→        \n   380→        Args:\n   381→            query: Search query string\n   382→            \n   383→        Returns:\n   384→            True if this appears to be a documentation query\n   385→        """"""\n   386→        query_lower = query.lower()\n   387→        \n   388→        # Check against document query patterns\n   389→        for pattern in self._compiled_doc_patterns:\n   390→            if pattern.search(query_lower):\n   391→                return True\n   392→        \n   393→        # Check for question words at the beginning\n   394→        question_starters = [\'how\', \'what\', \'where\', \'when\', \'why\', \'can\', \'is\', \'does\', \'should\']\n   395→        first_word = query_lower.split()[0] if query_lower.split() else \'\'\n   396→        if first_word in question_starters:\n   397→            return True\n   398→        \n   399→        return False\n   400→    \n   401→    def _expand_document_query(self, query: str) -> List[str]:\n   402→        """"""Expand a document query with related terms for better search coverage.\n   403→        \n   404→        Args:\n   405→            query: Original search query\n   406→            \n   407→        Returns:\n   408→            List of expanded query variations\n   409→        """"""\n   410→        expanded_queries = [query]  # Always include original\n   411→        query_lower = query.lower()\n   412→        \n   413→        # Common expansions for documentation queries\n   414→        expansions = {\n   415→            \'install\': [\'installation\', \'setup\', \'getting started\', \'requirements\'],\n   416→            \'config\': [\'configuration\', \'configure\', \'settings\', \'options\', \'parameters\'],\n   417→            \'api\': [\'api documentation\', \'api reference\', \'endpoint\', \'method\'],\n   418→            \'how to\': [\'tutorial\', \'guide\', \'example\', \'usage\'],\n   419→            \'example\': [\'sample\', \'snippet\', \'demo\', \'code example\'],\n   420→            \'error\': [\'troubleshoot\', \'debug\', \'issue\', \'problem\', \'fix\'],\n   421→            \'getting started\': [\'quickstart\', \'tutorial\', \'introduction\', \'setup\'],\n   422→            \'guide\': [\'tutorial\', \'documentation\', \'walkthrough\', \'how to\'],\n   423→            \'usage\': [\'how to use\', \'example\', \'api\', \'reference\']\n   424→        }\n   425→        \n   426→        # Apply expansions\n   427→        for term, related_terms in expansions.items():\n   428→            if term in query_lower:\n   429→                for related in related_terms:\n   430→                    # Replace the term with related term\n   431→                    expanded = query_lower.replace(term, related)\n   432→                    if expanded != query_lower and expanded not in expanded_queries:\n   433→                        expanded_queries.append(expanded)\n   434→                \n   435→                # Also add queries with additional terms\n   436→                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437→                    expanded = f""{query} {related}""\n   438→                    if expanded not in expanded_queries:\n   439→                        expanded_queries.append(expanded)\n   440→        \n   441→        # Add file-specific searches for common documentation files\n   442→        if self._is_document_query(query):\n   443→            # Extract the main topic from the query\n   444→            topic_words = []\n   445→            for word in query.lower().split():\n   446→                if word not in [\'how\', \'to\', \'the\', \'a\', \'an\', \'is\', \'are\', \'what\', \'where\', \'when\']:\n   447→                    topic_words.append(word)\n   448→            \n   449→            if topic_words:\n   450→                topic = \' \'.join(topic_words[:2])  # Use first two topic words\n   451→                expanded_queries.extend([\n   452→                    f""README {topic}"",\n   453→                    f""{topic} documentation"",\n   454→                    f""{topic} docs"",\n   455→                    f""{topic} guide""\n   456→                ])\n   457→        \n   458→        # Remove duplicates while preserving order\n   459→        seen = set()\n   460→        unique_queries = []\n   461→        for q in expanded_queries:\n   462→            if q not in seen:\n   463→                seen.add(q)\n   464→                unique_queries.append(q)\n   465→        \n   466→        return unique_queries[:10]  # Limit to 10 queries max\n   467→    \n   468→    def _is_documentation_file(self, file_path: str) -> bool:\n   469→        """"""Check if a file path is likely a documentation file.\n   470→        \n   471→        Args:\n   472→            file_path: Path to check\n   473→            \n   474→        Returns:\n   475→            True if this appears to be a documentation file\n   476→        """"""\n   477→        path_lower = file_path.lower()\n   478→        \n   479→        for pattern in self._compiled_file_patterns:\n   480→            if pattern.search(path_lower):\n   481→                return True\n   482→        \n   483→        return False\n   484→    \n   485→    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486→        """"""Adjust ranking to prioritize documentation files for document queries.\n   487→        \n   488→        Args:\n   489→            query: Original search query\n   490→            results: List of aggregated results\n   491→            \n   492→        Returns:\n   493→            Re-ranked results with documentation prioritized\n   494→        """"""\n   495→        if not self._is_document_query(query):\n   496→            return results\n   497→        \n   498→        # Separate documentation and code results\n   499→        doc_results = []\n   500→        code_results = []\n   501→        \n   502→        for result in results:\n   503→            if self._is_documentation_file(result.primary_result.get(\'file\', \'\')):\n   504→                # Boost documentation files for document queries\n   505→                result.rank_score *= 1.5\n   506→                result.metadata[\'doc_boost\'] = True\n   507→                doc_results.append(result)\n   508→            else:\n   509→                code_results.append(result)\n   510→        \n   511→        # Sort each group by rank score\n   512→        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513→        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514→        \n   515→        # Combine with documentation files first\n   516→        return doc_results + code_results\n   517→    \n   518→    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519→        """"""Search for code and documentation across all plugins.""""""\n   520→        start_time = time.time()\n   521→        \n   522→        try:\n   523→            # For search, we may need to search across all languages\n   524→            # Load all plugins if using lazy loading\n   525→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526→                self._load_all_plugins()\n   527→            \n   528→            # Detect if this is a document query\n   529→            is_doc_query = self._is_document_query(query)\n   530→            \n   531→            # Expand query if it\'s a document query\n   532→            queries = [query]\n   533→            if is_doc_query:\n   534→                queries = self._expand_document_query(query)\n   535→                logger.info(f""Expanded document query \'{query}\' to {len(queries)} variations"")\n   536→                # Force semantic search for natural language queries\n   537→                semantic = True\n   538→            \n   539→            if self._enable_advanced and self._aggregator:\n   540→                # Use advanced aggregation\n   541→                all_results_by_plugin = {}\n   542→                opts = {""semantic"": semantic, ""limit"": limit * 2 if is_doc_query else limit}\n   543→                \n   544→                # Search with all query variations\n   545→                for search_query in queries:\n   546→                    for plugin in self._plugins:\n   547→                        try:\n   548→                            results = list(plugin.search(search_query, opts))\n   549→                            if results:\n   550→                                if plugin not in all_results_by_plugin:\n   551→                                    all_results_by_plugin[plugin] = []\n   552→                                all_results_by_plugin[plugin].extend(results)\n   553→                        except Exception as e:\n   554→                            logger.warning(f""Plugin {plugin.lang} failed to search for {search_query}: {e}"")\n   555→                \n   556→                # Deduplicate results per plugin\n   557→                for plugin, results in all_results_by_plugin.items():\n   558→                    seen = set()\n   559→                    unique_results = []\n   560→                    for result in results:\n   561→                        key = f""{result[\'file\']}:{result[\'line\']}""\n   562→                        if key not in seen:\n   563→                            seen.add(key)\n   564→                            unique_results.append(result)\n   565→                    all_results_by_plugin[plugin] = unique_results\n   566→                \n   567→                # Configure aggregator for document queries\n   568→                if is_doc_query and self._enable_advanced:\n   569→                    # Adjust ranking criteria for documentation\n   570→                    doc_criteria = RankingCriteria(\n   571→                        relevance_weight=0.5,      # Increase relevance weight\n   572→                        confidence_weight=0.2,     # Reduce confidence weight\n   573→                        frequency_weight=0.2,      # Keep frequency weight\n   574→                        recency_weight=0.1,        # Keep recency weight\n   575→                        prefer_exact_matches=False,  # Natural language doesn\'t need exact matches\n   576→                        boost_multiple_sources=True,\n   577→                        boost_common_extensions=True\n   578→                    )\n   579→                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580→                \n   581→                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582→                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583→                )\n   584→                \n   585→                # Adjust ranking for document queries\n   586→                if is_doc_query:\n   587→                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588→                \n   589→                # Apply final limit\n   590→                if limit and len(aggregated_results) > limit:\n   591→                    aggregated_results = aggregated_results[:limit]\n   592→                \n   593→                logger.debug(f""Search aggregation stats: {stats.total_results} total, ""\n   594→                           f""{stats.unique_results} unique, {stats.plugins_used} plugins used, ""\n   595→                           f""document_query={is_doc_query}"")\n   596→                \n   597→                self._operation_stats[\'searches\'] += 1\n   598→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   599→                \n   600→                # Yield primary results from aggregated results\n   601→                for aggregated in aggregated_results:\n   602→                    yield aggregated.primary_result\n   603→            else:\n   604→                # Fallback to basic search\n   605→                # Detect if this is a document query\n   606→                is_doc_query = self._is_document_query(query)\n   607→                \n   608→                # Expand query if it\'s a document query\n   609→                queries = [query]\n   610→                if is_doc_query:\n   611→                    queries = self._expand_document_query(query)\n   612→                    semantic = True  # Force semantic search for natural language\n   613→                \n   614→                opts = {""semantic"": semantic, ""limit"": limit}\n   615→                all_results = []\n   616→                \n   617→                # Search with all query variations\n   618→                for search_query in queries:\n   619→                    for p in self._plugins:\n   620→                        try:\n   621→                            for result in p.search(search_query, opts):\n   622→                                all_results.append(result)\n   623→                        except Exception as e:\n   624→                            logger.warning(f""Plugin {p.lang} failed to search for {search_query}: {e}"")\n   625→                \n   626→                # Deduplicate results\n   627→                seen = set()\n   628→                unique_results = []\n   629→                for result in all_results:\n   630→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   631→                    if key not in seen:\n   632→                        seen.add(key)\n   633→                        unique_results.append(result)\n   634→                \n   635→                # Sort by score if available\n   636→                unique_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   637→                \n   638→                # Prioritize documentation files for document queries\n   639→                if is_doc_query:\n   640→                    doc_results = []\n   641→                    code_results = []\n   642→                    for result in unique_results:\n   643→                        if self._is_documentation_file(result.get(\'file\', \'\')):\n   644→                            doc_results.append(result)\n   645→                        else:\n   646→                            code_results.append(result)\n   647→                    unique_results = doc_results + code_results\n   648→                \n   649→                # Apply limit\n   650→                count = 0\n   651→                for result in unique_results:\n   652→                    if limit and count >= limit:\n   653→                        break\n   654→                    yield result\n   655→                    count += 1\n   656→                \n   657→                self._operation_stats[\'searches\'] += 1\n   658→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   659→                \n   660→        except Exception as e:\n   661→            logger.error(f""Error in search for {query}: {e}"", exc_info=True)\n   662→    \n   663→    def index_file(self, path: Path) -> None:\n   664→        """"""Index a single file if it has changed.""""""\n   665→        try:\n   666→            # Ensure path is absolute to avoid relative/absolute path issues\n   667→            path = path.resolve()\n   668→            \n   669→            # Find the appropriate plugin\n   670→            plugin = self._match_plugin(path)\n   671→            \n   672→            # Read file content\n   673→            try:\n   674→                content = path.read_text(encoding=\'utf-8\')\n   675→            except UnicodeDecodeError:\n   676→                # Try with different encodings\n   677→                try:\n   678→                    content = path.read_text(encoding=\'latin-1\')\n   679→                except Exception as e:\n   680→                    logger.error(f""Failed to read {path}: {e}"")\n   681→                    return\n   682→            \n   683→            # Check if we need to re-index (simplified for now)\n   684→            # TODO: Implement proper caching logic\n   685→            \n   686→            # Index the file\n   687→            start_time = time.time()\n   688→            logger.info(f""Indexing {path} with {plugin.lang} plugin"")\n   689→            shard = plugin.indexFile(path, content)\n   690→            \n   691→            # Record performance if advanced features enabled\n   692→            if self._enable_advanced and self._router:\n   693→                execution_time = time.time() - start_time\n   694→                self._router.record_performance(plugin, execution_time)\n   695→            \n   696→            self._operation_stats[\'indexings\'] += 1\n   697→            self._operation_stats[\'total_time\'] += time.time() - start_time\n   698→            \n   699→            logger.info(f""Successfully indexed {path}: {len(shard.get(\'symbols\', []))} symbols found"")\n   700→            \n   701→        except RuntimeError as e:\n   702→            # No plugin found for this file type\n   703→            logger.debug(f""No plugin for {path}: {e}"")\n   704→        except Exception as e:\n   705→            logger.error(f""Error indexing {path}: {e}"", exc_info=True)\n   706→    \n   707→    def get_statistics(self) -> dict:\n   708→        """"""Get comprehensive statistics across all plugins and components.""""""\n   709→        stats = {\n   710→            ""total_plugins"": len(self._plugins),\n   711→            ""loaded_languages"": sorted(list(self._loaded_languages)),\n   712→            ""supported_languages"": len(self.supported_languages),\n   713→            ""operations"": self._operation_stats.copy()\n   714→        }\n   715→        \n   716→        # Add language breakdown\n   717→        stats[""by_language""] = {}\n   718→        for lang, plugin in self._by_lang.items():\n   719→            plugin_info = {\n   720→                ""loaded"": True,\n   721→                ""class"": plugin.__class__.__name__\n   722→            }\n   723→            if hasattr(plugin, \'get_indexed_count\'):\n   724→                plugin_info[""indexed_files""] = plugin.get_indexed_count()\n   725→            stats[""by_language""][lang] = plugin_info\n   726→        \n   727→        return stats\n   728→    \n   729→    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730→        """"""\n   731→        Index all files in a directory, respecting ignore patterns.\n   732→        \n   733→        Args:\n   734→            directory: Directory to index\n   735→            recursive: Whether to index subdirectories\n   736→            \n   737→        Returns:\n   738→            Statistics about indexed files\n   739→        """"""\n   740→        logger.info(f""Indexing directory: {directory} (recursive={recursive})"")\n   741→        \n   742→        # Note: We don\'t use ignore patterns during indexing\n   743→        # ALL files are indexed for local search capability\n   744→        # Filtering happens only during export/sharing\n   745→        \n   746→        # Get all supported extensions\n   747→        supported_extensions = get_all_extensions()\n   748→        \n   749→        stats = {\n   750→            ""total_files"": 0,\n   751→            ""indexed_files"": 0,\n   752→            ""ignored_files"": 0,\n   753→            ""failed_files"": 0,\n   754→            ""by_language"": {}\n   755→        }\n   756→        \n   757→        # Walk directory\n   758→        if recursive:\n   759→            file_iterator = directory.rglob(""*"")\n   760→        else:\n   761→            file_iterator = directory.glob(""*"")\n   762→            \n   763→        for path in file_iterator:\n   764→            if not path.is_file():\n   765→                continue\n   766→                \n   767→            stats[""total_files""] += 1\n   768→            \n   769→            # NOTE: We index ALL files locally, including gitignored ones\n   770→            # Filtering happens only during export/sharing\n   771→            # This allows local search of .env, secrets, etc.\n   772→                \n   773→            # Try to find a plugin that supports this file\n   774→            # This allows us to index ALL files, including .env, .key, etc.\n   775→            try:\n   776→                # First try to match by extension\n   777→                if path.suffix in supported_extensions:\n   778→                    self.index_file(path)\n   779→                    stats[""indexed_files""] += 1\n   780→                # For files without recognized extensions, try each plugin\'s supports() method\n   781→                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782→                else:\n   783→                    matched = False\n   784→                    for plugin in self._plugins:\n   785→                        if plugin.supports(path):\n   786→                            self.index_file(path)\n   787→                            stats[""indexed_files""] += 1\n   788→                            matched = True\n   789→                            break\n   790→                    \n   791→                    # If no plugin matched but we want to index everything,\n   792→                    # we could add a fallback here to index as plaintext\n   793→                    # For now, we\'ll skip unmatched files\n   794→                    if not matched:\n   795→                        logger.debug(f""No plugin found for {path}"")\n   796→                \n   797→                # Track by language\n   798→                language = get_language_by_extension(path.suffix)\n   799→                if language:\n   800→                    stats[""by_language""][language] = stats[""by_language""].get(language, 0) + 1\n   801→                    \n   802→            except Exception as e:\n   803→                logger.error(f""Failed to index {path}: {e}"")\n   804→                stats[""failed_files""] += 1\n   805→                \n   806→        logger.info(f""Directory indexing complete: {stats[\'indexed_files\']} indexed, ""\n   807→                   f""{stats[\'ignored_files\']} ignored, {stats[\'failed_files\']} failed"")\n   808→        \n   809→        return stats\n   810→    \n   811→    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812→        """"""Search specifically across documentation files.\n   813→        \n   814→        Args:\n   815→            topic: Topic to search for (e.g., ""installation"", ""configuration"")\n   816→            doc_types: Optional list of document types to search (e.g., [""readme"", ""guide"", ""api""])\n   817→            limit: Maximum number of results\n   818→            \n   819→        Returns:\n   820→            Search results from documentation files\n   821→        """"""\n   822→        # Default document types if not specified\n   823→        if doc_types is None:\n   824→            doc_types = [""readme"", ""documentation"", ""guide"", ""tutorial"", ""api"", ""changelog"", ""contributing""]\n   825→        \n   826→        # Build search queries for different document types\n   827→        queries = []\n   828→        for doc_type in doc_types:\n   829→            queries.extend([\n   830→                f""{doc_type} {topic}"",\n   831→                f""{topic} {doc_type}"",\n   832→                f""{topic} in {doc_type}""\n   833→            ])\n   834→        \n   835→        # Also search for the topic in common doc filenames\n   836→        queries.extend([\n   837→            f""README {topic}"",\n   838→            f""CONTRIBUTING {topic}"",\n   839→            f""docs {topic}"",\n   840→            f""documentation {topic}""\n   841→        ])\n   842→        \n   843→        # Deduplicate queries\n   844→        queries = list(dict.fromkeys(queries))\n   845→        \n   846→        logger.info(f""Cross-document search for \'{topic}\' with {len(queries)} query variations"")\n   847→        \n   848→        # Use the enhanced search with document-specific handling\n   849→        all_results = []\n   850→        seen = set()\n   851→        \n   852→        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853→            for result in self.search(query, semantic=True, limit=limit):\n   854→                # Only include documentation files\n   855→                if self._is_documentation_file(result.get(\'file\', \'\')):\n   856→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   857→                    if key not in seen:\n   858→                        seen.add(key)\n   859→                        all_results.append(result)\n   860→        \n   861→        # Sort by relevance (score) and return top results\n   862→        all_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   863→        \n   864→        count = 0\n   865→        for result in all_results:\n   866→            if count >= limit:\n   867→                break\n   868→            yield result\n   869→            count += 1\n   870→    \n   871→    def health_check(self) -> Dict[str, Any]:\n   872→        """"""Perform a health check on all components.""""""\n   873→        health = {\n   874→            \'status\': \'healthy\',\n   875→            \'components\': {\n   876→                \'dispatcher\': {\n   877→                    \'status\': \'healthy\',\n   878→                    \'plugins_loaded\': len(self._plugins),\n   879→                    \'languages_supported\': len(self.supported_languages),\n   880→                    \'factory_enabled\': self._use_factory,\n   881→                    \'lazy_loading\': self._lazy_load\n   882→                }\n   883→            },\n   884→            \'plugins\': {},\n   885→            \'errors\': []\n   886→        }\n   887→        \n   888→        # Check plugin health\n   889→        for lang, plugin in self._by_lang.items():\n   890→            try:\n   891→                plugin_health = {\n   892→                    \'status\': \'healthy\',\n   893→                    \'class\': plugin.__class__.__name__,\n   894→                    \'semantic_enabled\': getattr(plugin, \'_enable_semantic\', False)\n   895→                }\n   896→                if hasattr(plugin, \'get_indexed_count\'):\n   897→                    plugin_health[\'indexed_files\'] = plugin.get_indexed_count()\n   898→            except Exception as e:\n   899→                plugin_health = {\n   900→                    \'status\': \'error\',\n   901→                    \'error\': str(e)\n   902→                }\n   903→                health[\'errors\'].append(f""Plugin {lang}: {str(e)}"")\n   904→            \n   905→            health[\'plugins\'][lang] = plugin_health\n   906→        \n   907→        # Determine overall health\n   908→        if len(health[\'errors\']) > 0:\n   909→            health[\'status\'] = \'degraded\' if len(health[\'errors\']) < 3 else \'unhealthy\'\n   910→        \n   911→        return health\n   912→    \n   913→    def remove_file(self, path: Union[Path, str]) -> None:\n   914→        """"""Remove a file from all indexes.\n   915→        \n   916→        Args:\n   917→            path: File path to remove\n   918→        """"""\n   919→        path = Path(path).resolve()\n   920→        logger.info(f""Removing file from index: {path}"")\n   921→        \n   922→        try:\n   923→            # Remove from SQLite if available\n   924→            if self._sqlite_store:\n   925→                from ..core.path_resolver import PathResolver\n   926→                path_resolver = PathResolver()\n   927→                try:\n   928→                    relative_path = path_resolver.normalize_path(path)\n   929→                    # Get repository ID - for now assume 1\n   930→                    # TODO: Properly detect repository\n   931→                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932→                except Exception as e:\n   933→                    logger.error(f""Error removing from SQLite: {e}"")\n   934→            \n   935→            # Remove from semantic index if available\n   936→            try:\n   937→                plugin = self._match_plugin(path)\n   938→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   939→                    plugin._indexer.remove_file(path)\n   940→                    logger.info(f""Removed from semantic index: {path}"")\n   941→            except Exception as e:\n   942→                logger.warning(f""Error removing from semantic index: {e}"")\n   943→            \n   944→            # Update statistics\n   945→            self._operation_stats[\'deletions\'] = self._operation_stats.get(\'deletions\', 0) + 1\n   946→            \n   947→        except Exception as e:\n   948→            logger.error(f""Error removing file {path}: {e}"", exc_info=True)\n   949→    \n   950→    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951→        """"""Move a file in all indexes.\n   952→        \n   953→        Args:\n   954→            old_path: Original file path\n   955→            new_path: New file path\n   956→            content_hash: Optional content hash to verify unchanged content\n   957→        """"""\n   958→        old_path = Path(old_path).resolve()\n   959→        new_path = Path(new_path).resolve()\n   960→        logger.info(f""Moving file in index: {old_path} -> {new_path}"")\n   961→        \n   962→        try:\n   963→            # Move in SQLite if available\n   964→            if self._sqlite_store:\n   965→                from ..core.path_resolver import PathResolver\n   966→                path_resolver = PathResolver()\n   967→                try:\n   968→                    old_relative = path_resolver.normalize_path(old_path)\n   969→                    new_relative = path_resolver.normalize_path(new_path)\n   970→                    # Get repository ID - for now assume 1\n   971→                    # TODO: Properly detect repository\n   972→                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973→                except Exception as e:\n   974→                    logger.error(f""Error moving in SQLite: {e}"")\n   975→            \n   976→            # Move in semantic index if available\n   977→            try:\n   978→                plugin = self._match_plugin(new_path)\n   979→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   980→                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981→                    logger.info(f""Moved in semantic index: {old_path} -> {new_path}"")\n   982→            except Exception as e:\n   983→                logger.warning(f""Error moving in semantic index: {e}"")\n   984→            \n   985→            # Update statistics\n   986→            self._operation_stats[\'moves\'] = self._operation_stats.get(\'moves\', 0) + 1\n   987→            \n   988→        except Exception as e:\n   989→            logger.error(f""Error moving file {old_path} -> {new_path}: {e}"", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,1,1,14366,13324,1,0.5,0.25,0,,,,,transcript_response
0eae0e31-aa5c-4b13-83d0-28bb4143fd22,2025-06-26T07:31:01.656Z,transcript_0eae0e31-aa5c-4b13-83d0-28bb4143fd22,Find the BM25Indexer class definition,transcript_query,unknown,mcp,0,True,4,0,0,0,4,0,0,0,1,1,0,13905,1,0.5,0.25,0,,,,,transcript_response
0eae0e31-aa5c-4b13-83d0-28bb4143fd22,2025-06-26T07:31:05.854Z,transcript_0eae0e31-aa5c-4b13-83d0-28bb4143fd22,"{'tool_use_id': 'toolu_01SjDWcufiuQoWCxEHUFnfiR', 'type': 'tool_result', 'content': 'Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/bm25_indexer.py'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,1,1,13905,228,1,0.5,0.25,0,,,,,transcript_response
e514801b-3ad0-4d1c-9f08-c28e03f0b0a9,2025-06-26T07:31:09.557Z,transcript_e514801b-3ad0-4d1c-9f08-c28e03f0b0a9,Find the EnhancedDispatcher class and show its main methods,transcript_query,unknown,mcp,0,True,4,0,0,0,4,0,0,0,1,1,13160,749,1,0.5,0.25,0,,,,,transcript_response
e514801b-3ad0-4d1c-9f08-c28e03f0b0a9,2025-06-26T07:31:12.704Z,transcript_e514801b-3ad0-4d1c-9f08-c28e03f0b0a9,"{'tool_use_id': 'toolu_013RYNe4zYErbjT6yFJfJHP3', 'type': 'tool_result', 'content': 'Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/path_management_architecture.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/path_management.puml'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,29,29,13909,455,29,0.5,0.25,0,,,,,transcript_response
e514801b-3ad0-4d1c-9f08-c28e03f0b0a9,2025-06-26T07:31:16.894Z,transcript_e514801b-3ad0-4d1c-9f08-c28e03f0b0a9,"{'tool_use_id': 'toolu_01WBt7AwPtsSUCPSJqPhzqRa', 'type': 'tool_result', 'content': '     1→""""""Enhanced dispatcher with dynamic plugin loading via PluginFactory.""""""\n     2→from pathlib import Path\n     3→from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4→import logging\n     5→import hashlib\n     6→import time\n     7→import re\n     8→from datetime import datetime\n     9→import os\n    10→\n    11→from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12→from ..plugins.plugin_factory import PluginFactory\n    13→from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14→from ..storage.sqlite_store import SQLiteStore\n    15→from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16→from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17→# Note: We\'ve removed ignore pattern checks to allow indexing ALL files\n    18→# Filtering happens only during export via SecureIndexExporter\n    19→# from ..core.ignore_patterns import get_ignore_manager\n    20→\n    21→logger = logging.getLogger(__name__)\n    22→\n    23→\n    24→class EnhancedDispatcher:\n    25→    """"""Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.""""""\n    26→    \n    27→    # Document query patterns - common documentation search terms\n    28→    DOCUMENT_QUERY_PATTERNS = [\n    29→        r\'\\b(how\\s+to|howto)\\b\',\n    30→        r\'\\b(getting\\s+started|get\\s+started)\\b\',\n    31→        r\'\\b(installation|install|setup)\\b\',\n    32→        r\'\\b(configuration|configure|config)\\b\',\n    33→        r\'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b\',\n    34→        r\'\\b(tutorial|guide|walkthrough)\\b\',\n    35→        r\'\\b(example|sample|snippet)\\b\',\n    36→        r\'\\b(readme|documentation|docs)\\b\',\n    37→        r\'\\b(usage|use\\s+case|using)\\b\',\n    38→        r\'\\b(reference|manual)\\b\',\n    39→        r\'\\b(faq|frequently\\s+asked)\\b\',\n    40→        r\'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b\',\n    41→        r\'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b\',\n    42→        r\'\\b(architecture|design|overview)\\b\',\n    43→        r\'\\b(changelog|release\\s+notes|migration)\\b\'\n    44→    ]\n    45→    \n    46→    # Documentation file patterns\n    47→    DOCUMENTATION_FILE_PATTERNS = [\n    48→        r\'readme(\\.\\w+)?$\',\n    49→        r\'changelog(\\.\\w+)?$\',\n    50→        r\'contributing(\\.\\w+)?$\',\n    51→        r\'license(\\.\\w+)?$\',\n    52→        r\'install(\\.\\w+)?$\',\n    53→        r\'setup(\\.\\w+)?$\',\n    54→        r\'guide(\\.\\w+)?$\',\n    55→        r\'tutorial(\\.\\w+)?$\',\n    56→        r\'\\.md$\',\n    57→        r\'\\.rst$\',\n    58→        r\'\\.txt$\',\n    59→        r\'docs?/\',\n    60→        r\'documentation/\'\n    61→    ]\n    62→    \n    63→    def __init__(self, \n    64→                 plugins: Optional[List[IPlugin]] = None,\n    65→                 sqlite_store: Optional[SQLiteStore] = None,\n    66→                 enable_advanced_features: bool = True,\n    67→                 use_plugin_factory: bool = True,\n    68→                 lazy_load: bool = True,\n    69→                 semantic_search_enabled: bool = True):\n    70→        """"""Initialize the enhanced dispatcher.\n    71→        \n    72→        Args:\n    73→            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74→            sqlite_store: SQLite store for plugin persistence\n    75→            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76→            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77→            lazy_load: Whether to lazy-load plugins on demand\n    78→            semantic_search_enabled: Whether to enable semantic search in plugins\n    79→        """"""\n    80→        self._sqlite_store = sqlite_store\n    81→        self._enable_advanced = enable_advanced_features\n    82→        self._use_factory = use_plugin_factory\n    83→        self._lazy_load = lazy_load\n    84→        self._semantic_enabled = semantic_search_enabled\n    85→        \n    86→        # Plugin storage\n    87→        self._plugins: List[IPlugin] = []\n    88→        self._by_lang: Dict[str, IPlugin] = {}\n    89→        self._loaded_languages: set[str] = set()\n    90→        \n    91→        # Cache for file hashes to avoid re-indexing unchanged files\n    92→        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93→        \n    94→        # Advanced components\n    95→        if self._enable_advanced:\n    96→            self._file_matcher = FileTypeMatcher()\n    97→            self._router = PluginRouter(self._file_matcher)\n    98→            self._aggregator = ResultAggregator()\n    99→        \n   100→        # Performance tracking\n   101→        self._operation_stats = {\n   102→            \'searches\': 0,\n   103→            \'lookups\': 0,\n   104→            \'indexings\': 0,\n   105→            \'total_time\': 0.0,\n   106→            \'plugins_loaded\': 0\n   107→        }\n   108→        \n   109→        # Initialize plugins\n   110→        if plugins:\n   111→            # Use provided plugins (backward compatibility)\n   112→            self._plugins = plugins\n   113→            self._by_lang = {p.lang: p for p in plugins}\n   114→            for plugin in plugins:\n   115→                self._loaded_languages.add(getattr(plugin, \'lang\', \'unknown\'))\n   116→            if self._enable_advanced:\n   117→                self._register_plugins_with_router()\n   118→        elif use_plugin_factory and not lazy_load:\n   119→            # Load all plugins immediately\n   120→            self._load_all_plugins()\n   121→        # If lazy_load is True, plugins will be loaded on demand\n   122→        \n   123→        # Compile document query patterns for performance\n   124→        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125→        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126→        \n   127→        logger.info(f""Enhanced dispatcher initialized with {len(self._plugins)} plugins"")\n   128→    \n   129→    def _load_all_plugins(self):\n   130→        """"""Load all available plugins using PluginFactory.""""""\n   131→        logger.info(""Loading all available plugins..."")\n   132→        \n   133→        all_plugins = PluginFactory.create_all_plugins(\n   134→            sqlite_store=self._sqlite_store,\n   135→            enable_semantic=self._semantic_enabled\n   136→        )\n   137→        \n   138→        for lang, plugin in all_plugins.items():\n   139→            self._plugins.append(plugin)\n   140→            self._by_lang[lang] = plugin\n   141→            self._loaded_languages.add(lang)\n   142→            self._operation_stats[\'plugins_loaded\'] += 1\n   143→        \n   144→        if self._enable_advanced:\n   145→            self._register_plugins_with_router()\n   146→        \n   147→        logger.info(f""Loaded {len(all_plugins)} plugins: {\', \'.join(sorted(all_plugins.keys()))}"")\n   148→    \n   149→    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150→        """"""Ensure a plugin for the given language is loaded.\n   151→        \n   152→        Args:\n   153→            language: Language code (e.g., \'python\', \'go\')\n   154→            \n   155→        Returns:\n   156→            Plugin instance or None if not available\n   157→        """"""\n   158→        # Normalize language\n   159→        language = language.lower().replace(\'-\', \'_\')\n   160→        \n   161→        # Check if already loaded\n   162→        if language in self._by_lang:\n   163→            return self._by_lang[language]\n   164→        \n   165→        # If not using factory or already tried to load, return None\n   166→        if not self._use_factory or language in self._loaded_languages:\n   167→            return None\n   168→        \n   169→        # Try to load the plugin\n   170→        try:\n   171→            logger.info(f""Lazy loading plugin for {language}"")\n   172→            plugin = PluginFactory.create_plugin(\n   173→                language,\n   174→                sqlite_store=self._sqlite_store,\n   175→                enable_semantic=self._semantic_enabled\n   176→            )\n   177→            \n   178→            # Add to collections\n   179→            self._plugins.append(plugin)\n   180→            self._by_lang[language] = plugin\n   181→            self._loaded_languages.add(language)\n   182→            self._operation_stats[\'plugins_loaded\'] += 1\n   183→            \n   184→            # Register with router if needed\n   185→            if self._enable_advanced:\n   186→                capabilities = self._detect_plugin_capabilities(plugin)\n   187→                self._router.register_plugin(plugin, capabilities)\n   188→            \n   189→            logger.info(f""Successfully loaded {language} plugin"")\n   190→            return plugin\n   191→            \n   192→        except ValueError as e:\n   193→            logger.warning(f""No plugin available for {language}: {e}"")\n   194→            self._loaded_languages.add(language)  # Mark as attempted\n   195→            return None\n   196→        except Exception as e:\n   197→            logger.error(f""Error loading plugin for {language}: {e}"")\n   198→            self._loaded_languages.add(language)  # Mark as attempted\n   199→            return None\n   200→    \n   201→    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202→        """"""Ensure a plugin is loaded for the given file.\n   203→        \n   204→        Args:\n   205→            path: File path\n   206→            \n   207→        Returns:\n   208→            Plugin instance or None if not available\n   209→        """"""\n   210→        # Get language from file extension\n   211→        extension = path.suffix.lower()\n   212→        language = get_language_by_extension(extension)\n   213→        \n   214→        if language:\n   215→            return self._ensure_plugin_loaded(language)\n   216→        \n   217→        # Fallback: try all loaded plugins\n   218→        for plugin in self._plugins:\n   219→            if plugin.supports(path):\n   220→                return plugin\n   221→        \n   222→        return None\n   223→    \n   224→    def _register_plugins_with_router(self):\n   225→        """"""Register plugins with the router and assign capabilities.""""""\n   226→        for plugin in self._plugins:\n   227→            # Determine capabilities based on plugin type/language\n   228→            capabilities = self._detect_plugin_capabilities(plugin)\n   229→            self._router.register_plugin(plugin, capabilities)\n   230→    \n   231→    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232→        """"""Detect capabilities for a plugin based on its language and features.""""""\n   233→        capabilities = []\n   234→        lang = getattr(plugin, \'lang\', \'unknown\')\n   235→        \n   236→        # Base capabilities all plugins have\n   237→        capabilities.append(PluginCapability(\n   238→            \'syntax_analysis\', \'1.0\', f\'{lang} syntax analysis\', \n   239→            priority=70, metadata={\'language\': lang}\n   240→        ))\n   241→        \n   242→        capabilities.append(PluginCapability(\n   243→            \'code_search\', \'1.0\', f\'{lang} code search\', \n   244→            priority=80, metadata={\'language\': lang}\n   245→        ))\n   246→        \n   247→        # Check for semantic search capability\n   248→        if hasattr(plugin, \'_enable_semantic\') and plugin._enable_semantic:\n   249→            capabilities.append(PluginCapability(\n   250→                \'semantic_search\', \'1.0\', f\'{lang} semantic search\', \n   251→                priority=90, metadata={\'language\': lang}\n   252→            ))\n   253→        \n   254→        # Language-specific capabilities\n   255→        if lang == \'python\':\n   256→            capabilities.extend([\n   257→                PluginCapability(\'refactoring\', \'1.0\', \'Python refactoring support\', 75),\n   258→                PluginCapability(\'type_analysis\', \'1.0\', \'Python type analysis\', 85)\n   259→            ])\n   260→        elif lang in [\'javascript\', \'typescript\']:\n   261→            capabilities.extend([\n   262→                PluginCapability(\'linting\', \'1.0\', \'JavaScript/TypeScript linting\', 85),\n   263→                PluginCapability(\'bundling_analysis\', \'1.0\', \'Module bundling analysis\', 70),\n   264→                PluginCapability(\'framework_support\', \'1.0\', \'Framework-specific support\', 75)\n   265→            ])\n   266→        elif lang in [\'c\', \'cpp\']:\n   267→            capabilities.extend([\n   268→                PluginCapability(\'compilation_analysis\', \'1.0\', \'Compilation analysis\', 80),\n   269→                PluginCapability(\'memory_analysis\', \'1.0\', \'Memory usage analysis\', 70),\n   270→                PluginCapability(\'performance_profiling\', \'1.0\', \'Performance profiling\', 75)\n   271→            ])\n   272→        elif lang in [\'go\', \'rust\']:\n   273→            capabilities.extend([\n   274→                PluginCapability(\'package_analysis\', \'1.0\', f\'{lang} package analysis\', 80),\n   275→                PluginCapability(\'concurrency_analysis\', \'1.0\', f\'{lang} concurrency analysis\', 75)\n   276→            ])\n   277→        elif lang in [\'java\', \'kotlin\', \'scala\']:\n   278→            capabilities.extend([\n   279→                PluginCapability(\'jvm_analysis\', \'1.0\', \'JVM bytecode analysis\', 75),\n   280→                PluginCapability(\'build_tool_integration\', \'1.0\', \'Build tool integration\', 70)\n   281→            ])\n   282→        \n   283→        return capabilities\n   284→    \n   285→    @property\n   286→    def plugins(self):\n   287→        """"""Get the dictionary of loaded plugins by language.""""""\n   288→        return self._by_lang\n   289→    \n   290→    @property\n   291→    def supported_languages(self) -> List[str]:\n   292→        """"""Get list of all supported languages (loaded and available).""""""\n   293→        if self._use_factory:\n   294→            return PluginFactory.get_supported_languages()\n   295→        else:\n   296→            return list(self._by_lang.keys())\n   297→    \n   298→    def _match_plugin(self, path: Path) -> IPlugin:\n   299→        """"""Match a plugin for the given file path.""""""\n   300→        # Ensure plugin is loaded if using lazy loading\n   301→        if self._lazy_load and self._use_factory:\n   302→            plugin = self._ensure_plugin_for_file(path)\n   303→            if plugin:\n   304→                return plugin\n   305→        \n   306→        # Use advanced routing if available\n   307→        if self._enable_advanced and self._router:\n   308→            route_result = self._router.get_best_plugin(path)\n   309→            if route_result:\n   310→                return route_result.plugin\n   311→        \n   312→        # Fallback to basic matching\n   313→        for p in self._plugins:\n   314→            if p.supports(path):\n   315→                return p\n   316→        \n   317→        raise RuntimeError(f""No plugin found for {path}"")\n   318→    \n   319→    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320→        """"""Get all plugins that can handle a file with confidence scores.""""""\n   321→        # Ensure plugin is loaded if using lazy loading\n   322→        if self._lazy_load and self._use_factory:\n   323→            self._ensure_plugin_for_file(path)\n   324→        \n   325→        if self._enable_advanced and self._router:\n   326→            route_results = self._router.route_file(path)\n   327→            return [(result.plugin, result.confidence) for result in route_results]\n   328→        else:\n   329→            # Basic fallback\n   330→            matching_plugins = []\n   331→            for plugin in self._plugins:\n   332→                if plugin.supports(path):\n   333→                    matching_plugins.append((plugin, 1.0))\n   334→            return matching_plugins\n   335→    \n   336→    def lookup(self, symbol: str) -> SymbolDef | None:\n   337→        """"""Look up symbol definition across all plugins.""""""\n   338→        start_time = time.time()\n   339→        \n   340→        try:\n   341→            # For symbol lookup, we may need to search across all languages\n   342→            # Load all plugins if using lazy loading\n   343→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344→                self._load_all_plugins()\n   345→            \n   346→            if self._enable_advanced and self._aggregator:\n   347→                # Use advanced aggregation\n   348→                definitions_by_plugin = {}\n   349→                for plugin in self._plugins:\n   350→                    try:\n   351→                        definition = plugin.getDefinition(symbol)\n   352→                        definitions_by_plugin[plugin] = definition\n   353→                    except Exception as e:\n   354→                        logger.warning(f""Plugin {plugin.lang} failed to get definition for {symbol}: {e}"")\n   355→                        definitions_by_plugin[plugin] = None\n   356→                \n   357→                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358→                \n   359→                self._operation_stats[\'lookups\'] += 1\n   360→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   361→                \n   362→                return result\n   363→            else:\n   364→                # Fallback to basic lookup\n   365→                for p in self._plugins:\n   366→                    res = p.getDefinition(symbol)\n   367→                    if res:\n   368→                        self._operation_stats[\'lookups\'] += 1\n   369→                        self._operation_stats[\'total_time\'] += time.time() - start_time\n   370→                        return res\n   371→                return None\n   372→                \n   373→        except Exception as e:\n   374→            logger.error(f""Error in symbol lookup for {symbol}: {e}"", exc_info=True)\n   375→            return None\n   376→    \n   377→    def _is_document_query(self, query: str) -> bool:\n   378→        """"""Check if the query is looking for documentation.\n   379→        \n   380→        Args:\n   381→            query: Search query string\n   382→            \n   383→        Returns:\n   384→            True if this appears to be a documentation query\n   385→        """"""\n   386→        query_lower = query.lower()\n   387→        \n   388→        # Check against document query patterns\n   389→        for pattern in self._compiled_doc_patterns:\n   390→            if pattern.search(query_lower):\n   391→                return True\n   392→        \n   393→        # Check for question words at the beginning\n   394→        question_starters = [\'how\', \'what\', \'where\', \'when\', \'why\', \'can\', \'is\', \'does\', \'should\']\n   395→        first_word = query_lower.split()[0] if query_lower.split() else \'\'\n   396→        if first_word in question_starters:\n   397→            return True\n   398→        \n   399→        return False\n   400→    \n   401→    def _expand_document_query(self, query: str) -> List[str]:\n   402→        """"""Expand a document query with related terms for better search coverage.\n   403→        \n   404→        Args:\n   405→            query: Original search query\n   406→            \n   407→        Returns:\n   408→            List of expanded query variations\n   409→        """"""\n   410→        expanded_queries = [query]  # Always include original\n   411→        query_lower = query.lower()\n   412→        \n   413→        # Common expansions for documentation queries\n   414→        expansions = {\n   415→            \'install\': [\'installation\', \'setup\', \'getting started\', \'requirements\'],\n   416→            \'config\': [\'configuration\', \'configure\', \'settings\', \'options\', \'parameters\'],\n   417→            \'api\': [\'api documentation\', \'api reference\', \'endpoint\', \'method\'],\n   418→            \'how to\': [\'tutorial\', \'guide\', \'example\', \'usage\'],\n   419→            \'example\': [\'sample\', \'snippet\', \'demo\', \'code example\'],\n   420→            \'error\': [\'troubleshoot\', \'debug\', \'issue\', \'problem\', \'fix\'],\n   421→            \'getting started\': [\'quickstart\', \'tutorial\', \'introduction\', \'setup\'],\n   422→            \'guide\': [\'tutorial\', \'documentation\', \'walkthrough\', \'how to\'],\n   423→            \'usage\': [\'how to use\', \'example\', \'api\', \'reference\']\n   424→        }\n   425→        \n   426→        # Apply expansions\n   427→        for term, related_terms in expansions.items():\n   428→            if term in query_lower:\n   429→                for related in related_terms:\n   430→                    # Replace the term with related term\n   431→                    expanded = query_lower.replace(term, related)\n   432→                    if expanded != query_lower and expanded not in expanded_queries:\n   433→                        expanded_queries.append(expanded)\n   434→                \n   435→                # Also add queries with additional terms\n   436→                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437→                    expanded = f""{query} {related}""\n   438→                    if expanded not in expanded_queries:\n   439→                        expanded_queries.append(expanded)\n   440→        \n   441→        # Add file-specific searches for common documentation files\n   442→        if self._is_document_query(query):\n   443→            # Extract the main topic from the query\n   444→            topic_words = []\n   445→            for word in query.lower().split():\n   446→                if word not in [\'how\', \'to\', \'the\', \'a\', \'an\', \'is\', \'are\', \'what\', \'where\', \'when\']:\n   447→                    topic_words.append(word)\n   448→            \n   449→            if topic_words:\n   450→                topic = \' \'.join(topic_words[:2])  # Use first two topic words\n   451→                expanded_queries.extend([\n   452→                    f""README {topic}"",\n   453→                    f""{topic} documentation"",\n   454→                    f""{topic} docs"",\n   455→                    f""{topic} guide""\n   456→                ])\n   457→        \n   458→        # Remove duplicates while preserving order\n   459→        seen = set()\n   460→        unique_queries = []\n   461→        for q in expanded_queries:\n   462→            if q not in seen:\n   463→                seen.add(q)\n   464→                unique_queries.append(q)\n   465→        \n   466→        return unique_queries[:10]  # Limit to 10 queries max\n   467→    \n   468→    def _is_documentation_file(self, file_path: str) -> bool:\n   469→        """"""Check if a file path is likely a documentation file.\n   470→        \n   471→        Args:\n   472→            file_path: Path to check\n   473→            \n   474→        Returns:\n   475→            True if this appears to be a documentation file\n   476→        """"""\n   477→        path_lower = file_path.lower()\n   478→        \n   479→        for pattern in self._compiled_file_patterns:\n   480→            if pattern.search(path_lower):\n   481→                return True\n   482→        \n   483→        return False\n   484→    \n   485→    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486→        """"""Adjust ranking to prioritize documentation files for document queries.\n   487→        \n   488→        Args:\n   489→            query: Original search query\n   490→            results: List of aggregated results\n   491→            \n   492→        Returns:\n   493→            Re-ranked results with documentation prioritized\n   494→        """"""\n   495→        if not self._is_document_query(query):\n   496→            return results\n   497→        \n   498→        # Separate documentation and code results\n   499→        doc_results = []\n   500→        code_results = []\n   501→        \n   502→        for result in results:\n   503→            if self._is_documentation_file(result.primary_result.get(\'file\', \'\')):\n   504→                # Boost documentation files for document queries\n   505→                result.rank_score *= 1.5\n   506→                result.metadata[\'doc_boost\'] = True\n   507→                doc_results.append(result)\n   508→            else:\n   509→                code_results.append(result)\n   510→        \n   511→        # Sort each group by rank score\n   512→        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513→        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514→        \n   515→        # Combine with documentation files first\n   516→        return doc_results + code_results\n   517→    \n   518→    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519→        """"""Search for code and documentation across all plugins.""""""\n   520→        start_time = time.time()\n   521→        \n   522→        try:\n   523→            # For search, we may need to search across all languages\n   524→            # Load all plugins if using lazy loading\n   525→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526→                self._load_all_plugins()\n   527→            \n   528→            # Detect if this is a document query\n   529→            is_doc_query = self._is_document_query(query)\n   530→            \n   531→            # Expand query if it\'s a document query\n   532→            queries = [query]\n   533→            if is_doc_query:\n   534→                queries = self._expand_document_query(query)\n   535→                logger.info(f""Expanded document query \'{query}\' to {len(queries)} variations"")\n   536→                # Force semantic search for natural language queries\n   537→                semantic = True\n   538→            \n   539→            if self._enable_advanced and self._aggregator:\n   540→                # Use advanced aggregation\n   541→                all_results_by_plugin = {}\n   542→                opts = {""semantic"": semantic, ""limit"": limit * 2 if is_doc_query else limit}\n   543→                \n   544→                # Search with all query variations\n   545→                for search_query in queries:\n   546→                    for plugin in self._plugins:\n   547→                        try:\n   548→                            results = list(plugin.search(search_query, opts))\n   549→                            if results:\n   550→                                if plugin not in all_results_by_plugin:\n   551→                                    all_results_by_plugin[plugin] = []\n   552→                                all_results_by_plugin[plugin].extend(results)\n   553→                        except Exception as e:\n   554→                            logger.warning(f""Plugin {plugin.lang} failed to search for {search_query}: {e}"")\n   555→                \n   556→                # Deduplicate results per plugin\n   557→                for plugin, results in all_results_by_plugin.items():\n   558→                    seen = set()\n   559→                    unique_results = []\n   560→                    for result in results:\n   561→                        key = f""{result[\'file\']}:{result[\'line\']}""\n   562→                        if key not in seen:\n   563→                            seen.add(key)\n   564→                            unique_results.append(result)\n   565→                    all_results_by_plugin[plugin] = unique_results\n   566→                \n   567→                # Configure aggregator for document queries\n   568→                if is_doc_query and self._enable_advanced:\n   569→                    # Adjust ranking criteria for documentation\n   570→                    doc_criteria = RankingCriteria(\n   571→                        relevance_weight=0.5,      # Increase relevance weight\n   572→                        confidence_weight=0.2,     # Reduce confidence weight\n   573→                        frequency_weight=0.2,      # Keep frequency weight\n   574→                        recency_weight=0.1,        # Keep recency weight\n   575→                        prefer_exact_matches=False,  # Natural language doesn\'t need exact matches\n   576→                        boost_multiple_sources=True,\n   577→                        boost_common_extensions=True\n   578→                    )\n   579→                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580→                \n   581→                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582→                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583→                )\n   584→                \n   585→                # Adjust ranking for document queries\n   586→                if is_doc_query:\n   587→                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588→                \n   589→                # Apply final limit\n   590→                if limit and len(aggregated_results) > limit:\n   591→                    aggregated_results = aggregated_results[:limit]\n   592→                \n   593→                logger.debug(f""Search aggregation stats: {stats.total_results} total, ""\n   594→                           f""{stats.unique_results} unique, {stats.plugins_used} plugins used, ""\n   595→                           f""document_query={is_doc_query}"")\n   596→                \n   597→                self._operation_stats[\'searches\'] += 1\n   598→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   599→                \n   600→                # Yield primary results from aggregated results\n   601→                for aggregated in aggregated_results:\n   602→                    yield aggregated.primary_result\n   603→            else:\n   604→                # Fallback to basic search\n   605→                # Detect if this is a document query\n   606→                is_doc_query = self._is_document_query(query)\n   607→                \n   608→                # Expand query if it\'s a document query\n   609→                queries = [query]\n   610→                if is_doc_query:\n   611→                    queries = self._expand_document_query(query)\n   612→                    semantic = True  # Force semantic search for natural language\n   613→                \n   614→                opts = {""semantic"": semantic, ""limit"": limit}\n   615→                all_results = []\n   616→                \n   617→                # Search with all query variations\n   618→                for search_query in queries:\n   619→                    for p in self._plugins:\n   620→                        try:\n   621→                            for result in p.search(search_query, opts):\n   622→                                all_results.append(result)\n   623→                        except Exception as e:\n   624→                            logger.warning(f""Plugin {p.lang} failed to search for {search_query}: {e}"")\n   625→                \n   626→                # Deduplicate results\n   627→                seen = set()\n   628→                unique_results = []\n   629→                for result in all_results:\n   630→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   631→                    if key not in seen:\n   632→                        seen.add(key)\n   633→                        unique_results.append(result)\n   634→                \n   635→                # Sort by score if available\n   636→                unique_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   637→                \n   638→                # Prioritize documentation files for document queries\n   639→                if is_doc_query:\n   640→                    doc_results = []\n   641→                    code_results = []\n   642→                    for result in unique_results:\n   643→                        if self._is_documentation_file(result.get(\'file\', \'\')):\n   644→                            doc_results.append(result)\n   645→                        else:\n   646→                            code_results.append(result)\n   647→                    unique_results = doc_results + code_results\n   648→                \n   649→                # Apply limit\n   650→                count = 0\n   651→                for result in unique_results:\n   652→                    if limit and count >= limit:\n   653→                        break\n   654→                    yield result\n   655→                    count += 1\n   656→                \n   657→                self._operation_stats[\'searches\'] += 1\n   658→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   659→                \n   660→        except Exception as e:\n   661→            logger.error(f""Error in search for {query}: {e}"", exc_info=True)\n   662→    \n   663→    def index_file(self, path: Path) -> None:\n   664→        """"""Index a single file if it has changed.""""""\n   665→        try:\n   666→            # Ensure path is absolute to avoid relative/absolute path issues\n   667→            path = path.resolve()\n   668→            \n   669→            # Find the appropriate plugin\n   670→            plugin = self._match_plugin(path)\n   671→            \n   672→            # Read file content\n   673→            try:\n   674→                content = path.read_text(encoding=\'utf-8\')\n   675→            except UnicodeDecodeError:\n   676→                # Try with different encodings\n   677→                try:\n   678→                    content = path.read_text(encoding=\'latin-1\')\n   679→                except Exception as e:\n   680→                    logger.error(f""Failed to read {path}: {e}"")\n   681→                    return\n   682→            \n   683→            # Check if we need to re-index (simplified for now)\n   684→            # TODO: Implement proper caching logic\n   685→            \n   686→            # Index the file\n   687→            start_time = time.time()\n   688→            logger.info(f""Indexing {path} with {plugin.lang} plugin"")\n   689→            shard = plugin.indexFile(path, content)\n   690→            \n   691→            # Record performance if advanced features enabled\n   692→            if self._enable_advanced and self._router:\n   693→                execution_time = time.time() - start_time\n   694→                self._router.record_performance(plugin, execution_time)\n   695→            \n   696→            self._operation_stats[\'indexings\'] += 1\n   697→            self._operation_stats[\'total_time\'] += time.time() - start_time\n   698→            \n   699→            logger.info(f""Successfully indexed {path}: {len(shard.get(\'symbols\', []))} symbols found"")\n   700→            \n   701→        except RuntimeError as e:\n   702→            # No plugin found for this file type\n   703→            logger.debug(f""No plugin for {path}: {e}"")\n   704→        except Exception as e:\n   705→            logger.error(f""Error indexing {path}: {e}"", exc_info=True)\n   706→    \n   707→    def get_statistics(self) -> dict:\n   708→        """"""Get comprehensive statistics across all plugins and components.""""""\n   709→        stats = {\n   710→            ""total_plugins"": len(self._plugins),\n   711→            ""loaded_languages"": sorted(list(self._loaded_languages)),\n   712→            ""supported_languages"": len(self.supported_languages),\n   713→            ""operations"": self._operation_stats.copy()\n   714→        }\n   715→        \n   716→        # Add language breakdown\n   717→        stats[""by_language""] = {}\n   718→        for lang, plugin in self._by_lang.items():\n   719→            plugin_info = {\n   720→                ""loaded"": True,\n   721→                ""class"": plugin.__class__.__name__\n   722→            }\n   723→            if hasattr(plugin, \'get_indexed_count\'):\n   724→                plugin_info[""indexed_files""] = plugin.get_indexed_count()\n   725→            stats[""by_language""][lang] = plugin_info\n   726→        \n   727→        return stats\n   728→    \n   729→    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730→        """"""\n   731→        Index all files in a directory, respecting ignore patterns.\n   732→        \n   733→        Args:\n   734→            directory: Directory to index\n   735→            recursive: Whether to index subdirectories\n   736→            \n   737→        Returns:\n   738→            Statistics about indexed files\n   739→        """"""\n   740→        logger.info(f""Indexing directory: {directory} (recursive={recursive})"")\n   741→        \n   742→        # Note: We don\'t use ignore patterns during indexing\n   743→        # ALL files are indexed for local search capability\n   744→        # Filtering happens only during export/sharing\n   745→        \n   746→        # Get all supported extensions\n   747→        supported_extensions = get_all_extensions()\n   748→        \n   749→        stats = {\n   750→            ""total_files"": 0,\n   751→            ""indexed_files"": 0,\n   752→            ""ignored_files"": 0,\n   753→            ""failed_files"": 0,\n   754→            ""by_language"": {}\n   755→        }\n   756→        \n   757→        # Walk directory\n   758→        if recursive:\n   759→            file_iterator = directory.rglob(""*"")\n   760→        else:\n   761→            file_iterator = directory.glob(""*"")\n   762→            \n   763→        for path in file_iterator:\n   764→            if not path.is_file():\n   765→                continue\n   766→                \n   767→            stats[""total_files""] += 1\n   768→            \n   769→            # NOTE: We index ALL files locally, including gitignored ones\n   770→            # Filtering happens only during export/sharing\n   771→            # This allows local search of .env, secrets, etc.\n   772→                \n   773→            # Try to find a plugin that supports this file\n   774→            # This allows us to index ALL files, including .env, .key, etc.\n   775→            try:\n   776→                # First try to match by extension\n   777→                if path.suffix in supported_extensions:\n   778→                    self.index_file(path)\n   779→                    stats[""indexed_files""] += 1\n   780→                # For files without recognized extensions, try each plugin\'s supports() method\n   781→                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782→                else:\n   783→                    matched = False\n   784→                    for plugin in self._plugins:\n   785→                        if plugin.supports(path):\n   786→                            self.index_file(path)\n   787→                            stats[""indexed_files""] += 1\n   788→                            matched = True\n   789→                            break\n   790→                    \n   791→                    # If no plugin matched but we want to index everything,\n   792→                    # we could add a fallback here to index as plaintext\n   793→                    # For now, we\'ll skip unmatched files\n   794→                    if not matched:\n   795→                        logger.debug(f""No plugin found for {path}"")\n   796→                \n   797→                # Track by language\n   798→                language = get_language_by_extension(path.suffix)\n   799→                if language:\n   800→                    stats[""by_language""][language] = stats[""by_language""].get(language, 0) + 1\n   801→                    \n   802→            except Exception as e:\n   803→                logger.error(f""Failed to index {path}: {e}"")\n   804→                stats[""failed_files""] += 1\n   805→                \n   806→        logger.info(f""Directory indexing complete: {stats[\'indexed_files\']} indexed, ""\n   807→                   f""{stats[\'ignored_files\']} ignored, {stats[\'failed_files\']} failed"")\n   808→        \n   809→        return stats\n   810→    \n   811→    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812→        """"""Search specifically across documentation files.\n   813→        \n   814→        Args:\n   815→            topic: Topic to search for (e.g., ""installation"", ""configuration"")\n   816→            doc_types: Optional list of document types to search (e.g., [""readme"", ""guide"", ""api""])\n   817→            limit: Maximum number of results\n   818→            \n   819→        Returns:\n   820→            Search results from documentation files\n   821→        """"""\n   822→        # Default document types if not specified\n   823→        if doc_types is None:\n   824→            doc_types = [""readme"", ""documentation"", ""guide"", ""tutorial"", ""api"", ""changelog"", ""contributing""]\n   825→        \n   826→        # Build search queries for different document types\n   827→        queries = []\n   828→        for doc_type in doc_types:\n   829→            queries.extend([\n   830→                f""{doc_type} {topic}"",\n   831→                f""{topic} {doc_type}"",\n   832→                f""{topic} in {doc_type}""\n   833→            ])\n   834→        \n   835→        # Also search for the topic in common doc filenames\n   836→        queries.extend([\n   837→            f""README {topic}"",\n   838→            f""CONTRIBUTING {topic}"",\n   839→            f""docs {topic}"",\n   840→            f""documentation {topic}""\n   841→        ])\n   842→        \n   843→        # Deduplicate queries\n   844→        queries = list(dict.fromkeys(queries))\n   845→        \n   846→        logger.info(f""Cross-document search for \'{topic}\' with {len(queries)} query variations"")\n   847→        \n   848→        # Use the enhanced search with document-specific handling\n   849→        all_results = []\n   850→        seen = set()\n   851→        \n   852→        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853→            for result in self.search(query, semantic=True, limit=limit):\n   854→                # Only include documentation files\n   855→                if self._is_documentation_file(result.get(\'file\', \'\')):\n   856→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   857→                    if key not in seen:\n   858→                        seen.add(key)\n   859→                        all_results.append(result)\n   860→        \n   861→        # Sort by relevance (score) and return top results\n   862→        all_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   863→        \n   864→        count = 0\n   865→        for result in all_results:\n   866→            if count >= limit:\n   867→                break\n   868→            yield result\n   869→            count += 1\n   870→    \n   871→    def health_check(self) -> Dict[str, Any]:\n   872→        """"""Perform a health check on all components.""""""\n   873→        health = {\n   874→            \'status\': \'healthy\',\n   875→            \'components\': {\n   876→                \'dispatcher\': {\n   877→                    \'status\': \'healthy\',\n   878→                    \'plugins_loaded\': len(self._plugins),\n   879→                    \'languages_supported\': len(self.supported_languages),\n   880→                    \'factory_enabled\': self._use_factory,\n   881→                    \'lazy_loading\': self._lazy_load\n   882→                }\n   883→            },\n   884→            \'plugins\': {},\n   885→            \'errors\': []\n   886→        }\n   887→        \n   888→        # Check plugin health\n   889→        for lang, plugin in self._by_lang.items():\n   890→            try:\n   891→                plugin_health = {\n   892→                    \'status\': \'healthy\',\n   893→                    \'class\': plugin.__class__.__name__,\n   894→                    \'semantic_enabled\': getattr(plugin, \'_enable_semantic\', False)\n   895→                }\n   896→                if hasattr(plugin, \'get_indexed_count\'):\n   897→                    plugin_health[\'indexed_files\'] = plugin.get_indexed_count()\n   898→            except Exception as e:\n   899→                plugin_health = {\n   900→                    \'status\': \'error\',\n   901→                    \'error\': str(e)\n   902→                }\n   903→                health[\'errors\'].append(f""Plugin {lang}: {str(e)}"")\n   904→            \n   905→            health[\'plugins\'][lang] = plugin_health\n   906→        \n   907→        # Determine overall health\n   908→        if len(health[\'errors\']) > 0:\n   909→            health[\'status\'] = \'degraded\' if len(health[\'errors\']) < 3 else \'unhealthy\'\n   910→        \n   911→        return health\n   912→    \n   913→    def remove_file(self, path: Union[Path, str]) -> None:\n   914→        """"""Remove a file from all indexes.\n   915→        \n   916→        Args:\n   917→            path: File path to remove\n   918→        """"""\n   919→        path = Path(path).resolve()\n   920→        logger.info(f""Removing file from index: {path}"")\n   921→        \n   922→        try:\n   923→            # Remove from SQLite if available\n   924→            if self._sqlite_store:\n   925→                from ..core.path_resolver import PathResolver\n   926→                path_resolver = PathResolver()\n   927→                try:\n   928→                    relative_path = path_resolver.normalize_path(path)\n   929→                    # Get repository ID - for now assume 1\n   930→                    # TODO: Properly detect repository\n   931→                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932→                except Exception as e:\n   933→                    logger.error(f""Error removing from SQLite: {e}"")\n   934→            \n   935→            # Remove from semantic index if available\n   936→            try:\n   937→                plugin = self._match_plugin(path)\n   938→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   939→                    plugin._indexer.remove_file(path)\n   940→                    logger.info(f""Removed from semantic index: {path}"")\n   941→            except Exception as e:\n   942→                logger.warning(f""Error removing from semantic index: {e}"")\n   943→            \n   944→            # Update statistics\n   945→            self._operation_stats[\'deletions\'] = self._operation_stats.get(\'deletions\', 0) + 1\n   946→            \n   947→        except Exception as e:\n   948→            logger.error(f""Error removing file {path}: {e}"", exc_info=True)\n   949→    \n   950→    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951→        """"""Move a file in all indexes.\n   952→        \n   953→        Args:\n   954→            old_path: Original file path\n   955→            new_path: New file path\n   956→            content_hash: Optional content hash to verify unchanged content\n   957→        """"""\n   958→        old_path = Path(old_path).resolve()\n   959→        new_path = Path(new_path).resolve()\n   960→        logger.info(f""Moving file in index: {old_path} -> {new_path}"")\n   961→        \n   962→        try:\n   963→            # Move in SQLite if available\n   964→            if self._sqlite_store:\n   965→                from ..core.path_resolver import PathResolver\n   966→                path_resolver = PathResolver()\n   967→                try:\n   968→                    old_relative = path_resolver.normalize_path(old_path)\n   969→                    new_relative = path_resolver.normalize_path(new_path)\n   970→                    # Get repository ID - for now assume 1\n   971→                    # TODO: Properly detect repository\n   972→                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973→                except Exception as e:\n   974→                    logger.error(f""Error moving in SQLite: {e}"")\n   975→            \n   976→            # Move in semantic index if available\n   977→            try:\n   978→                plugin = self._match_plugin(new_path)\n   979→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   980→                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981→                    logger.info(f""Moved in semantic index: {old_path} -> {new_path}"")\n   982→            except Exception as e:\n   983→                logger.warning(f""Error moving in semantic index: {e}"")\n   984→            \n   985→            # Update statistics\n   986→            self._operation_stats[\'moves\'] = self._operation_stats.get(\'moves\', 0) + 1\n   987→            \n   988→        except Exception as e:\n   989→            logger.error(f""Error moving file {old_path} -> {new_path}: {e}"", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,1,1,14364,13324,1,0.5,0.25,0,,,,,transcript_response
78758208-1282-42af-ac35-ffe5b8f3d015,2025-06-26T07:36:25.905Z,transcript_78758208-1282-42af-ac35-ffe5b8f3d015,Find the EnhancedDispatcher class and show its main methods,transcript_query,unknown,mcp,0,True,4,0,0,0,4,0,0,0,3,3,13909,0,3,0.5,0.25,0,,,,,transcript_response
78758208-1282-42af-ac35-ffe5b8f3d015,2025-06-26T07:36:28.940Z,transcript_78758208-1282-42af-ac35-ffe5b8f3d015,"{'tool_use_id': 'toolu_01D1TMX7fxfiKurBHf3kehbw', 'type': 'tool_result', 'content': 'Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/path_management_architecture.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/path_management.puml'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,27,27,14364,0,27,0.5,0.25,0,,,,,transcript_response
78758208-1282-42af-ac35-ffe5b8f3d015,2025-06-26T07:36:32.890Z,transcript_78758208-1282-42af-ac35-ffe5b8f3d015,"{'tool_use_id': 'toolu_01LU7y2MWZcf6K9pxbxitKFQ', 'type': 'tool_result', 'content': '     1→""""""Enhanced dispatcher with dynamic plugin loading via PluginFactory.""""""\n     2→from pathlib import Path\n     3→from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4→import logging\n     5→import hashlib\n     6→import time\n     7→import re\n     8→from datetime import datetime\n     9→import os\n    10→\n    11→from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12→from ..plugins.plugin_factory import PluginFactory\n    13→from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14→from ..storage.sqlite_store import SQLiteStore\n    15→from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16→from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17→# Note: We\'ve removed ignore pattern checks to allow indexing ALL files\n    18→# Filtering happens only during export via SecureIndexExporter\n    19→# from ..core.ignore_patterns import get_ignore_manager\n    20→\n    21→logger = logging.getLogger(__name__)\n    22→\n    23→\n    24→class EnhancedDispatcher:\n    25→    """"""Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.""""""\n    26→    \n    27→    # Document query patterns - common documentation search terms\n    28→    DOCUMENT_QUERY_PATTERNS = [\n    29→        r\'\\b(how\\s+to|howto)\\b\',\n    30→        r\'\\b(getting\\s+started|get\\s+started)\\b\',\n    31→        r\'\\b(installation|install|setup)\\b\',\n    32→        r\'\\b(configuration|configure|config)\\b\',\n    33→        r\'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b\',\n    34→        r\'\\b(tutorial|guide|walkthrough)\\b\',\n    35→        r\'\\b(example|sample|snippet)\\b\',\n    36→        r\'\\b(readme|documentation|docs)\\b\',\n    37→        r\'\\b(usage|use\\s+case|using)\\b\',\n    38→        r\'\\b(reference|manual)\\b\',\n    39→        r\'\\b(faq|frequently\\s+asked)\\b\',\n    40→        r\'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b\',\n    41→        r\'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b\',\n    42→        r\'\\b(architecture|design|overview)\\b\',\n    43→        r\'\\b(changelog|release\\s+notes|migration)\\b\'\n    44→    ]\n    45→    \n    46→    # Documentation file patterns\n    47→    DOCUMENTATION_FILE_PATTERNS = [\n    48→        r\'readme(\\.\\w+)?$\',\n    49→        r\'changelog(\\.\\w+)?$\',\n    50→        r\'contributing(\\.\\w+)?$\',\n    51→        r\'license(\\.\\w+)?$\',\n    52→        r\'install(\\.\\w+)?$\',\n    53→        r\'setup(\\.\\w+)?$\',\n    54→        r\'guide(\\.\\w+)?$\',\n    55→        r\'tutorial(\\.\\w+)?$\',\n    56→        r\'\\.md$\',\n    57→        r\'\\.rst$\',\n    58→        r\'\\.txt$\',\n    59→        r\'docs?/\',\n    60→        r\'documentation/\'\n    61→    ]\n    62→    \n    63→    def __init__(self, \n    64→                 plugins: Optional[List[IPlugin]] = None,\n    65→                 sqlite_store: Optional[SQLiteStore] = None,\n    66→                 enable_advanced_features: bool = True,\n    67→                 use_plugin_factory: bool = True,\n    68→                 lazy_load: bool = True,\n    69→                 semantic_search_enabled: bool = True):\n    70→        """"""Initialize the enhanced dispatcher.\n    71→        \n    72→        Args:\n    73→            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74→            sqlite_store: SQLite store for plugin persistence\n    75→            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76→            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77→            lazy_load: Whether to lazy-load plugins on demand\n    78→            semantic_search_enabled: Whether to enable semantic search in plugins\n    79→        """"""\n    80→        self._sqlite_store = sqlite_store\n    81→        self._enable_advanced = enable_advanced_features\n    82→        self._use_factory = use_plugin_factory\n    83→        self._lazy_load = lazy_load\n    84→        self._semantic_enabled = semantic_search_enabled\n    85→        \n    86→        # Plugin storage\n    87→        self._plugins: List[IPlugin] = []\n    88→        self._by_lang: Dict[str, IPlugin] = {}\n    89→        self._loaded_languages: set[str] = set()\n    90→        \n    91→        # Cache for file hashes to avoid re-indexing unchanged files\n    92→        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93→        \n    94→        # Advanced components\n    95→        if self._enable_advanced:\n    96→            self._file_matcher = FileTypeMatcher()\n    97→            self._router = PluginRouter(self._file_matcher)\n    98→            self._aggregator = ResultAggregator()\n    99→        \n   100→        # Performance tracking\n   101→        self._operation_stats = {\n   102→            \'searches\': 0,\n   103→            \'lookups\': 0,\n   104→            \'indexings\': 0,\n   105→            \'total_time\': 0.0,\n   106→            \'plugins_loaded\': 0\n   107→        }\n   108→        \n   109→        # Initialize plugins\n   110→        if plugins:\n   111→            # Use provided plugins (backward compatibility)\n   112→            self._plugins = plugins\n   113→            self._by_lang = {p.lang: p for p in plugins}\n   114→            for plugin in plugins:\n   115→                self._loaded_languages.add(getattr(plugin, \'lang\', \'unknown\'))\n   116→            if self._enable_advanced:\n   117→                self._register_plugins_with_router()\n   118→        elif use_plugin_factory and not lazy_load:\n   119→            # Load all plugins immediately\n   120→            self._load_all_plugins()\n   121→        # If lazy_load is True, plugins will be loaded on demand\n   122→        \n   123→        # Compile document query patterns for performance\n   124→        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125→        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126→        \n   127→        logger.info(f""Enhanced dispatcher initialized with {len(self._plugins)} plugins"")\n   128→    \n   129→    def _load_all_plugins(self):\n   130→        """"""Load all available plugins using PluginFactory.""""""\n   131→        logger.info(""Loading all available plugins..."")\n   132→        \n   133→        all_plugins = PluginFactory.create_all_plugins(\n   134→            sqlite_store=self._sqlite_store,\n   135→            enable_semantic=self._semantic_enabled\n   136→        )\n   137→        \n   138→        for lang, plugin in all_plugins.items():\n   139→            self._plugins.append(plugin)\n   140→            self._by_lang[lang] = plugin\n   141→            self._loaded_languages.add(lang)\n   142→            self._operation_stats[\'plugins_loaded\'] += 1\n   143→        \n   144→        if self._enable_advanced:\n   145→            self._register_plugins_with_router()\n   146→        \n   147→        logger.info(f""Loaded {len(all_plugins)} plugins: {\', \'.join(sorted(all_plugins.keys()))}"")\n   148→    \n   149→    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150→        """"""Ensure a plugin for the given language is loaded.\n   151→        \n   152→        Args:\n   153→            language: Language code (e.g., \'python\', \'go\')\n   154→            \n   155→        Returns:\n   156→            Plugin instance or None if not available\n   157→        """"""\n   158→        # Normalize language\n   159→        language = language.lower().replace(\'-\', \'_\')\n   160→        \n   161→        # Check if already loaded\n   162→        if language in self._by_lang:\n   163→            return self._by_lang[language]\n   164→        \n   165→        # If not using factory or already tried to load, return None\n   166→        if not self._use_factory or language in self._loaded_languages:\n   167→            return None\n   168→        \n   169→        # Try to load the plugin\n   170→        try:\n   171→            logger.info(f""Lazy loading plugin for {language}"")\n   172→            plugin = PluginFactory.create_plugin(\n   173→                language,\n   174→                sqlite_store=self._sqlite_store,\n   175→                enable_semantic=self._semantic_enabled\n   176→            )\n   177→            \n   178→            # Add to collections\n   179→            self._plugins.append(plugin)\n   180→            self._by_lang[language] = plugin\n   181→            self._loaded_languages.add(language)\n   182→            self._operation_stats[\'plugins_loaded\'] += 1\n   183→            \n   184→            # Register with router if needed\n   185→            if self._enable_advanced:\n   186→                capabilities = self._detect_plugin_capabilities(plugin)\n   187→                self._router.register_plugin(plugin, capabilities)\n   188→            \n   189→            logger.info(f""Successfully loaded {language} plugin"")\n   190→            return plugin\n   191→            \n   192→        except ValueError as e:\n   193→            logger.warning(f""No plugin available for {language}: {e}"")\n   194→            self._loaded_languages.add(language)  # Mark as attempted\n   195→            return None\n   196→        except Exception as e:\n   197→            logger.error(f""Error loading plugin for {language}: {e}"")\n   198→            self._loaded_languages.add(language)  # Mark as attempted\n   199→            return None\n   200→    \n   201→    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202→        """"""Ensure a plugin is loaded for the given file.\n   203→        \n   204→        Args:\n   205→            path: File path\n   206→            \n   207→        Returns:\n   208→            Plugin instance or None if not available\n   209→        """"""\n   210→        # Get language from file extension\n   211→        extension = path.suffix.lower()\n   212→        language = get_language_by_extension(extension)\n   213→        \n   214→        if language:\n   215→            return self._ensure_plugin_loaded(language)\n   216→        \n   217→        # Fallback: try all loaded plugins\n   218→        for plugin in self._plugins:\n   219→            if plugin.supports(path):\n   220→                return plugin\n   221→        \n   222→        return None\n   223→    \n   224→    def _register_plugins_with_router(self):\n   225→        """"""Register plugins with the router and assign capabilities.""""""\n   226→        for plugin in self._plugins:\n   227→            # Determine capabilities based on plugin type/language\n   228→            capabilities = self._detect_plugin_capabilities(plugin)\n   229→            self._router.register_plugin(plugin, capabilities)\n   230→    \n   231→    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232→        """"""Detect capabilities for a plugin based on its language and features.""""""\n   233→        capabilities = []\n   234→        lang = getattr(plugin, \'lang\', \'unknown\')\n   235→        \n   236→        # Base capabilities all plugins have\n   237→        capabilities.append(PluginCapability(\n   238→            \'syntax_analysis\', \'1.0\', f\'{lang} syntax analysis\', \n   239→            priority=70, metadata={\'language\': lang}\n   240→        ))\n   241→        \n   242→        capabilities.append(PluginCapability(\n   243→            \'code_search\', \'1.0\', f\'{lang} code search\', \n   244→            priority=80, metadata={\'language\': lang}\n   245→        ))\n   246→        \n   247→        # Check for semantic search capability\n   248→        if hasattr(plugin, \'_enable_semantic\') and plugin._enable_semantic:\n   249→            capabilities.append(PluginCapability(\n   250→                \'semantic_search\', \'1.0\', f\'{lang} semantic search\', \n   251→                priority=90, metadata={\'language\': lang}\n   252→            ))\n   253→        \n   254→        # Language-specific capabilities\n   255→        if lang == \'python\':\n   256→            capabilities.extend([\n   257→                PluginCapability(\'refactoring\', \'1.0\', \'Python refactoring support\', 75),\n   258→                PluginCapability(\'type_analysis\', \'1.0\', \'Python type analysis\', 85)\n   259→            ])\n   260→        elif lang in [\'javascript\', \'typescript\']:\n   261→            capabilities.extend([\n   262→                PluginCapability(\'linting\', \'1.0\', \'JavaScript/TypeScript linting\', 85),\n   263→                PluginCapability(\'bundling_analysis\', \'1.0\', \'Module bundling analysis\', 70),\n   264→                PluginCapability(\'framework_support\', \'1.0\', \'Framework-specific support\', 75)\n   265→            ])\n   266→        elif lang in [\'c\', \'cpp\']:\n   267→            capabilities.extend([\n   268→                PluginCapability(\'compilation_analysis\', \'1.0\', \'Compilation analysis\', 80),\n   269→                PluginCapability(\'memory_analysis\', \'1.0\', \'Memory usage analysis\', 70),\n   270→                PluginCapability(\'performance_profiling\', \'1.0\', \'Performance profiling\', 75)\n   271→            ])\n   272→        elif lang in [\'go\', \'rust\']:\n   273→            capabilities.extend([\n   274→                PluginCapability(\'package_analysis\', \'1.0\', f\'{lang} package analysis\', 80),\n   275→                PluginCapability(\'concurrency_analysis\', \'1.0\', f\'{lang} concurrency analysis\', 75)\n   276→            ])\n   277→        elif lang in [\'java\', \'kotlin\', \'scala\']:\n   278→            capabilities.extend([\n   279→                PluginCapability(\'jvm_analysis\', \'1.0\', \'JVM bytecode analysis\', 75),\n   280→                PluginCapability(\'build_tool_integration\', \'1.0\', \'Build tool integration\', 70)\n   281→            ])\n   282→        \n   283→        return capabilities\n   284→    \n   285→    @property\n   286→    def plugins(self):\n   287→        """"""Get the dictionary of loaded plugins by language.""""""\n   288→        return self._by_lang\n   289→    \n   290→    @property\n   291→    def supported_languages(self) -> List[str]:\n   292→        """"""Get list of all supported languages (loaded and available).""""""\n   293→        if self._use_factory:\n   294→            return PluginFactory.get_supported_languages()\n   295→        else:\n   296→            return list(self._by_lang.keys())\n   297→    \n   298→    def _match_plugin(self, path: Path) -> IPlugin:\n   299→        """"""Match a plugin for the given file path.""""""\n   300→        # Ensure plugin is loaded if using lazy loading\n   301→        if self._lazy_load and self._use_factory:\n   302→            plugin = self._ensure_plugin_for_file(path)\n   303→            if plugin:\n   304→                return plugin\n   305→        \n   306→        # Use advanced routing if available\n   307→        if self._enable_advanced and self._router:\n   308→            route_result = self._router.get_best_plugin(path)\n   309→            if route_result:\n   310→                return route_result.plugin\n   311→        \n   312→        # Fallback to basic matching\n   313→        for p in self._plugins:\n   314→            if p.supports(path):\n   315→                return p\n   316→        \n   317→        raise RuntimeError(f""No plugin found for {path}"")\n   318→    \n   319→    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320→        """"""Get all plugins that can handle a file with confidence scores.""""""\n   321→        # Ensure plugin is loaded if using lazy loading\n   322→        if self._lazy_load and self._use_factory:\n   323→            self._ensure_plugin_for_file(path)\n   324→        \n   325→        if self._enable_advanced and self._router:\n   326→            route_results = self._router.route_file(path)\n   327→            return [(result.plugin, result.confidence) for result in route_results]\n   328→        else:\n   329→            # Basic fallback\n   330→            matching_plugins = []\n   331→            for plugin in self._plugins:\n   332→                if plugin.supports(path):\n   333→                    matching_plugins.append((plugin, 1.0))\n   334→            return matching_plugins\n   335→    \n   336→    def lookup(self, symbol: str) -> SymbolDef | None:\n   337→        """"""Look up symbol definition across all plugins.""""""\n   338→        start_time = time.time()\n   339→        \n   340→        try:\n   341→            # For symbol lookup, we may need to search across all languages\n   342→            # Load all plugins if using lazy loading\n   343→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344→                self._load_all_plugins()\n   345→            \n   346→            if self._enable_advanced and self._aggregator:\n   347→                # Use advanced aggregation\n   348→                definitions_by_plugin = {}\n   349→                for plugin in self._plugins:\n   350→                    try:\n   351→                        definition = plugin.getDefinition(symbol)\n   352→                        definitions_by_plugin[plugin] = definition\n   353→                    except Exception as e:\n   354→                        logger.warning(f""Plugin {plugin.lang} failed to get definition for {symbol}: {e}"")\n   355→                        definitions_by_plugin[plugin] = None\n   356→                \n   357→                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358→                \n   359→                self._operation_stats[\'lookups\'] += 1\n   360→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   361→                \n   362→                return result\n   363→            else:\n   364→                # Fallback to basic lookup\n   365→                for p in self._plugins:\n   366→                    res = p.getDefinition(symbol)\n   367→                    if res:\n   368→                        self._operation_stats[\'lookups\'] += 1\n   369→                        self._operation_stats[\'total_time\'] += time.time() - start_time\n   370→                        return res\n   371→                return None\n   372→                \n   373→        except Exception as e:\n   374→            logger.error(f""Error in symbol lookup for {symbol}: {e}"", exc_info=True)\n   375→            return None\n   376→    \n   377→    def _is_document_query(self, query: str) -> bool:\n   378→        """"""Check if the query is looking for documentation.\n   379→        \n   380→        Args:\n   381→            query: Search query string\n   382→            \n   383→        Returns:\n   384→            True if this appears to be a documentation query\n   385→        """"""\n   386→        query_lower = query.lower()\n   387→        \n   388→        # Check against document query patterns\n   389→        for pattern in self._compiled_doc_patterns:\n   390→            if pattern.search(query_lower):\n   391→                return True\n   392→        \n   393→        # Check for question words at the beginning\n   394→        question_starters = [\'how\', \'what\', \'where\', \'when\', \'why\', \'can\', \'is\', \'does\', \'should\']\n   395→        first_word = query_lower.split()[0] if query_lower.split() else \'\'\n   396→        if first_word in question_starters:\n   397→            return True\n   398→        \n   399→        return False\n   400→    \n   401→    def _expand_document_query(self, query: str) -> List[str]:\n   402→        """"""Expand a document query with related terms for better search coverage.\n   403→        \n   404→        Args:\n   405→            query: Original search query\n   406→            \n   407→        Returns:\n   408→            List of expanded query variations\n   409→        """"""\n   410→        expanded_queries = [query]  # Always include original\n   411→        query_lower = query.lower()\n   412→        \n   413→        # Common expansions for documentation queries\n   414→        expansions = {\n   415→            \'install\': [\'installation\', \'setup\', \'getting started\', \'requirements\'],\n   416→            \'config\': [\'configuration\', \'configure\', \'settings\', \'options\', \'parameters\'],\n   417→            \'api\': [\'api documentation\', \'api reference\', \'endpoint\', \'method\'],\n   418→            \'how to\': [\'tutorial\', \'guide\', \'example\', \'usage\'],\n   419→            \'example\': [\'sample\', \'snippet\', \'demo\', \'code example\'],\n   420→            \'error\': [\'troubleshoot\', \'debug\', \'issue\', \'problem\', \'fix\'],\n   421→            \'getting started\': [\'quickstart\', \'tutorial\', \'introduction\', \'setup\'],\n   422→            \'guide\': [\'tutorial\', \'documentation\', \'walkthrough\', \'how to\'],\n   423→            \'usage\': [\'how to use\', \'example\', \'api\', \'reference\']\n   424→        }\n   425→        \n   426→        # Apply expansions\n   427→        for term, related_terms in expansions.items():\n   428→            if term in query_lower:\n   429→                for related in related_terms:\n   430→                    # Replace the term with related term\n   431→                    expanded = query_lower.replace(term, related)\n   432→                    if expanded != query_lower and expanded not in expanded_queries:\n   433→                        expanded_queries.append(expanded)\n   434→                \n   435→                # Also add queries with additional terms\n   436→                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437→                    expanded = f""{query} {related}""\n   438→                    if expanded not in expanded_queries:\n   439→                        expanded_queries.append(expanded)\n   440→        \n   441→        # Add file-specific searches for common documentation files\n   442→        if self._is_document_query(query):\n   443→            # Extract the main topic from the query\n   444→            topic_words = []\n   445→            for word in query.lower().split():\n   446→                if word not in [\'how\', \'to\', \'the\', \'a\', \'an\', \'is\', \'are\', \'what\', \'where\', \'when\']:\n   447→                    topic_words.append(word)\n   448→            \n   449→            if topic_words:\n   450→                topic = \' \'.join(topic_words[:2])  # Use first two topic words\n   451→                expanded_queries.extend([\n   452→                    f""README {topic}"",\n   453→                    f""{topic} documentation"",\n   454→                    f""{topic} docs"",\n   455→                    f""{topic} guide""\n   456→                ])\n   457→        \n   458→        # Remove duplicates while preserving order\n   459→        seen = set()\n   460→        unique_queries = []\n   461→        for q in expanded_queries:\n   462→            if q not in seen:\n   463→                seen.add(q)\n   464→                unique_queries.append(q)\n   465→        \n   466→        return unique_queries[:10]  # Limit to 10 queries max\n   467→    \n   468→    def _is_documentation_file(self, file_path: str) -> bool:\n   469→        """"""Check if a file path is likely a documentation file.\n   470→        \n   471→        Args:\n   472→            file_path: Path to check\n   473→            \n   474→        Returns:\n   475→            True if this appears to be a documentation file\n   476→        """"""\n   477→        path_lower = file_path.lower()\n   478→        \n   479→        for pattern in self._compiled_file_patterns:\n   480→            if pattern.search(path_lower):\n   481→                return True\n   482→        \n   483→        return False\n   484→    \n   485→    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486→        """"""Adjust ranking to prioritize documentation files for document queries.\n   487→        \n   488→        Args:\n   489→            query: Original search query\n   490→            results: List of aggregated results\n   491→            \n   492→        Returns:\n   493→            Re-ranked results with documentation prioritized\n   494→        """"""\n   495→        if not self._is_document_query(query):\n   496→            return results\n   497→        \n   498→        # Separate documentation and code results\n   499→        doc_results = []\n   500→        code_results = []\n   501→        \n   502→        for result in results:\n   503→            if self._is_documentation_file(result.primary_result.get(\'file\', \'\')):\n   504→                # Boost documentation files for document queries\n   505→                result.rank_score *= 1.5\n   506→                result.metadata[\'doc_boost\'] = True\n   507→                doc_results.append(result)\n   508→            else:\n   509→                code_results.append(result)\n   510→        \n   511→        # Sort each group by rank score\n   512→        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513→        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514→        \n   515→        # Combine with documentation files first\n   516→        return doc_results + code_results\n   517→    \n   518→    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519→        """"""Search for code and documentation across all plugins.""""""\n   520→        start_time = time.time()\n   521→        \n   522→        try:\n   523→            # For search, we may need to search across all languages\n   524→            # Load all plugins if using lazy loading\n   525→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526→                self._load_all_plugins()\n   527→            \n   528→            # Detect if this is a document query\n   529→            is_doc_query = self._is_document_query(query)\n   530→            \n   531→            # Expand query if it\'s a document query\n   532→            queries = [query]\n   533→            if is_doc_query:\n   534→                queries = self._expand_document_query(query)\n   535→                logger.info(f""Expanded document query \'{query}\' to {len(queries)} variations"")\n   536→                # Force semantic search for natural language queries\n   537→                semantic = True\n   538→            \n   539→            if self._enable_advanced and self._aggregator:\n   540→                # Use advanced aggregation\n   541→                all_results_by_plugin = {}\n   542→                opts = {""semantic"": semantic, ""limit"": limit * 2 if is_doc_query else limit}\n   543→                \n   544→                # Search with all query variations\n   545→                for search_query in queries:\n   546→                    for plugin in self._plugins:\n   547→                        try:\n   548→                            results = list(plugin.search(search_query, opts))\n   549→                            if results:\n   550→                                if plugin not in all_results_by_plugin:\n   551→                                    all_results_by_plugin[plugin] = []\n   552→                                all_results_by_plugin[plugin].extend(results)\n   553→                        except Exception as e:\n   554→                            logger.warning(f""Plugin {plugin.lang} failed to search for {search_query}: {e}"")\n   555→                \n   556→                # Deduplicate results per plugin\n   557→                for plugin, results in all_results_by_plugin.items():\n   558→                    seen = set()\n   559→                    unique_results = []\n   560→                    for result in results:\n   561→                        key = f""{result[\'file\']}:{result[\'line\']}""\n   562→                        if key not in seen:\n   563→                            seen.add(key)\n   564→                            unique_results.append(result)\n   565→                    all_results_by_plugin[plugin] = unique_results\n   566→                \n   567→                # Configure aggregator for document queries\n   568→                if is_doc_query and self._enable_advanced:\n   569→                    # Adjust ranking criteria for documentation\n   570→                    doc_criteria = RankingCriteria(\n   571→                        relevance_weight=0.5,      # Increase relevance weight\n   572→                        confidence_weight=0.2,     # Reduce confidence weight\n   573→                        frequency_weight=0.2,      # Keep frequency weight\n   574→                        recency_weight=0.1,        # Keep recency weight\n   575→                        prefer_exact_matches=False,  # Natural language doesn\'t need exact matches\n   576→                        boost_multiple_sources=True,\n   577→                        boost_common_extensions=True\n   578→                    )\n   579→                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580→                \n   581→                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582→                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583→                )\n   584→                \n   585→                # Adjust ranking for document queries\n   586→                if is_doc_query:\n   587→                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588→                \n   589→                # Apply final limit\n   590→                if limit and len(aggregated_results) > limit:\n   591→                    aggregated_results = aggregated_results[:limit]\n   592→                \n   593→                logger.debug(f""Search aggregation stats: {stats.total_results} total, ""\n   594→                           f""{stats.unique_results} unique, {stats.plugins_used} plugins used, ""\n   595→                           f""document_query={is_doc_query}"")\n   596→                \n   597→                self._operation_stats[\'searches\'] += 1\n   598→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   599→                \n   600→                # Yield primary results from aggregated results\n   601→                for aggregated in aggregated_results:\n   602→                    yield aggregated.primary_result\n   603→            else:\n   604→                # Fallback to basic search\n   605→                # Detect if this is a document query\n   606→                is_doc_query = self._is_document_query(query)\n   607→                \n   608→                # Expand query if it\'s a document query\n   609→                queries = [query]\n   610→                if is_doc_query:\n   611→                    queries = self._expand_document_query(query)\n   612→                    semantic = True  # Force semantic search for natural language\n   613→                \n   614→                opts = {""semantic"": semantic, ""limit"": limit}\n   615→                all_results = []\n   616→                \n   617→                # Search with all query variations\n   618→                for search_query in queries:\n   619→                    for p in self._plugins:\n   620→                        try:\n   621→                            for result in p.search(search_query, opts):\n   622→                                all_results.append(result)\n   623→                        except Exception as e:\n   624→                            logger.warning(f""Plugin {p.lang} failed to search for {search_query}: {e}"")\n   625→                \n   626→                # Deduplicate results\n   627→                seen = set()\n   628→                unique_results = []\n   629→                for result in all_results:\n   630→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   631→                    if key not in seen:\n   632→                        seen.add(key)\n   633→                        unique_results.append(result)\n   634→                \n   635→                # Sort by score if available\n   636→                unique_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   637→                \n   638→                # Prioritize documentation files for document queries\n   639→                if is_doc_query:\n   640→                    doc_results = []\n   641→                    code_results = []\n   642→                    for result in unique_results:\n   643→                        if self._is_documentation_file(result.get(\'file\', \'\')):\n   644→                            doc_results.append(result)\n   645→                        else:\n   646→                            code_results.append(result)\n   647→                    unique_results = doc_results + code_results\n   648→                \n   649→                # Apply limit\n   650→                count = 0\n   651→                for result in unique_results:\n   652→                    if limit and count >= limit:\n   653→                        break\n   654→                    yield result\n   655→                    count += 1\n   656→                \n   657→                self._operation_stats[\'searches\'] += 1\n   658→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   659→                \n   660→        except Exception as e:\n   661→            logger.error(f""Error in search for {query}: {e}"", exc_info=True)\n   662→    \n   663→    def index_file(self, path: Path) -> None:\n   664→        """"""Index a single file if it has changed.""""""\n   665→        try:\n   666→            # Ensure path is absolute to avoid relative/absolute path issues\n   667→            path = path.resolve()\n   668→            \n   669→            # Find the appropriate plugin\n   670→            plugin = self._match_plugin(path)\n   671→            \n   672→            # Read file content\n   673→            try:\n   674→                content = path.read_text(encoding=\'utf-8\')\n   675→            except UnicodeDecodeError:\n   676→                # Try with different encodings\n   677→                try:\n   678→                    content = path.read_text(encoding=\'latin-1\')\n   679→                except Exception as e:\n   680→                    logger.error(f""Failed to read {path}: {e}"")\n   681→                    return\n   682→            \n   683→            # Check if we need to re-index (simplified for now)\n   684→            # TODO: Implement proper caching logic\n   685→            \n   686→            # Index the file\n   687→            start_time = time.time()\n   688→            logger.info(f""Indexing {path} with {plugin.lang} plugin"")\n   689→            shard = plugin.indexFile(path, content)\n   690→            \n   691→            # Record performance if advanced features enabled\n   692→            if self._enable_advanced and self._router:\n   693→                execution_time = time.time() - start_time\n   694→                self._router.record_performance(plugin, execution_time)\n   695→            \n   696→            self._operation_stats[\'indexings\'] += 1\n   697→            self._operation_stats[\'total_time\'] += time.time() - start_time\n   698→            \n   699→            logger.info(f""Successfully indexed {path}: {len(shard.get(\'symbols\', []))} symbols found"")\n   700→            \n   701→        except RuntimeError as e:\n   702→            # No plugin found for this file type\n   703→            logger.debug(f""No plugin for {path}: {e}"")\n   704→        except Exception as e:\n   705→            logger.error(f""Error indexing {path}: {e}"", exc_info=True)\n   706→    \n   707→    def get_statistics(self) -> dict:\n   708→        """"""Get comprehensive statistics across all plugins and components.""""""\n   709→        stats = {\n   710→            ""total_plugins"": len(self._plugins),\n   711→            ""loaded_languages"": sorted(list(self._loaded_languages)),\n   712→            ""supported_languages"": len(self.supported_languages),\n   713→            ""operations"": self._operation_stats.copy()\n   714→        }\n   715→        \n   716→        # Add language breakdown\n   717→        stats[""by_language""] = {}\n   718→        for lang, plugin in self._by_lang.items():\n   719→            plugin_info = {\n   720→                ""loaded"": True,\n   721→                ""class"": plugin.__class__.__name__\n   722→            }\n   723→            if hasattr(plugin, \'get_indexed_count\'):\n   724→                plugin_info[""indexed_files""] = plugin.get_indexed_count()\n   725→            stats[""by_language""][lang] = plugin_info\n   726→        \n   727→        return stats\n   728→    \n   729→    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730→        """"""\n   731→        Index all files in a directory, respecting ignore patterns.\n   732→        \n   733→        Args:\n   734→            directory: Directory to index\n   735→            recursive: Whether to index subdirectories\n   736→            \n   737→        Returns:\n   738→            Statistics about indexed files\n   739→        """"""\n   740→        logger.info(f""Indexing directory: {directory} (recursive={recursive})"")\n   741→        \n   742→        # Note: We don\'t use ignore patterns during indexing\n   743→        # ALL files are indexed for local search capability\n   744→        # Filtering happens only during export/sharing\n   745→        \n   746→        # Get all supported extensions\n   747→        supported_extensions = get_all_extensions()\n   748→        \n   749→        stats = {\n   750→            ""total_files"": 0,\n   751→            ""indexed_files"": 0,\n   752→            ""ignored_files"": 0,\n   753→            ""failed_files"": 0,\n   754→            ""by_language"": {}\n   755→        }\n   756→        \n   757→        # Walk directory\n   758→        if recursive:\n   759→            file_iterator = directory.rglob(""*"")\n   760→        else:\n   761→            file_iterator = directory.glob(""*"")\n   762→            \n   763→        for path in file_iterator:\n   764→            if not path.is_file():\n   765→                continue\n   766→                \n   767→            stats[""total_files""] += 1\n   768→            \n   769→            # NOTE: We index ALL files locally, including gitignored ones\n   770→            # Filtering happens only during export/sharing\n   771→            # This allows local search of .env, secrets, etc.\n   772→                \n   773→            # Try to find a plugin that supports this file\n   774→            # This allows us to index ALL files, including .env, .key, etc.\n   775→            try:\n   776→                # First try to match by extension\n   777→                if path.suffix in supported_extensions:\n   778→                    self.index_file(path)\n   779→                    stats[""indexed_files""] += 1\n   780→                # For files without recognized extensions, try each plugin\'s supports() method\n   781→                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782→                else:\n   783→                    matched = False\n   784→                    for plugin in self._plugins:\n   785→                        if plugin.supports(path):\n   786→                            self.index_file(path)\n   787→                            stats[""indexed_files""] += 1\n   788→                            matched = True\n   789→                            break\n   790→                    \n   791→                    # If no plugin matched but we want to index everything,\n   792→                    # we could add a fallback here to index as plaintext\n   793→                    # For now, we\'ll skip unmatched files\n   794→                    if not matched:\n   795→                        logger.debug(f""No plugin found for {path}"")\n   796→                \n   797→                # Track by language\n   798→                language = get_language_by_extension(path.suffix)\n   799→                if language:\n   800→                    stats[""by_language""][language] = stats[""by_language""].get(language, 0) + 1\n   801→                    \n   802→            except Exception as e:\n   803→                logger.error(f""Failed to index {path}: {e}"")\n   804→                stats[""failed_files""] += 1\n   805→                \n   806→        logger.info(f""Directory indexing complete: {stats[\'indexed_files\']} indexed, ""\n   807→                   f""{stats[\'ignored_files\']} ignored, {stats[\'failed_files\']} failed"")\n   808→        \n   809→        return stats\n   810→    \n   811→    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812→        """"""Search specifically across documentation files.\n   813→        \n   814→        Args:\n   815→            topic: Topic to search for (e.g., ""installation"", ""configuration"")\n   816→            doc_types: Optional list of document types to search (e.g., [""readme"", ""guide"", ""api""])\n   817→            limit: Maximum number of results\n   818→            \n   819→        Returns:\n   820→            Search results from documentation files\n   821→        """"""\n   822→        # Default document types if not specified\n   823→        if doc_types is None:\n   824→            doc_types = [""readme"", ""documentation"", ""guide"", ""tutorial"", ""api"", ""changelog"", ""contributing""]\n   825→        \n   826→        # Build search queries for different document types\n   827→        queries = []\n   828→        for doc_type in doc_types:\n   829→            queries.extend([\n   830→                f""{doc_type} {topic}"",\n   831→                f""{topic} {doc_type}"",\n   832→                f""{topic} in {doc_type}""\n   833→            ])\n   834→        \n   835→        # Also search for the topic in common doc filenames\n   836→        queries.extend([\n   837→            f""README {topic}"",\n   838→            f""CONTRIBUTING {topic}"",\n   839→            f""docs {topic}"",\n   840→            f""documentation {topic}""\n   841→        ])\n   842→        \n   843→        # Deduplicate queries\n   844→        queries = list(dict.fromkeys(queries))\n   845→        \n   846→        logger.info(f""Cross-document search for \'{topic}\' with {len(queries)} query variations"")\n   847→        \n   848→        # Use the enhanced search with document-specific handling\n   849→        all_results = []\n   850→        seen = set()\n   851→        \n   852→        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853→            for result in self.search(query, semantic=True, limit=limit):\n   854→                # Only include documentation files\n   855→                if self._is_documentation_file(result.get(\'file\', \'\')):\n   856→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   857→                    if key not in seen:\n   858→                        seen.add(key)\n   859→                        all_results.append(result)\n   860→        \n   861→        # Sort by relevance (score) and return top results\n   862→        all_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   863→        \n   864→        count = 0\n   865→        for result in all_results:\n   866→            if count >= limit:\n   867→                break\n   868→            yield result\n   869→            count += 1\n   870→    \n   871→    def health_check(self) -> Dict[str, Any]:\n   872→        """"""Perform a health check on all components.""""""\n   873→        health = {\n   874→            \'status\': \'healthy\',\n   875→            \'components\': {\n   876→                \'dispatcher\': {\n   877→                    \'status\': \'healthy\',\n   878→                    \'plugins_loaded\': len(self._plugins),\n   879→                    \'languages_supported\': len(self.supported_languages),\n   880→                    \'factory_enabled\': self._use_factory,\n   881→                    \'lazy_loading\': self._lazy_load\n   882→                }\n   883→            },\n   884→            \'plugins\': {},\n   885→            \'errors\': []\n   886→        }\n   887→        \n   888→        # Check plugin health\n   889→        for lang, plugin in self._by_lang.items():\n   890→            try:\n   891→                plugin_health = {\n   892→                    \'status\': \'healthy\',\n   893→                    \'class\': plugin.__class__.__name__,\n   894→                    \'semantic_enabled\': getattr(plugin, \'_enable_semantic\', False)\n   895→                }\n   896→                if hasattr(plugin, \'get_indexed_count\'):\n   897→                    plugin_health[\'indexed_files\'] = plugin.get_indexed_count()\n   898→            except Exception as e:\n   899→                plugin_health = {\n   900→                    \'status\': \'error\',\n   901→                    \'error\': str(e)\n   902→                }\n   903→                health[\'errors\'].append(f""Plugin {lang}: {str(e)}"")\n   904→            \n   905→            health[\'plugins\'][lang] = plugin_health\n   906→        \n   907→        # Determine overall health\n   908→        if len(health[\'errors\']) > 0:\n   909→            health[\'status\'] = \'degraded\' if len(health[\'errors\']) < 3 else \'unhealthy\'\n   910→        \n   911→        return health\n   912→    \n   913→    def remove_file(self, path: Union[Path, str]) -> None:\n   914→        """"""Remove a file from all indexes.\n   915→        \n   916→        Args:\n   917→            path: File path to remove\n   918→        """"""\n   919→        path = Path(path).resolve()\n   920→        logger.info(f""Removing file from index: {path}"")\n   921→        \n   922→        try:\n   923→            # Remove from SQLite if available\n   924→            if self._sqlite_store:\n   925→                from ..core.path_resolver import PathResolver\n   926→                path_resolver = PathResolver()\n   927→                try:\n   928→                    relative_path = path_resolver.normalize_path(path)\n   929→                    # Get repository ID - for now assume 1\n   930→                    # TODO: Properly detect repository\n   931→                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932→                except Exception as e:\n   933→                    logger.error(f""Error removing from SQLite: {e}"")\n   934→            \n   935→            # Remove from semantic index if available\n   936→            try:\n   937→                plugin = self._match_plugin(path)\n   938→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   939→                    plugin._indexer.remove_file(path)\n   940→                    logger.info(f""Removed from semantic index: {path}"")\n   941→            except Exception as e:\n   942→                logger.warning(f""Error removing from semantic index: {e}"")\n   943→            \n   944→            # Update statistics\n   945→            self._operation_stats[\'deletions\'] = self._operation_stats.get(\'deletions\', 0) + 1\n   946→            \n   947→        except Exception as e:\n   948→            logger.error(f""Error removing file {path}: {e}"", exc_info=True)\n   949→    \n   950→    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951→        """"""Move a file in all indexes.\n   952→        \n   953→        Args:\n   954→            old_path: Original file path\n   955→            new_path: New file path\n   956→            content_hash: Optional content hash to verify unchanged content\n   957→        """"""\n   958→        old_path = Path(old_path).resolve()\n   959→        new_path = Path(new_path).resolve()\n   960→        logger.info(f""Moving file in index: {old_path} -> {new_path}"")\n   961→        \n   962→        try:\n   963→            # Move in SQLite if available\n   964→            if self._sqlite_store:\n   965→                from ..core.path_resolver import PathResolver\n   966→                path_resolver = PathResolver()\n   967→                try:\n   968→                    old_relative = path_resolver.normalize_path(old_path)\n   969→                    new_relative = path_resolver.normalize_path(new_path)\n   970→                    # Get repository ID - for now assume 1\n   971→                    # TODO: Properly detect repository\n   972→                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973→                except Exception as e:\n   974→                    logger.error(f""Error moving in SQLite: {e}"")\n   975→            \n   976→            # Move in semantic index if available\n   977→            try:\n   978→                plugin = self._match_plugin(new_path)\n   979→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   980→                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981→                    logger.info(f""Moved in semantic index: {old_path} -> {new_path}"")\n   982→            except Exception as e:\n   983→                logger.warning(f""Error moving in semantic index: {e}"")\n   984→            \n   985→            # Update statistics\n   986→            self._operation_stats[\'moves\'] = self._operation_stats.get(\'moves\', 0) + 1\n   987→            \n   988→        except Exception as e:\n   989→            logger.error(f""Error moving file {old_path} -> {new_path}: {e}"", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,1,1,27688,0,1,0.5,0.25,0,,,,,transcript_response
c3c8369b-4664-47f4-b3f9-7436700938d0,2025-06-26T07:36:47.054Z,transcript_c3c8369b-4664-47f4-b3f9-7436700938d0,Find all functions that use asyncio,transcript_query,unknown,mcp,0,True,4,0,0,0,4,0,0,0,1,1,13902,0,1,0.5,0.25,0,,,,,transcript_response
c3c8369b-4664-47f4-b3f9-7436700938d0,2025-06-26T07:36:50.620Z,transcript_c3c8369b-4664-47f4-b3f9-7436700938d0,"{'tool_use_id': 'toolu_01AcEHuEH4mB9MzGjyLaSgkV', 'type': 'tool_result', 'content': 'Found 78 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/conftest.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_developer_workflows.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_repository_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_semantic_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_plugins_direct.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/populate_bm25_quick.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/pytest.ini\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/requirements-production.txt\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/requirements.txt\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_reranking_working.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/docs/development/TESTING-GUIDE.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/codex-setup.sh\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/prometheus_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/watchdog.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/file_watcher.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/grpc_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/MCP.md'}",transcript_query,unknown,mcp,0,True,7,0,0,0,7,0,0,0,33,33,13902,5379,33,0.5,0.25,0,,,,,transcript_response
f86b4d58-381b-4fe7-9d8b-38906cd581f9,2025-06-26T07:32:52.426Z,transcript_f86b4d58-381b-4fe7-9d8b-38906cd581f9,Find the EnhancedDispatcher class and show its main methods,transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,122,122,13920,0,122,0.5,0.25,0,,,,,transcript_response
f86b4d58-381b-4fe7-9d8b-38906cd581f9,2025-06-26T07:32:56.802Z,transcript_f86b4d58-381b-4fe7-9d8b-38906cd581f9,"Search for the EnhancedDispatcher class in the codebase. Find the file that contains this class definition and identify its main methods. Return the file path, line numbers, and a list of the main methods with their signatures.",transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,91,91,9986,783,91,0.5,0.25,0,,,,,transcript_response
f86b4d58-381b-4fe7-9d8b-38906cd581f9,2025-06-26T07:33:00.558Z,transcript_f86b4d58-381b-4fe7-9d8b-38906cd581f9,"{'tool_use_id': 'toolu_01YFRY2V6iZSnN5beWPsuq6n', 'type': 'tool_result', 'content': 'Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/level4/path_management.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/path_management_architecture.md'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,106,106,10769,236,106,0.5,0.25,0,,,,,transcript_response
f86b4d58-381b-4fe7-9d8b-38906cd581f9,2025-06-26T07:33:04.269Z,transcript_f86b4d58-381b-4fe7-9d8b-38906cd581f9,"{'tool_use_id': 'toolu_01UM3pix73LxqfJCVR9R39iB', 'type': 'tool_result', 'content': '     1→""""""Enhanced dispatcher with dynamic plugin loading via PluginFactory.""""""\n     2→from pathlib import Path\n     3→from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4→import logging\n     5→import hashlib\n     6→import time\n     7→import re\n     8→from datetime import datetime\n     9→import os\n    10→\n    11→from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12→from ..plugins.plugin_factory import PluginFactory\n    13→from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14→from ..storage.sqlite_store import SQLiteStore\n    15→from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16→from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17→# Note: We\'ve removed ignore pattern checks to allow indexing ALL files\n    18→# Filtering happens only during export via SecureIndexExporter\n    19→# from ..core.ignore_patterns import get_ignore_manager\n    20→\n    21→logger = logging.getLogger(__name__)\n    22→\n    23→\n    24→class EnhancedDispatcher:\n    25→    """"""Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.""""""\n    26→    \n    27→    # Document query patterns - common documentation search terms\n    28→    DOCUMENT_QUERY_PATTERNS = [\n    29→        r\'\\b(how\\s+to|howto)\\b\',\n    30→        r\'\\b(getting\\s+started|get\\s+started)\\b\',\n    31→        r\'\\b(installation|install|setup)\\b\',\n    32→        r\'\\b(configuration|configure|config)\\b\',\n    33→        r\'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b\',\n    34→        r\'\\b(tutorial|guide|walkthrough)\\b\',\n    35→        r\'\\b(example|sample|snippet)\\b\',\n    36→        r\'\\b(readme|documentation|docs)\\b\',\n    37→        r\'\\b(usage|use\\s+case|using)\\b\',\n    38→        r\'\\b(reference|manual)\\b\',\n    39→        r\'\\b(faq|frequently\\s+asked)\\b\',\n    40→        r\'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b\',\n    41→        r\'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b\',\n    42→        r\'\\b(architecture|design|overview)\\b\',\n    43→        r\'\\b(changelog|release\\s+notes|migration)\\b\'\n    44→    ]\n    45→    \n    46→    # Documentation file patterns\n    47→    DOCUMENTATION_FILE_PATTERNS = [\n    48→        r\'readme(\\.\\w+)?$\',\n    49→        r\'changelog(\\.\\w+)?$\',\n    50→        r\'contributing(\\.\\w+)?$\',\n    51→        r\'license(\\.\\w+)?$\',\n    52→        r\'install(\\.\\w+)?$\',\n    53→        r\'setup(\\.\\w+)?$\',\n    54→        r\'guide(\\.\\w+)?$\',\n    55→        r\'tutorial(\\.\\w+)?$\',\n    56→        r\'\\.md$\',\n    57→        r\'\\.rst$\',\n    58→        r\'\\.txt$\',\n    59→        r\'docs?/\',\n    60→        r\'documentation/\'\n    61→    ]\n    62→    \n    63→    def __init__(self, \n    64→                 plugins: Optional[List[IPlugin]] = None,\n    65→                 sqlite_store: Optional[SQLiteStore] = None,\n    66→                 enable_advanced_features: bool = True,\n    67→                 use_plugin_factory: bool = True,\n    68→                 lazy_load: bool = True,\n    69→                 semantic_search_enabled: bool = True):\n    70→        """"""Initialize the enhanced dispatcher.\n    71→        \n    72→        Args:\n    73→            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74→            sqlite_store: SQLite store for plugin persistence\n    75→            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76→            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77→            lazy_load: Whether to lazy-load plugins on demand\n    78→            semantic_search_enabled: Whether to enable semantic search in plugins\n    79→        """"""\n    80→        self._sqlite_store = sqlite_store\n    81→        self._enable_advanced = enable_advanced_features\n    82→        self._use_factory = use_plugin_factory\n    83→        self._lazy_load = lazy_load\n    84→        self._semantic_enabled = semantic_search_enabled\n    85→        \n    86→        # Plugin storage\n    87→        self._plugins: List[IPlugin] = []\n    88→        self._by_lang: Dict[str, IPlugin] = {}\n    89→        self._loaded_languages: set[str] = set()\n    90→        \n    91→        # Cache for file hashes to avoid re-indexing unchanged files\n    92→        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93→        \n    94→        # Advanced components\n    95→        if self._enable_advanced:\n    96→            self._file_matcher = FileTypeMatcher()\n    97→            self._router = PluginRouter(self._file_matcher)\n    98→            self._aggregator = ResultAggregator()\n    99→        \n   100→        # Performance tracking\n   101→        self._operation_stats = {\n   102→            \'searches\': 0,\n   103→            \'lookups\': 0,\n   104→            \'indexings\': 0,\n   105→            \'total_time\': 0.0,\n   106→            \'plugins_loaded\': 0\n   107→        }\n   108→        \n   109→        # Initialize plugins\n   110→        if plugins:\n   111→            # Use provided plugins (backward compatibility)\n   112→            self._plugins = plugins\n   113→            self._by_lang = {p.lang: p for p in plugins}\n   114→            for plugin in plugins:\n   115→                self._loaded_languages.add(getattr(plugin, \'lang\', \'unknown\'))\n   116→            if self._enable_advanced:\n   117→                self._register_plugins_with_router()\n   118→        elif use_plugin_factory and not lazy_load:\n   119→            # Load all plugins immediately\n   120→            self._load_all_plugins()\n   121→        # If lazy_load is True, plugins will be loaded on demand\n   122→        \n   123→        # Compile document query patterns for performance\n   124→        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125→        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126→        \n   127→        logger.info(f""Enhanced dispatcher initialized with {len(self._plugins)} plugins"")\n   128→    \n   129→    def _load_all_plugins(self):\n   130→        """"""Load all available plugins using PluginFactory.""""""\n   131→        logger.info(""Loading all available plugins..."")\n   132→        \n   133→        all_plugins = PluginFactory.create_all_plugins(\n   134→            sqlite_store=self._sqlite_store,\n   135→            enable_semantic=self._semantic_enabled\n   136→        )\n   137→        \n   138→        for lang, plugin in all_plugins.items():\n   139→            self._plugins.append(plugin)\n   140→            self._by_lang[lang] = plugin\n   141→            self._loaded_languages.add(lang)\n   142→            self._operation_stats[\'plugins_loaded\'] += 1\n   143→        \n   144→        if self._enable_advanced:\n   145→            self._register_plugins_with_router()\n   146→        \n   147→        logger.info(f""Loaded {len(all_plugins)} plugins: {\', \'.join(sorted(all_plugins.keys()))}"")\n   148→    \n   149→    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150→        """"""Ensure a plugin for the given language is loaded.\n   151→        \n   152→        Args:\n   153→            language: Language code (e.g., \'python\', \'go\')\n   154→            \n   155→        Returns:\n   156→            Plugin instance or None if not available\n   157→        """"""\n   158→        # Normalize language\n   159→        language = language.lower().replace(\'-\', \'_\')\n   160→        \n   161→        # Check if already loaded\n   162→        if language in self._by_lang:\n   163→            return self._by_lang[language]\n   164→        \n   165→        # If not using factory or already tried to load, return None\n   166→        if not self._use_factory or language in self._loaded_languages:\n   167→            return None\n   168→        \n   169→        # Try to load the plugin\n   170→        try:\n   171→            logger.info(f""Lazy loading plugin for {language}"")\n   172→            plugin = PluginFactory.create_plugin(\n   173→                language,\n   174→                sqlite_store=self._sqlite_store,\n   175→                enable_semantic=self._semantic_enabled\n   176→            )\n   177→            \n   178→            # Add to collections\n   179→            self._plugins.append(plugin)\n   180→            self._by_lang[language] = plugin\n   181→            self._loaded_languages.add(language)\n   182→            self._operation_stats[\'plugins_loaded\'] += 1\n   183→            \n   184→            # Register with router if needed\n   185→            if self._enable_advanced:\n   186→                capabilities = self._detect_plugin_capabilities(plugin)\n   187→                self._router.register_plugin(plugin, capabilities)\n   188→            \n   189→            logger.info(f""Successfully loaded {language} plugin"")\n   190→            return plugin\n   191→            \n   192→        except ValueError as e:\n   193→            logger.warning(f""No plugin available for {language}: {e}"")\n   194→            self._loaded_languages.add(language)  # Mark as attempted\n   195→            return None\n   196→        except Exception as e:\n   197→            logger.error(f""Error loading plugin for {language}: {e}"")\n   198→            self._loaded_languages.add(language)  # Mark as attempted\n   199→            return None\n   200→    \n   201→    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202→        """"""Ensure a plugin is loaded for the given file.\n   203→        \n   204→        Args:\n   205→            path: File path\n   206→            \n   207→        Returns:\n   208→            Plugin instance or None if not available\n   209→        """"""\n   210→        # Get language from file extension\n   211→        extension = path.suffix.lower()\n   212→        language = get_language_by_extension(extension)\n   213→        \n   214→        if language:\n   215→            return self._ensure_plugin_loaded(language)\n   216→        \n   217→        # Fallback: try all loaded plugins\n   218→        for plugin in self._plugins:\n   219→            if plugin.supports(path):\n   220→                return plugin\n   221→        \n   222→        return None\n   223→    \n   224→    def _register_plugins_with_router(self):\n   225→        """"""Register plugins with the router and assign capabilities.""""""\n   226→        for plugin in self._plugins:\n   227→            # Determine capabilities based on plugin type/language\n   228→            capabilities = self._detect_plugin_capabilities(plugin)\n   229→            self._router.register_plugin(plugin, capabilities)\n   230→    \n   231→    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232→        """"""Detect capabilities for a plugin based on its language and features.""""""\n   233→        capabilities = []\n   234→        lang = getattr(plugin, \'lang\', \'unknown\')\n   235→        \n   236→        # Base capabilities all plugins have\n   237→        capabilities.append(PluginCapability(\n   238→            \'syntax_analysis\', \'1.0\', f\'{lang} syntax analysis\', \n   239→            priority=70, metadata={\'language\': lang}\n   240→        ))\n   241→        \n   242→        capabilities.append(PluginCapability(\n   243→            \'code_search\', \'1.0\', f\'{lang} code search\', \n   244→            priority=80, metadata={\'language\': lang}\n   245→        ))\n   246→        \n   247→        # Check for semantic search capability\n   248→        if hasattr(plugin, \'_enable_semantic\') and plugin._enable_semantic:\n   249→            capabilities.append(PluginCapability(\n   250→                \'semantic_search\', \'1.0\', f\'{lang} semantic search\', \n   251→                priority=90, metadata={\'language\': lang}\n   252→            ))\n   253→        \n   254→        # Language-specific capabilities\n   255→        if lang == \'python\':\n   256→            capabilities.extend([\n   257→                PluginCapability(\'refactoring\', \'1.0\', \'Python refactoring support\', 75),\n   258→                PluginCapability(\'type_analysis\', \'1.0\', \'Python type analysis\', 85)\n   259→            ])\n   260→        elif lang in [\'javascript\', \'typescript\']:\n   261→            capabilities.extend([\n   262→                PluginCapability(\'linting\', \'1.0\', \'JavaScript/TypeScript linting\', 85),\n   263→                PluginCapability(\'bundling_analysis\', \'1.0\', \'Module bundling analysis\', 70),\n   264→                PluginCapability(\'framework_support\', \'1.0\', \'Framework-specific support\', 75)\n   265→            ])\n   266→        elif lang in [\'c\', \'cpp\']:\n   267→            capabilities.extend([\n   268→                PluginCapability(\'compilation_analysis\', \'1.0\', \'Compilation analysis\', 80),\n   269→                PluginCapability(\'memory_analysis\', \'1.0\', \'Memory usage analysis\', 70),\n   270→                PluginCapability(\'performance_profiling\', \'1.0\', \'Performance profiling\', 75)\n   271→            ])\n   272→        elif lang in [\'go\', \'rust\']:\n   273→            capabilities.extend([\n   274→                PluginCapability(\'package_analysis\', \'1.0\', f\'{lang} package analysis\', 80),\n   275→                PluginCapability(\'concurrency_analysis\', \'1.0\', f\'{lang} concurrency analysis\', 75)\n   276→            ])\n   277→        elif lang in [\'java\', \'kotlin\', \'scala\']:\n   278→            capabilities.extend([\n   279→                PluginCapability(\'jvm_analysis\', \'1.0\', \'JVM bytecode analysis\', 75),\n   280→                PluginCapability(\'build_tool_integration\', \'1.0\', \'Build tool integration\', 70)\n   281→            ])\n   282→        \n   283→        return capabilities\n   284→    \n   285→    @property\n   286→    def plugins(self):\n   287→        """"""Get the dictionary of loaded plugins by language.""""""\n   288→        return self._by_lang\n   289→    \n   290→    @property\n   291→    def supported_languages(self) -> List[str]:\n   292→        """"""Get list of all supported languages (loaded and available).""""""\n   293→        if self._use_factory:\n   294→            return PluginFactory.get_supported_languages()\n   295→        else:\n   296→            return list(self._by_lang.keys())\n   297→    \n   298→    def _match_plugin(self, path: Path) -> IPlugin:\n   299→        """"""Match a plugin for the given file path.""""""\n   300→        # Ensure plugin is loaded if using lazy loading\n   301→        if self._lazy_load and self._use_factory:\n   302→            plugin = self._ensure_plugin_for_file(path)\n   303→            if plugin:\n   304→                return plugin\n   305→        \n   306→        # Use advanced routing if available\n   307→        if self._enable_advanced and self._router:\n   308→            route_result = self._router.get_best_plugin(path)\n   309→            if route_result:\n   310→                return route_result.plugin\n   311→        \n   312→        # Fallback to basic matching\n   313→        for p in self._plugins:\n   314→            if p.supports(path):\n   315→                return p\n   316→        \n   317→        raise RuntimeError(f""No plugin found for {path}"")\n   318→    \n   319→    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320→        """"""Get all plugins that can handle a file with confidence scores.""""""\n   321→        # Ensure plugin is loaded if using lazy loading\n   322→        if self._lazy_load and self._use_factory:\n   323→            self._ensure_plugin_for_file(path)\n   324→        \n   325→        if self._enable_advanced and self._router:\n   326→            route_results = self._router.route_file(path)\n   327→            return [(result.plugin, result.confidence) for result in route_results]\n   328→        else:\n   329→            # Basic fallback\n   330→            matching_plugins = []\n   331→            for plugin in self._plugins:\n   332→                if plugin.supports(path):\n   333→                    matching_plugins.append((plugin, 1.0))\n   334→            return matching_plugins\n   335→    \n   336→    def lookup(self, symbol: str) -> SymbolDef | None:\n   337→        """"""Look up symbol definition across all plugins.""""""\n   338→        start_time = time.time()\n   339→        \n   340→        try:\n   341→            # For symbol lookup, we may need to search across all languages\n   342→            # Load all plugins if using lazy loading\n   343→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344→                self._load_all_plugins()\n   345→            \n   346→            if self._enable_advanced and self._aggregator:\n   347→                # Use advanced aggregation\n   348→                definitions_by_plugin = {}\n   349→                for plugin in self._plugins:\n   350→                    try:\n   351→                        definition = plugin.getDefinition(symbol)\n   352→                        definitions_by_plugin[plugin] = definition\n   353→                    except Exception as e:\n   354→                        logger.warning(f""Plugin {plugin.lang} failed to get definition for {symbol}: {e}"")\n   355→                        definitions_by_plugin[plugin] = None\n   356→                \n   357→                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358→                \n   359→                self._operation_stats[\'lookups\'] += 1\n   360→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   361→                \n   362→                return result\n   363→            else:\n   364→                # Fallback to basic lookup\n   365→                for p in self._plugins:\n   366→                    res = p.getDefinition(symbol)\n   367→                    if res:\n   368→                        self._operation_stats[\'lookups\'] += 1\n   369→                        self._operation_stats[\'total_time\'] += time.time() - start_time\n   370→                        return res\n   371→                return None\n   372→                \n   373→        except Exception as e:\n   374→            logger.error(f""Error in symbol lookup for {symbol}: {e}"", exc_info=True)\n   375→            return None\n   376→    \n   377→    def _is_document_query(self, query: str) -> bool:\n   378→        """"""Check if the query is looking for documentation.\n   379→        \n   380→        Args:\n   381→            query: Search query string\n   382→            \n   383→        Returns:\n   384→            True if this appears to be a documentation query\n   385→        """"""\n   386→        query_lower = query.lower()\n   387→        \n   388→        # Check against document query patterns\n   389→        for pattern in self._compiled_doc_patterns:\n   390→            if pattern.search(query_lower):\n   391→                return True\n   392→        \n   393→        # Check for question words at the beginning\n   394→        question_starters = [\'how\', \'what\', \'where\', \'when\', \'why\', \'can\', \'is\', \'does\', \'should\']\n   395→        first_word = query_lower.split()[0] if query_lower.split() else \'\'\n   396→        if first_word in question_starters:\n   397→            return True\n   398→        \n   399→        return False\n   400→    \n   401→    def _expand_document_query(self, query: str) -> List[str]:\n   402→        """"""Expand a document query with related terms for better search coverage.\n   403→        \n   404→        Args:\n   405→            query: Original search query\n   406→            \n   407→        Returns:\n   408→            List of expanded query variations\n   409→        """"""\n   410→        expanded_queries = [query]  # Always include original\n   411→        query_lower = query.lower()\n   412→        \n   413→        # Common expansions for documentation queries\n   414→        expansions = {\n   415→            \'install\': [\'installation\', \'setup\', \'getting started\', \'requirements\'],\n   416→            \'config\': [\'configuration\', \'configure\', \'settings\', \'options\', \'parameters\'],\n   417→            \'api\': [\'api documentation\', \'api reference\', \'endpoint\', \'method\'],\n   418→            \'how to\': [\'tutorial\', \'guide\', \'example\', \'usage\'],\n   419→            \'example\': [\'sample\', \'snippet\', \'demo\', \'code example\'],\n   420→            \'error\': [\'troubleshoot\', \'debug\', \'issue\', \'problem\', \'fix\'],\n   421→            \'getting started\': [\'quickstart\', \'tutorial\', \'introduction\', \'setup\'],\n   422→            \'guide\': [\'tutorial\', \'documentation\', \'walkthrough\', \'how to\'],\n   423→            \'usage\': [\'how to use\', \'example\', \'api\', \'reference\']\n   424→        }\n   425→        \n   426→        # Apply expansions\n   427→        for term, related_terms in expansions.items():\n   428→            if term in query_lower:\n   429→                for related in related_terms:\n   430→                    # Replace the term with related term\n   431→                    expanded = query_lower.replace(term, related)\n   432→                    if expanded != query_lower and expanded not in expanded_queries:\n   433→                        expanded_queries.append(expanded)\n   434→                \n   435→                # Also add queries with additional terms\n   436→                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437→                    expanded = f""{query} {related}""\n   438→                    if expanded not in expanded_queries:\n   439→                        expanded_queries.append(expanded)\n   440→        \n   441→        # Add file-specific searches for common documentation files\n   442→        if self._is_document_query(query):\n   443→            # Extract the main topic from the query\n   444→            topic_words = []\n   445→            for word in query.lower().split():\n   446→                if word not in [\'how\', \'to\', \'the\', \'a\', \'an\', \'is\', \'are\', \'what\', \'where\', \'when\']:\n   447→                    topic_words.append(word)\n   448→            \n   449→            if topic_words:\n   450→                topic = \' \'.join(topic_words[:2])  # Use first two topic words\n   451→                expanded_queries.extend([\n   452→                    f""README {topic}"",\n   453→                    f""{topic} documentation"",\n   454→                    f""{topic} docs"",\n   455→                    f""{topic} guide""\n   456→                ])\n   457→        \n   458→        # Remove duplicates while preserving order\n   459→        seen = set()\n   460→        unique_queries = []\n   461→        for q in expanded_queries:\n   462→            if q not in seen:\n   463→                seen.add(q)\n   464→                unique_queries.append(q)\n   465→        \n   466→        return unique_queries[:10]  # Limit to 10 queries max\n   467→    \n   468→    def _is_documentation_file(self, file_path: str) -> bool:\n   469→        """"""Check if a file path is likely a documentation file.\n   470→        \n   471→        Args:\n   472→            file_path: Path to check\n   473→            \n   474→        Returns:\n   475→            True if this appears to be a documentation file\n   476→        """"""\n   477→        path_lower = file_path.lower()\n   478→        \n   479→        for pattern in self._compiled_file_patterns:\n   480→            if pattern.search(path_lower):\n   481→                return True\n   482→        \n   483→        return False\n   484→    \n   485→    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486→        """"""Adjust ranking to prioritize documentation files for document queries.\n   487→        \n   488→        Args:\n   489→            query: Original search query\n   490→            results: List of aggregated results\n   491→            \n   492→        Returns:\n   493→            Re-ranked results with documentation prioritized\n   494→        """"""\n   495→        if not self._is_document_query(query):\n   496→            return results\n   497→        \n   498→        # Separate documentation and code results\n   499→        doc_results = []\n   500→        code_results = []\n   501→        \n   502→        for result in results:\n   503→            if self._is_documentation_file(result.primary_result.get(\'file\', \'\')):\n   504→                # Boost documentation files for document queries\n   505→                result.rank_score *= 1.5\n   506→                result.metadata[\'doc_boost\'] = True\n   507→                doc_results.append(result)\n   508→            else:\n   509→                code_results.append(result)\n   510→        \n   511→        # Sort each group by rank score\n   512→        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513→        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514→        \n   515→        # Combine with documentation files first\n   516→        return doc_results + code_results\n   517→    \n   518→    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519→        """"""Search for code and documentation across all plugins.""""""\n   520→        start_time = time.time()\n   521→        \n   522→        try:\n   523→            # For search, we may need to search across all languages\n   524→            # Load all plugins if using lazy loading\n   525→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526→                self._load_all_plugins()\n   527→            \n   528→            # Detect if this is a document query\n   529→            is_doc_query = self._is_document_query(query)\n   530→            \n   531→            # Expand query if it\'s a document query\n   532→            queries = [query]\n   533→            if is_doc_query:\n   534→                queries = self._expand_document_query(query)\n   535→                logger.info(f""Expanded document query \'{query}\' to {len(queries)} variations"")\n   536→                # Force semantic search for natural language queries\n   537→                semantic = True\n   538→            \n   539→            if self._enable_advanced and self._aggregator:\n   540→                # Use advanced aggregation\n   541→                all_results_by_plugin = {}\n   542→                opts = {""semantic"": semantic, ""limit"": limit * 2 if is_doc_query else limit}\n   543→                \n   544→                # Search with all query variations\n   545→                for search_query in queries:\n   546→                    for plugin in self._plugins:\n   547→                        try:\n   548→                            results = list(plugin.search(search_query, opts))\n   549→                            if results:\n   550→                                if plugin not in all_results_by_plugin:\n   551→                                    all_results_by_plugin[plugin] = []\n   552→                                all_results_by_plugin[plugin].extend(results)\n   553→                        except Exception as e:\n   554→                            logger.warning(f""Plugin {plugin.lang} failed to search for {search_query}: {e}"")\n   555→                \n   556→                # Deduplicate results per plugin\n   557→                for plugin, results in all_results_by_plugin.items():\n   558→                    seen = set()\n   559→                    unique_results = []\n   560→                    for result in results:\n   561→                        key = f""{result[\'file\']}:{result[\'line\']}""\n   562→                        if key not in seen:\n   563→                            seen.add(key)\n   564→                            unique_results.append(result)\n   565→                    all_results_by_plugin[plugin] = unique_results\n   566→                \n   567→                # Configure aggregator for document queries\n   568→                if is_doc_query and self._enable_advanced:\n   569→                    # Adjust ranking criteria for documentation\n   570→                    doc_criteria = RankingCriteria(\n   571→                        relevance_weight=0.5,      # Increase relevance weight\n   572→                        confidence_weight=0.2,     # Reduce confidence weight\n   573→                        frequency_weight=0.2,      # Keep frequency weight\n   574→                        recency_weight=0.1,        # Keep recency weight\n   575→                        prefer_exact_matches=False,  # Natural language doesn\'t need exact matches\n   576→                        boost_multiple_sources=True,\n   577→                        boost_common_extensions=True\n   578→                    )\n   579→                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580→                \n   581→                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582→                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583→                )\n   584→                \n   585→                # Adjust ranking for document queries\n   586→                if is_doc_query:\n   587→                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588→                \n   589→                # Apply final limit\n   590→                if limit and len(aggregated_results) > limit:\n   591→                    aggregated_results = aggregated_results[:limit]\n   592→                \n   593→                logger.debug(f""Search aggregation stats: {stats.total_results} total, ""\n   594→                           f""{stats.unique_results} unique, {stats.plugins_used} plugins used, ""\n   595→                           f""document_query={is_doc_query}"")\n   596→                \n   597→                self._operation_stats[\'searches\'] += 1\n   598→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   599→                \n   600→                # Yield primary results from aggregated results\n   601→                for aggregated in aggregated_results:\n   602→                    yield aggregated.primary_result\n   603→            else:\n   604→                # Fallback to basic search\n   605→                # Detect if this is a document query\n   606→                is_doc_query = self._is_document_query(query)\n   607→                \n   608→                # Expand query if it\'s a document query\n   609→                queries = [query]\n   610→                if is_doc_query:\n   611→                    queries = self._expand_document_query(query)\n   612→                    semantic = True  # Force semantic search for natural language\n   613→                \n   614→                opts = {""semantic"": semantic, ""limit"": limit}\n   615→                all_results = []\n   616→                \n   617→                # Search with all query variations\n   618→                for search_query in queries:\n   619→                    for p in self._plugins:\n   620→                        try:\n   621→                            for result in p.search(search_query, opts):\n   622→                                all_results.append(result)\n   623→                        except Exception as e:\n   624→                            logger.warning(f""Plugin {p.lang} failed to search for {search_query}: {e}"")\n   625→                \n   626→                # Deduplicate results\n   627→                seen = set()\n   628→                unique_results = []\n   629→                for result in all_results:\n   630→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   631→                    if key not in seen:\n   632→                        seen.add(key)\n   633→                        unique_results.append(result)\n   634→                \n   635→                # Sort by score if available\n   636→                unique_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   637→                \n   638→                # Prioritize documentation files for document queries\n   639→                if is_doc_query:\n   640→                    doc_results = []\n   641→                    code_results = []\n   642→                    for result in unique_results:\n   643→                        if self._is_documentation_file(result.get(\'file\', \'\')):\n   644→                            doc_results.append(result)\n   645→                        else:\n   646→                            code_results.append(result)\n   647→                    unique_results = doc_results + code_results\n   648→                \n   649→                # Apply limit\n   650→                count = 0\n   651→                for result in unique_results:\n   652→                    if limit and count >= limit:\n   653→                        break\n   654→                    yield result\n   655→                    count += 1\n   656→                \n   657→                self._operation_stats[\'searches\'] += 1\n   658→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   659→                \n   660→        except Exception as e:\n   661→            logger.error(f""Error in search for {query}: {e}"", exc_info=True)\n   662→    \n   663→    def index_file(self, path: Path) -> None:\n   664→        """"""Index a single file if it has changed.""""""\n   665→        try:\n   666→            # Ensure path is absolute to avoid relative/absolute path issues\n   667→            path = path.resolve()\n   668→            \n   669→            # Find the appropriate plugin\n   670→            plugin = self._match_plugin(path)\n   671→            \n   672→            # Read file content\n   673→            try:\n   674→                content = path.read_text(encoding=\'utf-8\')\n   675→            except UnicodeDecodeError:\n   676→                # Try with different encodings\n   677→                try:\n   678→                    content = path.read_text(encoding=\'latin-1\')\n   679→                except Exception as e:\n   680→                    logger.error(f""Failed to read {path}: {e}"")\n   681→                    return\n   682→            \n   683→            # Check if we need to re-index (simplified for now)\n   684→            # TODO: Implement proper caching logic\n   685→            \n   686→            # Index the file\n   687→            start_time = time.time()\n   688→            logger.info(f""Indexing {path} with {plugin.lang} plugin"")\n   689→            shard = plugin.indexFile(path, content)\n   690→            \n   691→            # Record performance if advanced features enabled\n   692→            if self._enable_advanced and self._router:\n   693→                execution_time = time.time() - start_time\n   694→                self._router.record_performance(plugin, execution_time)\n   695→            \n   696→            self._operation_stats[\'indexings\'] += 1\n   697→            self._operation_stats[\'total_time\'] += time.time() - start_time\n   698→            \n   699→            logger.info(f""Successfully indexed {path}: {len(shard.get(\'symbols\', []))} symbols found"")\n   700→            \n   701→        except RuntimeError as e:\n   702→            # No plugin found for this file type\n   703→            logger.debug(f""No plugin for {path}: {e}"")\n   704→        except Exception as e:\n   705→            logger.error(f""Error indexing {path}: {e}"", exc_info=True)\n   706→    \n   707→    def get_statistics(self) -> dict:\n   708→        """"""Get comprehensive statistics across all plugins and components.""""""\n   709→        stats = {\n   710→            ""total_plugins"": len(self._plugins),\n   711→            ""loaded_languages"": sorted(list(self._loaded_languages)),\n   712→            ""supported_languages"": len(self.supported_languages),\n   713→            ""operations"": self._operation_stats.copy()\n   714→        }\n   715→        \n   716→        # Add language breakdown\n   717→        stats[""by_language""] = {}\n   718→        for lang, plugin in self._by_lang.items():\n   719→            plugin_info = {\n   720→                ""loaded"": True,\n   721→                ""class"": plugin.__class__.__name__\n   722→            }\n   723→            if hasattr(plugin, \'get_indexed_count\'):\n   724→                plugin_info[""indexed_files""] = plugin.get_indexed_count()\n   725→            stats[""by_language""][lang] = plugin_info\n   726→        \n   727→        return stats\n   728→    \n   729→    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730→        """"""\n   731→        Index all files in a directory, respecting ignore patterns.\n   732→        \n   733→        Args:\n   734→            directory: Directory to index\n   735→            recursive: Whether to index subdirectories\n   736→            \n   737→        Returns:\n   738→            Statistics about indexed files\n   739→        """"""\n   740→        logger.info(f""Indexing directory: {directory} (recursive={recursive})"")\n   741→        \n   742→        # Note: We don\'t use ignore patterns during indexing\n   743→        # ALL files are indexed for local search capability\n   744→        # Filtering happens only during export/sharing\n   745→        \n   746→        # Get all supported extensions\n   747→        supported_extensions = get_all_extensions()\n   748→        \n   749→        stats = {\n   750→            ""total_files"": 0,\n   751→            ""indexed_files"": 0,\n   752→            ""ignored_files"": 0,\n   753→            ""failed_files"": 0,\n   754→            ""by_language"": {}\n   755→        }\n   756→        \n   757→        # Walk directory\n   758→        if recursive:\n   759→            file_iterator = directory.rglob(""*"")\n   760→        else:\n   761→            file_iterator = directory.glob(""*"")\n   762→            \n   763→        for path in file_iterator:\n   764→            if not path.is_file():\n   765→                continue\n   766→                \n   767→            stats[""total_files""] += 1\n   768→            \n   769→            # NOTE: We index ALL files locally, including gitignored ones\n   770→            # Filtering happens only during export/sharing\n   771→            # This allows local search of .env, secrets, etc.\n   772→                \n   773→            # Try to find a plugin that supports this file\n   774→            # This allows us to index ALL files, including .env, .key, etc.\n   775→            try:\n   776→                # First try to match by extension\n   777→                if path.suffix in supported_extensions:\n   778→                    self.index_file(path)\n   779→                    stats[""indexed_files""] += 1\n   780→                # For files without recognized extensions, try each plugin\'s supports() method\n   781→                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782→                else:\n   783→                    matched = False\n   784→                    for plugin in self._plugins:\n   785→                        if plugin.supports(path):\n   786→                            self.index_file(path)\n   787→                            stats[""indexed_files""] += 1\n   788→                            matched = True\n   789→                            break\n   790→                    \n   791→                    # If no plugin matched but we want to index everything,\n   792→                    # we could add a fallback here to index as plaintext\n   793→                    # For now, we\'ll skip unmatched files\n   794→                    if not matched:\n   795→                        logger.debug(f""No plugin found for {path}"")\n   796→                \n   797→                # Track by language\n   798→                language = get_language_by_extension(path.suffix)\n   799→                if language:\n   800→                    stats[""by_language""][language] = stats[""by_language""].get(language, 0) + 1\n   801→                    \n   802→            except Exception as e:\n   803→                logger.error(f""Failed to index {path}: {e}"")\n   804→                stats[""failed_files""] += 1\n   805→                \n   806→        logger.info(f""Directory indexing complete: {stats[\'indexed_files\']} indexed, ""\n   807→                   f""{stats[\'ignored_files\']} ignored, {stats[\'failed_files\']} failed"")\n   808→        \n   809→        return stats\n   810→    \n   811→    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812→        """"""Search specifically across documentation files.\n   813→        \n   814→        Args:\n   815→            topic: Topic to search for (e.g., ""installation"", ""configuration"")\n   816→            doc_types: Optional list of document types to search (e.g., [""readme"", ""guide"", ""api""])\n   817→            limit: Maximum number of results\n   818→            \n   819→        Returns:\n   820→            Search results from documentation files\n   821→        """"""\n   822→        # Default document types if not specified\n   823→        if doc_types is None:\n   824→            doc_types = [""readme"", ""documentation"", ""guide"", ""tutorial"", ""api"", ""changelog"", ""contributing""]\n   825→        \n   826→        # Build search queries for different document types\n   827→        queries = []\n   828→        for doc_type in doc_types:\n   829→            queries.extend([\n   830→                f""{doc_type} {topic}"",\n   831→                f""{topic} {doc_type}"",\n   832→                f""{topic} in {doc_type}""\n   833→            ])\n   834→        \n   835→        # Also search for the topic in common doc filenames\n   836→        queries.extend([\n   837→            f""README {topic}"",\n   838→            f""CONTRIBUTING {topic}"",\n   839→            f""docs {topic}"",\n   840→            f""documentation {topic}""\n   841→        ])\n   842→        \n   843→        # Deduplicate queries\n   844→        queries = list(dict.fromkeys(queries))\n   845→        \n   846→        logger.info(f""Cross-document search for \'{topic}\' with {len(queries)} query variations"")\n   847→        \n   848→        # Use the enhanced search with document-specific handling\n   849→        all_results = []\n   850→        seen = set()\n   851→        \n   852→        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853→            for result in self.search(query, semantic=True, limit=limit):\n   854→                # Only include documentation files\n   855→                if self._is_documentation_file(result.get(\'file\', \'\')):\n   856→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   857→                    if key not in seen:\n   858→                        seen.add(key)\n   859→                        all_results.append(result)\n   860→        \n   861→        # Sort by relevance (score) and return top results\n   862→        all_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   863→        \n   864→        count = 0\n   865→        for result in all_results:\n   866→            if count >= limit:\n   867→                break\n   868→            yield result\n   869→            count += 1\n   870→    \n   871→    def health_check(self) -> Dict[str, Any]:\n   872→        """"""Perform a health check on all components.""""""\n   873→        health = {\n   874→            \'status\': \'healthy\',\n   875→            \'components\': {\n   876→                \'dispatcher\': {\n   877→                    \'status\': \'healthy\',\n   878→                    \'plugins_loaded\': len(self._plugins),\n   879→                    \'languages_supported\': len(self.supported_languages),\n   880→                    \'factory_enabled\': self._use_factory,\n   881→                    \'lazy_loading\': self._lazy_load\n   882→                }\n   883→            },\n   884→            \'plugins\': {},\n   885→            \'errors\': []\n   886→        }\n   887→        \n   888→        # Check plugin health\n   889→        for lang, plugin in self._by_lang.items():\n   890→            try:\n   891→                plugin_health = {\n   892→                    \'status\': \'healthy\',\n   893→                    \'class\': plugin.__class__.__name__,\n   894→                    \'semantic_enabled\': getattr(plugin, \'_enable_semantic\', False)\n   895→                }\n   896→                if hasattr(plugin, \'get_indexed_count\'):\n   897→                    plugin_health[\'indexed_files\'] = plugin.get_indexed_count()\n   898→            except Exception as e:\n   899→                plugin_health = {\n   900→                    \'status\': \'error\',\n   901→                    \'error\': str(e)\n   902→                }\n   903→                health[\'errors\'].append(f""Plugin {lang}: {str(e)}"")\n   904→            \n   905→            health[\'plugins\'][lang] = plugin_health\n   906→        \n   907→        # Determine overall health\n   908→        if len(health[\'errors\']) > 0:\n   909→            health[\'status\'] = \'degraded\' if len(health[\'errors\']) < 3 else \'unhealthy\'\n   910→        \n   911→        return health\n   912→    \n   913→    def remove_file(self, path: Union[Path, str]) -> None:\n   914→        """"""Remove a file from all indexes.\n   915→        \n   916→        Args:\n   917→            path: File path to remove\n   918→        """"""\n   919→        path = Path(path).resolve()\n   920→        logger.info(f""Removing file from index: {path}"")\n   921→        \n   922→        try:\n   923→            # Remove from SQLite if available\n   924→            if self._sqlite_store:\n   925→                from ..core.path_resolver import PathResolver\n   926→                path_resolver = PathResolver()\n   927→                try:\n   928→                    relative_path = path_resolver.normalize_path(path)\n   929→                    # Get repository ID - for now assume 1\n   930→                    # TODO: Properly detect repository\n   931→                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932→                except Exception as e:\n   933→                    logger.error(f""Error removing from SQLite: {e}"")\n   934→            \n   935→            # Remove from semantic index if available\n   936→            try:\n   937→                plugin = self._match_plugin(path)\n   938→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   939→                    plugin._indexer.remove_file(path)\n   940→                    logger.info(f""Removed from semantic index: {path}"")\n   941→            except Exception as e:\n   942→                logger.warning(f""Error removing from semantic index: {e}"")\n   943→            \n   944→            # Update statistics\n   945→            self._operation_stats[\'deletions\'] = self._operation_stats.get(\'deletions\', 0) + 1\n   946→            \n   947→        except Exception as e:\n   948→            logger.error(f""Error removing file {path}: {e}"", exc_info=True)\n   949→    \n   950→    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951→        """"""Move a file in all indexes.\n   952→        \n   953→        Args:\n   954→            old_path: Original file path\n   955→            new_path: New file path\n   956→            content_hash: Optional content hash to verify unchanged content\n   957→        """"""\n   958→        old_path = Path(old_path).resolve()\n   959→        new_path = Path(new_path).resolve()\n   960→        logger.info(f""Moving file in index: {old_path} -> {new_path}"")\n   961→        \n   962→        try:\n   963→            # Move in SQLite if available\n   964→            if self._sqlite_store:\n   965→                from ..core.path_resolver import PathResolver\n   966→                path_resolver = PathResolver()\n   967→                try:\n   968→                    old_relative = path_resolver.normalize_path(old_path)\n   969→                    new_relative = path_resolver.normalize_path(new_path)\n   970→                    # Get repository ID - for now assume 1\n   971→                    # TODO: Properly detect repository\n   972→                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973→                except Exception as e:\n   974→                    logger.error(f""Error moving in SQLite: {e}"")\n   975→            \n   976→            # Move in semantic index if available\n   977→            try:\n   978→                plugin = self._match_plugin(new_path)\n   979→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   980→                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981→                    logger.info(f""Moved in semantic index: {old_path} -> {new_path}"")\n   982→            except Exception as e:\n   983→                logger.warning(f""Error moving in semantic index: {e}"")\n   984→            \n   985→            # Update statistics\n   986→            self._operation_stats[\'moves\'] = self._operation_stats.get(\'moves\', 0) + 1\n   987→            \n   988→        except Exception as e:\n   989→            logger.error(f""Error moving file {old_path} -> {new_path}: {e}"", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,124,124,11005,13343,124,0.5,0.25,0,,,,,transcript_response
f86b4d58-381b-4fe7-9d8b-38906cd581f9,2025-06-26T07:33:14.213Z,transcript_f86b4d58-381b-4fe7-9d8b-38906cd581f9,"{'tool_use_id': 'toolu_01HjgKNWmNXxZh1nFpM6MCSV', 'type': 'tool_result', 'content': '     1→#!/usr/bin/env python3\n     2→""""""Test the enhanced dispatcher\'s document query functionality.""""""\n     3→\n     4→import logging\n     5→from pathlib import Path\n     6→from mcp_server.dispatcher.dispatcher_enhanced import EnhancedDispatcher\n     7→from mcp_server.storage.sqlite_store import SQLiteStore\n     8→\n     9→# Setup logging\n    10→logging.basicConfig(level=logging.INFO, format=\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n    11→logger = logging.getLogger(__name__)\n    12→\n    13→\n    14→def test_document_query_detection():\n    15→    """"""Test document query detection.""""""\n    16→    dispatcher = EnhancedDispatcher(enable_advanced_features=True)\n    17→    \n    18→    # Test queries that should be detected as document queries\n    19→    doc_queries = [\n    20→        ""How to install the package"",\n    21→        ""getting started with Python"",\n    22→        ""API documentation for search"",\n    23→        ""configuration options"",\n    24→        ""What is the architecture"",\n    25→        ""troubleshooting errors"",\n    26→        ""best practices for coding"",\n    27→        ""installation guide"",\n    28→        ""where can I find examples""\n    29→    ]\n    30→    \n    31→    # Test queries that should NOT be document queries\n    32→    code_queries = [\n    33→        ""def search"",\n    34→        ""class EnhancedDispatcher"",\n    35→        ""import logging"",\n    36→        ""search_function"",\n    37→        ""dispatcher.search""\n    38→    ]\n    39→    \n    40→    print(""Testing Document Query Detection:"")\n    41→    print(""="" * 50)\n    42→    \n    43→    print(""\\nDocument Queries (should return True):"")\n    44→    for query in doc_queries:\n    45→        is_doc = dispatcher._is_document_query(query)\n    46→        print(f""  \'{query}\' -> {is_doc}"")\n    47→        assert is_doc, f""Expected \'{query}\' to be detected as document query""\n    48→    \n    49→    print(""\\nCode Queries (should return False):"")\n    50→    for query in code_queries:\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,1175,1175,24348,786,1175,0.5,0.25,0,,,,,transcript_response
f86b4d58-381b-4fe7-9d8b-38906cd581f9,2025-06-26T07:33:39.675Z,transcript_f86b4d58-381b-4fe7-9d8b-38906cd581f9,"{'tool_use_id': 'toolu_01YKtKp5iH7i555oHcdft66C', 'type': 'tool_result', 'content': [{'type': 'text', 'text': 'Based on my analysis of the codebase, I found the EnhancedDispatcher class definition. Here are the details:\n\n## File Path\n**File:** `/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher_enhanced.py`\n\n## Class Definition Location\n**Lines:** 24-989 (the class spans from line 24 to the end of the file at line 989)\n\n## Main Methods with Signatures\n\nHere are the main methods of the EnhancedDispatcher class with their signatures:\n\n### Constructor\n- **`__init__(self, plugins: Optional[List[IPlugin]] = None, sqlite_store: Optional[SQLiteStore] = None, enable_advanced_features: bool = True, use_plugin_factory: bool = True, lazy_load: bool = True, semantic_search_enabled: bool = True)`** (lines 63-127)\n\n### Plugin Management Methods\n- **`_load_all_plugins(self)`** (lines 129-147) - Load all available plugins using PluginFactory\n- **`_ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]`** (lines 149-199) - Ensure a plugin for the given language is loaded\n- **`_ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]`** (lines 201-222) - Ensure a plugin is loaded for the given file\n- **`_register_plugins_with_router(self)`** (lines 224-229) - Register plugins with the router\n- **`_detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]`** (lines 231-283) - Detect capabilities for a plugin\n\n### Core Search and Lookup Methods\n- **`lookup(self, symbol: str) -> SymbolDef | None`** (lines 336-375) - Look up symbol definition across all plugins\n- **`search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]`** (lines 518-661) - Search for code and documentation across all plugins\n- **`search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]`** (lines 811-869) - Search specifically across documentation files\n\n### File Operations\n- **`index_file(self, path: Path) -> None`** (lines 663-705) - Index a single file if it has changed\n- **`index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]`** (lines 729-809) - Index all files in a directory\n- **`remove_file(self, path: Union[Path, str]) -> None`** (lines 913-948) - Remove a file from all indexes\n- **`move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None`** (lines 950-989) - Move a file in all indexes\n\n### Plugin Routing and Matching\n- **`_match_plugin(self, path: Path) -> IPlugin`** (lines 298-317) - Match a plugin for the given file path\n- **`get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]`** (lines 319-334) - Get all plugins that can handle a file with confidence scores\n\n### Document Query Processing\n- **`_is_document_query(self, query: str) -> bool`** (lines 377-399) - Check if the query is looking for documentation\n- **`_expand_document_query(self, query: str) -> List[str]`** (lines 401-466) - Expand a document query with related terms\n- **`_is_documentation_file(self, file_path: str) -> bool`** (lines 468-483) - Check if a file path is likely a documentation file\n- **`_adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]`** (lines 485-516) - Adjust ranking to prioritize documentation files\n\n### Properties and Utilities\n- **`plugins` (property)** (lines 286-288) - Get the dictionary of loaded plugins by language\n- **`supported_languages` (property)** (lines 291-296) - Get list of all supported languages\n- **`get_statistics(self) -> dict`** (lines 707-727) - Get comprehensive statistics across all plugins\n- **`health_check(self) -> Dict[str, Any]`** (lines 871-911) - Perform a health check on all components\n\nThe EnhancedDispatcher class is a comprehensive dispatcher that provides advanced plugin management, document-aware search capabilities, and extensive file indexing functionality with support for lazy loading, semantic search, and result aggregation.'}]}",transcript_query,unknown,native,0,True,6,0,0,0,6,0,0,0,1,1,13920,2609,1,0.5,0.25,0,,,,,transcript_response
3bea4874-d851-464f-90ea-6e0b51bf9716,2025-06-26T07:31:32.662Z,transcript_3bea4874-d851-464f-90ea-6e0b51bf9716,Locate the SQLiteStore class and explain its search method,transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,126,126,13172,746,126,0.5,0.25,0,,,,,transcript_response
41333a35-f5b8-4f90-875a-e853cef49417,2025-06-26T07:36:47.071Z,transcript_41333a35-f5b8-4f90-875a-e853cef49417,Find all functions that use asyncio,transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,202,202,13913,0,202,0.5,0.25,0,,,,,transcript_response
41333a35-f5b8-4f90-875a-e853cef49417,2025-06-26T07:36:52.766Z,transcript_41333a35-f5b8-4f90-875a-e853cef49417,"Search the codebase for all functions that use asyncio. Look for:
1. Functions that import asyncio
2. Functions that use async/await syntax
3. Functions that call asyncio methods like asyncio.run(), asyncio.gather(), asyncio.create_task(), etc.
4. Functions that use asyncio.sleep(), asyncio.wait(), etc.

Please provide a comprehensive list of all functions that use asyncio, including:
- File path and line number for each function
- Function name
- Brief description of how asyncio is being used

Return the results in a clear, organized format.",transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,101,101,9986,867,101,0.5,0.25,0,,,,,transcript_response
41333a35-f5b8-4f90-875a-e853cef49417,2025-06-26T07:36:56.627Z,transcript_41333a35-f5b8-4f90-875a-e853cef49417,"{'tool_use_id': 'toolu_015ZpnHSYVmftdP9EQxJ72RE', 'type': 'tool_result', 'content': 'Found 132 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/conftest.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_cross_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_developer_workflows.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_repository_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_semantic_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_dart_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_js_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_python_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_utils.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_data/markdown/tutorial.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_plugins_direct.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_metadata_extractor.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_multi_language_indexing_with_ignore.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_natural_language_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_readme_parsing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_section_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/swift_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/security/auth_manager.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/security/security_middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_test_prompts.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_test_results_template.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/populate_bm25_quick.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/swift_demo.swift\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_languages_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_plugins_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_specialized_plugins.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_data/markdown/api_docs.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/api_gateway_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/cache_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/dispatcher_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/indexing_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/metrics_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/security_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/shared_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/storage_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/type_analyzer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/dart_plugin/AGENTS.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/js_plugin/AGENTS.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/AGENTS.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_runner.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp-index-kit/install.sh\n(Results are truncated. Consider using a more specific path or pattern.)'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,223,223,10853,3457,223,0.5,0.25,0,,,,,transcript_response
41333a35-f5b8-4f90-875a-e853cef49417,2025-06-26T07:37:04.637Z,transcript_41333a35-f5b8-4f90-875a-e853cef49417,"{'tool_use_id': 'toolu_01BkuvhxXCgkT6UfrRGvksK5', 'type': 'tool_result', 'content': 'Found 93 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_python_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_data/markdown/tutorial.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_section_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/security/auth_manager.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/security/security_middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_test_prompts.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_test_results_template.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_data/markdown/api_docs.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/api_gateway_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/cache_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/dispatcher_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/indexing_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/metrics_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/security_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/shared_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/storage_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/AGENTS.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_runner.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/docs/development/TESTING-GUIDE.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/celery_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/fastapi_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/grpc_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/jwt_authentication_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/MCP.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/prometheus_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/pydantic_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/tree_sitter_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/watchdog.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/specialized_plugins_architecture.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/docs/DEPLOYMENT-GUIDE.md'}",transcript_query,unknown,native,0,True,1920,0,0,0,1920,0,0,0,315,315,14310,5638,315,0.5,0.25,0,,,,,transcript_response
41333a35-f5b8-4f90-875a-e853cef49417,2025-06-26T07:37:12.789Z,transcript_41333a35-f5b8-4f90-875a-e853cef49417,"{'tool_use_id': 'toolu_01XKQUXYgVqEq7jx4Nq5Ltic', 'type': 'tool_result', 'content': '     1→from fastapi import FastAPI, HTTPException, Depends, status, Response\n     2→from fastapi.responses import JSONResponse, PlainTextResponse\n     3→from typing import Optional, Dict, Any, List\n     4→from pathlib import Path\n     5→import logging\n     6→import os\n     7→import time\n     8→import threading\n     9→from .dispatcher import Dispatcher\n    10→from .plugin_base import SymbolDef, SearchResult\n    11→from .storage.sqlite_store import SQLiteStore\n    12→from .watcher import FileWatcher\n    13→from .core.logging import setup_logging\n    14→from .utils.index_discovery import IndexDiscovery\n    15→from .plugin_system import PluginManager, PluginSystemConfig\n    16→from .indexer.bm25_indexer import BM25Indexer\n    17→from .indexer.hybrid_search import HybridSearch, HybridSearchConfig\n    18→from .utils.fuzzy_indexer import FuzzyIndexer\n    19→from .security import (\n    20→    SecurityConfig, AuthManager, SecurityMiddlewareStack,\n    21→    AuthCredentials, User, UserRole, Permission,\n    22→    get_current_user, get_current_active_user, require_permission, require_role\n    23→)\n    24→from .metrics import (\n    25→    get_metrics_collector, get_health_checker, HealthStatus\n    26→)\n    27→from .metrics.middleware import setup_metrics_middleware, get_business_metrics\n    28→from .metrics.prometheus_exporter import get_prometheus_exporter\n    29→from .cache import (\n    30→    CacheManagerFactory, CacheConfig, CacheBackendType, \n    31→    QueryResultCache, QueryCacheConfig, QueryType\n    32→)\n    33→\n    34→# Set up logging\n    35→setup_logging(log_level=""INFO"")\n    36→logger = logging.getLogger(__name__)\n    37→\n    38→app = FastAPI(title=""MCP Server"", description=""Code Index MCP Server with Security, Metrics, and Health Checks"")\n    39→dispatcher: Dispatcher | None = None\n    40→sqlite_store: SQLiteStore | None = None\n    41→file_watcher: FileWatcher | None = None\n    42→plugin_manager: PluginManager | None = None\n    43→plugin_loader = None  # Dynamic plugin loader\n    44→auth_manager: AuthManager | None = None\n    45→security_config: SecurityConfig | None = None\n    46→cache_manager = None\n    47→query_cache: QueryResultCache | None = None\n    48→bm25_indexer: BM25Indexer | None = None\n    49→hybrid_search: HybridSearch | None = None\n    50→fuzzy_indexer: FuzzyIndexer | None = None\n    51→\n    52→# Initialize metrics and health checking\n    53→metrics_collector = get_metrics_collector()\n    54→health_checker = get_health_checker()\n    55→business_metrics = get_business_metrics()\n    56→\n    57→# Setup metrics middleware\n    58→setup_metrics_middleware(app, enable_detailed_metrics=True)\n    59→\n    60→@app.on_event(""startup"")\n    61→async def startup_event():\n    62→    """"""Initialize the dispatcher and register plugins on startup.""""""\n    63→    global dispatcher, sqlite_store, file_watcher, plugin_manager, plugin_loader, auth_manager, security_config, cache_manager, query_cache, bm25_indexer, hybrid_search, fuzzy_indexer\n    64→    \n    65→    try:\n    66→        # Initialize security configuration\n    67→        logger.info(""Initializing security configuration..."")\n    68→        security_config = SecurityConfig(\n    69→            jwt_secret_key=os.getenv(""JWT_SECRET_KEY"", ""your-super-secret-jwt-key-change-in-production-min-32-chars""),\n    70→            jwt_algorithm=""HS256"",\n    71→            access_token_expire_minutes=int(os.getenv(""ACCESS_TOKEN_EXPIRE_MINUTES"", ""30"")),\n    72→            refresh_token_expire_days=int(os.getenv(""REFRESH_TOKEN_EXPIRE_DAYS"", ""7"")),\n    73→            password_min_length=int(os.getenv(""PASSWORD_MIN_LENGTH"", ""8"")),\n    74→            max_login_attempts=int(os.getenv(""MAX_LOGIN_ATTEMPTS"", ""5"")),\n    75→            lockout_duration_minutes=int(os.getenv(""LOCKOUT_DURATION_MINUTES"", ""15"")),\n    76→            rate_limit_requests=int(os.getenv(""RATE_LIMIT_REQUESTS"", ""100"")),\n    77→            rate_limit_window_minutes=int(os.getenv(""RATE_LIMIT_WINDOW_MINUTES"", ""1"")),\n    78→            cors_origins=os.getenv(""CORS_ORIGINS"", ""*"").split("",""),\n    79→            cors_methods=[""GET"", ""POST"", ""PUT"", ""DELETE"", ""OPTIONS""],\n    80→            cors_headers=[""*""]\n    81→        )\n    82→        \n    83→        # Initialize authentication manager\n    84→        logger.info(""Initializing authentication manager..."")\n    85→        auth_manager = AuthManager(security_config)\n    86→        \n    87→        # Create default admin user if it doesn\'t exist\n    88→        admin_user = await auth_manager.get_user_by_username(""admin"")\n    89→        if not admin_user:\n    90→            admin_password = os.getenv(""DEFAULT_ADMIN_PASSWORD"", ""admin123!"")\n    91→            logger.info(""Creating default admin user..."")\n    92→            await auth_manager.create_user(\n    93→                username=""admin"",\n    94→                password=admin_password,\n    95→                email=os.getenv(""DEFAULT_ADMIN_EMAIL"", ""admin@localhost""),\n    96→                role=UserRole.ADMIN\n    97→            )\n    98→            logger.info(""Default admin user created"")\n    99→        \n   100→        # Set up security middleware\n   101→        logger.info(""Setting up security middleware..."")\n   102→        security_middleware = SecurityMiddlewareStack(app, security_config, auth_manager)\n   103→        security_middleware.setup_middleware()\n   104→        logger.info(""Security middleware configured successfully"")\n   105→        \n   106→        # Initialize cache system\n   107→        logger.info(""Initializing cache system..."")\n   108→        cache_backend_type = os.getenv(""CACHE_BACKEND"", ""memory"").lower()\n   109→        redis_url = os.getenv(""REDIS_URL"", ""redis://localhost:6379"")\n   110→        \n   111→        if cache_backend_type == ""redis"":\n   112→            try:\n   113→                cache_manager = CacheManagerFactory.create_redis_cache(\n   114→                    redis_url=redis_url,\n   115→                    default_ttl=int(os.getenv(""CACHE_DEFAULT_TTL"", ""3600""))\n   116→                )\n   117→                logger.info(""Using Redis cache backend"")\n   118→            except Exception as e:\n   119→                logger.warning(f""Failed to initialize Redis cache, falling back to memory: {e}"")\n   120→                cache_manager = CacheManagerFactory.create_memory_cache()\n   121→        elif cache_backend_type == ""hybrid"":\n   122→            try:\n   123→                cache_manager = CacheManagerFactory.create_hybrid_cache(\n   124→                    redis_url=redis_url,\n   125→                    max_entries=int(os.getenv(""CACHE_MAX_ENTRIES"", ""1000"")),\n   126→                    max_memory_mb=int(os.getenv(""CACHE_MAX_MEMORY_MB"", ""100"")),\n   127→                    default_ttl=int(os.getenv(""CACHE_DEFAULT_TTL"", ""3600""))\n   128→                )\n   129→                logger.info(""Using hybrid cache backend"")\n   130→            except Exception as e:\n   131→                logger.warning(f""Failed to initialize hybrid cache, falling back to memory: {e}"")\n   132→                cache_manager = CacheManagerFactory.create_memory_cache()\n   133→        else:\n   134→            cache_manager = CacheManagerFactory.create_memory_cache(\n   135→                max_entries=int(os.getenv(""CACHE_MAX_ENTRIES"", ""1000"")),\n   136→                max_memory_mb=int(os.getenv(""CACHE_MAX_MEMORY_MB"", ""100"")),\n   137→                default_ttl=int(os.getenv(""CACHE_DEFAULT_TTL"", ""3600""))\n   138→            )\n   139→            logger.info(""Using memory cache backend"")\n   140→        \n   141→        await cache_manager.initialize()\n   142→        \n   143→        # Initialize query result cache\n   144→        query_cache_config = QueryCacheConfig(\n   145→            enabled=os.getenv(""QUERY_CACHE_ENABLED"", ""true"").lower() == ""true"",\n   146→            default_ttl=int(os.getenv(""QUERY_CACHE_DEFAULT_TTL"", ""300"")),\n   147→            symbol_lookup_ttl=int(os.getenv(""QUERY_CACHE_SYMBOL_TTL"", ""1800"")),\n   148→            search_ttl=int(os.getenv(""QUERY_CACHE_SEARCH_TTL"", ""600"")),\n   149→            semantic_search_ttl=int(os.getenv(""QUERY_CACHE_SEMANTIC_TTL"", ""3600""))\n   150→        )\n   151→        query_cache = QueryResultCache(cache_manager, query_cache_config)\n   152→        logger.info(""Query result cache initialized successfully"")\n   153→        # Check for portable index first\n   154→        workspace_root = Path(""."")\n   155→        discovery = IndexDiscovery(workspace_root)\n   156→        \n   157→        if discovery.is_index_enabled():\n   158→            logger.info(""MCP portable index detected"")\n   159→            \n   160→            # Try to use existing index\n   161→            index_path = discovery.get_local_index_path()\n   162→            \n   163→            if not index_path and discovery.should_download_index():\n   164→                logger.info(""Attempting to download index from GitHub artifacts..."")\n   165→                if discovery.download_latest_index():\n   166→                    index_path = discovery.get_local_index_path()\n   167→                    logger.info(""Successfully downloaded index from artifacts"")\n   168→                else:\n   169→                    logger.info(""Could not download index, will use default"")\n   170→            \n   171→            if index_path:\n   172→                logger.info(f""Using portable index: {index_path}"")\n   173→                sqlite_store = SQLiteStore(str(index_path))\n   174→                \n   175→                # Log index info\n   176→                info = discovery.get_index_info()\n   177→                if info[\'metadata\']:\n   178→                    meta = info[\'metadata\']\n   179→                    logger.info(f""Index created: {meta.get(\'created_at\', \'unknown\')}"")\n   180→                    logger.info(f""Index commit: {meta.get(\'commit\', \'unknown\')[:8]}"")\n   181→            else:\n   182→                logger.info(""No portable index found, using default"")\n   183→                sqlite_store = SQLiteStore(""code_index.db"")\n   184→        else:\n   185→            # Initialize SQLite store with default\n   186→            logger.info(""Initializing SQLite store with default path..."")\n   187→            sqlite_store = SQLiteStore(""code_index.db"")\n   188→        \n   189→        logger.info(""SQLite store initialized successfully"")\n   190→        \n   191→        # Initialize plugin system with dynamic discovery\n   192→        logger.info(""Initializing plugin system with dynamic discovery..."")\n   193→        from .plugin_system.discovery import get_plugin_discovery\n   194→        from .plugin_system.loader import get_plugin_loader\n   195→        \n   196→        # Discover all available plugins\n   197→        plugin_discovery = get_plugin_discovery()\n   198→        discovered = plugin_discovery.discover_plugins()\n   199→        logger.info(f""Discovered {len(discovered)} plugins: {list(discovered.keys())}"")\n   200→        \n   201→        # Initialize plugin loader\n   202→        plugin_loader = get_plugin_loader()\n   203→        \n   204→        # Load plugins based on configuration or all discovered plugins\n   205→        config_path = Path(""plugins.yaml"")\n   206→        if config_path.exists():\n   207→            # Load specific plugins from config\n   208→            import yaml\n   209→            with open(config_path, \'r\') as f:\n   210→                plugin_config = yaml.safe_load(f)\n   211→            \n   212→            enabled_languages = plugin_config.get(\'enabled_languages\', list(discovered.keys()))\n   213→            logger.info(f""Loading plugins for languages: {enabled_languages}"")\n   214→        else:\n   215→            # Load all discovered plugins\n   216→            enabled_languages = list(discovered.keys())\n   217→            logger.info(""No plugins.yaml found, loading all discovered plugins"")\n   218→        \n   219→        # Load plugins\n   220→        plugin_instances = []\n   221→        for language in enabled_languages:\n   222→            try:\n   223→                plugin = plugin_loader.load_plugin(language)\n   224→                if plugin:\n   225→                    plugin_instances.append(plugin)\n   226→                    logger.info(f""Successfully loaded plugin for {language}"")\n   227→            except Exception as e:\n   228→                logger.error(f""Failed to load plugin for {language}: {e}"")\n   229→        \n   230→        logger.info(f""Loaded {len(plugin_instances)} plugins"")\n   231→        \n   232→        # Create plugin manager for compatibility\n   233→        plugin_manager = PluginManager(sqlite_store=sqlite_store)\n   234→        # Register loaded plugins with manager\n   235→        for plugin in plugin_instances:\n   236→            if hasattr(plugin, \'get_language\'):\n   237→                lang = plugin.get_language()\n   238→                plugin_manager._plugins[lang] = plugin\n   239→        \n   240→        logger.info(f""Loaded {len(plugin_instances)} active plugins"")\n   241→        \n   242→        # Create a new Dispatcher instance with the loaded plugins\n   243→        logger.info(""Creating dispatcher..."")\n   244→        dispatcher = Dispatcher(plugin_instances)\n   245→        logger.info(f""Dispatcher created with {len(plugin_instances)} plugins"")\n   246→        \n   247→        # Initialize BM25 indexer\n   248→        logger.info(""Initializing BM25 indexer..."")\n   249→        bm25_indexer = BM25Indexer(sqlite_store)\n   250→        logger.info(""BM25 indexer initialized successfully"")\n   251→        \n   252→        # Initialize Fuzzy indexer\n   253→        logger.info(""Initializing Fuzzy indexer..."")\n   254→        fuzzy_indexer = FuzzyIndexer(sqlite_store)\n   255→        logger.info(""Fuzzy indexer initialized successfully"")\n   256→        \n   257→        # Check if semantic indexer is available\n   258→        semantic_indexer = None\n   259→        try:\n   260→            from .utils.semantic_indexer import SemanticIndexer\n   261→            semantic_indexer = SemanticIndexer(sqlite_store)\n   262→            logger.info(""Semantic indexer initialized successfully"")\n   263→        except ImportError:\n   264→            logger.warning(""Semantic indexer not available (missing dependencies)"")\n   265→        \n   266→        # Initialize Hybrid Search\n   267→        logger.info(""Initializing Hybrid Search..."")\n   268→        hybrid_config = HybridSearchConfig(\n   269→            bm25_weight=float(os.getenv(""HYBRID_BM25_WEIGHT"", ""0.5"")),\n   270→            semantic_weight=float(os.getenv(""HYBRID_SEMANTIC_WEIGHT"", ""0.3"")),\n   271→            fuzzy_weight=float(os.getenv(""HYBRID_FUZZY_WEIGHT"", ""0.2"")),\n   272→            enable_bm25=True,\n   273→            enable_semantic=semantic_indexer is not None,\n   274→            enable_fuzzy=True,\n   275→            parallel_execution=True,\n   276→            cache_results=True\n   277→        )\n   278→        hybrid_search = HybridSearch(\n   279→            storage=sqlite_store,\n   280→            bm25_indexer=bm25_indexer,\n   281→            semantic_indexer=semantic_indexer,\n   282→            fuzzy_indexer=fuzzy_indexer,\n   283→            config=hybrid_config\n   284→        )\n   285→        logger.info(f""Hybrid Search initialized (BM25: {hybrid_config.enable_bm25}, Semantic: {hybrid_config.enable_semantic}, Fuzzy: {hybrid_config.enable_fuzzy})"")\n   286→        \n   287→        # Initialize file watcher with dispatcher and query cache\n   288→        logger.info(""Starting file watcher..."")\n   289→        file_watcher = FileWatcher(Path("".""), dispatcher, query_cache)\n   290→        file_watcher.start()\n   291→        logger.info(""File watcher started for current directory with cache invalidation"")\n   292→        \n   293→        # Store in app.state for potential future use\n   294→        app.state.dispatcher = dispatcher\n   295→        app.state.sqlite_store = sqlite_store\n   296→        app.state.file_watcher = file_watcher\n   297→        app.state.plugin_manager = plugin_manager\n   298→        app.state.auth_manager = auth_manager\n   299→        app.state.security_config = security_config\n   300→        app.state.cache_manager = cache_manager\n   301→        app.state.query_cache = query_cache\n   302→        app.state.metrics_collector = metrics_collector\n   303→        app.state.health_checker = health_checker\n   304→        app.state.business_metrics = business_metrics\n   305→        app.state.bm25_indexer = bm25_indexer\n   306→        app.state.hybrid_search = hybrid_search\n   307→        app.state.fuzzy_indexer = fuzzy_indexer\n   308→        \n   309→        # Update status to include search capabilities\n   310→        search_capabilities = []\n   311→        if bm25_indexer:\n   312→            search_capabilities.append(""bm25"")\n   313→        if fuzzy_indexer:\n   314→            search_capabilities.append(""fuzzy"") \n   315→        if semantic_indexer:\n   316→            search_capabilities.append(""semantic"")\n   317→        if hybrid_search:\n   318→            search_capabilities.append(""hybrid"")\n   319→        \n   320→        logger.info(f""Search capabilities: {\', \'.join(search_capabilities)}"")\n   321→        \n   322→        # Register health checks for system components\n   323→        logger.info(""Registering component health checks..."")\n   324→        health_checker.register_health_check(\n   325→            ""database"", \n   326→            health_checker.create_database_health_check(""code_index.db"")\n   327→        )\n   328→        health_checker.register_health_check(\n   329→            ""plugins"",\n   330→            health_checker.create_plugin_health_check(plugin_manager)\n   331→        )\n   332→        \n   333→        # Update system metrics\n   334→        business_metrics.update_system_metrics(\n   335→            active_plugins=len(plugin_instances),\n   336→            indexed_files=0,  # Will be updated as files are indexed\n   337→            database_size=0,  # Will be updated periodically\n   338→            memory_usage=0    # Will be updated by middleware\n   339→        )\n   340→        \n   341→        # Log loaded plugins with detailed status\n   342→        plugin_status = plugin_manager.get_detailed_plugin_status()\n   343→        for name, status in plugin_status.items():\n   344→            basic_info = status[\'basic_info\']\n   345→            runtime_info = status[\'runtime_info\']\n   346→            logger.info(f""Plugin \'{name}\': {runtime_info[\'state\']} (v{basic_info[\'version\']}, language: {basic_info[\'language\']}, enabled: {runtime_info[\'enabled\']})"")\n   347→            if runtime_info.get(\'error\'):\n   348→                logger.warning(f""Plugin \'{name}\' has error: {runtime_info[\'error\']}"")\n   349→        \n   350→        logger.info(""MCP Server initialized successfully with dynamic plugin system, SQLite persistence, and file watcher"")\n   351→    except Exception as e:\n   352→        logger.error(f""Failed to initialize MCP Server: {e}"", exc_info=True)\n   353→        raise\n   354→\n   355→@app.on_event(""shutdown"")\n   356→async def shutdown_event():\n   357→    """"""Clean up resources on shutdown.""""""\n   358→    global file_watcher, plugin_manager, cache_manager\n   359→    \n   360→    if file_watcher:\n   361→        try:\n   362→            file_watcher.stop()\n   363→            logger.info(""File watcher stopped successfully"")\n   364→        except Exception as e:\n   365→            logger.error(f""Error stopping file watcher: {e}"", exc_info=True)\n   366→    \n   367→    if plugin_manager:\n   368→        try:\n   369→            shutdown_result = plugin_manager.shutdown_safe()\n   370→            if shutdown_result.success:\n   371→                logger.info(""Plugin manager shutdown successfully"")\n   372→            else:\n   373→                logger.error(f""Plugin manager shutdown failed: {shutdown_result.error.message}"")\n   374→                logger.error(f""Shutdown error details: {shutdown_result.error.details}"")\n   375→        except Exception as e:\n   376→            logger.error(f""Error shutting down plugin manager: {e}"", exc_info=True)\n   377→    \n   378→    if cache_manager:\n   379→        try:\n   380→            await cache_manager.shutdown()\n   381→            logger.info(""Cache manager shutdown successfully"")\n   382→        except Exception as e:\n   383→            logger.error(f""Error shutting down cache manager: {e}"", exc_info=True)\n   384→\n   385→# Authentication endpoints\n   386→\n   387→@app.post(""/api/v1/auth/login"")\n   388→async def login(credentials: AuthCredentials) -> Dict[str, Any]:\n   389→    """"""User login endpoint.""""""\n   390→    if auth_manager is None:\n   391→        raise HTTPException(503, ""Authentication service not ready"")\n   392→    \n   393→    try:\n   394→        user = await auth_manager.authenticate_user(credentials)\n   395→        if not user:\n   396→            raise HTTPException(401, ""Invalid credentials"")\n   397→        \n   398→        access_token = await auth_manager.create_access_token(user)\n   399→        refresh_token = await auth_manager.create_refresh_token(user)\n   400→        \n   401→        return {\n   402→            ""access_token"": access_token,\n   403→            ""refresh_token"": refresh_token,\n   404→            ""token_type"": ""bearer"",\n   405→            ""expires_in"": security_config.access_token_expire_minutes * 60,\n   406→            ""user"": {\n   407→                ""id"": user.id,\n   408→                ""username"": user.username,\n   409→                ""role"": user.role.value,\n   410→                ""permissions"": [p.value for p in user.permissions]\n   411→            }\n   412→        }\n   413→    except Exception as e:\n   414→        logger.error(f""Login failed for user \'{credentials.username}\': {e}"")\n   415→        raise HTTPException(401, ""Authentication failed"")\n   416→\n   417→@app.post(""/api/v1/auth/refresh"")\n   418→async def refresh_token(refresh_token: str) -> Dict[str, Any]:\n   419→    """"""Refresh access token.""""""\n   420→    if auth_manager is None:\n   421→        raise HTTPException(503, ""Authentication service not ready"")\n   422→    \n   423→    try:\n   424→        new_access_token = await auth_manager.refresh_access_token(refresh_token)\n   425→        if not new_access_token:\n   426→            raise HTTPException(401, ""Invalid refresh token"")\n   427→        \n   428→        return {\n   429→            ""access_token"": new_access_token,\n   430→            ""token_type"": ""bearer"",\n   431→            ""expires_in"": security_config.access_token_expire_minutes * 60\n   432→        }\n   433→    except Exception as e:\n   434→        logger.error(f""Token refresh failed: {e}"")\n   435→        raise HTTPException(401, ""Token refresh failed"")\n   436→\n   437→@app.post(""/api/v1/auth/logout"")\n   438→async def logout(\n   439→    refresh_token: Optional[str] = None,\n   440→    current_user: User = Depends(get_current_active_user)\n   441→) -> Dict[str, str]:\n   442→    """"""User logout endpoint.""""""\n   443→    if auth_manager is None:\n   444→        raise HTTPException(503, ""Authentication service not ready"")\n   445→    \n   446→    try:\n   447→        if refresh_token:\n   448→            await auth_manager.revoke_refresh_token(refresh_token)\n   449→        \n   450→        await auth_manager._log_security_event(\n   451→            ""user_logout"",\n   452→            user_id=current_user.id,\n   453→            username=current_user.username\n   454→        )\n   455→        \n   456→        return {""message"": ""Successfully logged out""}\n   457→    except Exception as e:\n   458→        logger.error(f""Logout failed: {e}"")\n   459→        raise HTTPException(500, ""Logout failed"")\n   460→\n   461→@app.get(""/api/v1/auth/me"")\n   462→async def get_current_user_info(current_user: User = Depends(get_current_active_user)) -> Dict[str, Any]:\n   463→    """"""Get current user information.""""""\n   464→    return {\n   465→        ""id"": current_user.id,\n   466→        ""username"": current_user.username,\n   467→        ""email"": current_user.email,\n   468→        ""role"": current_user.role.value,\n   469→        ""permissions"": [p.value for p in current_user.permissions],\n   470→        ""is_active"": current_user.is_active,\n   471→        ""created_at"": current_user.created_at.isoformat(),\n   472→        ""last_login"": current_user.last_login.isoformat() if current_user.last_login else None\n   473→    }\n   474→\n   475→@app.post(""/api/v1/auth/register"")\n   476→async def register(\n   477→    credentials: AuthCredentials,\n   478→    email: Optional[str] = None,\n   479→    admin_user: User = Depends(require_role(UserRole.ADMIN))\n   480→) -> Dict[str, Any]:\n   481→    """"""Register new user (admin only).""""""\n   482→    if auth_manager is None:\n   483→        raise HTTPException(503, ""Authentication service not ready"")\n   484→    \n   485→    try:\n   486→        user = await auth_manager.create_user(\n   487→            username=credentials.username,\n   488→            password=credentials.password,\n   489→            email=email,\n   490→            role=UserRole.USER\n   491→        )\n   492→        \n   493→        return {\n   494→            ""message"": ""User created successfully"",\n   495→            ""user"": {\n   496→                ""id"": user.id,\n   497→                ""username"": user.username,\n   498→                ""email"": user.email,\n   499→                ""role"": user.role.value\n   500→            }\n   501→        }\n   502→    except Exception as e:\n   503→        logger.error(f""User registration failed: {e}"")\n   504→        raise HTTPException(400, str(e))\n   505→\n   506→# Security management endpoints\n   507→\n   508→@app.get(""/api/v1/security/events"")\n   509→async def get_security_events(\n   510→    limit: int = 100,\n   511→    admin_user: User = Depends(require_role(UserRole.ADMIN))\n   512→) -> Dict[str, Any]:\n   513→    """"""Get security events (admin only).""""""\n   514→    if auth_manager is None:\n   515→        raise HTTPException(503, ""Authentication service not ready"")\n   516→    \n   517→    try:\n   518→        events = await auth_manager.get_security_events(limit)\n   519→        return {\n   520→            ""events"": [\n   521→                {\n   522→                    ""id"": event.id,\n   523→                    ""event_type"": event.event_type,\n   524→                    ""user_id"": event.user_id,\n   525→                    ""username"": event.username,\n   526→                    ""ip_address"": event.ip_address,\n   527→                    ""timestamp"": event.timestamp.isoformat(),\n   528→                    ""details"": event.details,\n   529→                    ""severity"": event.severity\n   530→                }\n   531→                for event in events\n   532→            ]\n   533→        }\n   534→    except Exception as e:\n   535→        logger.error(f""Failed to get security events: {e}"")\n   536→        raise HTTPException(500, ""Failed to retrieve security events"")\n   537→\n   538→# Health check endpoints (public)\n   539→@app.get(""/health"")\n   540→async def health_check() -> Dict[str, Any]:\n   541→    """"""Basic health check endpoint.""""""\n   542→    return {""status"": ""healthy"", ""service"": ""mcp-server"", ""timestamp"": time.time()}\n   543→\n   544→@app.get(""/health/detailed"")\n   545→async def detailed_health_check() -> Dict[str, Any]:\n   546→    """"""Detailed health check endpoint.""""""\n   547→    try:\n   548→        overall_health = await health_checker.get_overall_health()\n   549→        component_results = await health_checker.check_all_components()\n   550→        \n   551→        return {\n   552→            ""status"": overall_health.status.value,\n   553→            ""message"": overall_health.message,\n   554→            ""timestamp"": overall_health.timestamp,\n   555→            ""details"": overall_health.details,\n   556→            ""components"": [\n   557→                {\n   558→                    ""component"": result.component,\n   559→                    ""status"": result.status.value,\n   560→                    ""message"": result.message,\n   561→                    ""details"": result.details\n   562→                }\n   563→                for result in component_results\n   564→            ]\n   565→        }\n   566→    except Exception as e:\n   567→        logger.error(f""Health check failed: {e}"", exc_info=True)\n   568→        return {\n   569→            ""status"": ""unhealthy"",\n   570→            ""message"": f""Health check failed: {str(e)}"",\n   571→            ""timestamp"": time.time()\n   572→        }\n   573→\n   574→@app.get(""/health/{component}"")\n   575→async def component_health_check(component: str) -> Dict[str, Any]:\n   576→    """"""Health check for a specific component.""""""\n   577→    try:\n   578→        result = await health_checker.check_component(component)\n   579→        return {\n   580→            ""component"": result.component,\n   581→            ""status"": result.status.value,\n   582→            ""message"": result.message,\n   583→            ""timestamp"": result.timestamp,\n   584→            ""details"": result.details\n   585→        }\n   586→    except Exception as e:\n   587→        logger.error(f""Component health check failed for {component}: {e}"", exc_info=True)\n   588→        raise HTTPException(500, f""Health check failed: {str(e)}"")\n   589→\n   590→# Metrics endpoints\n   591→@app.get(""/metrics"", response_class=PlainTextResponse)\n   592→def get_metrics() -> str:\n   593→    """"""Prometheus metrics endpoint.""""""\n   594→    try:\n   595→        return metrics_collector.get_metrics()\n   596→    except Exception as e:\n   597→        logger.error(f""Failed to get metrics: {e}"", exc_info=True)\n   598→        raise HTTPException(500, f""Failed to get metrics: {str(e)}"")\n   599→\n   600→@app.get(""/metrics"")\n   601→def get_prometheus_metrics() -> Response:\n   602→    """"""Prometheus metrics endpoint.""""""\n   603→    try:\n   604→        prometheus_exporter = get_prometheus_exporter()\n   605→        \n   606→        # Update build info\n   607→        prometheus_exporter.set_build_info(\n   608→            version=""1.0.0"",\n   609→            commit=os.getenv(""GIT_COMMIT"", ""unknown""),\n   610→            build_time=os.getenv(""BUILD_TIME"", ""unknown"")\n   611→        )\n   612→        \n   613→        # Update system metrics\n   614→        import psutil\n   615→        process = psutil.Process()\n   616→        memory_info = process.memory_info()\n   617→        prometheus_exporter.set_memory_usage(memory_info.rss, memory_info.vms)\n   618→        prometheus_exporter.set_cpu_usage(process.cpu_percent())\n   619→        prometheus_exporter.set_active_threads(threading.active_count())\n   620→        \n   621→        # Update plugin metrics\n   622→        if plugin_loader:\n   623→            stats = plugin_loader.get_statistics()\n   624→            for lang, plugin in plugin_loader.get_active_plugins().items():\n   625→                prometheus_exporter.plugin_status.labels(\n   626→                    plugin=plugin.__class__.__name__,\n   627→                    language=lang\n   628→                ).set(1)\n   629→        \n   630→        # Update file watcher metrics\n   631→        if file_watcher:\n   632→            # This would need to be implemented in FileWatcher\n   633→            # prometheus_exporter.set_files_watched(file_watcher.get_watched_count())\n   634→            pass\n   635→        \n   636→        # Generate metrics\n   637→        metrics = prometheus_exporter.generate_metrics()\n   638→        return Response(\n   639→            content=metrics,\n   640→            media_type=prometheus_exporter.get_content_type()\n   641→        )\n   642→    except Exception as e:\n   643→        logger.error(f""Failed to generate Prometheus metrics: {e}"", exc_info=True)\n   644→        raise HTTPException(500, f""Failed to generate metrics: {str(e)}"")\n   645→\n   646→@app.get(""/metrics/json"")\n   647→def get_metrics_json(current_user: User = Depends(require_permission(Permission.READ))) -> Dict[str, Any]:\n   648→    """"""JSON metrics endpoint for programmatic access.""""""\n   649→    try:\n   650→        families = metrics_collector.get_metric_families()\n   651→        stats = metrics_collector.get_stats()\n   652→        \n   653→        return {\n   654→            ""timestamp"": time.time(),\n   655→            ""collector_stats"": stats,\n   656→            ""metric_families"": families\n   657→        }\n   658→    except Exception as e:\n   659→        logger.error(f""Failed to get JSON metrics: {e}"", exc_info=True)\n   660→        raise HTTPException(500, f""Failed to get metrics: {str(e)}"")\n   661→\n   662→@app.get(""/symbol"", response_model=SymbolDef | None)\n   663→async def symbol(\n   664→    symbol: str,\n   665→    current_user: User = Depends(require_permission(Permission.READ))\n   666→):\n   667→    if dispatcher is None:\n   668→        logger.error(""Symbol lookup attempted but dispatcher not ready"")\n   669→        raise HTTPException(503, ""Dispatcher not ready"")\n   670→    \n   671→    start_time = time.time()\n   672→    try:\n   673→        logger.debug(f""Looking up symbol: {symbol} for user: {current_user.username}"")\n   674→        \n   675→        # Try cache first if query cache is available\n   676→        cached_result = None\n   677→        if query_cache and query_cache.config.enabled:\n   678→            cached_result = await query_cache.get_cached_result(\n   679→                QueryType.SYMBOL_LOOKUP,\n   680→                symbol=symbol\n   681→            )\n   682→        \n   683→        if cached_result is not None:\n   684→            logger.debug(f""Found cached symbol: {symbol}"")\n   685→            duration = time.time() - start_time\n   686→            business_metrics.record_search_performed(\n   687→                query=symbol,\n   688→                semantic=False,\n   689→                results_count=1,\n   690→                duration=duration\n   691→            )\n   692→            return cached_result\n   693→        \n   694→        # Record symbol lookup metrics\n   695→        with metrics_collector.time_function(""symbol_lookup""):\n   696→            result = dispatcher.lookup(symbol)\n   697→        \n   698→        # Cache the result if available\n   699→        if query_cache and query_cache.config.enabled and result:\n   700→            await query_cache.cache_result(\n   701→                QueryType.SYMBOL_LOOKUP,\n   702→                result,\n   703→                symbol=symbol\n   704→            )\n   705→        \n   706→        # Record business metrics\n   707→        duration = time.time() - start_time\n   708→        business_metrics.record_search_performed(\n   709→            query=symbol,\n   710→            semantic=False,\n   711→            results_count=1 if result else 0,\n   712→            duration=duration\n   713→        )\n   714→        \n   715→        if result:\n   716→            logger.debug(f""Found symbol: {symbol}"")\n   717→        else:\n   718→            logger.debug(f""Symbol not found: {symbol}"")\n   719→        return result\n   720→    except Exception as e:\n   721→        duration = time.time() - start_time\n   722→        business_metrics.record_search_performed(\n   723→            query=symbol,\n   724→            semantic=False,\n   725→            results_count=0,\n   726→            duration=duration\n   727→        )\n   728→        logger.error(f""Error looking up symbol \'{symbol}\': {e}"", exc_info=True)\n   729→        raise HTTPException(500, f""Internal error during symbol lookup: {str(e)}"")\n   730→\n   731→@app.get(""/search"", response_model=list[SearchResult])\n   732→async def search(\n   733→    q: str,\n   734→    semantic: bool = False,\n   735→    limit: int = 20,\n   736→    mode: str = ""auto"",  # ""auto"", ""hybrid"", ""bm25"", ""semantic"", ""fuzzy"", ""classic""\n   737→    language: Optional[str] = None,\n   738→    file_filter: Optional[str] = None,\n   739→    current_user: User = Depends(require_permission(Permission.READ))\n   740→):\n   741→    """"""Search with support for multiple modes including hybrid search.\n   742→    \n   743→    Args:\n   744→        q: Search query\n   745→        semantic: Whether to use semantic search (for backward compatibility)\n   746→        limit: Maximum number of results\n   747→        mode: Search mode - ""auto"" (default), ""hybrid"", ""bm25"", ""semantic"", ""fuzzy"", or ""classic""\n   748→        language: Filter by programming language\n   749→        file_filter: Filter by file path pattern\n   750→        current_user: Authenticated user\n   751→    """"""\n   752→    if dispatcher is None and mode == ""classic"":\n   753→        logger.error(""Search attempted but dispatcher not ready"")\n   754→        raise HTTPException(503, ""Dispatcher not ready"")\n   755→    \n   756→    start_time = time.time()\n   757→    try:\n   758→        # Determine effective search mode\n   759→        effective_mode = mode\n   760→        if mode == ""auto"":\n   761→            # Auto mode: use hybrid if available, otherwise fall back\n   762→            if hybrid_search is not None:\n   763→                effective_mode = ""hybrid""\n   764→            elif semantic and hasattr(dispatcher, \'search\'):\n   765→                effective_mode = ""classic""\n   766→            else:\n   767→                effective_mode = ""bm25"" if bm25_indexer else ""classic""\n   768→        \n   769→        logger.debug(f""Searching for: \'{q}\' (mode={effective_mode}, limit={limit}, language={language}) for user: {current_user.username}"")\n   770→        \n   771→        # Build filters\n   772→        filters = {}\n   773→        if language:\n   774→            filters[\'language\'] = language\n   775→        if file_filter:\n   776→            filters[\'file_filter\'] = file_filter\n   777→        \n   778→        # Try cache first if query cache is available\n   779→        cache_key_parts = [q, effective_mode, str(limit)]\n   780→        if filters:\n   781→            cache_key_parts.extend([f""{k}:{v}"" for k, v in sorted(filters.items())])\n   782→        \n   783→        cached_results = None\n   784→        if query_cache and query_cache.config.enabled:\n   785→            query_type = QueryType.SEMANTIC_SEARCH if effective_mode == ""semantic"" else QueryType.SEARCH\n   786→            cached_results = await query_cache.get_cached_result(\n   787→                query_type,\n   788→                q=q,\n   789→                semantic=(effective_mode == ""semantic""),\n   790→                limit=limit\n   791→            )\n   792→        \n   793→        if cached_results is not None:\n   794→            logger.debug(f""Found cached search results for: \'{q}\' ({len(cached_results)} results)"")\n   795→            duration = time.time() - start_time\n   796→            business_metrics.record_search_performed(\n   797→                query=q,\n   798→                semantic=(effective_mode == ""semantic""),\n   799→                results_count=len(cached_results),\n   800→                duration=duration\n   801→            )\n   802→            return cached_results\n   803→        \n   804→        # Perform search based on mode\n   805→        results = []\n   806→        \n   807→        if effective_mode == ""hybrid"" and hybrid_search:\n   808→            # Use hybrid search\n   809→            with metrics_collector.time_function(""search"", labels={""mode"": ""hybrid""}):\n   810→                hybrid_results = await hybrid_search.search(\n   811→                    query=q,\n   812→                    filters=filters,\n   813→                    limit=limit\n   814→                )\n   815→                # Convert to SearchResult format\n   816→                for r in hybrid_results:\n   817→                    results.append(SearchResult(\n   818→                        file_path=r[\'filepath\'],\n   819→                        snippet=r[\'snippet\'],\n   820→                        score=r[\'score\']\n   821→                    ))\n   822→        \n   823→        elif effective_mode == ""bm25"" and bm25_indexer:\n   824→            # Direct BM25 search\n   825→            with metrics_collector.time_function(""search"", labels={""mode"": ""bm25""}):\n   826→                bm25_results = bm25_indexer.search(q, limit=limit, **filters)\n   827→                for r in bm25_results:\n   828→                    results.append(SearchResult(\n   829→                        file_path=r[\'filepath\'],\n   830→                        snippet=r.get(\'snippet\', \'\'),\n   831→                        score=r[\'score\']\n   832→                    ))\n   833→        \n   834→        elif effective_mode == ""fuzzy"" and fuzzy_indexer:\n   835→            # Direct fuzzy search\n   836→            with metrics_collector.time_function(""search"", labels={""mode"": ""fuzzy""}):\n   837→                fuzzy_results = fuzzy_indexer.search_fuzzy(q, max_results=limit)\n   838→                for r in fuzzy_results:\n   839→                    results.append(SearchResult(\n   840→                        file_path=r.get(\'file_path\', \'\'),\n   841→                        snippet=r.get(\'context\', \'\'),\n   842→                        score=r.get(\'score\', 0.0)\n   843→                    ))\n   844→        \n   845→        elif effective_mode == ""semantic"":\n   846→            # Use classic dispatcher with semantic=True\n   847→            if dispatcher:\n   848→                with metrics_collector.time_function(""search"", labels={""mode"": ""semantic""}):\n   849→                    results = list(dispatcher.search(q, semantic=True, limit=limit))\n   850→            else:\n   851→                raise HTTPException(\n   852→                    503, \n   853→                    detail={\n   854→                        ""error"": ""Semantic search not available"",\n   855→                        ""reason"": ""Missing Voyage AI API key configuration"",\n   856→                        ""setup"": {\n   857→                            ""method_1_mcp_json"": [\n   858→                                ""Configure in .mcp.json (recommended for Claude Code):"",\n   859→                                ""{"",\n   860→                                \'  ""mcpServers"": {\',\n   861→                                \'    ""code-index-mcp"": {\',\n   862→                                \'      ""command"": ""uvicorn"",\',\n   863→                                \'      ""args"": [""mcp_server.gateway:app""],\',\n   864→                                \'      ""env"": {\',\n   865→                                \'        ""VOYAGE_AI_API_KEY"": ""your-key-here"",\',\n   866→                                \'        ""SEMANTIC_SEARCH_ENABLED"": ""true""\',\n   867→                                \'      }\',\n   868→                                \'    }\',\n   869→                                \'  }\',\n   870→                                \'}\'\n   871→                            ],\n   872→                            ""method_2_cli"": [\n   873→                                ""Or use Claude Code CLI:"",\n   874→                                ""claude mcp add code-index-mcp -e VOYAGE_AI_API_KEY=your_key -e SEMANTIC_SEARCH_ENABLED=true -- uvicorn mcp_server.gateway:app""\n   875→                            ],\n   876→                            ""method_3_env"": [\n   877→                                ""Or set environment variables:"",\n   878→                                ""export VOYAGE_AI_API_KEY=your_key"",\n   879→                                ""export SEMANTIC_SEARCH_ENABLED=true""\n   880→                            ],\n   881→                            ""method_4_dotenv"": [\n   882→                                ""Or add to .env file:"",\n   883→                                ""VOYAGE_AI_API_KEY=your_key"",\n   884→                                ""SEMANTIC_SEARCH_ENABLED=true""\n   885→                            ],\n   886→                            ""get_api_key"": ""Get your API key from: https://www.voyageai.com/"",\n   887→                            ""alternative"": ""Use mode=\'hybrid\' or mode=\'bm25\' for keyword-based search""\n   888→                        }\n   889→                    }\n   890→                )\n   891→        \n   892→        else:\n   893→            # Classic search through dispatcher\n   894→            if dispatcher:\n   895→                with metrics_collector.time_function(""search"", labels={""mode"": ""classic""}):\n   896→                    results = list(dispatcher.search(q, semantic=False, limit=limit))\n   897→            else:\n   898→                raise HTTPException(503, ""Classic search not available"")\n   899→        \n   900→        # Cache the results if available\n   901→        if query_cache and query_cache.config.enabled and results:\n   902→            query_type = QueryType.SEMANTIC_SEARCH if effective_mode == ""semantic"" else QueryType.SEARCH\n   903→            await query_cache.cache_result(\n   904→                query_type,\n   905→                results,\n   906→                q=q,\n   907→                semantic=(effective_mode == ""semantic""),\n   908→                limit=limit\n   909→            )\n   910→        \n   911→        # Record business metrics\n   912→        duration = time.time() - start_time\n   913→        business_metrics.record_search_performed(\n   914→            query=q,\n   915→            semantic=(effective_mode == ""semantic""),\n   916→            results_count=len(results),\n   917→            duration=duration\n   918→        )\n   919→        \n   920→        logger.debug(f""Search returned {len(results)} results using {effective_mode} mode"")\n   921→        return results\n   922→    except Exception as e:\n   923→        duration = time.time() - start_time\n   924→        business_metrics.record_search_performed(\n   925→            query=q,\n   926→            semantic=semantic,\n   927→            results_count=0,\n   928→            duration=duration\n   929→        )\n   930→        logger.error(f""Error during search for \'{q}\': {e}"", exc_info=True)\n   931→        raise HTTPException(500, f""Internal error during search: {str(e)}"")\n   932→\n   933→@app.get(""/search/capabilities"")\n   934→async def get_search_capabilities() -> Dict[str, Any]:\n   935→    """"""Get available search capabilities and configuration guidance.""""""\n   936→    voyage_key = os.environ.get(\'VOYAGE_API_KEY\') or os.environ.get(\'VOYAGE_AI_API_KEY\')\n   937→    semantic_enabled = os.environ.get(\'SEMANTIC_SEARCH_ENABLED\', \'false\').lower() == \'true\'\n   938→    \n   939→    return {\n   940→        ""available_modes"": {\n   941→            ""bm25"": bm25_indexer is not None,\n   942→            ""fuzzy"": fuzzy_indexer is not None,\n   943→            ""semantic"": semantic_indexer is not None,\n   944→            ""hybrid"": hybrid_search is not None,\n   945→            ""classic"": dispatcher is not None\n   946→        },\n   947→        ""semantic_config"": {\n   948→            ""enabled"": semantic_indexer is not None,\n   949→            ""api_key_configured"": bool(voyage_key),\n   950→            ""semantic_enabled_flag"": semantic_enabled,\n   951→            ""status"": ""ready"" if semantic_indexer else ""not_configured""\n   952→        },\n   953→        ""configuration_guide"": {\n   954→            ""mcp_json_example"": {\n   955→                ""description"": ""Add to .mcp.json for Claude Code (recommended)"",\n   956→                ""config"": {\n   957→                    ""mcpServers"": {\n   958→                        ""code-index-mcp"": {\n   959→                            ""command"": ""uvicorn"",\n   960→                            ""args"": [""mcp_server.gateway:app""],\n   961→                            ""env"": {\n   962→                                ""VOYAGE_AI_API_KEY"": ""your-key-here"",\n   963→                                ""SEMANTIC_SEARCH_ENABLED"": ""true""\n   964→                            }\n   965→                        }\n   966→                    }\n   967→                }\n   968→            },\n   969→            ""cli_command"": ""claude mcp add code-index-mcp -e VOYAGE_AI_API_KEY=key -e SEMANTIC_SEARCH_ENABLED=true -- uvicorn mcp_server.gateway:app"",\n   970→            ""env_file"": ""Add to .env: VOYAGE_AI_API_KEY=key and SEMANTIC_SEARCH_ENABLED=true"",\n   971→            ""get_api_key"": ""https://www.voyageai.com/""\n   972→        }\n   973→    }\n   974→\n   975→@app.get(""/status"")\n   976→async def status(current_user: User = Depends(require_permission(Permission.READ))) -> Dict[str, Any]:\n   977→    """"""Returns server status including plugin information and statistics.""""""\n   978→    if dispatcher is None:\n   979→        return {\n   980→            ""status"": ""error"",\n   981→            ""plugins"": 0,\n   982→            ""indexed_files"": {""total"": 0, ""by_language"": {}},\n   983→            ""version"": ""0.1.0"",\n   984→            ""message"": ""Dispatcher not initialized""\n   985→        }\n   986→    \n   987→    try:\n   988→        # Try cache first if query cache is available\n   989→        cached_status = None\n   990→        if query_cache and query_cache.config.enabled:\n   991→            cached_status = await query_cache.get_cached_result(\n   992→                QueryType.PROJECT_STATUS\n   993→            )\n   994→        \n   995→        if cached_status is not None:\n   996→            return cached_status\n   997→        \n   998→        # Get plugin count\n   999→        plugin_count = len(dispatcher._plugins) if hasattr(dispatcher, \'_plugins\') else 0\n  1000→        \n  1001→        # Get indexed files statistics\n  1002→        indexed_stats = {""total"": 0, ""by_language"": {}}\n  1003→        if hasattr(dispatcher, \'get_statistics\'):\n  1004→            indexed_stats = dispatcher.get_statistics()\n  1005→        elif hasattr(dispatcher, \'_plugins\'):\n  1006→            # Calculate basic statistics from plugins\n  1007→            for plugin in dispatcher._plugins:\n  1008→                if hasattr(plugin, \'get_indexed_count\'):\n  1009→                    count = plugin.get_indexed_count()\n  1010→                    indexed_stats[""total""] += count\n  1011→                    lang = getattr(plugin, \'language\', getattr(plugin, \'lang\', \'unknown\'))\n  1012→                    indexed_stats[""by_language""][lang] = count\n  1013→        \n  1014→        # Add database statistics if available\n  1015→        db_stats = {}\n  1016→        if sqlite_store:\n  1017→            db_stats = sqlite_store.get_statistics()\n  1018→        \n  1019→        # Add cache statistics if available\n  1020→        cache_stats = {}\n  1021→        if cache_manager:\n  1022→            try:\n  1023→                cache_metrics = await cache_manager.get_metrics()\n  1024→                cache_stats = {\n  1025→                    ""hit_rate"": cache_metrics.hit_rate,\n  1026→                    ""entries"": cache_metrics.entries_count,\n  1027→                    ""memory_usage_mb"": cache_metrics.memory_usage_mb\n  1028→                }\n  1029→            except Exception as e:\n  1030→                logger.warning(f""Failed to get cache stats: {e}"")\n  1031→        \n  1032→        status_data = {\n  1033→            ""status"": ""operational"",\n  1034→            ""plugins"": plugin_count,\n  1035→            ""indexed_files"": indexed_stats,\n  1036→            ""database"": db_stats,\n  1037→            ""cache"": cache_stats,\n  1038→            ""search_capabilities"": [],\n  1039→            ""version"": ""0.1.0""\n  1040→        }\n  1041→        \n  1042→        # Add search capabilities\n  1043→        if bm25_indexer:\n  1044→            status_data[""search_capabilities""].append(""bm25"")\n  1045→        if fuzzy_indexer:\n  1046→            status_data[""search_capabilities""].append(""fuzzy"") \n  1047→        if hasattr(app.state, \'semantic_indexer\') and app.state.semantic_indexer:\n  1048→            status_data[""search_capabilities""].append(""semantic"")\n  1049→        if hybrid_search:\n  1050→            status_data[""search_capabilities""].append(""hybrid"")\n  1051→        \n  1052→        # Cache the status\n  1053→        if query_cache and query_cache.config.enabled:\n  1054→            await query_cache.cache_result(\n  1055→                QueryType.PROJECT_STATUS,\n  1056→                status_data\n  1057→            )\n  1058→        \n  1059→        return status_data\n  1060→    except Exception as e:\n  1061→        logger.error(f""Error getting server status: {e}"", exc_info=True)\n  1062→        return {\n  1063→            ""status"": ""error"",\n  1064→            ""plugins"": 0,\n  1065→            ""indexed_files"": {""total"": 0, ""by_language"": {}},\n  1066→            ""version"": ""0.1.0"",\n  1067→            ""message"": str(e)\n  1068→        }\n  1069→\n  1070→@app.get(""/plugins"")\n  1071→def plugins(current_user: User = Depends(require_permission(Permission.READ))) -> List[Dict[str, Any]]:\n  1072→    """"""Returns list of loaded plugins with their information.""""""\n  1073→    if plugin_manager is None:\n  1074→        logger.error(""Plugin list requested but plugin manager not ready"")\n  1075→        raise HTTPException(503, ""Plugin manager not ready"")\n  1076→    \n  1077→    try:\n  1078→        plugin_list = []\n  1079→        plugin_infos = plugin_manager._registry.list_plugins()\n  1080→        plugin_status = plugin_manager.get_plugin_status()\n  1081→        \n  1082→        for info in plugin_infos:\n  1083→            status = plugin_status.get(info.name, {})\n  1084→            plugin_data = {\n  1085→                ""name"": info.name,\n  1086→                ""version"": info.version,\n  1087→                ""description"": info.description,\n  1088→                ""author"": info.author,\n  1089→                ""language"": info.language,\n  1090→                ""file_extensions"": info.file_extensions,\n  1091→                ""state"": status.get(\'state\', \'unknown\'),\n  1092→                ""enabled"": status.get(\'enabled\', False)\n  1093→            }\n  1094→            plugin_list.append(plugin_data)\n  1095→        \n  1096→        logger.debug(f""Returning {len(plugin_list)} plugins"")\n  1097→        return plugin_list\n  1098→    except Exception as e:\n  1099→        logger.error(f""Error getting plugin list: {e}"", exc_info=True)\n  1100→        raise HTTPException(500, f""Internal error getting plugins: {str(e)}"")\n  1101→\n  1102→@app.post(""/reindex"")\n  1103→async def reindex(\n  1104→    path: Optional[str] = None,\n  1105→    current_user: User = Depends(require_permission(Permission.EXECUTE))\n  1106→) -> Dict[str, str]:\n  1107→    """"""Triggers manual reindexing of files.\n  1108→    \n  1109→    Args:\n  1110→        path: Optional specific directory path to reindex. If not provided,\n  1111→              reindexes all configured paths.\n  1112→    \n  1113→    Returns:\n  1114→        Task status information.\n  1115→    """"""\n  1116→    if dispatcher is None:\n  1117→        logger.error(""Reindex requested but dispatcher not ready"")\n  1118→        raise HTTPException(503, ""Dispatcher not ready"")\n  1119→    \n  1120→    try:\n  1121→        logger.info(f""Manual reindex requested for path: {path or \'all\'}"")\n  1122→        # Since dispatcher has index_file method, we can use it for reindexing\n  1123→        if path:\n  1124→            # Reindex specific path\n  1125→            target_path = Path(path)\n  1126→            if not target_path.exists():\n  1127→                raise HTTPException(404, f""Path not found: {path}"")\n  1128→            \n  1129→            indexed_count = 0\n  1130→            if target_path.is_file():\n  1131→                # Single file\n  1132→                dispatcher.index_file(target_path)\n  1133→                indexed_count = 1\n  1134→            else:\n  1135→                # Directory - find all supported files\n  1136→                for file_path in target_path.rglob(""*""):\n  1137→                    if file_path.is_file():\n  1138→                        try:\n  1139→                            # Check if any plugin supports this file\n  1140→                            for plugin in dispatcher._plugins:\n  1141→                                if plugin.supports(file_path):\n  1142→                                    dispatcher.index_file(file_path)\n  1143→                                    indexed_count += 1\n  1144→                                    break\n  1145→                        except Exception as e:\n  1146→                            # Log but continue with other files\n  1147→                            logger.warning(f""Failed to index {file_path}: {e}"")\n  1148→            \n  1149→            logger.info(f""Successfully reindexed {indexed_count} files in {path}"")\n  1150→            return {\n  1151→                ""status"": ""completed"",\n  1152→                ""message"": f""Reindexed {indexed_count} files in {path}""\n  1153→            }\n  1154→        else:\n  1155→            # Reindex all supported files\n  1156→            indexed_count = 0\n  1157→            indexed_by_type = {}\n  1158→            \n  1159→            # Find all files and check if any plugin supports them\n  1160→            for file_path in Path(""."").rglob(""*""):\n  1161→                if file_path.is_file():\n  1162→                    try:\n  1163→                        # Check if any plugin supports this file\n  1164→                        for plugin in dispatcher._plugins:\n  1165→                            if plugin.supports(file_path):\n  1166→                                dispatcher.index_file(file_path)\n  1167→                                indexed_count += 1\n  1168→                                \n  1169→                                # Track by file type\n  1170→                                suffix = file_path.suffix.lower()\n  1171→                                indexed_by_type[suffix] = indexed_by_type.get(suffix, 0) + 1\n  1172→                                break\n  1173→                    except Exception as e:\n  1174→                        # Log but continue with other files\n  1175→                        logger.warning(f""Failed to index {file_path}: {e}"")\n  1176→            \n  1177→            # Build summary message\n  1178→            type_summary = "", "".join([f""{count} {ext} files"" for ext, count in indexed_by_type.items()])\n  1179→            logger.info(f""Successfully reindexed {indexed_count} files: {type_summary}"")\n  1180→            return {\n  1181→                ""status"": ""completed"",\n  1182→                ""message"": f""Reindexed {indexed_count} files ({type_summary})""\n  1183→            }\n  1184→    except HTTPException:\n  1185→        raise\n  1186→    except Exception as e:\n  1187→        logger.error(f""Reindexing failed: {e}"", exc_info=True)\n  1188→        raise HTTPException(500, f""Reindexing failed: {str(e)}"")\n  1189→\n  1190→@app.post(""/plugins/{plugin_name}/reload"")\n  1191→async def reload_plugin(\n  1192→    plugin_name: str,\n  1193→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1194→) -> Dict[str, str]:\n  1195→    """"""Reload a specific plugin.\n  1196→    \n  1197→    Args:\n  1198→        plugin_name: Name of the plugin to reload\n  1199→        \n  1200→    Returns:\n  1201→        Status message\n  1202→    """"""\n  1203→    if plugin_manager is None:\n  1204→        logger.error(""Plugin reload requested but plugin manager not ready"")\n  1205→        raise HTTPException(503, ""Plugin manager not ready"")\n  1206→    \n  1207→    try:\n  1208→        plugin_manager.reload_plugin(plugin_name)\n  1209→        return {\n  1210→            ""status"": ""success"",\n  1211→            ""message"": f""Plugin \'{plugin_name}\' reloaded successfully""\n  1212→        }\n  1213→    except Exception as e:\n  1214→        logger.error(f""Failed to reload plugin \'{plugin_name}\': {e}"", exc_info=True)\n  1215→        raise HTTPException(500, f""Failed to reload plugin: {str(e)}"")\n  1216→\n  1217→@app.post(""/plugins/{plugin_name}/enable"")\n  1218→async def enable_plugin(\n  1219→    plugin_name: str,\n  1220→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1221→) -> Dict[str, str]:\n  1222→    """"""Enable a disabled plugin.\n  1223→    \n  1224→    Args:\n  1225→        plugin_name: Name of the plugin to enable\n  1226→        \n  1227→    Returns:\n  1228→        Status message\n  1229→    """"""\n  1230→    if plugin_manager is None:\n  1231→        logger.error(""Plugin enable requested but plugin manager not ready"")\n  1232→        raise HTTPException(503, ""Plugin manager not ready"")\n  1233→    \n  1234→    try:\n  1235→        plugin_manager.enable_plugin(plugin_name)\n  1236→        # Recreate dispatcher with updated plugins\n  1237→        active_plugins = plugin_manager.get_active_plugins()\n  1238→        global dispatcher\n  1239→        dispatcher = Dispatcher(list(active_plugins.values()))\n  1240→        \n  1241→        return {\n  1242→            ""status"": ""success"",\n  1243→            ""message"": f""Plugin \'{plugin_name}\' enabled successfully""\n  1244→        }\n  1245→    except Exception as e:\n  1246→        logger.error(f""Failed to enable plugin \'{plugin_name}\': {e}"", exc_info=True)\n  1247→        raise HTTPException(500, f""Failed to enable plugin: {str(e)}"")\n  1248→\n  1249→@app.post(""/plugins/{plugin_name}/disable"")\n  1250→async def disable_plugin(\n  1251→    plugin_name: str,\n  1252→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1253→) -> Dict[str, str]:\n  1254→    """"""Disable an active plugin.\n  1255→    \n  1256→    Args:\n  1257→        plugin_name: Name of the plugin to disable\n  1258→        \n  1259→    Returns:\n  1260→        Status message\n  1261→    """"""\n  1262→    if plugin_manager is None:\n  1263→        logger.error(""Plugin disable requested but plugin manager not ready"")\n  1264→        raise HTTPException(503, ""Plugin manager not ready"")\n  1265→    \n  1266→    try:\n  1267→        plugin_manager.disable_plugin(plugin_name)\n  1268→        # Recreate dispatcher with updated plugins\n  1269→        active_plugins = plugin_manager.get_active_plugins()\n  1270→        global dispatcher\n  1271→        dispatcher = Dispatcher(list(active_plugins.values()))\n  1272→        \n  1273→        return {\n  1274→            ""status"": ""success"",\n  1275→            ""message"": f""Plugin \'{plugin_name}\' disabled successfully""\n  1276→        }\n  1277→    except Exception as e:\n  1278→        logger.error(f""Failed to disable plugin \'{plugin_name}\': {e}"", exc_info=True)\n  1279→        raise HTTPException(500, f""Failed to disable plugin: {str(e)}"")\n  1280→\n  1281→# Cache management endpoints\n  1282→\n  1283→@app.get(""/cache/stats"")\n  1284→async def get_cache_stats(\n  1285→    current_user: User = Depends(require_permission(Permission.READ))\n  1286→) -> Dict[str, Any]:\n  1287→    """"""Get cache statistics and performance metrics.""""""\n  1288→    if not cache_manager:\n  1289→        raise HTTPException(503, ""Cache manager not ready"")\n  1290→    \n  1291→    try:\n  1292→        cache_metrics = await cache_manager.get_metrics()\n  1293→        backend_stats = await cache_manager.get_backend_stats()\n  1294→        \n  1295→        stats = {\n  1296→            ""cache_metrics"": {\n  1297→                ""hits"": cache_metrics.hits,\n  1298→                ""misses"": cache_metrics.misses,\n  1299→                ""sets"": cache_metrics.sets,\n  1300→                ""deletes"": cache_metrics.deletes,\n  1301→                ""hit_rate"": cache_metrics.hit_rate,\n  1302→                ""avg_response_time_ms"": cache_metrics.avg_response_time_ms,\n  1303→                ""entries_count"": cache_metrics.entries_count,\n  1304→                ""memory_usage_mb"": cache_metrics.memory_usage_mb\n  1305→            },\n  1306→            ""backend_stats"": backend_stats\n  1307→        }\n  1308→        \n  1309→        # Add query cache stats if available\n  1310→        if query_cache:\n  1311→            query_stats = await query_cache.get_cache_stats()\n  1312→            stats[""query_cache""] = query_stats\n  1313→        \n  1314→        return stats\n  1315→    except Exception as e:\n  1316→        logger.error(f""Failed to get cache stats: {e}"")\n  1317→        raise HTTPException(500, f""Failed to get cache statistics: {str(e)}"")\n  1318→\n  1319→@app.post(""/cache/clear"")\n  1320→async def clear_cache(\n  1321→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1322→) -> Dict[str, Any]:\n  1323→    """"""Clear all cache entries (admin only).""""""\n  1324→    if not cache_manager:\n  1325→        raise HTTPException(503, ""Cache manager not ready"")\n  1326→    \n  1327→    try:\n  1328→        count = await cache_manager.clear()\n  1329→        logger.info(f""Cache cleared by admin user {current_user.username}: {count} entries"")\n  1330→        \n  1331→        return {\n  1332→            ""status"": ""success"",\n  1333→            ""message"": f""Cleared {count} cache entries"",\n  1334→            ""cleared_entries"": count\n  1335→        }\n  1336→    except Exception as e:\n  1337→        logger.error(f""Failed to clear cache: {e}"")\n  1338→        raise HTTPException(500, f""Failed to clear cache: {str(e)}"")\n  1339→\n  1340→@app.post(""/cache/invalidate"")\n  1341→async def invalidate_cache_by_tags(\n  1342→    tags: List[str],\n  1343→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1344→) -> Dict[str, Any]:\n  1345→    """"""Invalidate cache entries by tags (admin only).""""""\n  1346→    if not cache_manager:\n  1347→        raise HTTPException(503, ""Cache manager not ready"")\n  1348→    \n  1349→    try:\n  1350→        tag_set = set(tags)\n  1351→        count = await cache_manager.invalidate_by_tags(tag_set)\n  1352→        logger.info(f""Cache invalidated by admin user {current_user.username}: {count} entries with tags {tags}"")\n  1353→        \n  1354→        return {\n  1355→            ""status"": ""success"",\n  1356→            ""message"": f""Invalidated {count} cache entries with tags {tags}"",\n  1357→            ""invalidated_entries"": count,\n  1358→            ""tags"": tags\n  1359→        }\n  1360→    except Exception as e:\n  1361→        logger.error(f""Failed to invalidate cache by tags: {e}"")\n  1362→        raise HTTPException(500, f""Failed to invalidate cache: {str(e)}"")\n  1363→\n  1364→@app.post(""/cache/invalidate/files"")\n  1365→async def invalidate_cache_by_files(\n  1366→    file_paths: List[str],\n  1367→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1368→) -> Dict[str, Any]:\n  1369→    """"""Invalidate cache entries that depend on specific files (admin only).""""""\n  1370→    if not query_cache:\n  1371→        raise HTTPException(503, ""Query cache not ready"")\n  1372→    \n  1373→    try:\n  1374→        total_count = 0\n  1375→        for file_path in file_paths:\n  1376→            count = await query_cache.invalidate_file_queries(file_path)\n  1377→            total_count += count\n  1378→        \n  1379→        logger.info(f""File-based cache invalidation by admin user {current_user.username}: {total_count} entries for {len(file_paths)} files"")\n  1380→        \n  1381→        return {\n  1382→            ""status"": ""success"",\n  1383→            ""message"": f""Invalidated {total_count} cache entries for {len(file_paths)} files"",\n  1384→            ""invalidated_entries"": total_count,\n  1385→            ""files"": file_paths\n  1386→        }\n  1387→    except Exception as e:\n  1388→        logger.error(f""Failed to invalidate cache by files: {e}"")\n  1389→        raise HTTPException(500, f""Failed to invalidate cache by files: {str(e)}"")\n  1390→\n  1391→@app.post(""/cache/invalidate/semantic"")\n  1392→async def invalidate_semantic_cache(\n  1393→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1394→) -> Dict[str, Any]:\n  1395→    """"""Invalidate all semantic search cache entries (admin only).""""""\n  1396→    if not query_cache:\n  1397→        raise HTTPException(503, ""Query cache not ready"")\n  1398→    \n  1399→    try:\n  1400→        count = await query_cache.invalidate_semantic_queries()\n  1401→        logger.info(f""Semantic cache invalidated by admin user {current_user.username}: {count} entries"")\n  1402→        \n  1403→        return {\n  1404→            ""status"": ""success"",\n  1405→            ""message"": f""Invalidated {count} semantic search cache entries"",\n  1406→            ""invalidated_entries"": count\n  1407→        }\n  1408→    except Exception as e:\n  1409→        logger.error(f""Failed to invalidate semantic cache: {e}"")\n  1410→        raise HTTPException(500, f""Failed to invalidate semantic cache: {str(e)}"")\n  1411→\n  1412→@app.post(""/cache/warm"")\n  1413→async def warm_cache(\n  1414→    keys: List[str],\n  1415→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1416→) -> Dict[str, Any]:\n  1417→    """"""Warm cache with predefined keys (admin only).""""""\n  1418→    if not cache_manager:\n  1419→        raise HTTPException(503, ""Cache manager not ready"")\n  1420→    \n  1421→    try:\n  1422→        # Simple factory function for warming - would need more sophisticated logic in production\n  1423→        async def factory(key: str):\n  1424→            # This is a placeholder - in real implementation would depend on key type\n  1425→            return f""warmed_value_for_{key}""\n  1426→        \n  1427→        count = await cache_manager.warm_cache(keys, factory)\n  1428→        logger.info(f""Cache warmed by admin user {current_user.username}: {count} entries"")\n  1429→        \n  1430→        return {\n  1431→            ""status"": ""success"",\n  1432→            ""message"": f""Warmed {count} cache entries"",\n  1433→            ""warmed_entries"": count,\n  1434→            ""requested_keys"": len(keys)\n  1435→        }\n  1436→    except Exception as e:\n  1437→        logger.error(f""Failed to warm cache: {e}"")\n  1438→        raise HTTPException(500, f""Failed to warm cache: {str(e)}"")\n  1439→\n  1440→@app.post(""/cache/cleanup"")\n  1441→async def cleanup_cache(\n  1442→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1443→) -> Dict[str, Any]:\n  1444→    """"""Manually trigger cache cleanup (admin only).""""""\n  1445→    if not cache_manager:\n  1446→        raise HTTPException(503, ""Cache manager not ready"")\n  1447→    \n  1448→    try:\n  1449→        count = await cache_manager.cleanup()\n  1450→        logger.info(f""Cache cleanup triggered by admin user {current_user.username}: {count} entries cleaned"")\n  1451→        \n  1452→        return {\n  1453→            ""status"": ""success"",\n  1454→            ""message"": f""Cleaned up {count} expired cache entries"",\n  1455→            ""cleaned_entries"": count\n  1456→        }\n  1457→    except Exception as e:\n  1458→        logger.error(f""Failed to cleanup cache: {e}"")\n  1459→        raise HTTPException(500, f""Failed to cleanup cache: {str(e)}"")\n  1460→\n  1461→# Hybrid Search endpoints\n  1462→\n  1463→@app.get(""/search/config"")\n  1464→async def get_search_config(\n  1465→    current_user: User = Depends(require_permission(Permission.READ))\n  1466→) -> Dict[str, Any]:\n  1467→    """"""Get current hybrid search configuration.""""""\n  1468→    if not hybrid_search:\n  1469→        raise HTTPException(503, ""Hybrid search not available"")\n  1470→    \n  1471→    config = hybrid_search.config\n  1472→    return {\n  1473→        ""weights"": {\n  1474→            ""bm25"": config.bm25_weight,\n  1475→            ""semantic"": config.semantic_weight,\n  1476→            ""fuzzy"": config.fuzzy_weight\n  1477→        },\n  1478→        ""enabled_methods"": {\n  1479→            ""bm25"": config.enable_bm25,\n  1480→            ""semantic"": config.enable_semantic,\n  1481→            ""fuzzy"": config.enable_fuzzy\n  1482→        },\n  1483→        ""rrf_k"": config.rrf_k,\n  1484→        ""parallel_execution"": config.parallel_execution,\n  1485→        ""cache_results"": config.cache_results,\n  1486→        ""limits"": {\n  1487→            ""individual_limit"": config.individual_limit,\n  1488→            ""final_limit"": config.final_limit\n  1489→        }\n  1490→    }\n  1491→\n  1492→@app.put(""/search/config/weights"")\n  1493→async def update_search_weights(\n  1494→    bm25: Optional[float] = None,\n  1495→    semantic: Optional[float] = None,\n  1496→    fuzzy: Optional[float] = None,\n  1497→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1498→) -> Dict[str, Any]:\n  1499→    """"""Update hybrid search weights (admin only).\n  1500→    \n  1501→    Weights will be normalized to sum to 1.0.\n  1502→    """"""\n  1503→    if not hybrid_search:\n  1504→        raise HTTPException(503, ""Hybrid search not available"")\n  1505→    \n  1506→    try:\n  1507→        hybrid_search.set_weights(bm25=bm25, semantic=semantic, fuzzy=fuzzy)\n  1508→        \n  1509→        # Get updated config\n  1510→        config = hybrid_search.config\n  1511→        \n  1512→        logger.info(f""Search weights updated by {current_user.username}: ""\n  1513→                   f""BM25={config.bm25_weight:.3f}, ""\n  1514→                   f""Semantic={config.semantic_weight:.3f}, ""\n  1515→                   f""Fuzzy={config.fuzzy_weight:.3f}"")\n  1516→        \n  1517→        return {\n  1518→            ""status"": ""success"",\n  1519→            ""weights"": {\n  1520→                ""bm25"": config.bm25_weight,\n  1521→                ""semantic"": config.semantic_weight,\n  1522→                ""fuzzy"": config.fuzzy_weight\n  1523→            }\n  1524→        }\n  1525→    except Exception as e:\n  1526→        logger.error(f""Failed to update search weights: {e}"")\n  1527→        raise HTTPException(500, f""Failed to update weights: {str(e)}"")\n  1528→\n  1529→@app.put(""/search/config/methods"")\n  1530→async def toggle_search_methods(\n  1531→    bm25: Optional[bool] = None,\n  1532→    semantic: Optional[bool] = None,\n  1533→    fuzzy: Optional[bool] = None,\n  1534→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1535→) -> Dict[str, Any]:\n  1536→    """"""Enable or disable search methods (admin only).""""""\n  1537→    if not hybrid_search:\n  1538→        raise HTTPException(503, ""Hybrid search not available"")\n  1539→    \n  1540→    try:\n  1541→        hybrid_search.enable_methods(bm25=bm25, semantic=semantic, fuzzy=fuzzy)\n  1542→        \n  1543→        # Get updated config\n  1544→        config = hybrid_search.config\n  1545→        \n  1546→        logger.info(f""Search methods updated by {current_user.username}: ""\n  1547→                   f""BM25={config.enable_bm25}, ""\n  1548→                   f""Semantic={config.enable_semantic}, ""\n  1549→                   f""Fuzzy={config.enable_fuzzy}"")\n  1550→        \n  1551→        return {\n  1552→            ""status"": ""success"",\n  1553→            ""enabled_methods"": {\n  1554→                ""bm25"": config.enable_bm25,\n  1555→                ""semantic"": config.enable_semantic,\n  1556→                ""fuzzy"": config.enable_fuzzy\n  1557→            }\n  1558→        }\n  1559→    except Exception as e:\n  1560→        logger.error(f""Failed to update search methods: {e}"")\n  1561→        raise HTTPException(500, f""Failed to update methods: {str(e)}"")\n  1562→\n  1563→@app.get(""/search/statistics"")\n  1564→async def get_search_statistics(\n  1565→    current_user: User = Depends(require_permission(Permission.READ))\n  1566→) -> Dict[str, Any]:\n  1567→    """"""Get search statistics and performance metrics.""""""\n  1568→    stats = {}\n  1569→    \n  1570→    # Hybrid search statistics\n  1571→    if hybrid_search:\n  1572→        stats[""hybrid_search""] = hybrid_search.get_statistics()\n  1573→    \n  1574→    # BM25 statistics\n  1575→    if bm25_indexer:\n  1576→        stats[""bm25""] = bm25_indexer.get_statistics()\n  1577→    \n  1578→    # Fuzzy search statistics\n  1579→    if fuzzy_indexer and hasattr(fuzzy_indexer, \'get_statistics\'):\n  1580→        stats[""fuzzy""] = fuzzy_indexer.get_statistics()\n  1581→    \n  1582→    # Add general search metrics from business metrics\n  1583→    if business_metrics:\n  1584→        search_metrics = business_metrics.get_search_metrics()\n  1585→        stats[""general""] = {\n  1586→            ""total_searches"": search_metrics.get(""total_searches"", 0),\n  1587→            ""average_response_time_ms"": search_metrics.get(""avg_response_time"", 0),\n  1588→            ""search_success_rate"": search_metrics.get(""success_rate"", 0)\n  1589→        }\n  1590→    \n  1591→    return stats\n  1592→\n  1593→@app.post(""/search/optimize"")\n  1594→async def optimize_search_indexes(\n  1595→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1596→) -> Dict[str, Any]:\n  1597→    """"""Optimize search indexes for better performance (admin only).""""""\n  1598→    results = {}\n  1599→    \n  1600→    try:\n  1601→        # Optimize BM25 indexes\n  1602→        if bm25_indexer:\n  1603→            bm25_indexer.optimize()\n  1604→            results[""bm25""] = ""optimized""\n  1605→            logger.info(""BM25 indexes optimized"")\n  1606→        \n  1607→        # Optimize FTS5 tables in SQLite\n  1608→        if sqlite_store:\n  1609→            sqlite_store.optimize_fts_tables()\n  1610→            results[""fts5""] = ""optimized""\n  1611→            logger.info(""FTS5 tables optimized"")\n  1612→        \n  1613→        # Clear hybrid search cache\n  1614→        if hybrid_search:\n  1615→            hybrid_search.clear_cache()\n  1616→            results[""hybrid_cache""] = ""cleared""\n  1617→            logger.info(""Hybrid search cache cleared"")\n  1618→        \n  1619→        logger.info(f""Search indexes optimized by {current_user.username}"")\n  1620→        \n  1621→        return {\n  1622→            ""status"": ""success"",\n  1623→            ""message"": ""Search indexes optimized successfully"",\n  1624→            ""results"": results\n  1625→        }\n  1626→    except Exception as e:\n  1627→        logger.error(f""Failed to optimize search indexes: {e}"")\n  1628→        raise HTTPException(500, f""Failed to optimize indexes: {str(e)}"")\n  1629→\n  1630→@app.get(""/search/term/{term}/stats"")\n  1631→async def get_term_statistics(\n  1632→    term: str,\n  1633→    current_user: User = Depends(require_permission(Permission.READ))\n  1634→) -> Dict[str, Any]:\n  1635→    """"""Get statistics for a specific search term.""""""\n  1636→    stats = {}\n  1637→    \n  1638→    try:\n  1639→        # BM25 term statistics\n  1640→        if bm25_indexer:\n  1641→            stats[""bm25""] = bm25_indexer.get_term_statistics(term)\n  1642→        \n  1643→        # SQLite FTS5 statistics\n  1644→        if sqlite_store:\n  1645→            stats[""fts5""] = sqlite_store.get_bm25_term_statistics(term)\n  1646→        \n  1647→        return {\n  1648→            ""term"": term,\n  1649→            ""statistics"": stats\n  1650→        }\n  1651→    except Exception as e:\n  1652→        logger.error(f""Failed to get term statistics: {e}"")\n  1653→        raise HTTPException(500, f""Failed to get term statistics: {str(e)}"")\n  1654→\n  1655→@app.post(""/search/rebuild"")\n  1656→async def rebuild_search_indexes(\n  1657→    index_type: str = ""all"",  # ""all"", ""bm25"", ""fuzzy"", ""semantic""\n  1658→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1659→) -> Dict[str, Any]:\n  1660→    """"""Rebuild search indexes (admin only).""""""\n  1661→    if index_type not in [""all"", ""bm25"", ""fuzzy"", ""semantic""]:\n  1662→        raise HTTPException(400, ""Invalid index_type. Must be \'all\', \'bm25\', \'fuzzy\', or \'semantic\'"")\n  1663→    \n  1664→    results = {}\n  1665→    \n  1666→    try:\n  1667→        if index_type in [""all"", ""bm25""] and bm25_indexer:\n  1668→            bm25_indexer.rebuild()\n  1669→            results[""bm25""] = ""rebuilt""\n  1670→            logger.info(""BM25 index rebuilt"")\n  1671→        \n  1672→        if index_type in [""all"", ""fuzzy""] and fuzzy_indexer:\n  1673→            fuzzy_indexer.clear()\n  1674→            # Re-index all files\n  1675→            if sqlite_store:\n  1676→                files = sqlite_store.get_all_files()\n  1677→                for file_info in files:\n  1678→                    try:\n  1679→                        with open(file_info[""path""], \'r\', encoding=\'utf-8\') as f:\n  1680→                            content = f.read()\n  1681→                        fuzzy_indexer.add_file(file_info[""path""], content)\n  1682→                    except Exception as e:\n  1683→                        logger.warning(f""Failed to re-index {file_info[\'path\']}: {e}"")\n  1684→            results[""fuzzy""] = ""rebuilt""\n  1685→            logger.info(""Fuzzy index rebuilt"")\n  1686→        \n  1687→        if index_type in [""all"", ""semantic""]:\n  1688→            # Semantic index rebuild would go here if available\n  1689→            if hasattr(hybrid_search, \'semantic_indexer\') and hybrid_search.semantic_indexer:\n  1690→                results[""semantic""] = ""rebuild_not_implemented""\n  1691→            else:\n  1692→                results[""semantic""] = ""not_available""\n  1693→        \n  1694→        logger.info(f""Search indexes rebuilt by {current_user.username}: {index_type}"")\n  1695→        \n  1696→        return {\n  1697→            ""status"": ""success"",\n  1698→            ""message"": f""Search indexes rebuilt successfully"",\n  1699→            ""index_type"": index_type,\n  1700→            ""results"": results\n  1701→        }\n  1702→    except Exception as e:\n  1703→        logger.error(f""Failed to rebuild search indexes: {e}"")\n  1704→        raise HTTPException(500, f""Failed to rebuild indexes: {str(e)}"")\n  1705→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,native,0,True,8706,0,0,0,8706,0,0,0,247,247,19948,31774,247,0.5,0.25,0,,,,,transcript_response
41333a35-f5b8-4f90-875a-e853cef49417,2025-06-26T07:37:20.543Z,transcript_41333a35-f5b8-4f90-875a-e853cef49417,"{'tool_use_id': 'toolu_016nDSL9GZzRYFaZLR7jm7rh', 'type': 'tool_result', 'content': '     1→""""""\n     2→Hybrid Search implementation combining BM25 and semantic search.\n     3→\n     4→This module implements reciprocal rank fusion (RRF) to combine results from\n     5→multiple search methods, providing better overall search quality.\n     6→""""""\n     7→\n     8→import logging\n     9→import asyncio\n    10→from typing import List, Dict, Any, Optional, Tuple, Set\n    11→from dataclasses import dataclass, field\n    12→from collections import defaultdict\n    13→import numpy as np\n    14→\n    15→from .bm25_indexer import BM25Indexer\n    16→from ..utils.semantic_indexer import SemanticIndexer\n    17→from ..storage.sqlite_store import SQLiteStore\n    18→from .query_optimizer import Query, QueryType\n    19→from .reranker import RerankerFactory, IReranker\n    20→# Import only what we need to avoid circular dependencies\n    21→from ..config.settings import RerankingSettings\n    22→\n    23→logger = logging.getLogger(__name__)\n    24→\n    25→\n    26→@dataclass\n    27→class SearchResult:\n    28→    """"""Individual search result from any search method.""""""\n    29→    doc_id: str\n    30→    filepath: str\n    31→    score: float\n    32→    snippet: str\n    33→    metadata: Dict[str, Any] = field(default_factory=dict)\n    34→    source: str = """"  # \'bm25\', \'semantic\', \'fuzzy\', etc.\n    35→    \n    36→    \n    37→@dataclass\n    38→class HybridSearchConfig:\n    39→    """"""Configuration for hybrid search.""""""\n    40→    # Weight configuration\n    41→    bm25_weight: float = 0.5\n    42→    semantic_weight: float = 0.3\n    43→    fuzzy_weight: float = 0.2\n    44→    \n    45→    # RRF parameters\n    46→    rrf_k: int = 60  # Reciprocal Rank Fusion constant\n    47→    \n    48→    # Search parameters\n    49→    enable_bm25: bool = True\n    50→    enable_semantic: bool = True\n    51→    enable_fuzzy: bool = True\n    52→    \n    53→    # Result limits\n    54→    individual_limit: int = 50  # Results per search method\n    55→    final_limit: int = 20  # Final results after fusion\n    56→    \n    57→    # Optimization\n    58→    parallel_execution: bool = True\n    59→    cache_results: bool = True\n    60→    \n    61→    # Minimum scores\n    62→    min_bm25_score: float = -10.0\n    63→    min_semantic_score: float = 0.5\n    64→    min_fuzzy_score: float = 0.3\n    65→\n    66→\n    67→class HybridSearch:\n    68→    """"""\n    69→    Hybrid search combining multiple search methods with reciprocal rank fusion.\n    70→    \n    71→    This class orchestrates multiple search backends (BM25, semantic, fuzzy) and\n    72→    combines their results using configurable fusion strategies.\n    73→    """"""\n    74→    \n    75→    def __init__(\n    76→        self,\n    77→        storage: SQLiteStore,\n    78→        bm25_indexer: Optional[BM25Indexer] = None,\n    79→        semantic_indexer: Optional[SemanticIndexer] = None,\n    80→        fuzzy_indexer: Optional[Any] = None,\n    81→        config: Optional[HybridSearchConfig] = None,\n    82→        reranking_settings: Optional[RerankingSettings] = None\n    83→    ):\n    84→        """"""\n    85→        Initialize hybrid search.\n    86→        \n    87→        Args:\n    88→            storage: SQLite storage backend\n    89→            bm25_indexer: BM25 full-text search indexer\n    90→            semantic_indexer: Semantic/vector search indexer\n    91→            fuzzy_indexer: Fuzzy search indexer\n    92→            config: Hybrid search configuration\n    93→            reranking_settings: Settings for result reranking\n    94→        """"""\n    95→        self.storage = storage\n    96→        self.bm25_indexer = bm25_indexer\n    97→        self.semantic_indexer = semantic_indexer\n    98→        self.fuzzy_indexer = fuzzy_indexer\n    99→        self.config = config or HybridSearchConfig()\n   100→        self.reranking_settings = reranking_settings\n   101→        \n   102→        # Initialize reranker if enabled\n   103→        self.reranker: Optional[IReranker] = None\n   104→        if reranking_settings and reranking_settings.enabled:\n   105→            self._initialize_reranker()\n   106→        \n   107→        # Result cache\n   108→        self._result_cache: Dict[str, List[SearchResult]] = {}\n   109→        \n   110→        # Statistics\n   111→        self._search_stats = defaultdict(int)\n   112→    \n   113→    def _initialize_reranker(self):\n   114→        """"""Initialize the reranker based on settings.""""""\n   115→        try:\n   116→            factory = RerankerFactory()\n   117→            config = {\n   118→                \'cohere_api_key\': self.reranking_settings.cohere_api_key,\n   119→                \'model\': self.reranking_settings.cohere_model,\n   120→                \'device\': self.reranking_settings.cross_encoder_device,\n   121→                \'primary_type\': self.reranking_settings.hybrid_primary_type,\n   122→                \'fallback_type\': self.reranking_settings.hybrid_fallback_type,\n   123→                \'weight_primary\': self.reranking_settings.hybrid_primary_weight,\n   124→                \'weight_fallback\': self.reranking_settings.hybrid_fallback_weight,\n   125→                \'cache_ttl\': self.reranking_settings.cache_ttl\n   126→            }\n   127→            \n   128→            self.reranker = factory.create_reranker(\n   129→                self.reranking_settings.reranker_type,\n   130→                config\n   131→            )\n   132→            \n   133→            # Initialize reranker asynchronously will be done on first use\n   134→            logger.info(f""Initialized {self.reranking_settings.reranker_type} reranker"")\n   135→        except Exception as e:\n   136→            logger.error(f""Failed to initialize reranker: {e}"")\n   137→            self.reranker = None\n   138→        \n   139→    async def search(\n   140→        self,\n   141→        query: str,\n   142→        query_type: Optional[QueryType] = None,\n   143→        filters: Optional[Dict[str, Any]] = None,\n   144→        limit: Optional[int] = None\n   145→    ) -> List[Dict[str, Any]]:\n   146→        """"""\n   147→        Perform hybrid search combining multiple search methods.\n   148→        \n   149→        Args:\n   150→            query: Search query\n   151→            query_type: Optional query type hint\n   152→            filters: Optional filters (language, file path, etc.)\n   153→            limit: Maximum number of results\n   154→            \n   155→        Returns:\n   156→            List of combined search results\n   157→        """"""\n   158→        limit = limit or self.config.final_limit\n   159→        \n   160→        # Check cache if enabled\n   161→        cache_key = self._get_cache_key(query, filters)\n   162→        if self.config.cache_results and cache_key in self._result_cache:\n   163→            self._search_stats[\'cache_hits\'] += 1\n   164→            cached_results = self._result_cache[cache_key]\n   165→            return self._format_results(cached_results[:limit])\n   166→        \n   167→        # Collect results from different search methods\n   168→        all_results = []\n   169→        \n   170→        if self.config.parallel_execution:\n   171→            # Execute searches in parallel\n   172→            all_results = await self._parallel_search(query, query_type, filters)\n   173→        else:\n   174→            # Execute searches sequentially\n   175→            all_results = await self._sequential_search(query, query_type, filters)\n   176→        \n   177→        # Combine results using reciprocal rank fusion\n   178→        combined_results = self._reciprocal_rank_fusion(all_results)\n   179→        \n   180→        # Apply reranking if enabled\n   181→        if self.reranker and self.reranking_settings and self.reranking_settings.enabled:\n   182→            combined_results = await self._rerank_results(query, combined_results)\n   183→        \n   184→        # Apply post-processing\n   185→        final_results = self._post_process_results(combined_results, limit)\n   186→        \n   187→        # Cache results if enabled\n   188→        if self.config.cache_results:\n   189→            self._result_cache[cache_key] = final_results\n   190→            self._cleanup_cache()\n   191→        \n   192→        # Update statistics\n   193→        self._search_stats[\'total_searches\'] += 1\n   194→        \n   195→        return self._format_results(final_results)\n   196→    \n   197→    async def _parallel_search(\n   198→        self,\n   199→        query: str,\n   200→        query_type: Optional[QueryType],\n   201→        filters: Optional[Dict[str, Any]]\n   202→    ) -> List[List[SearchResult]]:\n   203→        """"""Execute searches in parallel.""""""\n   204→        tasks = []\n   205→        \n   206→        if self.config.enable_bm25 and self.bm25_indexer:\n   207→            tasks.append(self._search_bm25(query, filters))\n   208→            \n   209→        if self.config.enable_semantic and self.semantic_indexer:\n   210→            tasks.append(self._search_semantic(query, filters))\n   211→            \n   212→        if self.config.enable_fuzzy and self.fuzzy_indexer:\n   213→            tasks.append(self._search_fuzzy(query, filters))\n   214→        \n   215→        # Execute all searches in parallel\n   216→        results = await asyncio.gather(*tasks, return_exceptions=True)\n   217→        \n   218→        # Filter out exceptions and empty results\n   219→        valid_results = []\n   220→        for i, result in enumerate(results):\n   221→            if isinstance(result, Exception):\n   222→                logger.error(f""Search error in method {i}: {result}"")\n   223→            elif result:\n   224→                valid_results.append(result)\n   225→                \n   226→        return valid_results\n   227→    \n   228→    async def _sequential_search(\n   229→        self,\n   230→        query: str,\n   231→        query_type: Optional[QueryType],\n   232→        filters: Optional[Dict[str, Any]]\n   233→    ) -> List[List[SearchResult]]:\n   234→        """"""Execute searches sequentially.""""""\n   235→        all_results = []\n   236→        \n   237→        if self.config.enable_bm25 and self.bm25_indexer:\n   238→            try:\n   239→                bm25_results = await self._search_bm25(query, filters)\n   240→                if bm25_results:\n   241→                    all_results.append(bm25_results)\n   242→            except Exception as e:\n   243→                logger.error(f""BM25 search error: {e}"")\n   244→                \n   245→        if self.config.enable_semantic and self.semantic_indexer:\n   246→            try:\n   247→                semantic_results = await self._search_semantic(query, filters)\n   248→                if semantic_results:\n   249→                    all_results.append(semantic_results)\n   250→            except Exception as e:\n   251→                logger.error(f""Semantic search error: {e}"")\n   252→                \n   253→        if self.config.enable_fuzzy and self.fuzzy_indexer:\n   254→            try:\n   255→                fuzzy_results = await self._search_fuzzy(query, filters)\n   256→                if fuzzy_results:\n   257→                    all_results.append(fuzzy_results)\n   258→            except Exception as e:\n   259→                logger.error(f""Fuzzy search error: {e}"")\n   260→                \n   261→        return all_results\n   262→    \n   263→    async def _search_bm25(self, query: str, filters: Optional[Dict[str, Any]]) -> List[SearchResult]:\n   264→        """"""Perform BM25 search.""""""\n   265→        # Run BM25 search in thread pool since it\'s synchronous\n   266→        loop = asyncio.get_event_loop()\n   267→        \n   268→        def run_search():\n   269→            kwargs = filters or {}\n   270→            results = self.bm25_indexer.search(query, limit=self.config.individual_limit, **kwargs)\n   271→            \n   272→            search_results = []\n   273→            for r in results:\n   274→                if r[\'score\'] >= self.config.min_bm25_score:\n   275→                    search_results.append(SearchResult(\n   276→                        doc_id=r.get(\'filepath\', \'\'),\n   277→                        filepath=r.get(\'filepath\', \'\'),\n   278→                        score=r[\'score\'],\n   279→                        snippet=r.get(\'snippet\', \'\'),\n   280→                        metadata=r,\n   281→                        source=\'bm25\'\n   282→                    ))\n   283→            return search_results\n   284→        \n   285→        results = await loop.run_in_executor(None, run_search)\n   286→        self._search_stats[\'bm25_searches\'] += 1\n   287→        return results\n   288→    \n   289→    async def _search_semantic(self, query: str, filters: Optional[Dict[str, Any]]) -> List[SearchResult]:\n   290→        """"""Perform semantic search.""""""\n   291→        # Run semantic search\n   292→        loop = asyncio.get_event_loop()\n   293→        \n   294→        def run_search():\n   295→            # Semantic search with filters\n   296→            kwargs = {\n   297→                \'k\': self.config.individual_limit,\n   298→                \'threshold\': self.config.min_semantic_score\n   299→            }\n   300→            if filters:\n   301→                kwargs.update(filters)\n   302→                \n   303→            results = self.semantic_indexer.search(query, **kwargs)\n   304→            \n   305→            search_results = []\n   306→            for r in results:\n   307→                search_results.append(SearchResult(\n   308→                    doc_id=r.get(\'filepath\', \'\'),\n   309→                    filepath=r.get(\'filepath\', \'\'),\n   310→                    score=r.get(\'score\', 0.0),\n   311→                    snippet=r.get(\'content\', \'\')[:200],\n   312→                    metadata=r,\n   313→                    source=\'semantic\'\n   314→                ))\n   315→            return search_results\n   316→        \n   317→        results = await loop.run_in_executor(None, run_search)\n   318→        self._search_stats[\'semantic_searches\'] += 1\n   319→        return results\n   320→    \n   321→    async def _search_fuzzy(self, query: str, filters: Optional[Dict[str, Any]]) -> List[SearchResult]:\n   322→        """"""Perform fuzzy search.""""""\n   323→        loop = asyncio.get_event_loop()\n   324→        \n   325→        def run_search():\n   326→            # Fuzzy search\n   327→            results = self.fuzzy_indexer.search_fuzzy(\n   328→                query,\n   329→                max_results=self.config.individual_limit,\n   330→                threshold=self.config.min_fuzzy_score\n   331→            )\n   332→            \n   333→            search_results = []\n   334→            for r in results:\n   335→                if r[\'score\'] >= self.config.min_fuzzy_score:\n   336→                    search_results.append(SearchResult(\n   337→                        doc_id=r.get(\'file_path\', \'\'),\n   338→                        filepath=r.get(\'file_path\', \'\'),\n   339→                        score=r[\'score\'],\n   340→                        snippet=r.get(\'context\', \'\'),\n   341→                        metadata=r,\n   342→                        source=\'fuzzy\'\n   343→                    ))\n   344→            return search_results\n   345→        \n   346→        results = await loop.run_in_executor(None, run_search)\n   347→        self._search_stats[\'fuzzy_searches\'] += 1\n   348→        return results\n   349→    \n   350→    def _reciprocal_rank_fusion(self, result_lists: List[List[SearchResult]]) -> List[SearchResult]:\n   351→        """"""\n   352→        Combine results using Reciprocal Rank Fusion (RRF).\n   353→        \n   354→        RRF score = Σ(1 / (k + rank_i)) for each result list\n   355→        where k is a constant (typically 60) and rank_i is the rank in list i\n   356→        """"""\n   357→        # Track scores for each document\n   358→        doc_scores: Dict[str, float] = defaultdict(float)\n   359→        doc_info: Dict[str, SearchResult] = {}\n   360→        \n   361→        # Calculate RRF scores\n   362→        for result_list in result_lists:\n   363→            for rank, result in enumerate(result_list):\n   364→                doc_id = result.doc_id\n   365→                \n   366→                # RRF score for this ranking\n   367→                rrf_score = 1.0 / (self.config.rrf_k + rank + 1)\n   368→                \n   369→                # Apply source-specific weights\n   370→                if result.source == \'bm25\':\n   371→                    rrf_score *= self.config.bm25_weight\n   372→                elif result.source == \'semantic\':\n   373→                    rrf_score *= self.config.semantic_weight\n   374→                elif result.source == \'fuzzy\':\n   375→                    rrf_score *= self.config.fuzzy_weight\n   376→                \n   377→                doc_scores[doc_id] += rrf_score\n   378→                \n   379→                # Keep the result with the best individual score\n   380→                if doc_id not in doc_info or result.score > doc_info[doc_id].score:\n   381→                    doc_info[doc_id] = result\n   382→        \n   383→        # Create combined results\n   384→        combined = []\n   385→        for doc_id, combined_score in doc_scores.items():\n   386→            result = doc_info[doc_id]\n   387→            # Update score to combined RRF score\n   388→            result.score = combined_score\n   389→            combined.append(result)\n   390→        \n   391→        # Sort by combined score\n   392→        combined.sort(key=lambda x: x.score, reverse=True)\n   393→        \n   394→        return combined\n   395→    \n   396→    async def _rerank_results(self, query: str, results: List[SearchResult]) -> List[SearchResult]:\n   397→        """"""Rerank search results using the configured reranker.""""""\n   398→        if not self.reranker or not results:\n   399→            return results\n   400→        \n   401→        try:\n   402→            # Initialize reranker if not already done\n   403→            if not hasattr(self, \'_reranker_initialized\'):\n   404→                init_result = await self.reranker.initialize({})\n   405→                if not init_result.is_success:\n   406→                    logger.error(f""Failed to initialize reranker: {init_result.error}"")\n   407→                    return results\n   408→                self._reranker_initialized = True\n   409→            \n   410→            # Import SearchResult from reranker module to avoid circular dependency\n   411→            from .reranker import SearchResult as RerankSearchResult\n   412→            \n   413→            # Convert our SearchResult to reranker\'s SearchResult format\n   414→            reranker_results = []\n   415→            for r in results:\n   416→                # Create a reranker-compatible search result\n   417→                reranker_result = RerankSearchResult(\n   418→                    file_path=r.filepath,\n   419→                    line=r.metadata.get(\'line\', 1),\n   420→                    column=r.metadata.get(\'column\', 0),\n   421→                    snippet=r.snippet,\n   422→                    match_type=r.source,  # Use source as match type\n   423→                    score=r.score,\n   424→                    context=r.metadata.get(\'context\', \'\')\n   425→                )\n   426→                reranker_results.append(reranker_result)\n   427→            \n   428→            # Perform reranking\n   429→            top_k = self.reranking_settings.top_k if self.reranking_settings else len(reranker_results)\n   430→            rerank_result = await self.reranker.rerank(\n   431→                query,\n   432→                reranker_results,\n   433→                top_k=top_k\n   434→            )\n   435→            \n   436→            if not rerank_result.is_success:\n   437→                logger.warning(f""Reranking failed: {rerank_result.error}"")\n   438→                return results\n   439→            \n   440→            # Handle the new RerankResult structure\n   441→            # rerank_result.data is a RerankResult object with \'results\' list and \'metadata\' dict\n   442→            rerank_data = rerank_result.data\n   443→            if not rerank_data:\n   444→                logger.warning(""Reranking returned no data"")\n   445→                return results\n   446→                \n   447→            if not hasattr(rerank_data, \'results\'):\n   448→                logger.error(""Unexpected rerank result structure"")\n   449→                return results\n   450→                \n   451→            if not rerank_data.results:\n   452→                logger.warning(""Reranking returned empty results"")\n   453→                return results\n   454→                \n   455→            # Convert back to our SearchResult format, preserving all original metadata\n   456→            reranked_results = []\n   457→            for rerank_item in rerank_data.results:\n   458→                # Validate original_rank is within bounds\n   459→                if rerank_item.original_rank < 0 or rerank_item.original_rank >= len(results):\n   460→                    logger.warning(f""Invalid original_rank {rerank_item.original_rank} for {len(results)} results"")\n   461→                    continue\n   462→                    \n   463→                # Get the original result using the original_rank\n   464→                original = results[rerank_item.original_rank]\n   465→                \n   466→                # Create a new SearchResult with updated score but preserved metadata\n   467→                updated_result = SearchResult(\n   468→                    doc_id=original.doc_id,\n   469→                    filepath=original.filepath,\n   470→                    score=rerank_item.rerank_score,  # Use the new rerank score\n   471→                    snippet=original.snippet,\n   472→                    metadata=original.metadata.copy(),  # Preserve all original metadata\n   473→                    source=original.source\n   474→                )\n   475→                \n   476→                # Add reranking metadata\n   477→                updated_result.metadata[\'original_score\'] = original.score\n   478→                updated_result.metadata[\'rerank_score\'] = rerank_item.rerank_score\n   479→                updated_result.metadata[\'original_rank\'] = rerank_item.original_rank\n   480→                updated_result.metadata[\'new_rank\'] = rerank_item.new_rank\n   481→                if rerank_item.explanation:\n   482→                    updated_result.metadata[\'rerank_explanation\'] = rerank_item.explanation\n   483→                \n   484→                reranked_results.append(updated_result)\n   485→            \n   486→            # Sort by new rank to ensure proper ordering\n   487→            reranked_results.sort(key=lambda x: x.metadata.get(\'new_rank\', float(\'inf\')))\n   488→            \n   489→            # Log reranking metadata if available\n   490→            if hasattr(rerank_data, \'metadata\') and rerank_data.metadata:\n   491→                logger.debug(f""Reranked {len(reranked_results)} results using {rerank_data.metadata.get(\'reranker\', \'unknown\')} reranker"")\n   492→            else:\n   493→                logger.debug(f""Reranked {len(reranked_results)} results"")\n   494→            \n   495→            return reranked_results\n   496→            \n   497→        except Exception as e:\n   498→            logger.error(f""Error during reranking: {e}"", exc_info=True)\n   499→            return results\n   500→    \n   501→    def _post_process_results(self, results: List[SearchResult], limit: int) -> List[SearchResult]:\n   502→        """"""Apply post-processing to results.""""""\n   503→        # Remove duplicates while preserving order\n   504→        seen = set()\n   505→        unique_results = []\n   506→        for result in results:\n   507→            if result.filepath not in seen:\n   508→                seen.add(result.filepath)\n   509→                unique_results.append(result)\n   510→        \n   511→        # Apply limit\n   512→        unique_results = unique_results[:limit]\n   513→        \n   514→        # Enhance snippets if needed\n   515→        for result in unique_results:\n   516→            if not result.snippet and result.filepath:\n   517→                # Try to generate a snippet\n   518→                result.snippet = self._generate_snippet(result.filepath, result.metadata)\n   519→        \n   520→        return unique_results\n   521→    \n   522→    def _generate_snippet(self, filepath: str, metadata: Dict[str, Any]) -> str:\n   523→        """"""Generate a snippet for a result.""""""\n   524→        # This is a placeholder - in practice, you\'d read the file\n   525→        # and extract relevant context\n   526→        return f""File: {filepath}""\n   527→    \n   528→    def _format_results(self, results: List[SearchResult]) -> List[Dict[str, Any]]:\n   529→        """"""Format results for output, including all metadata.""""""\n   530→        formatted = []\n   531→        for i, result in enumerate(results):\n   532→            # Ensure metadata exists\n   533→            metadata = result.metadata if result.metadata else {}\n   534→            \n   535→            # Build base result with all fields\n   536→            formatted_result = {\n   537→                \'rank\': i + 1,\n   538→                \'filepath\': result.filepath,\n   539→                \'score\': result.score,\n   540→                \'snippet\': result.snippet,\n   541→                \'source\': result.source,\n   542→                # Include all metadata fields\n   543→                \'line\': metadata.get(\'line\', 1),\n   544→                \'column\': metadata.get(\'column\', 0),\n   545→                \'context\': metadata.get(\'context\', \'\'),\n   546→                \'match_type\': metadata.get(\'match_type\', result.source),\n   547→                # Include language and symbol information if present\n   548→                \'language\': metadata.get(\'language\', \'\'),\n   549→                \'symbol\': metadata.get(\'symbol\', \'\'),\n   550→                \'symbol_type\': metadata.get(\'symbol_type\', \'\'),\n   551→                # Include reranking information if present\n   552→                \'original_score\': metadata.get(\'original_score\', result.score),\n   553→                \'rerank_score\': metadata.get(\'rerank_score\'),\n   554→                \'original_rank\': metadata.get(\'original_rank\'),\n   555→                \'new_rank\': metadata.get(\'new_rank\'),\n   556→                \'rerank_explanation\': metadata.get(\'rerank_explanation\'),\n   557→                # Include any additional metadata\n   558→                \'metadata\': metadata\n   559→            }\n   560→            \n   561→            # Remove None values and empty strings for cleaner output\n   562→            formatted_result = {k: v for k, v in formatted_result.items() \n   563→                              if v is not None and (not isinstance(v, str) or v != \'\')}\n   564→            \n   565→            formatted.append(formatted_result)\n   566→        return formatted\n   567→    \n   568→    def _get_cache_key(self, query: str, filters: Optional[Dict[str, Any]]) -> str:\n   569→        """"""Generate cache key for a query.""""""\n   570→        import hashlib\n   571→        key_parts = [query]\n   572→        if filters:\n   573→            key_parts.extend(f""{k}:{v}"" for k, v in sorted(filters.items()))\n   574→        key_string = ""|"".join(key_parts)\n   575→        return hashlib.md5(key_string.encode()).hexdigest()\n   576→    \n   577→    def _cleanup_cache(self):\n   578→        """"""Clean up cache if it gets too large.""""""\n   579→        max_cache_size = 1000\n   580→        if len(self._result_cache) > max_cache_size:\n   581→            # Remove oldest entries (simple FIFO)\n   582→            entries_to_remove = len(self._result_cache) - max_cache_size // 2\n   583→            for key in list(self._result_cache.keys())[:entries_to_remove]:\n   584→                del self._result_cache[key]\n   585→    \n   586→    # Configuration methods\n   587→    \n   588→    def set_weights(self, bm25: float = None, semantic: float = None, fuzzy: float = None):\n   589→        """"""\n   590→        Update search method weights.\n   591→        \n   592→        Args:\n   593→            bm25: Weight for BM25 search (0-1)\n   594→            semantic: Weight for semantic search (0-1)\n   595→            fuzzy: Weight for fuzzy search (0-1)\n   596→        """"""\n   597→        if bm25 is not None:\n   598→            self.config.bm25_weight = max(0, min(1, bm25))\n   599→        if semantic is not None:\n   600→            self.config.semantic_weight = max(0, min(1, semantic))\n   601→        if fuzzy is not None:\n   602→            self.config.fuzzy_weight = max(0, min(1, fuzzy))\n   603→            \n   604→        # Normalize weights\n   605→        total = self.config.bm25_weight + self.config.semantic_weight + self.config.fuzzy_weight\n   606→        if total > 0:\n   607→            self.config.bm25_weight /= total\n   608→            self.config.semantic_weight /= total\n   609→            self.config.fuzzy_weight /= total\n   610→    \n   611→    def enable_methods(self, bm25: bool = None, semantic: bool = None, fuzzy: bool = None):\n   612→        """"""\n   613→        Enable or disable search methods.\n   614→        \n   615→        Args:\n   616→            bm25: Enable/disable BM25 search\n   617→            semantic: Enable/disable semantic search\n   618→            fuzzy: Enable/disable fuzzy search\n   619→        """"""\n   620→        if bm25 is not None:\n   621→            self.config.enable_bm25 = bm25\n   622→        if semantic is not None:\n   623→            self.config.enable_semantic = semantic\n   624→        if fuzzy is not None:\n   625→            self.config.enable_fuzzy = fuzzy\n   626→    \n   627→    def get_statistics(self) -> Dict[str, Any]:\n   628→        """"""Get search statistics.""""""\n   629→        stats = dict(self._search_stats)\n   630→        \n   631→        # Add cache statistics\n   632→        stats[\'cache_size\'] = len(self._result_cache)\n   633→        cache_hit_rate = 0\n   634→        if stats.get(\'total_searches\', 0) > 0:\n   635→            cache_hit_rate = stats.get(\'cache_hits\', 0) / stats[\'total_searches\']\n   636→        stats[\'cache_hit_rate\'] = cache_hit_rate\n   637→        \n   638→        # Add configuration info\n   639→        stats[\'config\'] = {\n   640→            \'weights\': {\n   641→                \'bm25\': self.config.bm25_weight,\n   642→                \'semantic\': self.config.semantic_weight,\n   643→                \'fuzzy\': self.config.fuzzy_weight\n   644→            },\n   645→            \'enabled_methods\': {\n   646→                \'bm25\': self.config.enable_bm25,\n   647→                \'semantic\': self.config.enable_semantic,\n   648→                \'fuzzy\': self.config.enable_fuzzy\n   649→            },\n   650→            \'rrf_k\': self.config.rrf_k\n   651→        }\n   652→        \n   653→        return stats\n   654→    \n   655→    def clear_cache(self):\n   656→        """"""Clear the result cache.""""""\n   657→        self._result_cache.clear()\n   658→        logger.info(""Hybrid search cache cleared"")\n   659→\n   660→\n   661→class HybridSearchOptimizer:\n   662→    """"""\n   663→    Optimizer for hybrid search parameters based on user feedback and performance.\n   664→    """"""\n   665→    \n   666→    def __init__(self, hybrid_search: HybridSearch):\n   667→        """"""\n   668→        Initialize the optimizer.\n   669→        \n   670→        Args:\n   671→            hybrid_search: HybridSearch instance to optimize\n   672→        """"""\n   673→        self.hybrid_search = hybrid_search\n   674→        self.feedback_history: List[Dict[str, Any]] = []\n   675→        self.performance_history: List[Dict[str, Any]] = []\n   676→        \n   677→    def record_feedback(self, query: str, selected_result: int, results: List[Dict[str, Any]]):\n   678→        """"""\n   679→        Record user feedback on search results.\n   680→        \n   681→        Args:\n   682→            query: The search query\n   683→            selected_result: Index of the result the user selected\n   684→            results: The search results shown to the user\n   685→        """"""\n   686→        feedback = {\n   687→            \'query\': query,\n   688→            \'selected_rank\': selected_result + 1,\n   689→            \'selected_source\': results[selected_result][\'source\'] if selected_result < len(results) else None,\n   690→            \'num_results\': len(results),\n   691→            \'timestamp\': datetime.now()\n   692→        }\n   693→        self.feedback_history.append(feedback)\n   694→        \n   695→        # Optimize weights after collecting enough feedback\n   696→        if len(self.feedback_history) % 10 == 0:\n   697→            self.optimize_weights()\n   698→    \n   699→    def record_performance(self, query: str, search_time_ms: float, num_results: int):\n   700→        """"""\n   701→        Record search performance metrics.\n   702→        \n   703→        Args:\n   704→            query: The search query\n   705→            search_time_ms: Time taken for search in milliseconds\n   706→            num_results: Number of results returned\n   707→        """"""\n   708→        self.performance_history.append({\n   709→            \'query\': query,\n   710→            \'search_time_ms\': search_time_ms,\n   711→            \'num_results\': num_results,\n   712→            \'timestamp\': datetime.now()\n   713→        })\n   714→    \n   715→    def optimize_weights(self):\n   716→        """"""Optimize search weights based on feedback history.""""""\n   717→        if len(self.feedback_history) < 10:\n   718→            return\n   719→        \n   720→        # Count selections by source\n   721→        source_selections = defaultdict(int)\n   722→        source_ranks = defaultdict(list)\n   723→        \n   724→        for feedback in self.feedback_history[-100:]:  # Last 100 feedbacks\n   725→            source = feedback.get(\'selected_source\')\n   726→            if source:\n   727→                source_selections[source] += 1\n   728→                source_ranks[source].append(feedback[\'selected_rank\'])\n   729→        \n   730→        # Calculate average rank for each source\n   731→        avg_ranks = {}\n   732→        for source, ranks in source_ranks.items():\n   733→            avg_ranks[source] = sum(ranks) / len(ranks) if ranks else float(\'inf\')\n   734→        \n   735→        # Update weights based on selection frequency and average rank\n   736→        total_selections = sum(source_selections.values())\n   737→        if total_selections > 0:\n   738→            # Base weights on selection frequency\n   739→            bm25_weight = source_selections.get(\'bm25\', 0) / total_selections\n   740→            semantic_weight = source_selections.get(\'semantic\', 0) / total_selections\n   741→            fuzzy_weight = source_selections.get(\'fuzzy\', 0) / total_selections\n   742→            \n   743→            # Adjust based on average rank (lower is better)\n   744→            rank_factor = 0.2\n   745→            if \'bm25\' in avg_ranks:\n   746→                bm25_weight *= (1 + rank_factor * (1 / avg_ranks[\'bm25\']))\n   747→            if \'semantic\' in avg_ranks:\n   748→                semantic_weight *= (1 + rank_factor * (1 / avg_ranks[\'semantic\']))\n   749→            if \'fuzzy\' in avg_ranks:\n   750→                fuzzy_weight *= (1 + rank_factor * (1 / avg_ranks[\'fuzzy\']))\n   751→            \n   752→            # Apply new weights\n   753→            self.hybrid_search.set_weights(bm25_weight, semantic_weight, fuzzy_weight)\n   754→            \n   755→            logger.info(f""Optimized weights - BM25: {bm25_weight:.3f}, ""\n   756→                       f""Semantic: {semantic_weight:.3f}, Fuzzy: {fuzzy_weight:.3f}"")\n   757→    \n   758→    def get_optimization_report(self) -> Dict[str, Any]:\n   759→        """"""Get a report on optimization performance.""""""\n   760→        if not self.feedback_history:\n   761→            return {\'status\': \'no_data\'}\n   762→        \n   763→        # Analyze feedback\n   764→        source_stats = defaultdict(lambda: {\'count\': 0, \'avg_rank\': 0})\n   765→        for feedback in self.feedback_history:\n   766→            source = feedback.get(\'selected_source\')\n   767→            if source:\n   768→                stats = source_stats[source]\n   769→                stats[\'count\'] += 1\n   770→                stats[\'avg_rank\'] = (\n   771→                    (stats[\'avg_rank\'] * (stats[\'count\'] - 1) + feedback[\'selected_rank\']) \n   772→                    / stats[\'count\']\n   773→                )\n   774→        \n   775→        # Analyze performance\n   776→        avg_search_time = 0\n   777→        if self.performance_history:\n   778→            avg_search_time = sum(p[\'search_time_ms\'] for p in self.performance_history) / len(self.performance_history)\n   779→        \n   780→        return {\n   781→            \'feedback_count\': len(self.feedback_history),\n   782→            \'source_statistics\': dict(source_stats),\n   783→            \'average_search_time_ms\': avg_search_time,\n   784→            \'current_weights\': {\n   785→                \'bm25\': self.hybrid_search.config.bm25_weight,\n   786→                \'semantic\': self.hybrid_search.config.semantic_weight,\n   787→                \'fuzzy\': self.hybrid_search.config.fuzzy_weight\n   788→            }\n   789→        }\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,native,0,True,5239,0,0,0,5239,0,0,0,236,236,51722,25622,236,0.5,0.25,0,,,,,transcript_response
41333a35-f5b8-4f90-875a-e853cef49417,2025-06-26T07:37:34.020Z,transcript_41333a35-f5b8-4f90-875a-e853cef49417,"{'type': 'tool_result', 'content': ""Claude requested permissions to use Bash, but you haven't granted it yet."", 'is_error': True, 'tool_use_id': 'toolu_01PsXYECXJKL1mQGBxU1xTBU'}",transcript_query,unknown,native,0,True,8,0,0,0,8,0,0,0,157,157,77344,5558,157,0.5,0.25,0,,,,,transcript_response
41333a35-f5b8-4f90-875a-e853cef49417,2025-06-26T07:37:41.059Z,transcript_41333a35-f5b8-4f90-875a-e853cef49417,"{'tool_use_id': 'toolu_01Sqex9qf9pdjZKhP4Dn3zEU', 'type': 'tool_result', 'content': 'Found 51 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_indexer.py'}",transcript_query,unknown,native,0,True,8,0,0,0,8,0,0,0,210,210,82902,3606,210,0.5,0.25,0,,,,,transcript_response
41333a35-f5b8-4f90-875a-e853cef49417,2025-06-26T07:37:50.002Z,transcript_41333a35-f5b8-4f90-875a-e853cef49417,"{'tool_use_id': 'toolu_01MvVEFNoB8xyZZgDPLFXiKk', 'type': 'tool_result', 'content': '     1→""""""\n     2→Reranking Module for Search Result Optimization\n     3→\n     4→This module provides implementations for reranking search results to improve relevance.\n     5→It supports both Cohere\'s reranking API and local cross-encoder models as fallback.\n     6→""""""\n     7→\n     8→import os\n     9→import logging\n    10→import asyncio\n    11→from typing import Dict, List, Optional, Any, Tuple\n    12→from dataclasses import dataclass\n    13→import numpy as np\n    14→from abc import ABC, abstractmethod\n    15→\n    16→# Define interfaces inline for now\n    17→from abc import ABC, abstractmethod\n    18→from dataclasses import dataclass as dc\n    19→from typing import Protocol\n    20→\n    21→# Define SearchResult inline\n    22→@dc \n    23→class SearchResult:\n    24→    """"""Search result information""""""\n    25→    file_path: str\n    26→    line: int\n    27→    column: int\n    28→    snippet: str\n    29→    match_type: str  # exact, fuzzy, semantic\n    30→    score: float\n    31→    context: Optional[str] = None\n    32→\n    33→# Define RerankItem to wrap original SearchResult\n    34→@dc\n    35→class RerankItem:\n    36→    """"""A single reranked item that preserves the complete original SearchResult""""""\n    37→    original_result: SearchResult  # Complete original SearchResult\n    38→    rerank_score: float  # The reranking score\n    39→    original_rank: int  # Original position in results\n    40→    new_rank: int  # New position after reranking\n    41→    explanation: Optional[str] = None  # Optional explanation for the ranking\n    42→\n    43→# Define RerankResult with proper structure\n    44→@dc\n    45→class RerankResult:\n    46→    """"""Result from reranking operation""""""\n    47→    results: List[RerankItem]  # List of reranked items\n    48→    metadata: Dict[str, Any]  # Metadata about the reranking operation\n    49→\n    50→# Define IReranker interface\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,native,0,True,8,0,0,0,8,0,0,0,202,202,86508,1446,202,0.5,0.25,0,,,,,transcript_response
41333a35-f5b8-4f90-875a-e853cef49417,2025-06-26T07:37:56.950Z,transcript_41333a35-f5b8-4f90-875a-e853cef49417,"{'tool_use_id': 'toolu_0136yzG3KyXDvkvTqDKa2GVN', 'type': 'tool_result', 'content': '     1→#!/usr/bin/env python3\n     2→""""""Parallel test runner for document processing features.""""""\n     3→\n     4→import asyncio\n     5→import json\n     6→import time\n     7→import traceback\n     8→from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n     9→from dataclasses import dataclass, field\n    10→from datetime import datetime\n    11→from pathlib import Path\n    12→from typing import Dict, List, Optional, Tuple\n    13→import multiprocessing\n    14→import subprocess\n    15→import sys\n    16→import os\n    17→\n    18→# Add project root to path\n    19→sys.path.insert(0, str(Path(__file__).parent))\n    20→\n    21→\n    22→@dataclass\n    23→class TestResult:\n    24→    """"""Test execution result.""""""\n    25→    name: str\n    26→    group: str\n    27→    status: str  # \'passed\', \'failed\', \'error\'\n    28→    duration: float\n    29→    message: str = """"\n    30→    errors: List[str] = field(default_factory=list)\n    31→    metrics: Dict[str, float] = field(default_factory=dict)\n    32→\n    33→\n    34→@dataclass\n    35→class TestGroup:\n    36→    """"""Group of related tests to run in parallel.""""""\n    37→    name: str\n    38→    tests: List[str]\n    39→    max_workers: int = 4\n    40→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,native,0,True,8,0,0,0,8,0,0,0,2839,2839,87954,1244,2839,0.5,0.25,0,,,,,transcript_response
c6133498-4b89-4f96-b12c-b76e3435cbcf,2025-06-26T07:31:09.559Z,transcript_c6133498-4b89-4f96-b12c-b76e3435cbcf,Find the EnhancedDispatcher class and show its main methods,transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,1,1,13172,748,1,0.5,0.25,0,,,,,transcript_response
c6133498-4b89-4f96-b12c-b76e3435cbcf,2025-06-26T07:31:13.313Z,transcript_c6133498-4b89-4f96-b12c-b76e3435cbcf,"{'tool_use_id': 'toolu_01KT4mFAJVK3N8vkjF5ZBwmz', 'type': 'tool_result', 'content': 'Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/level4/path_management.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/path_management_architecture.md'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,26,26,13920,447,26,0.5,0.25,0,,,,,transcript_response
c6133498-4b89-4f96-b12c-b76e3435cbcf,2025-06-26T07:31:18.205Z,transcript_c6133498-4b89-4f96-b12c-b76e3435cbcf,"{'tool_use_id': 'toolu_0172221y9RdEtmQniTvUUMQi', 'type': 'tool_result', 'content': '     1→""""""Enhanced dispatcher with dynamic plugin loading via PluginFactory.""""""\n     2→from pathlib import Path\n     3→from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4→import logging\n     5→import hashlib\n     6→import time\n     7→import re\n     8→from datetime import datetime\n     9→import os\n    10→\n    11→from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12→from ..plugins.plugin_factory import PluginFactory\n    13→from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14→from ..storage.sqlite_store import SQLiteStore\n    15→from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16→from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17→# Note: We\'ve removed ignore pattern checks to allow indexing ALL files\n    18→# Filtering happens only during export via SecureIndexExporter\n    19→# from ..core.ignore_patterns import get_ignore_manager\n    20→\n    21→logger = logging.getLogger(__name__)\n    22→\n    23→\n    24→class EnhancedDispatcher:\n    25→    """"""Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.""""""\n    26→    \n    27→    # Document query patterns - common documentation search terms\n    28→    DOCUMENT_QUERY_PATTERNS = [\n    29→        r\'\\b(how\\s+to|howto)\\b\',\n    30→        r\'\\b(getting\\s+started|get\\s+started)\\b\',\n    31→        r\'\\b(installation|install|setup)\\b\',\n    32→        r\'\\b(configuration|configure|config)\\b\',\n    33→        r\'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b\',\n    34→        r\'\\b(tutorial|guide|walkthrough)\\b\',\n    35→        r\'\\b(example|sample|snippet)\\b\',\n    36→        r\'\\b(readme|documentation|docs)\\b\',\n    37→        r\'\\b(usage|use\\s+case|using)\\b\',\n    38→        r\'\\b(reference|manual)\\b\',\n    39→        r\'\\b(faq|frequently\\s+asked)\\b\',\n    40→        r\'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b\',\n    41→        r\'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b\',\n    42→        r\'\\b(architecture|design|overview)\\b\',\n    43→        r\'\\b(changelog|release\\s+notes|migration)\\b\'\n    44→    ]\n    45→    \n    46→    # Documentation file patterns\n    47→    DOCUMENTATION_FILE_PATTERNS = [\n    48→        r\'readme(\\.\\w+)?$\',\n    49→        r\'changelog(\\.\\w+)?$\',\n    50→        r\'contributing(\\.\\w+)?$\',\n    51→        r\'license(\\.\\w+)?$\',\n    52→        r\'install(\\.\\w+)?$\',\n    53→        r\'setup(\\.\\w+)?$\',\n    54→        r\'guide(\\.\\w+)?$\',\n    55→        r\'tutorial(\\.\\w+)?$\',\n    56→        r\'\\.md$\',\n    57→        r\'\\.rst$\',\n    58→        r\'\\.txt$\',\n    59→        r\'docs?/\',\n    60→        r\'documentation/\'\n    61→    ]\n    62→    \n    63→    def __init__(self, \n    64→                 plugins: Optional[List[IPlugin]] = None,\n    65→                 sqlite_store: Optional[SQLiteStore] = None,\n    66→                 enable_advanced_features: bool = True,\n    67→                 use_plugin_factory: bool = True,\n    68→                 lazy_load: bool = True,\n    69→                 semantic_search_enabled: bool = True):\n    70→        """"""Initialize the enhanced dispatcher.\n    71→        \n    72→        Args:\n    73→            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74→            sqlite_store: SQLite store for plugin persistence\n    75→            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76→            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77→            lazy_load: Whether to lazy-load plugins on demand\n    78→            semantic_search_enabled: Whether to enable semantic search in plugins\n    79→        """"""\n    80→        self._sqlite_store = sqlite_store\n    81→        self._enable_advanced = enable_advanced_features\n    82→        self._use_factory = use_plugin_factory\n    83→        self._lazy_load = lazy_load\n    84→        self._semantic_enabled = semantic_search_enabled\n    85→        \n    86→        # Plugin storage\n    87→        self._plugins: List[IPlugin] = []\n    88→        self._by_lang: Dict[str, IPlugin] = {}\n    89→        self._loaded_languages: set[str] = set()\n    90→        \n    91→        # Cache for file hashes to avoid re-indexing unchanged files\n    92→        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93→        \n    94→        # Advanced components\n    95→        if self._enable_advanced:\n    96→            self._file_matcher = FileTypeMatcher()\n    97→            self._router = PluginRouter(self._file_matcher)\n    98→            self._aggregator = ResultAggregator()\n    99→        \n   100→        # Performance tracking\n   101→        self._operation_stats = {\n   102→            \'searches\': 0,\n   103→            \'lookups\': 0,\n   104→            \'indexings\': 0,\n   105→            \'total_time\': 0.0,\n   106→            \'plugins_loaded\': 0\n   107→        }\n   108→        \n   109→        # Initialize plugins\n   110→        if plugins:\n   111→            # Use provided plugins (backward compatibility)\n   112→            self._plugins = plugins\n   113→            self._by_lang = {p.lang: p for p in plugins}\n   114→            for plugin in plugins:\n   115→                self._loaded_languages.add(getattr(plugin, \'lang\', \'unknown\'))\n   116→            if self._enable_advanced:\n   117→                self._register_plugins_with_router()\n   118→        elif use_plugin_factory and not lazy_load:\n   119→            # Load all plugins immediately\n   120→            self._load_all_plugins()\n   121→        # If lazy_load is True, plugins will be loaded on demand\n   122→        \n   123→        # Compile document query patterns for performance\n   124→        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125→        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126→        \n   127→        logger.info(f""Enhanced dispatcher initialized with {len(self._plugins)} plugins"")\n   128→    \n   129→    def _load_all_plugins(self):\n   130→        """"""Load all available plugins using PluginFactory.""""""\n   131→        logger.info(""Loading all available plugins..."")\n   132→        \n   133→        all_plugins = PluginFactory.create_all_plugins(\n   134→            sqlite_store=self._sqlite_store,\n   135→            enable_semantic=self._semantic_enabled\n   136→        )\n   137→        \n   138→        for lang, plugin in all_plugins.items():\n   139→            self._plugins.append(plugin)\n   140→            self._by_lang[lang] = plugin\n   141→            self._loaded_languages.add(lang)\n   142→            self._operation_stats[\'plugins_loaded\'] += 1\n   143→        \n   144→        if self._enable_advanced:\n   145→            self._register_plugins_with_router()\n   146→        \n   147→        logger.info(f""Loaded {len(all_plugins)} plugins: {\', \'.join(sorted(all_plugins.keys()))}"")\n   148→    \n   149→    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150→        """"""Ensure a plugin for the given language is loaded.\n   151→        \n   152→        Args:\n   153→            language: Language code (e.g., \'python\', \'go\')\n   154→            \n   155→        Returns:\n   156→            Plugin instance or None if not available\n   157→        """"""\n   158→        # Normalize language\n   159→        language = language.lower().replace(\'-\', \'_\')\n   160→        \n   161→        # Check if already loaded\n   162→        if language in self._by_lang:\n   163→            return self._by_lang[language]\n   164→        \n   165→        # If not using factory or already tried to load, return None\n   166→        if not self._use_factory or language in self._loaded_languages:\n   167→            return None\n   168→        \n   169→        # Try to load the plugin\n   170→        try:\n   171→            logger.info(f""Lazy loading plugin for {language}"")\n   172→            plugin = PluginFactory.create_plugin(\n   173→                language,\n   174→                sqlite_store=self._sqlite_store,\n   175→                enable_semantic=self._semantic_enabled\n   176→            )\n   177→            \n   178→            # Add to collections\n   179→            self._plugins.append(plugin)\n   180→            self._by_lang[language] = plugin\n   181→            self._loaded_languages.add(language)\n   182→            self._operation_stats[\'plugins_loaded\'] += 1\n   183→            \n   184→            # Register with router if needed\n   185→            if self._enable_advanced:\n   186→                capabilities = self._detect_plugin_capabilities(plugin)\n   187→                self._router.register_plugin(plugin, capabilities)\n   188→            \n   189→            logger.info(f""Successfully loaded {language} plugin"")\n   190→            return plugin\n   191→            \n   192→        except ValueError as e:\n   193→            logger.warning(f""No plugin available for {language}: {e}"")\n   194→            self._loaded_languages.add(language)  # Mark as attempted\n   195→            return None\n   196→        except Exception as e:\n   197→            logger.error(f""Error loading plugin for {language}: {e}"")\n   198→            self._loaded_languages.add(language)  # Mark as attempted\n   199→            return None\n   200→    \n   201→    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202→        """"""Ensure a plugin is loaded for the given file.\n   203→        \n   204→        Args:\n   205→            path: File path\n   206→            \n   207→        Returns:\n   208→            Plugin instance or None if not available\n   209→        """"""\n   210→        # Get language from file extension\n   211→        extension = path.suffix.lower()\n   212→        language = get_language_by_extension(extension)\n   213→        \n   214→        if language:\n   215→            return self._ensure_plugin_loaded(language)\n   216→        \n   217→        # Fallback: try all loaded plugins\n   218→        for plugin in self._plugins:\n   219→            if plugin.supports(path):\n   220→                return plugin\n   221→        \n   222→        return None\n   223→    \n   224→    def _register_plugins_with_router(self):\n   225→        """"""Register plugins with the router and assign capabilities.""""""\n   226→        for plugin in self._plugins:\n   227→            # Determine capabilities based on plugin type/language\n   228→            capabilities = self._detect_plugin_capabilities(plugin)\n   229→            self._router.register_plugin(plugin, capabilities)\n   230→    \n   231→    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232→        """"""Detect capabilities for a plugin based on its language and features.""""""\n   233→        capabilities = []\n   234→        lang = getattr(plugin, \'lang\', \'unknown\')\n   235→        \n   236→        # Base capabilities all plugins have\n   237→        capabilities.append(PluginCapability(\n   238→            \'syntax_analysis\', \'1.0\', f\'{lang} syntax analysis\', \n   239→            priority=70, metadata={\'language\': lang}\n   240→        ))\n   241→        \n   242→        capabilities.append(PluginCapability(\n   243→            \'code_search\', \'1.0\', f\'{lang} code search\', \n   244→            priority=80, metadata={\'language\': lang}\n   245→        ))\n   246→        \n   247→        # Check for semantic search capability\n   248→        if hasattr(plugin, \'_enable_semantic\') and plugin._enable_semantic:\n   249→            capabilities.append(PluginCapability(\n   250→                \'semantic_search\', \'1.0\', f\'{lang} semantic search\', \n   251→                priority=90, metadata={\'language\': lang}\n   252→            ))\n   253→        \n   254→        # Language-specific capabilities\n   255→        if lang == \'python\':\n   256→            capabilities.extend([\n   257→                PluginCapability(\'refactoring\', \'1.0\', \'Python refactoring support\', 75),\n   258→                PluginCapability(\'type_analysis\', \'1.0\', \'Python type analysis\', 85)\n   259→            ])\n   260→        elif lang in [\'javascript\', \'typescript\']:\n   261→            capabilities.extend([\n   262→                PluginCapability(\'linting\', \'1.0\', \'JavaScript/TypeScript linting\', 85),\n   263→                PluginCapability(\'bundling_analysis\', \'1.0\', \'Module bundling analysis\', 70),\n   264→                PluginCapability(\'framework_support\', \'1.0\', \'Framework-specific support\', 75)\n   265→            ])\n   266→        elif lang in [\'c\', \'cpp\']:\n   267→            capabilities.extend([\n   268→                PluginCapability(\'compilation_analysis\', \'1.0\', \'Compilation analysis\', 80),\n   269→                PluginCapability(\'memory_analysis\', \'1.0\', \'Memory usage analysis\', 70),\n   270→                PluginCapability(\'performance_profiling\', \'1.0\', \'Performance profiling\', 75)\n   271→            ])\n   272→        elif lang in [\'go\', \'rust\']:\n   273→            capabilities.extend([\n   274→                PluginCapability(\'package_analysis\', \'1.0\', f\'{lang} package analysis\', 80),\n   275→                PluginCapability(\'concurrency_analysis\', \'1.0\', f\'{lang} concurrency analysis\', 75)\n   276→            ])\n   277→        elif lang in [\'java\', \'kotlin\', \'scala\']:\n   278→            capabilities.extend([\n   279→                PluginCapability(\'jvm_analysis\', \'1.0\', \'JVM bytecode analysis\', 75),\n   280→                PluginCapability(\'build_tool_integration\', \'1.0\', \'Build tool integration\', 70)\n   281→            ])\n   282→        \n   283→        return capabilities\n   284→    \n   285→    @property\n   286→    def plugins(self):\n   287→        """"""Get the dictionary of loaded plugins by language.""""""\n   288→        return self._by_lang\n   289→    \n   290→    @property\n   291→    def supported_languages(self) -> List[str]:\n   292→        """"""Get list of all supported languages (loaded and available).""""""\n   293→        if self._use_factory:\n   294→            return PluginFactory.get_supported_languages()\n   295→        else:\n   296→            return list(self._by_lang.keys())\n   297→    \n   298→    def _match_plugin(self, path: Path) -> IPlugin:\n   299→        """"""Match a plugin for the given file path.""""""\n   300→        # Ensure plugin is loaded if using lazy loading\n   301→        if self._lazy_load and self._use_factory:\n   302→            plugin = self._ensure_plugin_for_file(path)\n   303→            if plugin:\n   304→                return plugin\n   305→        \n   306→        # Use advanced routing if available\n   307→        if self._enable_advanced and self._router:\n   308→            route_result = self._router.get_best_plugin(path)\n   309→            if route_result:\n   310→                return route_result.plugin\n   311→        \n   312→        # Fallback to basic matching\n   313→        for p in self._plugins:\n   314→            if p.supports(path):\n   315→                return p\n   316→        \n   317→        raise RuntimeError(f""No plugin found for {path}"")\n   318→    \n   319→    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320→        """"""Get all plugins that can handle a file with confidence scores.""""""\n   321→        # Ensure plugin is loaded if using lazy loading\n   322→        if self._lazy_load and self._use_factory:\n   323→            self._ensure_plugin_for_file(path)\n   324→        \n   325→        if self._enable_advanced and self._router:\n   326→            route_results = self._router.route_file(path)\n   327→            return [(result.plugin, result.confidence) for result in route_results]\n   328→        else:\n   329→            # Basic fallback\n   330→            matching_plugins = []\n   331→            for plugin in self._plugins:\n   332→                if plugin.supports(path):\n   333→                    matching_plugins.append((plugin, 1.0))\n   334→            return matching_plugins\n   335→    \n   336→    def lookup(self, symbol: str) -> SymbolDef | None:\n   337→        """"""Look up symbol definition across all plugins.""""""\n   338→        start_time = time.time()\n   339→        \n   340→        try:\n   341→            # For symbol lookup, we may need to search across all languages\n   342→            # Load all plugins if using lazy loading\n   343→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344→                self._load_all_plugins()\n   345→            \n   346→            if self._enable_advanced and self._aggregator:\n   347→                # Use advanced aggregation\n   348→                definitions_by_plugin = {}\n   349→                for plugin in self._plugins:\n   350→                    try:\n   351→                        definition = plugin.getDefinition(symbol)\n   352→                        definitions_by_plugin[plugin] = definition\n   353→                    except Exception as e:\n   354→                        logger.warning(f""Plugin {plugin.lang} failed to get definition for {symbol}: {e}"")\n   355→                        definitions_by_plugin[plugin] = None\n   356→                \n   357→                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358→                \n   359→                self._operation_stats[\'lookups\'] += 1\n   360→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   361→                \n   362→                return result\n   363→            else:\n   364→                # Fallback to basic lookup\n   365→                for p in self._plugins:\n   366→                    res = p.getDefinition(symbol)\n   367→                    if res:\n   368→                        self._operation_stats[\'lookups\'] += 1\n   369→                        self._operation_stats[\'total_time\'] += time.time() - start_time\n   370→                        return res\n   371→                return None\n   372→                \n   373→        except Exception as e:\n   374→            logger.error(f""Error in symbol lookup for {symbol}: {e}"", exc_info=True)\n   375→            return None\n   376→    \n   377→    def _is_document_query(self, query: str) -> bool:\n   378→        """"""Check if the query is looking for documentation.\n   379→        \n   380→        Args:\n   381→            query: Search query string\n   382→            \n   383→        Returns:\n   384→            True if this appears to be a documentation query\n   385→        """"""\n   386→        query_lower = query.lower()\n   387→        \n   388→        # Check against document query patterns\n   389→        for pattern in self._compiled_doc_patterns:\n   390→            if pattern.search(query_lower):\n   391→                return True\n   392→        \n   393→        # Check for question words at the beginning\n   394→        question_starters = [\'how\', \'what\', \'where\', \'when\', \'why\', \'can\', \'is\', \'does\', \'should\']\n   395→        first_word = query_lower.split()[0] if query_lower.split() else \'\'\n   396→        if first_word in question_starters:\n   397→            return True\n   398→        \n   399→        return False\n   400→    \n   401→    def _expand_document_query(self, query: str) -> List[str]:\n   402→        """"""Expand a document query with related terms for better search coverage.\n   403→        \n   404→        Args:\n   405→            query: Original search query\n   406→            \n   407→        Returns:\n   408→            List of expanded query variations\n   409→        """"""\n   410→        expanded_queries = [query]  # Always include original\n   411→        query_lower = query.lower()\n   412→        \n   413→        # Common expansions for documentation queries\n   414→        expansions = {\n   415→            \'install\': [\'installation\', \'setup\', \'getting started\', \'requirements\'],\n   416→            \'config\': [\'configuration\', \'configure\', \'settings\', \'options\', \'parameters\'],\n   417→            \'api\': [\'api documentation\', \'api reference\', \'endpoint\', \'method\'],\n   418→            \'how to\': [\'tutorial\', \'guide\', \'example\', \'usage\'],\n   419→            \'example\': [\'sample\', \'snippet\', \'demo\', \'code example\'],\n   420→            \'error\': [\'troubleshoot\', \'debug\', \'issue\', \'problem\', \'fix\'],\n   421→            \'getting started\': [\'quickstart\', \'tutorial\', \'introduction\', \'setup\'],\n   422→            \'guide\': [\'tutorial\', \'documentation\', \'walkthrough\', \'how to\'],\n   423→            \'usage\': [\'how to use\', \'example\', \'api\', \'reference\']\n   424→        }\n   425→        \n   426→        # Apply expansions\n   427→        for term, related_terms in expansions.items():\n   428→            if term in query_lower:\n   429→                for related in related_terms:\n   430→                    # Replace the term with related term\n   431→                    expanded = query_lower.replace(term, related)\n   432→                    if expanded != query_lower and expanded not in expanded_queries:\n   433→                        expanded_queries.append(expanded)\n   434→                \n   435→                # Also add queries with additional terms\n   436→                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437→                    expanded = f""{query} {related}""\n   438→                    if expanded not in expanded_queries:\n   439→                        expanded_queries.append(expanded)\n   440→        \n   441→        # Add file-specific searches for common documentation files\n   442→        if self._is_document_query(query):\n   443→            # Extract the main topic from the query\n   444→            topic_words = []\n   445→            for word in query.lower().split():\n   446→                if word not in [\'how\', \'to\', \'the\', \'a\', \'an\', \'is\', \'are\', \'what\', \'where\', \'when\']:\n   447→                    topic_words.append(word)\n   448→            \n   449→            if topic_words:\n   450→                topic = \' \'.join(topic_words[:2])  # Use first two topic words\n   451→                expanded_queries.extend([\n   452→                    f""README {topic}"",\n   453→                    f""{topic} documentation"",\n   454→                    f""{topic} docs"",\n   455→                    f""{topic} guide""\n   456→                ])\n   457→        \n   458→        # Remove duplicates while preserving order\n   459→        seen = set()\n   460→        unique_queries = []\n   461→        for q in expanded_queries:\n   462→            if q not in seen:\n   463→                seen.add(q)\n   464→                unique_queries.append(q)\n   465→        \n   466→        return unique_queries[:10]  # Limit to 10 queries max\n   467→    \n   468→    def _is_documentation_file(self, file_path: str) -> bool:\n   469→        """"""Check if a file path is likely a documentation file.\n   470→        \n   471→        Args:\n   472→            file_path: Path to check\n   473→            \n   474→        Returns:\n   475→            True if this appears to be a documentation file\n   476→        """"""\n   477→        path_lower = file_path.lower()\n   478→        \n   479→        for pattern in self._compiled_file_patterns:\n   480→            if pattern.search(path_lower):\n   481→                return True\n   482→        \n   483→        return False\n   484→    \n   485→    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486→        """"""Adjust ranking to prioritize documentation files for document queries.\n   487→        \n   488→        Args:\n   489→            query: Original search query\n   490→            results: List of aggregated results\n   491→            \n   492→        Returns:\n   493→            Re-ranked results with documentation prioritized\n   494→        """"""\n   495→        if not self._is_document_query(query):\n   496→            return results\n   497→        \n   498→        # Separate documentation and code results\n   499→        doc_results = []\n   500→        code_results = []\n   501→        \n   502→        for result in results:\n   503→            if self._is_documentation_file(result.primary_result.get(\'file\', \'\')):\n   504→                # Boost documentation files for document queries\n   505→                result.rank_score *= 1.5\n   506→                result.metadata[\'doc_boost\'] = True\n   507→                doc_results.append(result)\n   508→            else:\n   509→                code_results.append(result)\n   510→        \n   511→        # Sort each group by rank score\n   512→        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513→        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514→        \n   515→        # Combine with documentation files first\n   516→        return doc_results + code_results\n   517→    \n   518→    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519→        """"""Search for code and documentation across all plugins.""""""\n   520→        start_time = time.time()\n   521→        \n   522→        try:\n   523→            # For search, we may need to search across all languages\n   524→            # Load all plugins if using lazy loading\n   525→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526→                self._load_all_plugins()\n   527→            \n   528→            # Detect if this is a document query\n   529→            is_doc_query = self._is_document_query(query)\n   530→            \n   531→            # Expand query if it\'s a document query\n   532→            queries = [query]\n   533→            if is_doc_query:\n   534→                queries = self._expand_document_query(query)\n   535→                logger.info(f""Expanded document query \'{query}\' to {len(queries)} variations"")\n   536→                # Force semantic search for natural language queries\n   537→                semantic = True\n   538→            \n   539→            if self._enable_advanced and self._aggregator:\n   540→                # Use advanced aggregation\n   541→                all_results_by_plugin = {}\n   542→                opts = {""semantic"": semantic, ""limit"": limit * 2 if is_doc_query else limit}\n   543→                \n   544→                # Search with all query variations\n   545→                for search_query in queries:\n   546→                    for plugin in self._plugins:\n   547→                        try:\n   548→                            results = list(plugin.search(search_query, opts))\n   549→                            if results:\n   550→                                if plugin not in all_results_by_plugin:\n   551→                                    all_results_by_plugin[plugin] = []\n   552→                                all_results_by_plugin[plugin].extend(results)\n   553→                        except Exception as e:\n   554→                            logger.warning(f""Plugin {plugin.lang} failed to search for {search_query}: {e}"")\n   555→                \n   556→                # Deduplicate results per plugin\n   557→                for plugin, results in all_results_by_plugin.items():\n   558→                    seen = set()\n   559→                    unique_results = []\n   560→                    for result in results:\n   561→                        key = f""{result[\'file\']}:{result[\'line\']}""\n   562→                        if key not in seen:\n   563→                            seen.add(key)\n   564→                            unique_results.append(result)\n   565→                    all_results_by_plugin[plugin] = unique_results\n   566→                \n   567→                # Configure aggregator for document queries\n   568→                if is_doc_query and self._enable_advanced:\n   569→                    # Adjust ranking criteria for documentation\n   570→                    doc_criteria = RankingCriteria(\n   571→                        relevance_weight=0.5,      # Increase relevance weight\n   572→                        confidence_weight=0.2,     # Reduce confidence weight\n   573→                        frequency_weight=0.2,      # Keep frequency weight\n   574→                        recency_weight=0.1,        # Keep recency weight\n   575→                        prefer_exact_matches=False,  # Natural language doesn\'t need exact matches\n   576→                        boost_multiple_sources=True,\n   577→                        boost_common_extensions=True\n   578→                    )\n   579→                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580→                \n   581→                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582→                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583→                )\n   584→                \n   585→                # Adjust ranking for document queries\n   586→                if is_doc_query:\n   587→                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588→                \n   589→                # Apply final limit\n   590→                if limit and len(aggregated_results) > limit:\n   591→                    aggregated_results = aggregated_results[:limit]\n   592→                \n   593→                logger.debug(f""Search aggregation stats: {stats.total_results} total, ""\n   594→                           f""{stats.unique_results} unique, {stats.plugins_used} plugins used, ""\n   595→                           f""document_query={is_doc_query}"")\n   596→                \n   597→                self._operation_stats[\'searches\'] += 1\n   598→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   599→                \n   600→                # Yield primary results from aggregated results\n   601→                for aggregated in aggregated_results:\n   602→                    yield aggregated.primary_result\n   603→            else:\n   604→                # Fallback to basic search\n   605→                # Detect if this is a document query\n   606→                is_doc_query = self._is_document_query(query)\n   607→                \n   608→                # Expand query if it\'s a document query\n   609→                queries = [query]\n   610→                if is_doc_query:\n   611→                    queries = self._expand_document_query(query)\n   612→                    semantic = True  # Force semantic search for natural language\n   613→                \n   614→                opts = {""semantic"": semantic, ""limit"": limit}\n   615→                all_results = []\n   616→                \n   617→                # Search with all query variations\n   618→                for search_query in queries:\n   619→                    for p in self._plugins:\n   620→                        try:\n   621→                            for result in p.search(search_query, opts):\n   622→                                all_results.append(result)\n   623→                        except Exception as e:\n   624→                            logger.warning(f""Plugin {p.lang} failed to search for {search_query}: {e}"")\n   625→                \n   626→                # Deduplicate results\n   627→                seen = set()\n   628→                unique_results = []\n   629→                for result in all_results:\n   630→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   631→                    if key not in seen:\n   632→                        seen.add(key)\n   633→                        unique_results.append(result)\n   634→                \n   635→                # Sort by score if available\n   636→                unique_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   637→                \n   638→                # Prioritize documentation files for document queries\n   639→                if is_doc_query:\n   640→                    doc_results = []\n   641→                    code_results = []\n   642→                    for result in unique_results:\n   643→                        if self._is_documentation_file(result.get(\'file\', \'\')):\n   644→                            doc_results.append(result)\n   645→                        else:\n   646→                            code_results.append(result)\n   647→                    unique_results = doc_results + code_results\n   648→                \n   649→                # Apply limit\n   650→                count = 0\n   651→                for result in unique_results:\n   652→                    if limit and count >= limit:\n   653→                        break\n   654→                    yield result\n   655→                    count += 1\n   656→                \n   657→                self._operation_stats[\'searches\'] += 1\n   658→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   659→                \n   660→        except Exception as e:\n   661→            logger.error(f""Error in search for {query}: {e}"", exc_info=True)\n   662→    \n   663→    def index_file(self, path: Path) -> None:\n   664→        """"""Index a single file if it has changed.""""""\n   665→        try:\n   666→            # Ensure path is absolute to avoid relative/absolute path issues\n   667→            path = path.resolve()\n   668→            \n   669→            # Find the appropriate plugin\n   670→            plugin = self._match_plugin(path)\n   671→            \n   672→            # Read file content\n   673→            try:\n   674→                content = path.read_text(encoding=\'utf-8\')\n   675→            except UnicodeDecodeError:\n   676→                # Try with different encodings\n   677→                try:\n   678→                    content = path.read_text(encoding=\'latin-1\')\n   679→                except Exception as e:\n   680→                    logger.error(f""Failed to read {path}: {e}"")\n   681→                    return\n   682→            \n   683→            # Check if we need to re-index (simplified for now)\n   684→            # TODO: Implement proper caching logic\n   685→            \n   686→            # Index the file\n   687→            start_time = time.time()\n   688→            logger.info(f""Indexing {path} with {plugin.lang} plugin"")\n   689→            shard = plugin.indexFile(path, content)\n   690→            \n   691→            # Record performance if advanced features enabled\n   692→            if self._enable_advanced and self._router:\n   693→                execution_time = time.time() - start_time\n   694→                self._router.record_performance(plugin, execution_time)\n   695→            \n   696→            self._operation_stats[\'indexings\'] += 1\n   697→            self._operation_stats[\'total_time\'] += time.time() - start_time\n   698→            \n   699→            logger.info(f""Successfully indexed {path}: {len(shard.get(\'symbols\', []))} symbols found"")\n   700→            \n   701→        except RuntimeError as e:\n   702→            # No plugin found for this file type\n   703→            logger.debug(f""No plugin for {path}: {e}"")\n   704→        except Exception as e:\n   705→            logger.error(f""Error indexing {path}: {e}"", exc_info=True)\n   706→    \n   707→    def get_statistics(self) -> dict:\n   708→        """"""Get comprehensive statistics across all plugins and components.""""""\n   709→        stats = {\n   710→            ""total_plugins"": len(self._plugins),\n   711→            ""loaded_languages"": sorted(list(self._loaded_languages)),\n   712→            ""supported_languages"": len(self.supported_languages),\n   713→            ""operations"": self._operation_stats.copy()\n   714→        }\n   715→        \n   716→        # Add language breakdown\n   717→        stats[""by_language""] = {}\n   718→        for lang, plugin in self._by_lang.items():\n   719→            plugin_info = {\n   720→                ""loaded"": True,\n   721→                ""class"": plugin.__class__.__name__\n   722→            }\n   723→            if hasattr(plugin, \'get_indexed_count\'):\n   724→                plugin_info[""indexed_files""] = plugin.get_indexed_count()\n   725→            stats[""by_language""][lang] = plugin_info\n   726→        \n   727→        return stats\n   728→    \n   729→    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730→        """"""\n   731→        Index all files in a directory, respecting ignore patterns.\n   732→        \n   733→        Args:\n   734→            directory: Directory to index\n   735→            recursive: Whether to index subdirectories\n   736→            \n   737→        Returns:\n   738→            Statistics about indexed files\n   739→        """"""\n   740→        logger.info(f""Indexing directory: {directory} (recursive={recursive})"")\n   741→        \n   742→        # Note: We don\'t use ignore patterns during indexing\n   743→        # ALL files are indexed for local search capability\n   744→        # Filtering happens only during export/sharing\n   745→        \n   746→        # Get all supported extensions\n   747→        supported_extensions = get_all_extensions()\n   748→        \n   749→        stats = {\n   750→            ""total_files"": 0,\n   751→            ""indexed_files"": 0,\n   752→            ""ignored_files"": 0,\n   753→            ""failed_files"": 0,\n   754→            ""by_language"": {}\n   755→        }\n   756→        \n   757→        # Walk directory\n   758→        if recursive:\n   759→            file_iterator = directory.rglob(""*"")\n   760→        else:\n   761→            file_iterator = directory.glob(""*"")\n   762→            \n   763→        for path in file_iterator:\n   764→            if not path.is_file():\n   765→                continue\n   766→                \n   767→            stats[""total_files""] += 1\n   768→            \n   769→            # NOTE: We index ALL files locally, including gitignored ones\n   770→            # Filtering happens only during export/sharing\n   771→            # This allows local search of .env, secrets, etc.\n   772→                \n   773→            # Try to find a plugin that supports this file\n   774→            # This allows us to index ALL files, including .env, .key, etc.\n   775→            try:\n   776→                # First try to match by extension\n   777→                if path.suffix in supported_extensions:\n   778→                    self.index_file(path)\n   779→                    stats[""indexed_files""] += 1\n   780→                # For files without recognized extensions, try each plugin\'s supports() method\n   781→                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782→                else:\n   783→                    matched = False\n   784→                    for plugin in self._plugins:\n   785→                        if plugin.supports(path):\n   786→                            self.index_file(path)\n   787→                            stats[""indexed_files""] += 1\n   788→                            matched = True\n   789→                            break\n   790→                    \n   791→                    # If no plugin matched but we want to index everything,\n   792→                    # we could add a fallback here to index as plaintext\n   793→                    # For now, we\'ll skip unmatched files\n   794→                    if not matched:\n   795→                        logger.debug(f""No plugin found for {path}"")\n   796→                \n   797→                # Track by language\n   798→                language = get_language_by_extension(path.suffix)\n   799→                if language:\n   800→                    stats[""by_language""][language] = stats[""by_language""].get(language, 0) + 1\n   801→                    \n   802→            except Exception as e:\n   803→                logger.error(f""Failed to index {path}: {e}"")\n   804→                stats[""failed_files""] += 1\n   805→                \n   806→        logger.info(f""Directory indexing complete: {stats[\'indexed_files\']} indexed, ""\n   807→                   f""{stats[\'ignored_files\']} ignored, {stats[\'failed_files\']} failed"")\n   808→        \n   809→        return stats\n   810→    \n   811→    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812→        """"""Search specifically across documentation files.\n   813→        \n   814→        Args:\n   815→            topic: Topic to search for (e.g., ""installation"", ""configuration"")\n   816→            doc_types: Optional list of document types to search (e.g., [""readme"", ""guide"", ""api""])\n   817→            limit: Maximum number of results\n   818→            \n   819→        Returns:\n   820→            Search results from documentation files\n   821→        """"""\n   822→        # Default document types if not specified\n   823→        if doc_types is None:\n   824→            doc_types = [""readme"", ""documentation"", ""guide"", ""tutorial"", ""api"", ""changelog"", ""contributing""]\n   825→        \n   826→        # Build search queries for different document types\n   827→        queries = []\n   828→        for doc_type in doc_types:\n   829→            queries.extend([\n   830→                f""{doc_type} {topic}"",\n   831→                f""{topic} {doc_type}"",\n   832→                f""{topic} in {doc_type}""\n   833→            ])\n   834→        \n   835→        # Also search for the topic in common doc filenames\n   836→        queries.extend([\n   837→            f""README {topic}"",\n   838→            f""CONTRIBUTING {topic}"",\n   839→            f""docs {topic}"",\n   840→            f""documentation {topic}""\n   841→        ])\n   842→        \n   843→        # Deduplicate queries\n   844→        queries = list(dict.fromkeys(queries))\n   845→        \n   846→        logger.info(f""Cross-document search for \'{topic}\' with {len(queries)} query variations"")\n   847→        \n   848→        # Use the enhanced search with document-specific handling\n   849→        all_results = []\n   850→        seen = set()\n   851→        \n   852→        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853→            for result in self.search(query, semantic=True, limit=limit):\n   854→                # Only include documentation files\n   855→                if self._is_documentation_file(result.get(\'file\', \'\')):\n   856→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   857→                    if key not in seen:\n   858→                        seen.add(key)\n   859→                        all_results.append(result)\n   860→        \n   861→        # Sort by relevance (score) and return top results\n   862→        all_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   863→        \n   864→        count = 0\n   865→        for result in all_results:\n   866→            if count >= limit:\n   867→                break\n   868→            yield result\n   869→            count += 1\n   870→    \n   871→    def health_check(self) -> Dict[str, Any]:\n   872→        """"""Perform a health check on all components.""""""\n   873→        health = {\n   874→            \'status\': \'healthy\',\n   875→            \'components\': {\n   876→                \'dispatcher\': {\n   877→                    \'status\': \'healthy\',\n   878→                    \'plugins_loaded\': len(self._plugins),\n   879→                    \'languages_supported\': len(self.supported_languages),\n   880→                    \'factory_enabled\': self._use_factory,\n   881→                    \'lazy_loading\': self._lazy_load\n   882→                }\n   883→            },\n   884→            \'plugins\': {},\n   885→            \'errors\': []\n   886→        }\n   887→        \n   888→        # Check plugin health\n   889→        for lang, plugin in self._by_lang.items():\n   890→            try:\n   891→                plugin_health = {\n   892→                    \'status\': \'healthy\',\n   893→                    \'class\': plugin.__class__.__name__,\n   894→                    \'semantic_enabled\': getattr(plugin, \'_enable_semantic\', False)\n   895→                }\n   896→                if hasattr(plugin, \'get_indexed_count\'):\n   897→                    plugin_health[\'indexed_files\'] = plugin.get_indexed_count()\n   898→            except Exception as e:\n   899→                plugin_health = {\n   900→                    \'status\': \'error\',\n   901→                    \'error\': str(e)\n   902→                }\n   903→                health[\'errors\'].append(f""Plugin {lang}: {str(e)}"")\n   904→            \n   905→            health[\'plugins\'][lang] = plugin_health\n   906→        \n   907→        # Determine overall health\n   908→        if len(health[\'errors\']) > 0:\n   909→            health[\'status\'] = \'degraded\' if len(health[\'errors\']) < 3 else \'unhealthy\'\n   910→        \n   911→        return health\n   912→    \n   913→    def remove_file(self, path: Union[Path, str]) -> None:\n   914→        """"""Remove a file from all indexes.\n   915→        \n   916→        Args:\n   917→            path: File path to remove\n   918→        """"""\n   919→        path = Path(path).resolve()\n   920→        logger.info(f""Removing file from index: {path}"")\n   921→        \n   922→        try:\n   923→            # Remove from SQLite if available\n   924→            if self._sqlite_store:\n   925→                from ..core.path_resolver import PathResolver\n   926→                path_resolver = PathResolver()\n   927→                try:\n   928→                    relative_path = path_resolver.normalize_path(path)\n   929→                    # Get repository ID - for now assume 1\n   930→                    # TODO: Properly detect repository\n   931→                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932→                except Exception as e:\n   933→                    logger.error(f""Error removing from SQLite: {e}"")\n   934→            \n   935→            # Remove from semantic index if available\n   936→            try:\n   937→                plugin = self._match_plugin(path)\n   938→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   939→                    plugin._indexer.remove_file(path)\n   940→                    logger.info(f""Removed from semantic index: {path}"")\n   941→            except Exception as e:\n   942→                logger.warning(f""Error removing from semantic index: {e}"")\n   943→            \n   944→            # Update statistics\n   945→            self._operation_stats[\'deletions\'] = self._operation_stats.get(\'deletions\', 0) + 1\n   946→            \n   947→        except Exception as e:\n   948→            logger.error(f""Error removing file {path}: {e}"", exc_info=True)\n   949→    \n   950→    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951→        """"""Move a file in all indexes.\n   952→        \n   953→        Args:\n   954→            old_path: Original file path\n   955→            new_path: New file path\n   956→            content_hash: Optional content hash to verify unchanged content\n   957→        """"""\n   958→        old_path = Path(old_path).resolve()\n   959→        new_path = Path(new_path).resolve()\n   960→        logger.info(f""Moving file in index: {old_path} -> {new_path}"")\n   961→        \n   962→        try:\n   963→            # Move in SQLite if available\n   964→            if self._sqlite_store:\n   965→                from ..core.path_resolver import PathResolver\n   966→                path_resolver = PathResolver()\n   967→                try:\n   968→                    old_relative = path_resolver.normalize_path(old_path)\n   969→                    new_relative = path_resolver.normalize_path(new_path)\n   970→                    # Get repository ID - for now assume 1\n   971→                    # TODO: Properly detect repository\n   972→                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973→                except Exception as e:\n   974→                    logger.error(f""Error moving in SQLite: {e}"")\n   975→            \n   976→            # Move in semantic index if available\n   977→            try:\n   978→                plugin = self._match_plugin(new_path)\n   979→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   980→                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981→                    logger.info(f""Moved in semantic index: {old_path} -> {new_path}"")\n   982→            except Exception as e:\n   983→                logger.warning(f""Error moving in semantic index: {e}"")\n   984→            \n   985→            # Update statistics\n   986→            self._operation_stats[\'moves\'] = self._operation_stats.get(\'moves\', 0) + 1\n   987→            \n   988→        except Exception as e:\n   989→            logger.error(f""Error moving file {old_path} -> {new_path}: {e}"", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,1,1,14367,13323,1,0.5,0.25,0,,,,,transcript_response
1a4d62a8-4e27-4462-80ca-11c6a0771da9,2025-06-26T07:39:12.070Z,transcript_1a4d62a8-4e27-4462-80ca-11c6a0771da9,Find the BM25Indexer class definition,transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,1,1,13916,0,1,0.5,0.25,0,,,,,transcript_response
1a4d62a8-4e27-4462-80ca-11c6a0771da9,2025-06-26T07:39:14.927Z,transcript_1a4d62a8-4e27-4462-80ca-11c6a0771da9,"{'tool_use_id': 'toolu_014Ni45aGbsjdNWvSS15vpYS', 'type': 'tool_result', 'content': 'Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/bm25_indexer.py'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,24,24,14142,0,24,0.5,0.25,0,,,,,transcript_response
1a4d62a8-4e27-4462-80ca-11c6a0771da9,2025-06-26T07:39:20.291Z,transcript_1a4d62a8-4e27-4462-80ca-11c6a0771da9,"{'tool_use_id': 'toolu_012YEXU3G3P6HKmsmox7skdi', 'type': 'tool_result', 'content': '     1→""""""\n     2→BM25 Indexer using SQLite FTS5 for full-text search.\n     3→\n     4→This module provides BM25-based full-text search indexing using SQLite\'s FTS5 \n     5→extension, which includes built-in BM25 ranking algorithms.\n     6→""""""\n     7→\n     8→import sqlite3\n     9→import logging\n    10→import hashlib\n    11→import json\n    12→from typing import List, Dict, Any, Optional, Tuple\n    13→from datetime import datetime\n    14→from pathlib import Path\n    15→from contextlib import contextmanager\n    16→\n    17→from ..storage.sqlite_store import SQLiteStore\n    18→# Interface definition inline for now\n    19→from abc import ABC, abstractmethod\n    20→\n    21→logger = logging.getLogger(__name__)\n    22→\n    23→# Define IIndexer interface inline\n    24→class IIndexer(ABC):\n    25→    @abstractmethod\n    26→    def index_documents(self, documents: List[Dict[str, Any]]) -> bool:\n    27→        pass\n    28→    \n    29→    @abstractmethod\n    30→    def search(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:\n    31→        pass\n    32→\n    33→\n    34→class BM25Indexer(IIndexer):\n    35→    """"""\n    36→    BM25-based full-text search indexer using SQLite FTS5.\n    37→    \n    38→    This indexer provides:\n    39→    - Efficient full-text search using inverted indexes\n    40→    - Built-in BM25 ranking algorithm\n    41→    - Support for phrase searches, prefix searches, and boolean operators\n    42→    - Integration with existing SQLite storage\n    43→    """"""\n    44→    \n    45→    def __init__(self, storage: SQLiteStore, table_name: str = ""bm25_content""):\n    46→        """"""\n    47→        Initialize the BM25 indexer.\n    48→        \n    49→        Args:\n    50→            storage: SQLite storage instance\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,1,1,14142,727,1,0.5,0.25,0,,,,,transcript_response
a712ff2a-98c3-41fc-9a1c-0469a20a4330,2025-06-26T07:33:48.076Z,transcript_a712ff2a-98c3-41fc-9a1c-0469a20a4330,Find all functions that use asyncio,transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,1,1,13172,741,1,0.5,0.25,0,,,,,transcript_response
927dc689-c758-442e-be8e-cfd1ea400fd3,2025-06-26T07:39:25.623Z,transcript_927dc689-c758-442e-be8e-cfd1ea400fd3,Find the EnhancedDispatcher class and show its main methods,transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,1,1,13920,0,1,0.5,0.25,0,,,,,transcript_response
927dc689-c758-442e-be8e-cfd1ea400fd3,2025-06-26T07:39:28.848Z,transcript_927dc689-c758-442e-be8e-cfd1ea400fd3,"{'tool_use_id': 'toolu_01X2tS5FLZrPy5NEvFYaTCe7', 'type': 'tool_result', 'content': 'Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/level4/path_management.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/path_management_architecture.md'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,32,32,14367,0,32,0.5,0.25,0,,,,,transcript_response
927dc689-c758-442e-be8e-cfd1ea400fd3,2025-06-26T07:39:32.545Z,transcript_927dc689-c758-442e-be8e-cfd1ea400fd3,"{'tool_use_id': 'toolu_013j8zvX8FzhHhBx9BfQJuoK', 'type': 'tool_result', 'content': '     1→""""""Enhanced dispatcher with dynamic plugin loading via PluginFactory.""""""\n     2→from pathlib import Path\n     3→from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4→import logging\n     5→import hashlib\n     6→import time\n     7→import re\n     8→from datetime import datetime\n     9→import os\n    10→\n    11→from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12→from ..plugins.plugin_factory import PluginFactory\n    13→from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14→from ..storage.sqlite_store import SQLiteStore\n    15→from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16→from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17→# Note: We\'ve removed ignore pattern checks to allow indexing ALL files\n    18→# Filtering happens only during export via SecureIndexExporter\n    19→# from ..core.ignore_patterns import get_ignore_manager\n    20→\n    21→logger = logging.getLogger(__name__)\n    22→\n    23→\n    24→class EnhancedDispatcher:\n    25→    """"""Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.""""""\n    26→    \n    27→    # Document query patterns - common documentation search terms\n    28→    DOCUMENT_QUERY_PATTERNS = [\n    29→        r\'\\b(how\\s+to|howto)\\b\',\n    30→        r\'\\b(getting\\s+started|get\\s+started)\\b\',\n    31→        r\'\\b(installation|install|setup)\\b\',\n    32→        r\'\\b(configuration|configure|config)\\b\',\n    33→        r\'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b\',\n    34→        r\'\\b(tutorial|guide|walkthrough)\\b\',\n    35→        r\'\\b(example|sample|snippet)\\b\',\n    36→        r\'\\b(readme|documentation|docs)\\b\',\n    37→        r\'\\b(usage|use\\s+case|using)\\b\',\n    38→        r\'\\b(reference|manual)\\b\',\n    39→        r\'\\b(faq|frequently\\s+asked)\\b\',\n    40→        r\'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b\',\n    41→        r\'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b\',\n    42→        r\'\\b(architecture|design|overview)\\b\',\n    43→        r\'\\b(changelog|release\\s+notes|migration)\\b\'\n    44→    ]\n    45→    \n    46→    # Documentation file patterns\n    47→    DOCUMENTATION_FILE_PATTERNS = [\n    48→        r\'readme(\\.\\w+)?$\',\n    49→        r\'changelog(\\.\\w+)?$\',\n    50→        r\'contributing(\\.\\w+)?$\',\n    51→        r\'license(\\.\\w+)?$\',\n    52→        r\'install(\\.\\w+)?$\',\n    53→        r\'setup(\\.\\w+)?$\',\n    54→        r\'guide(\\.\\w+)?$\',\n    55→        r\'tutorial(\\.\\w+)?$\',\n    56→        r\'\\.md$\',\n    57→        r\'\\.rst$\',\n    58→        r\'\\.txt$\',\n    59→        r\'docs?/\',\n    60→        r\'documentation/\'\n    61→    ]\n    62→    \n    63→    def __init__(self, \n    64→                 plugins: Optional[List[IPlugin]] = None,\n    65→                 sqlite_store: Optional[SQLiteStore] = None,\n    66→                 enable_advanced_features: bool = True,\n    67→                 use_plugin_factory: bool = True,\n    68→                 lazy_load: bool = True,\n    69→                 semantic_search_enabled: bool = True):\n    70→        """"""Initialize the enhanced dispatcher.\n    71→        \n    72→        Args:\n    73→            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74→            sqlite_store: SQLite store for plugin persistence\n    75→            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76→            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77→            lazy_load: Whether to lazy-load plugins on demand\n    78→            semantic_search_enabled: Whether to enable semantic search in plugins\n    79→        """"""\n    80→        self._sqlite_store = sqlite_store\n    81→        self._enable_advanced = enable_advanced_features\n    82→        self._use_factory = use_plugin_factory\n    83→        self._lazy_load = lazy_load\n    84→        self._semantic_enabled = semantic_search_enabled\n    85→        \n    86→        # Plugin storage\n    87→        self._plugins: List[IPlugin] = []\n    88→        self._by_lang: Dict[str, IPlugin] = {}\n    89→        self._loaded_languages: set[str] = set()\n    90→        \n    91→        # Cache for file hashes to avoid re-indexing unchanged files\n    92→        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93→        \n    94→        # Advanced components\n    95→        if self._enable_advanced:\n    96→            self._file_matcher = FileTypeMatcher()\n    97→            self._router = PluginRouter(self._file_matcher)\n    98→            self._aggregator = ResultAggregator()\n    99→        \n   100→        # Performance tracking\n   101→        self._operation_stats = {\n   102→            \'searches\': 0,\n   103→            \'lookups\': 0,\n   104→            \'indexings\': 0,\n   105→            \'total_time\': 0.0,\n   106→            \'plugins_loaded\': 0\n   107→        }\n   108→        \n   109→        # Initialize plugins\n   110→        if plugins:\n   111→            # Use provided plugins (backward compatibility)\n   112→            self._plugins = plugins\n   113→            self._by_lang = {p.lang: p for p in plugins}\n   114→            for plugin in plugins:\n   115→                self._loaded_languages.add(getattr(plugin, \'lang\', \'unknown\'))\n   116→            if self._enable_advanced:\n   117→                self._register_plugins_with_router()\n   118→        elif use_plugin_factory and not lazy_load:\n   119→            # Load all plugins immediately\n   120→            self._load_all_plugins()\n   121→        # If lazy_load is True, plugins will be loaded on demand\n   122→        \n   123→        # Compile document query patterns for performance\n   124→        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125→        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126→        \n   127→        logger.info(f""Enhanced dispatcher initialized with {len(self._plugins)} plugins"")\n   128→    \n   129→    def _load_all_plugins(self):\n   130→        """"""Load all available plugins using PluginFactory.""""""\n   131→        logger.info(""Loading all available plugins..."")\n   132→        \n   133→        all_plugins = PluginFactory.create_all_plugins(\n   134→            sqlite_store=self._sqlite_store,\n   135→            enable_semantic=self._semantic_enabled\n   136→        )\n   137→        \n   138→        for lang, plugin in all_plugins.items():\n   139→            self._plugins.append(plugin)\n   140→            self._by_lang[lang] = plugin\n   141→            self._loaded_languages.add(lang)\n   142→            self._operation_stats[\'plugins_loaded\'] += 1\n   143→        \n   144→        if self._enable_advanced:\n   145→            self._register_plugins_with_router()\n   146→        \n   147→        logger.info(f""Loaded {len(all_plugins)} plugins: {\', \'.join(sorted(all_plugins.keys()))}"")\n   148→    \n   149→    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150→        """"""Ensure a plugin for the given language is loaded.\n   151→        \n   152→        Args:\n   153→            language: Language code (e.g., \'python\', \'go\')\n   154→            \n   155→        Returns:\n   156→            Plugin instance or None if not available\n   157→        """"""\n   158→        # Normalize language\n   159→        language = language.lower().replace(\'-\', \'_\')\n   160→        \n   161→        # Check if already loaded\n   162→        if language in self._by_lang:\n   163→            return self._by_lang[language]\n   164→        \n   165→        # If not using factory or already tried to load, return None\n   166→        if not self._use_factory or language in self._loaded_languages:\n   167→            return None\n   168→        \n   169→        # Try to load the plugin\n   170→        try:\n   171→            logger.info(f""Lazy loading plugin for {language}"")\n   172→            plugin = PluginFactory.create_plugin(\n   173→                language,\n   174→                sqlite_store=self._sqlite_store,\n   175→                enable_semantic=self._semantic_enabled\n   176→            )\n   177→            \n   178→            # Add to collections\n   179→            self._plugins.append(plugin)\n   180→            self._by_lang[language] = plugin\n   181→            self._loaded_languages.add(language)\n   182→            self._operation_stats[\'plugins_loaded\'] += 1\n   183→            \n   184→            # Register with router if needed\n   185→            if self._enable_advanced:\n   186→                capabilities = self._detect_plugin_capabilities(plugin)\n   187→                self._router.register_plugin(plugin, capabilities)\n   188→            \n   189→            logger.info(f""Successfully loaded {language} plugin"")\n   190→            return plugin\n   191→            \n   192→        except ValueError as e:\n   193→            logger.warning(f""No plugin available for {language}: {e}"")\n   194→            self._loaded_languages.add(language)  # Mark as attempted\n   195→            return None\n   196→        except Exception as e:\n   197→            logger.error(f""Error loading plugin for {language}: {e}"")\n   198→            self._loaded_languages.add(language)  # Mark as attempted\n   199→            return None\n   200→    \n   201→    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202→        """"""Ensure a plugin is loaded for the given file.\n   203→        \n   204→        Args:\n   205→            path: File path\n   206→            \n   207→        Returns:\n   208→            Plugin instance or None if not available\n   209→        """"""\n   210→        # Get language from file extension\n   211→        extension = path.suffix.lower()\n   212→        language = get_language_by_extension(extension)\n   213→        \n   214→        if language:\n   215→            return self._ensure_plugin_loaded(language)\n   216→        \n   217→        # Fallback: try all loaded plugins\n   218→        for plugin in self._plugins:\n   219→            if plugin.supports(path):\n   220→                return plugin\n   221→        \n   222→        return None\n   223→    \n   224→    def _register_plugins_with_router(self):\n   225→        """"""Register plugins with the router and assign capabilities.""""""\n   226→        for plugin in self._plugins:\n   227→            # Determine capabilities based on plugin type/language\n   228→            capabilities = self._detect_plugin_capabilities(plugin)\n   229→            self._router.register_plugin(plugin, capabilities)\n   230→    \n   231→    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232→        """"""Detect capabilities for a plugin based on its language and features.""""""\n   233→        capabilities = []\n   234→        lang = getattr(plugin, \'lang\', \'unknown\')\n   235→        \n   236→        # Base capabilities all plugins have\n   237→        capabilities.append(PluginCapability(\n   238→            \'syntax_analysis\', \'1.0\', f\'{lang} syntax analysis\', \n   239→            priority=70, metadata={\'language\': lang}\n   240→        ))\n   241→        \n   242→        capabilities.append(PluginCapability(\n   243→            \'code_search\', \'1.0\', f\'{lang} code search\', \n   244→            priority=80, metadata={\'language\': lang}\n   245→        ))\n   246→        \n   247→        # Check for semantic search capability\n   248→        if hasattr(plugin, \'_enable_semantic\') and plugin._enable_semantic:\n   249→            capabilities.append(PluginCapability(\n   250→                \'semantic_search\', \'1.0\', f\'{lang} semantic search\', \n   251→                priority=90, metadata={\'language\': lang}\n   252→            ))\n   253→        \n   254→        # Language-specific capabilities\n   255→        if lang == \'python\':\n   256→            capabilities.extend([\n   257→                PluginCapability(\'refactoring\', \'1.0\', \'Python refactoring support\', 75),\n   258→                PluginCapability(\'type_analysis\', \'1.0\', \'Python type analysis\', 85)\n   259→            ])\n   260→        elif lang in [\'javascript\', \'typescript\']:\n   261→            capabilities.extend([\n   262→                PluginCapability(\'linting\', \'1.0\', \'JavaScript/TypeScript linting\', 85),\n   263→                PluginCapability(\'bundling_analysis\', \'1.0\', \'Module bundling analysis\', 70),\n   264→                PluginCapability(\'framework_support\', \'1.0\', \'Framework-specific support\', 75)\n   265→            ])\n   266→        elif lang in [\'c\', \'cpp\']:\n   267→            capabilities.extend([\n   268→                PluginCapability(\'compilation_analysis\', \'1.0\', \'Compilation analysis\', 80),\n   269→                PluginCapability(\'memory_analysis\', \'1.0\', \'Memory usage analysis\', 70),\n   270→                PluginCapability(\'performance_profiling\', \'1.0\', \'Performance profiling\', 75)\n   271→            ])\n   272→        elif lang in [\'go\', \'rust\']:\n   273→            capabilities.extend([\n   274→                PluginCapability(\'package_analysis\', \'1.0\', f\'{lang} package analysis\', 80),\n   275→                PluginCapability(\'concurrency_analysis\', \'1.0\', f\'{lang} concurrency analysis\', 75)\n   276→            ])\n   277→        elif lang in [\'java\', \'kotlin\', \'scala\']:\n   278→            capabilities.extend([\n   279→                PluginCapability(\'jvm_analysis\', \'1.0\', \'JVM bytecode analysis\', 75),\n   280→                PluginCapability(\'build_tool_integration\', \'1.0\', \'Build tool integration\', 70)\n   281→            ])\n   282→        \n   283→        return capabilities\n   284→    \n   285→    @property\n   286→    def plugins(self):\n   287→        """"""Get the dictionary of loaded plugins by language.""""""\n   288→        return self._by_lang\n   289→    \n   290→    @property\n   291→    def supported_languages(self) -> List[str]:\n   292→        """"""Get list of all supported languages (loaded and available).""""""\n   293→        if self._use_factory:\n   294→            return PluginFactory.get_supported_languages()\n   295→        else:\n   296→            return list(self._by_lang.keys())\n   297→    \n   298→    def _match_plugin(self, path: Path) -> IPlugin:\n   299→        """"""Match a plugin for the given file path.""""""\n   300→        # Ensure plugin is loaded if using lazy loading\n   301→        if self._lazy_load and self._use_factory:\n   302→            plugin = self._ensure_plugin_for_file(path)\n   303→            if plugin:\n   304→                return plugin\n   305→        \n   306→        # Use advanced routing if available\n   307→        if self._enable_advanced and self._router:\n   308→            route_result = self._router.get_best_plugin(path)\n   309→            if route_result:\n   310→                return route_result.plugin\n   311→        \n   312→        # Fallback to basic matching\n   313→        for p in self._plugins:\n   314→            if p.supports(path):\n   315→                return p\n   316→        \n   317→        raise RuntimeError(f""No plugin found for {path}"")\n   318→    \n   319→    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320→        """"""Get all plugins that can handle a file with confidence scores.""""""\n   321→        # Ensure plugin is loaded if using lazy loading\n   322→        if self._lazy_load and self._use_factory:\n   323→            self._ensure_plugin_for_file(path)\n   324→        \n   325→        if self._enable_advanced and self._router:\n   326→            route_results = self._router.route_file(path)\n   327→            return [(result.plugin, result.confidence) for result in route_results]\n   328→        else:\n   329→            # Basic fallback\n   330→            matching_plugins = []\n   331→            for plugin in self._plugins:\n   332→                if plugin.supports(path):\n   333→                    matching_plugins.append((plugin, 1.0))\n   334→            return matching_plugins\n   335→    \n   336→    def lookup(self, symbol: str) -> SymbolDef | None:\n   337→        """"""Look up symbol definition across all plugins.""""""\n   338→        start_time = time.time()\n   339→        \n   340→        try:\n   341→            # For symbol lookup, we may need to search across all languages\n   342→            # Load all plugins if using lazy loading\n   343→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344→                self._load_all_plugins()\n   345→            \n   346→            if self._enable_advanced and self._aggregator:\n   347→                # Use advanced aggregation\n   348→                definitions_by_plugin = {}\n   349→                for plugin in self._plugins:\n   350→                    try:\n   351→                        definition = plugin.getDefinition(symbol)\n   352→                        definitions_by_plugin[plugin] = definition\n   353→                    except Exception as e:\n   354→                        logger.warning(f""Plugin {plugin.lang} failed to get definition for {symbol}: {e}"")\n   355→                        definitions_by_plugin[plugin] = None\n   356→                \n   357→                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358→                \n   359→                self._operation_stats[\'lookups\'] += 1\n   360→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   361→                \n   362→                return result\n   363→            else:\n   364→                # Fallback to basic lookup\n   365→                for p in self._plugins:\n   366→                    res = p.getDefinition(symbol)\n   367→                    if res:\n   368→                        self._operation_stats[\'lookups\'] += 1\n   369→                        self._operation_stats[\'total_time\'] += time.time() - start_time\n   370→                        return res\n   371→                return None\n   372→                \n   373→        except Exception as e:\n   374→            logger.error(f""Error in symbol lookup for {symbol}: {e}"", exc_info=True)\n   375→            return None\n   376→    \n   377→    def _is_document_query(self, query: str) -> bool:\n   378→        """"""Check if the query is looking for documentation.\n   379→        \n   380→        Args:\n   381→            query: Search query string\n   382→            \n   383→        Returns:\n   384→            True if this appears to be a documentation query\n   385→        """"""\n   386→        query_lower = query.lower()\n   387→        \n   388→        # Check against document query patterns\n   389→        for pattern in self._compiled_doc_patterns:\n   390→            if pattern.search(query_lower):\n   391→                return True\n   392→        \n   393→        # Check for question words at the beginning\n   394→        question_starters = [\'how\', \'what\', \'where\', \'when\', \'why\', \'can\', \'is\', \'does\', \'should\']\n   395→        first_word = query_lower.split()[0] if query_lower.split() else \'\'\n   396→        if first_word in question_starters:\n   397→            return True\n   398→        \n   399→        return False\n   400→    \n   401→    def _expand_document_query(self, query: str) -> List[str]:\n   402→        """"""Expand a document query with related terms for better search coverage.\n   403→        \n   404→        Args:\n   405→            query: Original search query\n   406→            \n   407→        Returns:\n   408→            List of expanded query variations\n   409→        """"""\n   410→        expanded_queries = [query]  # Always include original\n   411→        query_lower = query.lower()\n   412→        \n   413→        # Common expansions for documentation queries\n   414→        expansions = {\n   415→            \'install\': [\'installation\', \'setup\', \'getting started\', \'requirements\'],\n   416→            \'config\': [\'configuration\', \'configure\', \'settings\', \'options\', \'parameters\'],\n   417→            \'api\': [\'api documentation\', \'api reference\', \'endpoint\', \'method\'],\n   418→            \'how to\': [\'tutorial\', \'guide\', \'example\', \'usage\'],\n   419→            \'example\': [\'sample\', \'snippet\', \'demo\', \'code example\'],\n   420→            \'error\': [\'troubleshoot\', \'debug\', \'issue\', \'problem\', \'fix\'],\n   421→            \'getting started\': [\'quickstart\', \'tutorial\', \'introduction\', \'setup\'],\n   422→            \'guide\': [\'tutorial\', \'documentation\', \'walkthrough\', \'how to\'],\n   423→            \'usage\': [\'how to use\', \'example\', \'api\', \'reference\']\n   424→        }\n   425→        \n   426→        # Apply expansions\n   427→        for term, related_terms in expansions.items():\n   428→            if term in query_lower:\n   429→                for related in related_terms:\n   430→                    # Replace the term with related term\n   431→                    expanded = query_lower.replace(term, related)\n   432→                    if expanded != query_lower and expanded not in expanded_queries:\n   433→                        expanded_queries.append(expanded)\n   434→                \n   435→                # Also add queries with additional terms\n   436→                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437→                    expanded = f""{query} {related}""\n   438→                    if expanded not in expanded_queries:\n   439→                        expanded_queries.append(expanded)\n   440→        \n   441→        # Add file-specific searches for common documentation files\n   442→        if self._is_document_query(query):\n   443→            # Extract the main topic from the query\n   444→            topic_words = []\n   445→            for word in query.lower().split():\n   446→                if word not in [\'how\', \'to\', \'the\', \'a\', \'an\', \'is\', \'are\', \'what\', \'where\', \'when\']:\n   447→                    topic_words.append(word)\n   448→            \n   449→            if topic_words:\n   450→                topic = \' \'.join(topic_words[:2])  # Use first two topic words\n   451→                expanded_queries.extend([\n   452→                    f""README {topic}"",\n   453→                    f""{topic} documentation"",\n   454→                    f""{topic} docs"",\n   455→                    f""{topic} guide""\n   456→                ])\n   457→        \n   458→        # Remove duplicates while preserving order\n   459→        seen = set()\n   460→        unique_queries = []\n   461→        for q in expanded_queries:\n   462→            if q not in seen:\n   463→                seen.add(q)\n   464→                unique_queries.append(q)\n   465→        \n   466→        return unique_queries[:10]  # Limit to 10 queries max\n   467→    \n   468→    def _is_documentation_file(self, file_path: str) -> bool:\n   469→        """"""Check if a file path is likely a documentation file.\n   470→        \n   471→        Args:\n   472→            file_path: Path to check\n   473→            \n   474→        Returns:\n   475→            True if this appears to be a documentation file\n   476→        """"""\n   477→        path_lower = file_path.lower()\n   478→        \n   479→        for pattern in self._compiled_file_patterns:\n   480→            if pattern.search(path_lower):\n   481→                return True\n   482→        \n   483→        return False\n   484→    \n   485→    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486→        """"""Adjust ranking to prioritize documentation files for document queries.\n   487→        \n   488→        Args:\n   489→            query: Original search query\n   490→            results: List of aggregated results\n   491→            \n   492→        Returns:\n   493→            Re-ranked results with documentation prioritized\n   494→        """"""\n   495→        if not self._is_document_query(query):\n   496→            return results\n   497→        \n   498→        # Separate documentation and code results\n   499→        doc_results = []\n   500→        code_results = []\n   501→        \n   502→        for result in results:\n   503→            if self._is_documentation_file(result.primary_result.get(\'file\', \'\')):\n   504→                # Boost documentation files for document queries\n   505→                result.rank_score *= 1.5\n   506→                result.metadata[\'doc_boost\'] = True\n   507→                doc_results.append(result)\n   508→            else:\n   509→                code_results.append(result)\n   510→        \n   511→        # Sort each group by rank score\n   512→        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513→        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514→        \n   515→        # Combine with documentation files first\n   516→        return doc_results + code_results\n   517→    \n   518→    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519→        """"""Search for code and documentation across all plugins.""""""\n   520→        start_time = time.time()\n   521→        \n   522→        try:\n   523→            # For search, we may need to search across all languages\n   524→            # Load all plugins if using lazy loading\n   525→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526→                self._load_all_plugins()\n   527→            \n   528→            # Detect if this is a document query\n   529→            is_doc_query = self._is_document_query(query)\n   530→            \n   531→            # Expand query if it\'s a document query\n   532→            queries = [query]\n   533→            if is_doc_query:\n   534→                queries = self._expand_document_query(query)\n   535→                logger.info(f""Expanded document query \'{query}\' to {len(queries)} variations"")\n   536→                # Force semantic search for natural language queries\n   537→                semantic = True\n   538→            \n   539→            if self._enable_advanced and self._aggregator:\n   540→                # Use advanced aggregation\n   541→                all_results_by_plugin = {}\n   542→                opts = {""semantic"": semantic, ""limit"": limit * 2 if is_doc_query else limit}\n   543→                \n   544→                # Search with all query variations\n   545→                for search_query in queries:\n   546→                    for plugin in self._plugins:\n   547→                        try:\n   548→                            results = list(plugin.search(search_query, opts))\n   549→                            if results:\n   550→                                if plugin not in all_results_by_plugin:\n   551→                                    all_results_by_plugin[plugin] = []\n   552→                                all_results_by_plugin[plugin].extend(results)\n   553→                        except Exception as e:\n   554→                            logger.warning(f""Plugin {plugin.lang} failed to search for {search_query}: {e}"")\n   555→                \n   556→                # Deduplicate results per plugin\n   557→                for plugin, results in all_results_by_plugin.items():\n   558→                    seen = set()\n   559→                    unique_results = []\n   560→                    for result in results:\n   561→                        key = f""{result[\'file\']}:{result[\'line\']}""\n   562→                        if key not in seen:\n   563→                            seen.add(key)\n   564→                            unique_results.append(result)\n   565→                    all_results_by_plugin[plugin] = unique_results\n   566→                \n   567→                # Configure aggregator for document queries\n   568→                if is_doc_query and self._enable_advanced:\n   569→                    # Adjust ranking criteria for documentation\n   570→                    doc_criteria = RankingCriteria(\n   571→                        relevance_weight=0.5,      # Increase relevance weight\n   572→                        confidence_weight=0.2,     # Reduce confidence weight\n   573→                        frequency_weight=0.2,      # Keep frequency weight\n   574→                        recency_weight=0.1,        # Keep recency weight\n   575→                        prefer_exact_matches=False,  # Natural language doesn\'t need exact matches\n   576→                        boost_multiple_sources=True,\n   577→                        boost_common_extensions=True\n   578→                    )\n   579→                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580→                \n   581→                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582→                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583→                )\n   584→                \n   585→                # Adjust ranking for document queries\n   586→                if is_doc_query:\n   587→                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588→                \n   589→                # Apply final limit\n   590→                if limit and len(aggregated_results) > limit:\n   591→                    aggregated_results = aggregated_results[:limit]\n   592→                \n   593→                logger.debug(f""Search aggregation stats: {stats.total_results} total, ""\n   594→                           f""{stats.unique_results} unique, {stats.plugins_used} plugins used, ""\n   595→                           f""document_query={is_doc_query}"")\n   596→                \n   597→                self._operation_stats[\'searches\'] += 1\n   598→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   599→                \n   600→                # Yield primary results from aggregated results\n   601→                for aggregated in aggregated_results:\n   602→                    yield aggregated.primary_result\n   603→            else:\n   604→                # Fallback to basic search\n   605→                # Detect if this is a document query\n   606→                is_doc_query = self._is_document_query(query)\n   607→                \n   608→                # Expand query if it\'s a document query\n   609→                queries = [query]\n   610→                if is_doc_query:\n   611→                    queries = self._expand_document_query(query)\n   612→                    semantic = True  # Force semantic search for natural language\n   613→                \n   614→                opts = {""semantic"": semantic, ""limit"": limit}\n   615→                all_results = []\n   616→                \n   617→                # Search with all query variations\n   618→                for search_query in queries:\n   619→                    for p in self._plugins:\n   620→                        try:\n   621→                            for result in p.search(search_query, opts):\n   622→                                all_results.append(result)\n   623→                        except Exception as e:\n   624→                            logger.warning(f""Plugin {p.lang} failed to search for {search_query}: {e}"")\n   625→                \n   626→                # Deduplicate results\n   627→                seen = set()\n   628→                unique_results = []\n   629→                for result in all_results:\n   630→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   631→                    if key not in seen:\n   632→                        seen.add(key)\n   633→                        unique_results.append(result)\n   634→                \n   635→                # Sort by score if available\n   636→                unique_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   637→                \n   638→                # Prioritize documentation files for document queries\n   639→                if is_doc_query:\n   640→                    doc_results = []\n   641→                    code_results = []\n   642→                    for result in unique_results:\n   643→                        if self._is_documentation_file(result.get(\'file\', \'\')):\n   644→                            doc_results.append(result)\n   645→                        else:\n   646→                            code_results.append(result)\n   647→                    unique_results = doc_results + code_results\n   648→                \n   649→                # Apply limit\n   650→                count = 0\n   651→                for result in unique_results:\n   652→                    if limit and count >= limit:\n   653→                        break\n   654→                    yield result\n   655→                    count += 1\n   656→                \n   657→                self._operation_stats[\'searches\'] += 1\n   658→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   659→                \n   660→        except Exception as e:\n   661→            logger.error(f""Error in search for {query}: {e}"", exc_info=True)\n   662→    \n   663→    def index_file(self, path: Path) -> None:\n   664→        """"""Index a single file if it has changed.""""""\n   665→        try:\n   666→            # Ensure path is absolute to avoid relative/absolute path issues\n   667→            path = path.resolve()\n   668→            \n   669→            # Find the appropriate plugin\n   670→            plugin = self._match_plugin(path)\n   671→            \n   672→            # Read file content\n   673→            try:\n   674→                content = path.read_text(encoding=\'utf-8\')\n   675→            except UnicodeDecodeError:\n   676→                # Try with different encodings\n   677→                try:\n   678→                    content = path.read_text(encoding=\'latin-1\')\n   679→                except Exception as e:\n   680→                    logger.error(f""Failed to read {path}: {e}"")\n   681→                    return\n   682→            \n   683→            # Check if we need to re-index (simplified for now)\n   684→            # TODO: Implement proper caching logic\n   685→            \n   686→            # Index the file\n   687→            start_time = time.time()\n   688→            logger.info(f""Indexing {path} with {plugin.lang} plugin"")\n   689→            shard = plugin.indexFile(path, content)\n   690→            \n   691→            # Record performance if advanced features enabled\n   692→            if self._enable_advanced and self._router:\n   693→                execution_time = time.time() - start_time\n   694→                self._router.record_performance(plugin, execution_time)\n   695→            \n   696→            self._operation_stats[\'indexings\'] += 1\n   697→            self._operation_stats[\'total_time\'] += time.time() - start_time\n   698→            \n   699→            logger.info(f""Successfully indexed {path}: {len(shard.get(\'symbols\', []))} symbols found"")\n   700→            \n   701→        except RuntimeError as e:\n   702→            # No plugin found for this file type\n   703→            logger.debug(f""No plugin for {path}: {e}"")\n   704→        except Exception as e:\n   705→            logger.error(f""Error indexing {path}: {e}"", exc_info=True)\n   706→    \n   707→    def get_statistics(self) -> dict:\n   708→        """"""Get comprehensive statistics across all plugins and components.""""""\n   709→        stats = {\n   710→            ""total_plugins"": len(self._plugins),\n   711→            ""loaded_languages"": sorted(list(self._loaded_languages)),\n   712→            ""supported_languages"": len(self.supported_languages),\n   713→            ""operations"": self._operation_stats.copy()\n   714→        }\n   715→        \n   716→        # Add language breakdown\n   717→        stats[""by_language""] = {}\n   718→        for lang, plugin in self._by_lang.items():\n   719→            plugin_info = {\n   720→                ""loaded"": True,\n   721→                ""class"": plugin.__class__.__name__\n   722→            }\n   723→            if hasattr(plugin, \'get_indexed_count\'):\n   724→                plugin_info[""indexed_files""] = plugin.get_indexed_count()\n   725→            stats[""by_language""][lang] = plugin_info\n   726→        \n   727→        return stats\n   728→    \n   729→    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730→        """"""\n   731→        Index all files in a directory, respecting ignore patterns.\n   732→        \n   733→        Args:\n   734→            directory: Directory to index\n   735→            recursive: Whether to index subdirectories\n   736→            \n   737→        Returns:\n   738→            Statistics about indexed files\n   739→        """"""\n   740→        logger.info(f""Indexing directory: {directory} (recursive={recursive})"")\n   741→        \n   742→        # Note: We don\'t use ignore patterns during indexing\n   743→        # ALL files are indexed for local search capability\n   744→        # Filtering happens only during export/sharing\n   745→        \n   746→        # Get all supported extensions\n   747→        supported_extensions = get_all_extensions()\n   748→        \n   749→        stats = {\n   750→            ""total_files"": 0,\n   751→            ""indexed_files"": 0,\n   752→            ""ignored_files"": 0,\n   753→            ""failed_files"": 0,\n   754→            ""by_language"": {}\n   755→        }\n   756→        \n   757→        # Walk directory\n   758→        if recursive:\n   759→            file_iterator = directory.rglob(""*"")\n   760→        else:\n   761→            file_iterator = directory.glob(""*"")\n   762→            \n   763→        for path in file_iterator:\n   764→            if not path.is_file():\n   765→                continue\n   766→                \n   767→            stats[""total_files""] += 1\n   768→            \n   769→            # NOTE: We index ALL files locally, including gitignored ones\n   770→            # Filtering happens only during export/sharing\n   771→            # This allows local search of .env, secrets, etc.\n   772→                \n   773→            # Try to find a plugin that supports this file\n   774→            # This allows us to index ALL files, including .env, .key, etc.\n   775→            try:\n   776→                # First try to match by extension\n   777→                if path.suffix in supported_extensions:\n   778→                    self.index_file(path)\n   779→                    stats[""indexed_files""] += 1\n   780→                # For files without recognized extensions, try each plugin\'s supports() method\n   781→                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782→                else:\n   783→                    matched = False\n   784→                    for plugin in self._plugins:\n   785→                        if plugin.supports(path):\n   786→                            self.index_file(path)\n   787→                            stats[""indexed_files""] += 1\n   788→                            matched = True\n   789→                            break\n   790→                    \n   791→                    # If no plugin matched but we want to index everything,\n   792→                    # we could add a fallback here to index as plaintext\n   793→                    # For now, we\'ll skip unmatched files\n   794→                    if not matched:\n   795→                        logger.debug(f""No plugin found for {path}"")\n   796→                \n   797→                # Track by language\n   798→                language = get_language_by_extension(path.suffix)\n   799→                if language:\n   800→                    stats[""by_language""][language] = stats[""by_language""].get(language, 0) + 1\n   801→                    \n   802→            except Exception as e:\n   803→                logger.error(f""Failed to index {path}: {e}"")\n   804→                stats[""failed_files""] += 1\n   805→                \n   806→        logger.info(f""Directory indexing complete: {stats[\'indexed_files\']} indexed, ""\n   807→                   f""{stats[\'ignored_files\']} ignored, {stats[\'failed_files\']} failed"")\n   808→        \n   809→        return stats\n   810→    \n   811→    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812→        """"""Search specifically across documentation files.\n   813→        \n   814→        Args:\n   815→            topic: Topic to search for (e.g., ""installation"", ""configuration"")\n   816→            doc_types: Optional list of document types to search (e.g., [""readme"", ""guide"", ""api""])\n   817→            limit: Maximum number of results\n   818→            \n   819→        Returns:\n   820→            Search results from documentation files\n   821→        """"""\n   822→        # Default document types if not specified\n   823→        if doc_types is None:\n   824→            doc_types = [""readme"", ""documentation"", ""guide"", ""tutorial"", ""api"", ""changelog"", ""contributing""]\n   825→        \n   826→        # Build search queries for different document types\n   827→        queries = []\n   828→        for doc_type in doc_types:\n   829→            queries.extend([\n   830→                f""{doc_type} {topic}"",\n   831→                f""{topic} {doc_type}"",\n   832→                f""{topic} in {doc_type}""\n   833→            ])\n   834→        \n   835→        # Also search for the topic in common doc filenames\n   836→        queries.extend([\n   837→            f""README {topic}"",\n   838→            f""CONTRIBUTING {topic}"",\n   839→            f""docs {topic}"",\n   840→            f""documentation {topic}""\n   841→        ])\n   842→        \n   843→        # Deduplicate queries\n   844→        queries = list(dict.fromkeys(queries))\n   845→        \n   846→        logger.info(f""Cross-document search for \'{topic}\' with {len(queries)} query variations"")\n   847→        \n   848→        # Use the enhanced search with document-specific handling\n   849→        all_results = []\n   850→        seen = set()\n   851→        \n   852→        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853→            for result in self.search(query, semantic=True, limit=limit):\n   854→                # Only include documentation files\n   855→                if self._is_documentation_file(result.get(\'file\', \'\')):\n   856→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   857→                    if key not in seen:\n   858→                        seen.add(key)\n   859→                        all_results.append(result)\n   860→        \n   861→        # Sort by relevance (score) and return top results\n   862→        all_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   863→        \n   864→        count = 0\n   865→        for result in all_results:\n   866→            if count >= limit:\n   867→                break\n   868→            yield result\n   869→            count += 1\n   870→    \n   871→    def health_check(self) -> Dict[str, Any]:\n   872→        """"""Perform a health check on all components.""""""\n   873→        health = {\n   874→            \'status\': \'healthy\',\n   875→            \'components\': {\n   876→                \'dispatcher\': {\n   877→                    \'status\': \'healthy\',\n   878→                    \'plugins_loaded\': len(self._plugins),\n   879→                    \'languages_supported\': len(self.supported_languages),\n   880→                    \'factory_enabled\': self._use_factory,\n   881→                    \'lazy_loading\': self._lazy_load\n   882→                }\n   883→            },\n   884→            \'plugins\': {},\n   885→            \'errors\': []\n   886→        }\n   887→        \n   888→        # Check plugin health\n   889→        for lang, plugin in self._by_lang.items():\n   890→            try:\n   891→                plugin_health = {\n   892→                    \'status\': \'healthy\',\n   893→                    \'class\': plugin.__class__.__name__,\n   894→                    \'semantic_enabled\': getattr(plugin, \'_enable_semantic\', False)\n   895→                }\n   896→                if hasattr(plugin, \'get_indexed_count\'):\n   897→                    plugin_health[\'indexed_files\'] = plugin.get_indexed_count()\n   898→            except Exception as e:\n   899→                plugin_health = {\n   900→                    \'status\': \'error\',\n   901→                    \'error\': str(e)\n   902→                }\n   903→                health[\'errors\'].append(f""Plugin {lang}: {str(e)}"")\n   904→            \n   905→            health[\'plugins\'][lang] = plugin_health\n   906→        \n   907→        # Determine overall health\n   908→        if len(health[\'errors\']) > 0:\n   909→            health[\'status\'] = \'degraded\' if len(health[\'errors\']) < 3 else \'unhealthy\'\n   910→        \n   911→        return health\n   912→    \n   913→    def remove_file(self, path: Union[Path, str]) -> None:\n   914→        """"""Remove a file from all indexes.\n   915→        \n   916→        Args:\n   917→            path: File path to remove\n   918→        """"""\n   919→        path = Path(path).resolve()\n   920→        logger.info(f""Removing file from index: {path}"")\n   921→        \n   922→        try:\n   923→            # Remove from SQLite if available\n   924→            if self._sqlite_store:\n   925→                from ..core.path_resolver import PathResolver\n   926→                path_resolver = PathResolver()\n   927→                try:\n   928→                    relative_path = path_resolver.normalize_path(path)\n   929→                    # Get repository ID - for now assume 1\n   930→                    # TODO: Properly detect repository\n   931→                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932→                except Exception as e:\n   933→                    logger.error(f""Error removing from SQLite: {e}"")\n   934→            \n   935→            # Remove from semantic index if available\n   936→            try:\n   937→                plugin = self._match_plugin(path)\n   938→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   939→                    plugin._indexer.remove_file(path)\n   940→                    logger.info(f""Removed from semantic index: {path}"")\n   941→            except Exception as e:\n   942→                logger.warning(f""Error removing from semantic index: {e}"")\n   943→            \n   944→            # Update statistics\n   945→            self._operation_stats[\'deletions\'] = self._operation_stats.get(\'deletions\', 0) + 1\n   946→            \n   947→        except Exception as e:\n   948→            logger.error(f""Error removing file {path}: {e}"", exc_info=True)\n   949→    \n   950→    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951→        """"""Move a file in all indexes.\n   952→        \n   953→        Args:\n   954→            old_path: Original file path\n   955→            new_path: New file path\n   956→            content_hash: Optional content hash to verify unchanged content\n   957→        """"""\n   958→        old_path = Path(old_path).resolve()\n   959→        new_path = Path(new_path).resolve()\n   960→        logger.info(f""Moving file in index: {old_path} -> {new_path}"")\n   961→        \n   962→        try:\n   963→            # Move in SQLite if available\n   964→            if self._sqlite_store:\n   965→                from ..core.path_resolver import PathResolver\n   966→                path_resolver = PathResolver()\n   967→                try:\n   968→                    old_relative = path_resolver.normalize_path(old_path)\n   969→                    new_relative = path_resolver.normalize_path(new_path)\n   970→                    # Get repository ID - for now assume 1\n   971→                    # TODO: Properly detect repository\n   972→                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973→                except Exception as e:\n   974→                    logger.error(f""Error moving in SQLite: {e}"")\n   975→            \n   976→            # Move in semantic index if available\n   977→            try:\n   978→                plugin = self._match_plugin(new_path)\n   979→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   980→                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981→                    logger.info(f""Moved in semantic index: {old_path} -> {new_path}"")\n   982→            except Exception as e:\n   983→                logger.warning(f""Error moving in semantic index: {e}"")\n   984→            \n   985→            # Update statistics\n   986→            self._operation_stats[\'moves\'] = self._operation_stats.get(\'moves\', 0) + 1\n   987→            \n   988→        except Exception as e:\n   989→            logger.error(f""Error moving file {old_path} -> {new_path}: {e}"", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,1,1,27690,0,1,0.5,0.25,0,,,,,transcript_response
a9f87083-f603-4b60-a151-8cabc58f385f,2025-06-26T07:39:47.016Z,transcript_a9f87083-f603-4b60-a151-8cabc58f385f,Find all functions that use asyncio,transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,1,1,13913,0,1,0.5,0.25,0,,,,,transcript_response
a9f87083-f603-4b60-a151-8cabc58f385f,2025-06-26T07:39:50.500Z,transcript_a9f87083-f603-4b60-a151-8cabc58f385f,"{'tool_use_id': 'toolu_012WYtGvzXaiAUGSYUpSxZed', 'type': 'tool_result', 'content': 'Found 78 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/conftest.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_developer_workflows.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_repository_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_semantic_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_plugins_direct.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/populate_bm25_quick.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/pytest.ini\n/workspaces/Code-Index-MCP/testing-env/worktree-native/requirements-production.txt\n/workspaces/Code-Index-MCP/testing-env/worktree-native/requirements.txt\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/docs/development/TESTING-GUIDE.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/grpc_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/MCP.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/prometheus_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/watchdog.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/level4/file_watcher.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-native/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/codex-setup.sh\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_reranking_working.py'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,1,1,13913,5223,1,0.5,0.25,0,,,,,transcript_response
a9f87083-f603-4b60-a151-8cabc58f385f,2025-06-26T07:39:58.653Z,transcript_a9f87083-f603-4b60-a151-8cabc58f385f,"Find all functions that use asyncio in the codebase. I need you to:
1. Search for ""import asyncio"" imports
2. Search for ""async def"" function definitions
3. Search for asyncio.run, asyncio.create_task, asyncio.gather, and other asyncio method calls
4. For each file that contains asyncio usage, read the file and identify the specific function names that use asyncio
5. Create a comprehensive list showing the file path, line number, and function name for each function that uses asyncio

Focus on the main source code files in mcp_server/ directory, not just test files. Return a detailed list of all asyncio functions with their locations.",transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,343,343,9986,886,343,0.5,0.25,0,,,,,transcript_response
a9f87083-f603-4b60-a151-8cabc58f385f,2025-06-26T07:40:06.620Z,transcript_a9f87083-f603-4b60-a151-8cabc58f385f,"{'tool_use_id': 'toolu_012G7ePQUKtfvazbgfhxUTc4', 'type': 'tool_result', 'content': 'Found 62 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/conftest.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/docs/development/TESTING-GUIDE.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/grpc_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/MCP.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/prometheus_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/watchdog.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/level4/file_watcher.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-native/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_indexer.py'}",transcript_query,unknown,native,0,True,8,0,0,0,8,0,0,0,90,90,10872,7901,90,0.5,0.25,0,,,,,transcript_response
a9f87083-f603-4b60-a151-8cabc58f385f,2025-06-26T07:40:13.309Z,transcript_a9f87083-f603-4b60-a151-8cabc58f385f,"{'tool_use_id': 'toolu_012m3HDMDdRZbrD3EnF71hyN', 'type': 'tool_result', 'content': '/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_runner.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/__main__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_base_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_base.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/generic_treesitter_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/plugin_registry.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/plugin_manager.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/plugin_loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/plugin_discovery.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/models.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/discovery.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/config.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/prometheus_exporter.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/metrics_collector.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/storage_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/shared_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/security_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/plugin_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/metrics_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/indexing_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/dispatcher_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/cache_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/api_gateway_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/query_optimizer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/bm25_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/semantic_chunker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/metadata_extractor.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/document_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/chunk_optimizer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/base_document_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/result_aggregator.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/plugin_router.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/core/path_resolver.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/core/logging.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/core/ignore_patterns.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/core/errors.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/core/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cli/index_management.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cli/artifact_commands.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cli/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/config/validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/config/settings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/config/environment.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/config/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cache/query_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cache/cache_manager.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cache/backends.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cache/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/kotlin_plugin/java_interop.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/kotlin_plugin/coroutines_analyzer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/kotlin_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/js_plugin/plugin_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/js_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/js_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/java_plugin/type_analyzer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/java_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/java_plugin/import_resolver.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/java_plugin/build_system.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/java_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/html_css_plugin/plugin_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/html_css_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/html_css_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/go_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/go_plugin/package_analyzer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/go_plugin/module_resolver.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/go_plugin/interface_checker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/go_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/dart_plugin/plugin_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/dart_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/dart_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/type_analyzer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/nuget_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/namespace_resolver.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/cpp_plugin/plugin_semantic.py\n(Results are truncated. Consider using a more specific path or pattern.)'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,391,391,18773,3726,391,0.5,0.25,0,,,,,transcript_response
a9f87083-f603-4b60-a151-8cabc58f385f,2025-06-26T07:40:21.997Z,transcript_a9f87083-f603-4b60-a151-8cabc58f385f,"{'tool_use_id': 'toolu_01Gtbau7JDxAuhDZgWePDd1p', 'type': 'tool_result', 'content': '     1→""""""\n     2→Reranking Module for Search Result Optimization\n     3→\n     4→This module provides implementations for reranking search results to improve relevance.\n     5→It supports both Cohere\'s reranking API and local cross-encoder models as fallback.\n     6→""""""\n     7→\n     8→import os\n     9→import logging\n    10→import asyncio\n    11→from typing import Dict, List, Optional, Any, Tuple\n    12→from dataclasses import dataclass\n    13→import numpy as np\n    14→from abc import ABC, abstractmethod\n    15→\n    16→# Define interfaces inline for now\n    17→from abc import ABC, abstractmethod\n    18→from dataclasses import dataclass as dc\n    19→from typing import Protocol\n    20→\n    21→# Define SearchResult inline\n    22→@dc \n    23→class SearchResult:\n    24→    """"""Search result information""""""\n    25→    file_path: str\n    26→    line: int\n    27→    column: int\n    28→    snippet: str\n    29→    match_type: str  # exact, fuzzy, semantic\n    30→    score: float\n    31→    context: Optional[str] = None\n    32→\n    33→# Define RerankItem to wrap original SearchResult\n    34→@dc\n    35→class RerankItem:\n    36→    """"""A single reranked item that preserves the complete original SearchResult""""""\n    37→    original_result: SearchResult  # Complete original SearchResult\n    38→    rerank_score: float  # The reranking score\n    39→    original_rank: int  # Original position in results\n    40→    new_rank: int  # New position after reranking\n    41→    explanation: Optional[str] = None  # Optional explanation for the ranking\n    42→\n    43→# Define RerankResult with proper structure\n    44→@dc\n    45→class RerankResult:\n    46→    """"""Result from reranking operation""""""\n    47→    results: List[RerankItem]  # List of reranked items\n    48→    metadata: Dict[str, Any]  # Metadata about the reranking operation\n    49→\n    50→# Define IReranker interface\n    51→class IReranker(ABC):\n    52→    @abstractmethod\n    53→    async def rerank(self, query: str, results: List[SearchResult], top_k: int = 10) -> RerankResult:\n    54→        pass\n    55→\n    56→# Define IRerankerFactory interface  \n    57→class IRerankerFactory(ABC):\n    58→    @abstractmethod\n    59→    def create_reranker(self, config: Any) -> Optional[IReranker]:\n    60→        pass\n    61→\n    62→class MCPError(Exception):\n    63→    """"""Base exception for MCP errors.""""""\n    64→    pass\n    65→\n    66→# Simple Result class for this module\n    67→class Result:\n    68→    def __init__(self, success: bool, data=None, error=None):\n    69→        self.is_success = success\n    70→        self.data = data\n    71→        self.error = error\n    72→    \n    73→    @classmethod\n    74→    def ok(cls, data=None):\n    75→        return cls(True, data)\n    76→    \n    77→    @classmethod\n    78→    def error(cls, error):\n    79→        return cls(False, None, error)\n    80→\n    81→logger = logging.getLogger(__name__)\n    82→\n    83→\n    84→class BaseReranker(IReranker, ABC):\n    85→    """"""Base class for all reranker implementations""""""\n    86→    \n    87→    def __init__(self, config: Dict[str, Any]):\n    88→        self.config = config\n    89→        self._cache: Dict[str, Any] = {}  # Simple in-memory cache\n    90→        self.cache_ttl = config.get(\'cache_ttl\', 3600)  # 1 hour default\n    91→        self.initialized = False\n    92→    \n    93→    async def _get_cache_key(self, query: str, results: List[SearchResult]) -> str:\n    94→        """"""Generate cache key for reranking results""""""\n    95→        # Create a deterministic key based on query and result IDs\n    96→        result_ids = [f""{r.file_path}:{r.line}"" for r in results[:10]]  # Use top 10 for key\n    97→        return f""rerank:{self.__class__.__name__}:{hash(query)}:{hash(tuple(result_ids))}""\n    98→    \n    99→    async def _get_cached_results(\n   100→        self, \n   101→        query: str, \n   102→        results: List[SearchResult]\n   103→    ) -> Optional[List[RerankItem]]:\n   104→        """"""Get cached reranking results if available""""""\n   105→        cache_key = await self._get_cache_key(query, results)\n   106→        \n   107→        if cache_key in self._cache:\n   108→            cached_data = self._cache[cache_key]\n   109→            # Check if cache is still valid (simple time-based check)\n   110→            import time\n   111→            if time.time() - cached_data[\'timestamp\'] < self.cache_ttl:\n   112→                logger.debug(f""Cache hit for reranking query: {query}"")\n   113→                return cached_data[\'results\']\n   114→            else:\n   115→                # Cache expired\n   116→                del self._cache[cache_key]\n   117→        \n   118→        return None\n   119→    \n   120→    async def _cache_results(\n   121→        self, \n   122→        query: str, \n   123→        results: List[SearchResult], \n   124→        reranked: List[RerankItem]\n   125→    ):\n   126→        """"""Cache reranking results""""""\n   127→        import time\n   128→        cache_key = await self._get_cache_key(query, results)\n   129→        self._cache[cache_key] = {\n   130→            \'results\': reranked,\n   131→            \'timestamp\': time.time()\n   132→        }\n   133→        \n   134→        # Simple cache size limit\n   135→        if len(self._cache) > 1000:\n   136→            # Remove oldest entries\n   137→            sorted_keys = sorted(\n   138→                self._cache.keys(), \n   139→                key=lambda k: self._cache[k][\'timestamp\']\n   140→            )\n   141→            for key in sorted_keys[:100]:  # Remove oldest 100\n   142→                del self._cache[key]\n   143→\n   144→\n   145→class CohereReranker(BaseReranker):\n   146→    """"""Reranker using Cohere\'s reranking API""""""\n   147→    \n   148→    def __init__(self, config: Dict[str, Any]):\n   149→        super().__init__(config)\n   150→        self.api_key = config.get(\'cohere_api_key\') or os.getenv(\'COHERE_API_KEY\')\n   151→        self.model = config.get(\'model\', \'rerank-english-v2.0\')\n   152→        self.client = None\n   153→    \n   154→    async def initialize(self, config: Dict[str, Any]) -> Result:\n   155→        """"""Initialize Cohere client""""""\n   156→        try:\n   157→            if not self.api_key:\n   158→                return Result.error(""Cohere API key not configured"")\n   159→            \n   160→            # Lazy import to avoid dependency if not used\n   161→            try:\n   162→                import cohere\n   163→                self.client = cohere.Client(self.api_key)\n   164→                self.initialized = True\n   165→                logger.info(f""Initialized Cohere reranker with model: {self.model}"")\n   166→                return Result.ok(None)\n   167→            except ImportError:\n   168→                return Result.error(\n   169→                    ""Cohere library not installed. Install with: pip install cohere""\n   170→                )\n   171→        except Exception as e:\n   172→            logger.error(f""Failed to initialize Cohere reranker: {e}"")\n   173→            return Result.error(f""Initialization failed: {str(e)}"")\n   174→    \n   175→    async def shutdown(self) -> Result:\n   176→        """"""Shutdown Cohere client""""""\n   177→        self.client = None\n   178→        self.initialized = False\n   179→        return Result.ok(None)\n   180→    \n   181→    async def rerank(\n   182→        self, \n   183→        query: str, \n   184→        results: List[SearchResult], \n   185→        top_k: Optional[int] = None\n   186→    ) -> Result:\n   187→        """"""Rerank results using Cohere API""""""\n   188→        if not self.initialized:\n   189→            return Result.error(""Cohere reranker not initialized"")\n   190→        \n   191→        # Check cache first\n   192→        cached = await self._get_cached_results(query, results)\n   193→        if cached:\n   194→            rerank_result = RerankResult(\n   195→                results=cached[:top_k] if top_k else cached,\n   196→                metadata={\n   197→                    \'reranker\': \'cohere\',\n   198→                    \'model\': self.model,\n   199→                    \'from_cache\': True,\n   200→                    \'total_results\': len(results),\n   201→                    \'returned_results\': len(cached[:top_k]) if top_k else len(cached)\n   202→                }\n   203→            )\n   204→            return Result.ok(rerank_result)\n   205→        \n   206→        try:\n   207→            # Prepare documents for reranking\n   208→            documents = []\n   209→            for result in results:\n   210→                # Combine relevant information for reranking\n   211→                doc_text = f""{result.snippet}""\n   212→                if result.context:\n   213→                    doc_text = f""{doc_text} {result.context}""\n   214→                documents.append(doc_text)\n   215→            \n   216→            # Call Cohere rerank API\n   217→            response = await asyncio.to_thread(\n   218→                self.client.rerank,\n   219→                model=self.model,\n   220→                query=query,\n   221→                documents=documents,\n   222→                top_n=top_k or len(results)\n   223→            )\n   224→            \n   225→            # Build reranked results\n   226→            reranked_items = []\n   227→            for idx, result in enumerate(response.results):\n   228→                original_idx = result.index\n   229→                original_result = results[original_idx]\n   230→                \n   231→                rerank_item = RerankItem(\n   232→                    original_result=original_result,\n   233→                    rerank_score=result.relevance_score,\n   234→                    original_rank=original_idx,\n   235→                    new_rank=idx\n   236→                )\n   237→                reranked_items.append(rerank_item)\n   238→            \n   239→            # Cache results\n   240→            await self._cache_results(query, results, reranked_items)\n   241→            \n   242→            # Create RerankResult with metadata\n   243→            rerank_result = RerankResult(\n   244→                results=reranked_items,\n   245→                metadata={\n   246→                    \'reranker\': \'cohere\',\n   247→                    \'model\': self.model,\n   248→                    \'from_cache\': False,\n   249→                    \'total_results\': len(results),\n   250→                    \'returned_results\': len(reranked_items)\n   251→                }\n   252→            )\n   253→            \n   254→            return Result.ok(rerank_result)\n   255→            \n   256→        except Exception as e:\n   257→            logger.error(f""Cohere reranking failed: {e}"")\n   258→            return Result.error(f""Reranking failed: {str(e)}"")\n   259→    \n   260→    def get_capabilities(self) -> Dict[str, Any]:\n   261→        """"""Get Cohere reranker capabilities""""""\n   262→        return {\n   263→            \'name\': \'Cohere Reranker\',\n   264→            \'model\': self.model,\n   265→            \'supports_multilingual\': self.model.startswith(\'rerank-multilingual\'),\n   266→            \'max_documents\': 1000,\n   267→            \'requires_api_key\': True,\n   268→            \'initialized\': self.initialized\n   269→        }\n   270→\n   271→\n   272→class LocalCrossEncoderReranker(BaseReranker):\n   273→    """"""Local reranker using cross-encoder models""""""\n   274→    \n   275→    def __init__(self, config: Dict[str, Any]):\n   276→        super().__init__(config)\n   277→        self.model_name = config.get(\'model\', \'cross-encoder/ms-marco-MiniLM-L-6-v2\')\n   278→        self.device = config.get(\'device\', \'cpu\')\n   279→        self.model = None\n   280→        self.tokenizer = None\n   281→    \n   282→    async def initialize(self, config: Dict[str, Any]) -> Result:\n   283→        """"""Initialize cross-encoder model""""""\n   284→        try:\n   285→            # Lazy import to avoid dependency if not used\n   286→            try:\n   287→                from sentence_transformers import CrossEncoder\n   288→                \n   289→                logger.info(f""Loading cross-encoder model: {self.model_name}"")\n   290→                self.model = CrossEncoder(self.model_name, device=self.device)\n   291→                self.initialized = True\n   292→                logger.info(f""Initialized local cross-encoder reranker on {self.device}"")\n   293→                return Result.ok(None)\n   294→                \n   295→            except ImportError:\n   296→                return Result.error(\n   297→                    ""Sentence-transformers library not installed. ""\n   298→                    ""Install with: pip install sentence-transformers""\n   299→                )\n   300→        except Exception as e:\n   301→            logger.error(f""Failed to initialize cross-encoder: {e}"")\n   302→            return Result.error(f""Initialization failed: {str(e)}"")\n   303→    \n   304→    async def shutdown(self) -> Result:\n   305→        """"""Shutdown cross-encoder model""""""\n   306→        self.model = None\n   307→        self.initialized = False\n   308→        return Result.ok(None)\n   309→    \n   310→    async def rerank(\n   311→        self, \n   312→        query: str, \n   313→        results: List[SearchResult], \n   314→        top_k: Optional[int] = None\n   315→    ) -> Result:\n   316→        """"""Rerank results using cross-encoder model""""""\n   317→        if not self.initialized:\n   318→            return Result.error(""Cross-encoder reranker not initialized"")\n   319→        \n   320→        # Check cache first\n   321→        cached = await self._get_cached_results(query, results)\n   322→        if cached:\n   323→            rerank_result = RerankResult(\n   324→                results=cached[:top_k] if top_k else cached,\n   325→                metadata={\n   326→                    \'reranker\': \'cross-encoder\',\n   327→                    \'model\': self.model_name,\n   328→                    \'device\': self.device,\n   329→                    \'from_cache\': True,\n   330→                    \'total_results\': len(results),\n   331→                    \'returned_results\': len(cached[:top_k]) if top_k else len(cached)\n   332→                }\n   333→            )\n   334→            return Result.ok(rerank_result)\n   335→        \n   336→        try:\n   337→            # Prepare query-document pairs\n   338→            pairs = []\n   339→            for result in results:\n   340→                # Combine relevant information for reranking\n   341→                doc_text = f""{result.snippet}""\n   342→                if result.context:\n   343→                    doc_text = f""{doc_text} {result.context}""\n   344→                pairs.append([query, doc_text])\n   345→            \n   346→            # Get scores from cross-encoder\n   347→            scores = await asyncio.to_thread(self.model.predict, pairs)\n   348→            \n   349→            # Create indexed scores for sorting\n   350→            indexed_scores = [(score, idx) for idx, score in enumerate(scores)]\n   351→            indexed_scores.sort(reverse=True, key=lambda x: x[0])\n   352→            \n   353→            # Build reranked results\n   354→            reranked_items = []\n   355→            for new_rank, (score, original_idx) in enumerate(indexed_scores):\n   356→                if top_k and new_rank >= top_k:\n   357→                    break\n   358→                \n   359→                rerank_item = RerankItem(\n   360→                    original_result=results[original_idx],\n   361→                    rerank_score=float(score),\n   362→                    original_rank=original_idx,\n   363→                    new_rank=new_rank\n   364→                )\n   365→                reranked_items.append(rerank_item)\n   366→            \n   367→            # Cache results\n   368→            await self._cache_results(query, results, reranked_items)\n   369→            \n   370→            # Create RerankResult with metadata\n   371→            rerank_result = RerankResult(\n   372→                results=reranked_items,\n   373→                metadata={\n   374→                    \'reranker\': \'cross-encoder\',\n   375→                    \'model\': self.model_name,\n   376→                    \'device\': self.device,\n   377→                    \'from_cache\': False,\n   378→                    \'total_results\': len(results),\n   379→                    \'returned_results\': len(reranked_items)\n   380→                }\n   381→            )\n   382→            \n   383→            return Result.ok(rerank_result)\n   384→            \n   385→        except Exception as e:\n   386→            logger.error(f""Cross-encoder reranking failed: {e}"")\n   387→            return Result.error(f""Reranking failed: {str(e)}"")\n   388→    \n   389→    def get_capabilities(self) -> Dict[str, Any]:\n   390→        """"""Get cross-encoder reranker capabilities""""""\n   391→        return {\n   392→            \'name\': \'Local Cross-Encoder Reranker\',\n   393→            \'model\': self.model_name,\n   394→            \'device\': self.device,\n   395→            \'supports_multilingual\': \'multilingual\' in self.model_name.lower(),\n   396→            \'max_documents\': 10000,  # Limited by memory\n   397→            \'requires_api_key\': False,\n   398→            \'initialized\': self.initialized\n   399→        }\n   400→\n   401→\n   402→class TFIDFReranker(BaseReranker):\n   403→    """"""Simple TF-IDF based reranker as lightweight fallback""""""\n   404→    \n   405→    def __init__(self, config: Dict[str, Any]):\n   406→        super().__init__(config)\n   407→        self.vectorizer = None\n   408→    \n   409→    async def initialize(self, config: Dict[str, Any]) -> Result:\n   410→        """"""Initialize TF-IDF vectorizer""""""\n   411→        try:\n   412→            from sklearn.feature_extraction.text import TfidfVectorizer\n   413→            from sklearn.metrics.pairwise import cosine_similarity\n   414→            \n   415→            self.vectorizer = TfidfVectorizer(\n   416→                max_features=config.get(\'max_features\', 5000),\n   417→                ngram_range=(1, 2),\n   418→                stop_words=\'english\'\n   419→            )\n   420→            self.cosine_similarity = cosine_similarity\n   421→            self.initialized = True\n   422→            logger.info(""Initialized TF-IDF reranker"")\n   423→            return Result.ok(None)\n   424→            \n   425→        except ImportError:\n   426→            return Result.error(\n   427→                ""Scikit-learn not installed. Install with: pip install scikit-learn""\n   428→            )\n   429→        except Exception as e:\n   430→            logger.error(f""Failed to initialize TF-IDF reranker: {e}"")\n   431→            return Result.error(f""Initialization failed: {str(e)}"")\n   432→    \n   433→    async def shutdown(self) -> Result:\n   434→        """"""Shutdown TF-IDF reranker""""""\n   435→        self.vectorizer = None\n   436→        self.initialized = False\n   437→        return Result.ok(None)\n   438→    \n   439→    async def rerank(\n   440→        self, \n   441→        query: str, \n   442→        results: List[SearchResult], \n   443→        top_k: Optional[int] = None\n   444→    ) -> Result:\n   445→        """"""Rerank results using TF-IDF similarity""""""\n   446→        if not self.initialized:\n   447→            return Result.error(""TF-IDF reranker not initialized"")\n   448→        \n   449→        # Check cache first\n   450→        cached = await self._get_cached_results(query, results)\n   451→        if cached:\n   452→            rerank_result = RerankResult(\n   453→                results=cached[:top_k] if top_k else cached,\n   454→                metadata={\n   455→                    \'reranker\': \'tfidf\',\n   456→                    \'max_features\': self.config.get(\'max_features\', 5000),\n   457→                    \'from_cache\': True,\n   458→                    \'total_results\': len(results),\n   459→                    \'returned_results\': len(cached[:top_k]) if top_k else len(cached)\n   460→                }\n   461→            )\n   462→            return Result.ok(rerank_result)\n   463→        \n   464→        try:\n   465→            # Prepare documents\n   466→            documents = []\n   467→            for result in results:\n   468→                doc_text = f""{result.snippet}""\n   469→                if result.context:\n   470→                    doc_text = f""{doc_text} {result.context}""\n   471→                documents.append(doc_text)\n   472→            \n   473→            # Add query to documents for vectorization\n   474→            all_texts = [query] + documents\n   475→            \n   476→            # Vectorize texts\n   477→            tfidf_matrix = await asyncio.to_thread(\n   478→                self.vectorizer.fit_transform, all_texts\n   479→            )\n   480→            \n   481→            # Calculate similarities\n   482→            query_vector = tfidf_matrix[0:1]\n   483→            doc_vectors = tfidf_matrix[1:]\n   484→            similarities = self.cosine_similarity(query_vector, doc_vectors)[0]\n   485→            \n   486→            # Create indexed scores for sorting\n   487→            indexed_scores = [(score, idx) for idx, score in enumerate(similarities)]\n   488→            indexed_scores.sort(reverse=True, key=lambda x: x[0])\n   489→            \n   490→            # Build reranked results\n   491→            reranked_items = []\n   492→            for new_rank, (score, original_idx) in enumerate(indexed_scores):\n   493→                if top_k and new_rank >= top_k:\n   494→                    break\n   495→                \n   496→                rerank_item = RerankItem(\n   497→                    original_result=results[original_idx],\n   498→                    rerank_score=float(score),\n   499→                    original_rank=original_idx,\n   500→                    new_rank=new_rank\n   501→                )\n   502→                reranked_items.append(rerank_item)\n   503→            \n   504→            # Cache results\n   505→            await self._cache_results(query, results, reranked_items)\n   506→            \n   507→            # Create RerankResult with metadata\n   508→            rerank_result = RerankResult(\n   509→                results=reranked_items,\n   510→                metadata={\n   511→                    \'reranker\': \'tfidf\',\n   512→                    \'max_features\': self.config.get(\'max_features\', 5000),\n   513→                    \'from_cache\': False,\n   514→                    \'total_results\': len(results),\n   515→                    \'returned_results\': len(reranked_items)\n   516→                }\n   517→            )\n   518→            \n   519→            return Result.ok(rerank_result)\n   520→            \n   521→        except Exception as e:\n   522→            logger.error(f""TF-IDF reranking failed: {e}"")\n   523→            return Result.error(f""Reranking failed: {str(e)}"")\n   524→    \n   525→    def get_capabilities(self) -> Dict[str, Any]:\n   526→        """"""Get TF-IDF reranker capabilities""""""\n   527→        return {\n   528→            \'name\': \'TF-IDF Reranker\',\n   529→            \'algorithm\': \'TF-IDF with cosine similarity\',\n   530→            \'supports_multilingual\': False,\n   531→            \'max_documents\': 100000,\n   532→            \'requires_api_key\': False,\n   533→            \'initialized\': self.initialized\n   534→        }\n   535→\n   536→\n   537→class HybridReranker(BaseReranker):\n   538→    """"""Hybrid reranker that combines multiple reranking strategies""""""\n   539→    \n   540→    def __init__(self, config: Dict[str, Any]):\n   541→        super().__init__(config)\n   542→        self.primary_reranker: Optional[IReranker] = None\n   543→        self.fallback_reranker: Optional[IReranker] = None\n   544→        self.weight_primary = config.get(\'weight_primary\', 0.7)\n   545→        self.weight_fallback = config.get(\'weight_fallback\', 0.3)\n   546→    \n   547→    def set_rerankers(\n   548→        self, \n   549→        primary: IReranker, \n   550→        fallback: Optional[IReranker] = None\n   551→    ):\n   552→        """"""Set primary and fallback rerankers""""""\n   553→        self.primary_reranker = primary\n   554→        self.fallback_reranker = fallback\n   555→    \n   556→    async def initialize(self, config: Dict[str, Any]) -> Result:\n   557→        """"""Initialize hybrid reranker""""""\n   558→        if not self.primary_reranker:\n   559→            return Result.error(""Primary reranker not set"")\n   560→        \n   561→        # Initialize primary reranker\n   562→        primary_result = await self.primary_reranker.initialize(config)\n   563→        if not primary_result.is_success:\n   564→            return primary_result\n   565→        \n   566→        # Initialize fallback if available\n   567→        if self.fallback_reranker:\n   568→            fallback_result = await self.fallback_reranker.initialize(config)\n   569→            if not fallback_result.is_success:\n   570→                logger.warning(f""Fallback reranker initialization failed: {fallback_result.error}"")\n   571→        \n   572→        self.initialized = True\n   573→        return Result.ok(None)\n   574→    \n   575→    async def shutdown(self) -> Result:\n   576→        """"""Shutdown hybrid reranker""""""\n   577→        if self.primary_reranker:\n   578→            await self.primary_reranker.shutdown()\n   579→        if self.fallback_reranker:\n   580→            await self.fallback_reranker.shutdown()\n   581→        self.initialized = False\n   582→        return Result.ok(None)\n   583→    \n   584→    async def rerank(\n   585→        self, \n   586→        query: str, \n   587→        results: List[SearchResult], \n   588→        top_k: Optional[int] = None\n   589→    ) -> Result:\n   590→        """"""Rerank using hybrid approach""""""\n   591→        if not self.initialized:\n   592→            return Result.error(""Hybrid reranker not initialized"")\n   593→        \n   594→        # Try primary reranker first\n   595→        primary_result = await self.primary_reranker.rerank(query, results, top_k)\n   596→        \n   597→        if primary_result.is_success:\n   598→            # Update metadata to indicate hybrid reranker was used\n   599→            if isinstance(primary_result.data, RerankResult):\n   600→                primary_result.data.metadata[\'hybrid\'] = True\n   601→                primary_result.data.metadata[\'primary_succeeded\'] = True\n   602→            return primary_result\n   603→        \n   604→        # If primary fails and we have fallback, use it\n   605→        if self.fallback_reranker:\n   606→            logger.warning(\n   607→                f""Primary reranker failed: {primary_result.error}, using fallback""\n   608→            )\n   609→            fallback_result = await self.fallback_reranker.rerank(query, results, top_k)\n   610→            if fallback_result.is_success and isinstance(fallback_result.data, RerankResult):\n   611→                fallback_result.data.metadata[\'hybrid\'] = True\n   612→                fallback_result.data.metadata[\'primary_succeeded\'] = False\n   613→                fallback_result.data.metadata[\'fallback_reason\'] = str(primary_result.error)\n   614→            return fallback_result\n   615→        \n   616→        return primary_result\n   617→    \n   618→    def get_capabilities(self) -> Dict[str, Any]:\n   619→        """"""Get hybrid reranker capabilities""""""\n   620→        capabilities = {\n   621→            \'name\': \'Hybrid Reranker\',\n   622→            \'primary\': self.primary_reranker.get_capabilities() if self.primary_reranker else None,\n   623→            \'fallback\': self.fallback_reranker.get_capabilities() if self.fallback_reranker else None,\n   624→            \'weight_primary\': self.weight_primary,\n   625→            \'weight_fallback\': self.weight_fallback,\n   626→            \'initialized\': self.initialized\n   627→        }\n   628→        return capabilities\n   629→\n   630→\n   631→class RerankerFactory(IRerankerFactory):\n   632→    """"""Factory for creating reranker instances""""""\n   633→    \n   634→    def __init__(self):\n   635→        self.reranker_types = {\n   636→            \'cohere\': CohereReranker,\n   637→            \'cross-encoder\': LocalCrossEncoderReranker,\n   638→            \'tfidf\': TFIDFReranker,\n   639→            \'hybrid\': HybridReranker\n   640→        }\n   641→    \n   642→    def create_reranker(self, reranker_type: str, config: Dict[str, Any]) -> IReranker:\n   643→        """"""Create a reranker instance""""""\n   644→        if reranker_type not in self.reranker_types:\n   645→            raise ValueError(f""Unknown reranker type: {reranker_type}"")\n   646→        \n   647→        reranker_class = self.reranker_types[reranker_type]\n   648→        reranker = reranker_class(config)\n   649→        \n   650→        # Special handling for hybrid reranker\n   651→        if reranker_type == \'hybrid\':\n   652→            primary_type = config.get(\'primary_type\', \'cohere\')\n   653→            fallback_type = config.get(\'fallback_type\', \'tfidf\')\n   654→            \n   655→            primary = self.create_reranker(primary_type, config)\n   656→            fallback = self.create_reranker(fallback_type, config) if fallback_type else None\n   657→            \n   658→            reranker.set_rerankers(primary, fallback)\n   659→        \n   660→        return reranker\n   661→    \n   662→    def get_available_rerankers(self) -> List[str]:\n   663→        """"""Get list of available reranker types""""""\n   664→        return list(self.reranker_types.keys())\n   665→    \n   666→    def register_reranker(self, name: str, reranker_class: type):\n   667→        """"""Register a custom reranker type""""""\n   668→        if not issubclass(reranker_class, IReranker):\n   669→            raise ValueError(f""{reranker_class} must implement IReranker interface"")\n   670→        self.reranker_types[name] = reranker_class\n   671→\n   672→\n   673→# Default factory instance\n   674→default_reranker_factory = RerankerFactory()\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,native,0,True,8,0,0,0,8,0,0,0,237,237,22499,36743,237,0.5,0.25,0,,,,,transcript_response
a9f87083-f603-4b60-a151-8cabc58f385f,2025-06-26T07:40:30.654Z,transcript_a9f87083-f603-4b60-a151-8cabc58f385f,"{'tool_use_id': 'toolu_01BuEsq9qhx1CyeSJedDYa97', 'type': 'tool_result', 'content': '     1→from fastapi import FastAPI, HTTPException, Depends, status, Response\n     2→from fastapi.responses import JSONResponse, PlainTextResponse\n     3→from typing import Optional, Dict, Any, List\n     4→from pathlib import Path\n     5→import logging\n     6→import os\n     7→import time\n     8→import threading\n     9→from .dispatcher import Dispatcher\n    10→from .plugin_base import SymbolDef, SearchResult\n    11→from .storage.sqlite_store import SQLiteStore\n    12→from .watcher import FileWatcher\n    13→from .core.logging import setup_logging\n    14→from .utils.index_discovery import IndexDiscovery\n    15→from .plugin_system import PluginManager, PluginSystemConfig\n    16→from .indexer.bm25_indexer import BM25Indexer\n    17→from .indexer.hybrid_search import HybridSearch, HybridSearchConfig\n    18→from .utils.fuzzy_indexer import FuzzyIndexer\n    19→from .security import (\n    20→    SecurityConfig, AuthManager, SecurityMiddlewareStack,\n    21→    AuthCredentials, User, UserRole, Permission,\n    22→    get_current_user, get_current_active_user, require_permission, require_role\n    23→)\n    24→from .metrics import (\n    25→    get_metrics_collector, get_health_checker, HealthStatus\n    26→)\n    27→from .metrics.middleware import setup_metrics_middleware, get_business_metrics\n    28→from .metrics.prometheus_exporter import get_prometheus_exporter\n    29→from .cache import (\n    30→    CacheManagerFactory, CacheConfig, CacheBackendType, \n    31→    QueryResultCache, QueryCacheConfig, QueryType\n    32→)\n    33→\n    34→# Set up logging\n    35→setup_logging(log_level=""INFO"")\n    36→logger = logging.getLogger(__name__)\n    37→\n    38→app = FastAPI(title=""MCP Server"", description=""Code Index MCP Server with Security, Metrics, and Health Checks"")\n    39→dispatcher: Dispatcher | None = None\n    40→sqlite_store: SQLiteStore | None = None\n    41→file_watcher: FileWatcher | None = None\n    42→plugin_manager: PluginManager | None = None\n    43→plugin_loader = None  # Dynamic plugin loader\n    44→auth_manager: AuthManager | None = None\n    45→security_config: SecurityConfig | None = None\n    46→cache_manager = None\n    47→query_cache: QueryResultCache | None = None\n    48→bm25_indexer: BM25Indexer | None = None\n    49→hybrid_search: HybridSearch | None = None\n    50→fuzzy_indexer: FuzzyIndexer | None = None\n    51→\n    52→# Initialize metrics and health checking\n    53→metrics_collector = get_metrics_collector()\n    54→health_checker = get_health_checker()\n    55→business_metrics = get_business_metrics()\n    56→\n    57→# Setup metrics middleware\n    58→setup_metrics_middleware(app, enable_detailed_metrics=True)\n    59→\n    60→@app.on_event(""startup"")\n    61→async def startup_event():\n    62→    """"""Initialize the dispatcher and register plugins on startup.""""""\n    63→    global dispatcher, sqlite_store, file_watcher, plugin_manager, plugin_loader, auth_manager, security_config, cache_manager, query_cache, bm25_indexer, hybrid_search, fuzzy_indexer\n    64→    \n    65→    try:\n    66→        # Initialize security configuration\n    67→        logger.info(""Initializing security configuration..."")\n    68→        security_config = SecurityConfig(\n    69→            jwt_secret_key=os.getenv(""JWT_SECRET_KEY"", ""your-super-secret-jwt-key-change-in-production-min-32-chars""),\n    70→            jwt_algorithm=""HS256"",\n    71→            access_token_expire_minutes=int(os.getenv(""ACCESS_TOKEN_EXPIRE_MINUTES"", ""30"")),\n    72→            refresh_token_expire_days=int(os.getenv(""REFRESH_TOKEN_EXPIRE_DAYS"", ""7"")),\n    73→            password_min_length=int(os.getenv(""PASSWORD_MIN_LENGTH"", ""8"")),\n    74→            max_login_attempts=int(os.getenv(""MAX_LOGIN_ATTEMPTS"", ""5"")),\n    75→            lockout_duration_minutes=int(os.getenv(""LOCKOUT_DURATION_MINUTES"", ""15"")),\n    76→            rate_limit_requests=int(os.getenv(""RATE_LIMIT_REQUESTS"", ""100"")),\n    77→            rate_limit_window_minutes=int(os.getenv(""RATE_LIMIT_WINDOW_MINUTES"", ""1"")),\n    78→            cors_origins=os.getenv(""CORS_ORIGINS"", ""*"").split("",""),\n    79→            cors_methods=[""GET"", ""POST"", ""PUT"", ""DELETE"", ""OPTIONS""],\n    80→            cors_headers=[""*""]\n    81→        )\n    82→        \n    83→        # Initialize authentication manager\n    84→        logger.info(""Initializing authentication manager..."")\n    85→        auth_manager = AuthManager(security_config)\n    86→        \n    87→        # Create default admin user if it doesn\'t exist\n    88→        admin_user = await auth_manager.get_user_by_username(""admin"")\n    89→        if not admin_user:\n    90→            admin_password = os.getenv(""DEFAULT_ADMIN_PASSWORD"", ""admin123!"")\n    91→            logger.info(""Creating default admin user..."")\n    92→            await auth_manager.create_user(\n    93→                username=""admin"",\n    94→                password=admin_password,\n    95→                email=os.getenv(""DEFAULT_ADMIN_EMAIL"", ""admin@localhost""),\n    96→                role=UserRole.ADMIN\n    97→            )\n    98→            logger.info(""Default admin user created"")\n    99→        \n   100→        # Set up security middleware\n   101→        logger.info(""Setting up security middleware..."")\n   102→        security_middleware = SecurityMiddlewareStack(app, security_config, auth_manager)\n   103→        security_middleware.setup_middleware()\n   104→        logger.info(""Security middleware configured successfully"")\n   105→        \n   106→        # Initialize cache system\n   107→        logger.info(""Initializing cache system..."")\n   108→        cache_backend_type = os.getenv(""CACHE_BACKEND"", ""memory"").lower()\n   109→        redis_url = os.getenv(""REDIS_URL"", ""redis://localhost:6379"")\n   110→        \n   111→        if cache_backend_type == ""redis"":\n   112→            try:\n   113→                cache_manager = CacheManagerFactory.create_redis_cache(\n   114→                    redis_url=redis_url,\n   115→                    default_ttl=int(os.getenv(""CACHE_DEFAULT_TTL"", ""3600""))\n   116→                )\n   117→                logger.info(""Using Redis cache backend"")\n   118→            except Exception as e:\n   119→                logger.warning(f""Failed to initialize Redis cache, falling back to memory: {e}"")\n   120→                cache_manager = CacheManagerFactory.create_memory_cache()\n   121→        elif cache_backend_type == ""hybrid"":\n   122→            try:\n   123→                cache_manager = CacheManagerFactory.create_hybrid_cache(\n   124→                    redis_url=redis_url,\n   125→                    max_entries=int(os.getenv(""CACHE_MAX_ENTRIES"", ""1000"")),\n   126→                    max_memory_mb=int(os.getenv(""CACHE_MAX_MEMORY_MB"", ""100"")),\n   127→                    default_ttl=int(os.getenv(""CACHE_DEFAULT_TTL"", ""3600""))\n   128→                )\n   129→                logger.info(""Using hybrid cache backend"")\n   130→            except Exception as e:\n   131→                logger.warning(f""Failed to initialize hybrid cache, falling back to memory: {e}"")\n   132→                cache_manager = CacheManagerFactory.create_memory_cache()\n   133→        else:\n   134→            cache_manager = CacheManagerFactory.create_memory_cache(\n   135→                max_entries=int(os.getenv(""CACHE_MAX_ENTRIES"", ""1000"")),\n   136→                max_memory_mb=int(os.getenv(""CACHE_MAX_MEMORY_MB"", ""100"")),\n   137→                default_ttl=int(os.getenv(""CACHE_DEFAULT_TTL"", ""3600""))\n   138→            )\n   139→            logger.info(""Using memory cache backend"")\n   140→        \n   141→        await cache_manager.initialize()\n   142→        \n   143→        # Initialize query result cache\n   144→        query_cache_config = QueryCacheConfig(\n   145→            enabled=os.getenv(""QUERY_CACHE_ENABLED"", ""true"").lower() == ""true"",\n   146→            default_ttl=int(os.getenv(""QUERY_CACHE_DEFAULT_TTL"", ""300"")),\n   147→            symbol_lookup_ttl=int(os.getenv(""QUERY_CACHE_SYMBOL_TTL"", ""1800"")),\n   148→            search_ttl=int(os.getenv(""QUERY_CACHE_SEARCH_TTL"", ""600"")),\n   149→            semantic_search_ttl=int(os.getenv(""QUERY_CACHE_SEMANTIC_TTL"", ""3600""))\n   150→        )\n   151→        query_cache = QueryResultCache(cache_manager, query_cache_config)\n   152→        logger.info(""Query result cache initialized successfully"")\n   153→        # Check for portable index first\n   154→        workspace_root = Path(""."")\n   155→        discovery = IndexDiscovery(workspace_root)\n   156→        \n   157→        if discovery.is_index_enabled():\n   158→            logger.info(""MCP portable index detected"")\n   159→            \n   160→            # Try to use existing index\n   161→            index_path = discovery.get_local_index_path()\n   162→            \n   163→            if not index_path and discovery.should_download_index():\n   164→                logger.info(""Attempting to download index from GitHub artifacts..."")\n   165→                if discovery.download_latest_index():\n   166→                    index_path = discovery.get_local_index_path()\n   167→                    logger.info(""Successfully downloaded index from artifacts"")\n   168→                else:\n   169→                    logger.info(""Could not download index, will use default"")\n   170→            \n   171→            if index_path:\n   172→                logger.info(f""Using portable index: {index_path}"")\n   173→                sqlite_store = SQLiteStore(str(index_path))\n   174→                \n   175→                # Log index info\n   176→                info = discovery.get_index_info()\n   177→                if info[\'metadata\']:\n   178→                    meta = info[\'metadata\']\n   179→                    logger.info(f""Index created: {meta.get(\'created_at\', \'unknown\')}"")\n   180→                    logger.info(f""Index commit: {meta.get(\'commit\', \'unknown\')[:8]}"")\n   181→            else:\n   182→                logger.info(""No portable index found, using default"")\n   183→                sqlite_store = SQLiteStore(""code_index.db"")\n   184→        else:\n   185→            # Initialize SQLite store with default\n   186→            logger.info(""Initializing SQLite store with default path..."")\n   187→            sqlite_store = SQLiteStore(""code_index.db"")\n   188→        \n   189→        logger.info(""SQLite store initialized successfully"")\n   190→        \n   191→        # Initialize plugin system with dynamic discovery\n   192→        logger.info(""Initializing plugin system with dynamic discovery..."")\n   193→        from .plugin_system.discovery import get_plugin_discovery\n   194→        from .plugin_system.loader import get_plugin_loader\n   195→        \n   196→        # Discover all available plugins\n   197→        plugin_discovery = get_plugin_discovery()\n   198→        discovered = plugin_discovery.discover_plugins()\n   199→        logger.info(f""Discovered {len(discovered)} plugins: {list(discovered.keys())}"")\n   200→        \n   201→        # Initialize plugin loader\n   202→        plugin_loader = get_plugin_loader()\n   203→        \n   204→        # Load plugins based on configuration or all discovered plugins\n   205→        config_path = Path(""plugins.yaml"")\n   206→        if config_path.exists():\n   207→            # Load specific plugins from config\n   208→            import yaml\n   209→            with open(config_path, \'r\') as f:\n   210→                plugin_config = yaml.safe_load(f)\n   211→            \n   212→            enabled_languages = plugin_config.get(\'enabled_languages\', list(discovered.keys()))\n   213→            logger.info(f""Loading plugins for languages: {enabled_languages}"")\n   214→        else:\n   215→            # Load all discovered plugins\n   216→            enabled_languages = list(discovered.keys())\n   217→            logger.info(""No plugins.yaml found, loading all discovered plugins"")\n   218→        \n   219→        # Load plugins\n   220→        plugin_instances = []\n   221→        for language in enabled_languages:\n   222→            try:\n   223→                plugin = plugin_loader.load_plugin(language)\n   224→                if plugin:\n   225→                    plugin_instances.append(plugin)\n   226→                    logger.info(f""Successfully loaded plugin for {language}"")\n   227→            except Exception as e:\n   228→                logger.error(f""Failed to load plugin for {language}: {e}"")\n   229→        \n   230→        logger.info(f""Loaded {len(plugin_instances)} plugins"")\n   231→        \n   232→        # Create plugin manager for compatibility\n   233→        plugin_manager = PluginManager(sqlite_store=sqlite_store)\n   234→        # Register loaded plugins with manager\n   235→        for plugin in plugin_instances:\n   236→            if hasattr(plugin, \'get_language\'):\n   237→                lang = plugin.get_language()\n   238→                plugin_manager._plugins[lang] = plugin\n   239→        \n   240→        logger.info(f""Loaded {len(plugin_instances)} active plugins"")\n   241→        \n   242→        # Create a new Dispatcher instance with the loaded plugins\n   243→        logger.info(""Creating dispatcher..."")\n   244→        dispatcher = Dispatcher(plugin_instances)\n   245→        logger.info(f""Dispatcher created with {len(plugin_instances)} plugins"")\n   246→        \n   247→        # Initialize BM25 indexer\n   248→        logger.info(""Initializing BM25 indexer..."")\n   249→        bm25_indexer = BM25Indexer(sqlite_store)\n   250→        logger.info(""BM25 indexer initialized successfully"")\n   251→        \n   252→        # Initialize Fuzzy indexer\n   253→        logger.info(""Initializing Fuzzy indexer..."")\n   254→        fuzzy_indexer = FuzzyIndexer(sqlite_store)\n   255→        logger.info(""Fuzzy indexer initialized successfully"")\n   256→        \n   257→        # Check if semantic indexer is available\n   258→        semantic_indexer = None\n   259→        try:\n   260→            from .utils.semantic_indexer import SemanticIndexer\n   261→            semantic_indexer = SemanticIndexer(sqlite_store)\n   262→            logger.info(""Semantic indexer initialized successfully"")\n   263→        except ImportError:\n   264→            logger.warning(""Semantic indexer not available (missing dependencies)"")\n   265→        \n   266→        # Initialize Hybrid Search\n   267→        logger.info(""Initializing Hybrid Search..."")\n   268→        hybrid_config = HybridSearchConfig(\n   269→            bm25_weight=float(os.getenv(""HYBRID_BM25_WEIGHT"", ""0.5"")),\n   270→            semantic_weight=float(os.getenv(""HYBRID_SEMANTIC_WEIGHT"", ""0.3"")),\n   271→            fuzzy_weight=float(os.getenv(""HYBRID_FUZZY_WEIGHT"", ""0.2"")),\n   272→            enable_bm25=True,\n   273→            enable_semantic=semantic_indexer is not None,\n   274→            enable_fuzzy=True,\n   275→            parallel_execution=True,\n   276→            cache_results=True\n   277→        )\n   278→        hybrid_search = HybridSearch(\n   279→            storage=sqlite_store,\n   280→            bm25_indexer=bm25_indexer,\n   281→            semantic_indexer=semantic_indexer,\n   282→            fuzzy_indexer=fuzzy_indexer,\n   283→            config=hybrid_config\n   284→        )\n   285→        logger.info(f""Hybrid Search initialized (BM25: {hybrid_config.enable_bm25}, Semantic: {hybrid_config.enable_semantic}, Fuzzy: {hybrid_config.enable_fuzzy})"")\n   286→        \n   287→        # Initialize file watcher with dispatcher and query cache\n   288→        logger.info(""Starting file watcher..."")\n   289→        file_watcher = FileWatcher(Path("".""), dispatcher, query_cache)\n   290→        file_watcher.start()\n   291→        logger.info(""File watcher started for current directory with cache invalidation"")\n   292→        \n   293→        # Store in app.state for potential future use\n   294→        app.state.dispatcher = dispatcher\n   295→        app.state.sqlite_store = sqlite_store\n   296→        app.state.file_watcher = file_watcher\n   297→        app.state.plugin_manager = plugin_manager\n   298→        app.state.auth_manager = auth_manager\n   299→        app.state.security_config = security_config\n   300→        app.state.cache_manager = cache_manager\n   301→        app.state.query_cache = query_cache\n   302→        app.state.metrics_collector = metrics_collector\n   303→        app.state.health_checker = health_checker\n   304→        app.state.business_metrics = business_metrics\n   305→        app.state.bm25_indexer = bm25_indexer\n   306→        app.state.hybrid_search = hybrid_search\n   307→        app.state.fuzzy_indexer = fuzzy_indexer\n   308→        \n   309→        # Update status to include search capabilities\n   310→        search_capabilities = []\n   311→        if bm25_indexer:\n   312→            search_capabilities.append(""bm25"")\n   313→        if fuzzy_indexer:\n   314→            search_capabilities.append(""fuzzy"") \n   315→        if semantic_indexer:\n   316→            search_capabilities.append(""semantic"")\n   317→        if hybrid_search:\n   318→            search_capabilities.append(""hybrid"")\n   319→        \n   320→        logger.info(f""Search capabilities: {\', \'.join(search_capabilities)}"")\n   321→        \n   322→        # Register health checks for system components\n   323→        logger.info(""Registering component health checks..."")\n   324→        health_checker.register_health_check(\n   325→            ""database"", \n   326→            health_checker.create_database_health_check(""code_index.db"")\n   327→        )\n   328→        health_checker.register_health_check(\n   329→            ""plugins"",\n   330→            health_checker.create_plugin_health_check(plugin_manager)\n   331→        )\n   332→        \n   333→        # Update system metrics\n   334→        business_metrics.update_system_metrics(\n   335→            active_plugins=len(plugin_instances),\n   336→            indexed_files=0,  # Will be updated as files are indexed\n   337→            database_size=0,  # Will be updated periodically\n   338→            memory_usage=0    # Will be updated by middleware\n   339→        )\n   340→        \n   341→        # Log loaded plugins with detailed status\n   342→        plugin_status = plugin_manager.get_detailed_plugin_status()\n   343→        for name, status in plugin_status.items():\n   344→            basic_info = status[\'basic_info\']\n   345→            runtime_info = status[\'runtime_info\']\n   346→            logger.info(f""Plugin \'{name}\': {runtime_info[\'state\']} (v{basic_info[\'version\']}, language: {basic_info[\'language\']}, enabled: {runtime_info[\'enabled\']})"")\n   347→            if runtime_info.get(\'error\'):\n   348→                logger.warning(f""Plugin \'{name}\' has error: {runtime_info[\'error\']}"")\n   349→        \n   350→        logger.info(""MCP Server initialized successfully with dynamic plugin system, SQLite persistence, and file watcher"")\n   351→    except Exception as e:\n   352→        logger.error(f""Failed to initialize MCP Server: {e}"", exc_info=True)\n   353→        raise\n   354→\n   355→@app.on_event(""shutdown"")\n   356→async def shutdown_event():\n   357→    """"""Clean up resources on shutdown.""""""\n   358→    global file_watcher, plugin_manager, cache_manager\n   359→    \n   360→    if file_watcher:\n   361→        try:\n   362→            file_watcher.stop()\n   363→            logger.info(""File watcher stopped successfully"")\n   364→        except Exception as e:\n   365→            logger.error(f""Error stopping file watcher: {e}"", exc_info=True)\n   366→    \n   367→    if plugin_manager:\n   368→        try:\n   369→            shutdown_result = plugin_manager.shutdown_safe()\n   370→            if shutdown_result.success:\n   371→                logger.info(""Plugin manager shutdown successfully"")\n   372→            else:\n   373→                logger.error(f""Plugin manager shutdown failed: {shutdown_result.error.message}"")\n   374→                logger.error(f""Shutdown error details: {shutdown_result.error.details}"")\n   375→        except Exception as e:\n   376→            logger.error(f""Error shutting down plugin manager: {e}"", exc_info=True)\n   377→    \n   378→    if cache_manager:\n   379→        try:\n   380→            await cache_manager.shutdown()\n   381→            logger.info(""Cache manager shutdown successfully"")\n   382→        except Exception as e:\n   383→            logger.error(f""Error shutting down cache manager: {e}"", exc_info=True)\n   384→\n   385→# Authentication endpoints\n   386→\n   387→@app.post(""/api/v1/auth/login"")\n   388→async def login(credentials: AuthCredentials) -> Dict[str, Any]:\n   389→    """"""User login endpoint.""""""\n   390→    if auth_manager is None:\n   391→        raise HTTPException(503, ""Authentication service not ready"")\n   392→    \n   393→    try:\n   394→        user = await auth_manager.authenticate_user(credentials)\n   395→        if not user:\n   396→            raise HTTPException(401, ""Invalid credentials"")\n   397→        \n   398→        access_token = await auth_manager.create_access_token(user)\n   399→        refresh_token = await auth_manager.create_refresh_token(user)\n   400→        \n   401→        return {\n   402→            ""access_token"": access_token,\n   403→            ""refresh_token"": refresh_token,\n   404→            ""token_type"": ""bearer"",\n   405→            ""expires_in"": security_config.access_token_expire_minutes * 60,\n   406→            ""user"": {\n   407→                ""id"": user.id,\n   408→                ""username"": user.username,\n   409→                ""role"": user.role.value,\n   410→                ""permissions"": [p.value for p in user.permissions]\n   411→            }\n   412→        }\n   413→    except Exception as e:\n   414→        logger.error(f""Login failed for user \'{credentials.username}\': {e}"")\n   415→        raise HTTPException(401, ""Authentication failed"")\n   416→\n   417→@app.post(""/api/v1/auth/refresh"")\n   418→async def refresh_token(refresh_token: str) -> Dict[str, Any]:\n   419→    """"""Refresh access token.""""""\n   420→    if auth_manager is None:\n   421→        raise HTTPException(503, ""Authentication service not ready"")\n   422→    \n   423→    try:\n   424→        new_access_token = await auth_manager.refresh_access_token(refresh_token)\n   425→        if not new_access_token:\n   426→            raise HTTPException(401, ""Invalid refresh token"")\n   427→        \n   428→        return {\n   429→            ""access_token"": new_access_token,\n   430→            ""token_type"": ""bearer"",\n   431→            ""expires_in"": security_config.access_token_expire_minutes * 60\n   432→        }\n   433→    except Exception as e:\n   434→        logger.error(f""Token refresh failed: {e}"")\n   435→        raise HTTPException(401, ""Token refresh failed"")\n   436→\n   437→@app.post(""/api/v1/auth/logout"")\n   438→async def logout(\n   439→    refresh_token: Optional[str] = None,\n   440→    current_user: User = Depends(get_current_active_user)\n   441→) -> Dict[str, str]:\n   442→    """"""User logout endpoint.""""""\n   443→    if auth_manager is None:\n   444→        raise HTTPException(503, ""Authentication service not ready"")\n   445→    \n   446→    try:\n   447→        if refresh_token:\n   448→            await auth_manager.revoke_refresh_token(refresh_token)\n   449→        \n   450→        await auth_manager._log_security_event(\n   451→            ""user_logout"",\n   452→            user_id=current_user.id,\n   453→            username=current_user.username\n   454→        )\n   455→        \n   456→        return {""message"": ""Successfully logged out""}\n   457→    except Exception as e:\n   458→        logger.error(f""Logout failed: {e}"")\n   459→        raise HTTPException(500, ""Logout failed"")\n   460→\n   461→@app.get(""/api/v1/auth/me"")\n   462→async def get_current_user_info(current_user: User = Depends(get_current_active_user)) -> Dict[str, Any]:\n   463→    """"""Get current user information.""""""\n   464→    return {\n   465→        ""id"": current_user.id,\n   466→        ""username"": current_user.username,\n   467→        ""email"": current_user.email,\n   468→        ""role"": current_user.role.value,\n   469→        ""permissions"": [p.value for p in current_user.permissions],\n   470→        ""is_active"": current_user.is_active,\n   471→        ""created_at"": current_user.created_at.isoformat(),\n   472→        ""last_login"": current_user.last_login.isoformat() if current_user.last_login else None\n   473→    }\n   474→\n   475→@app.post(""/api/v1/auth/register"")\n   476→async def register(\n   477→    credentials: AuthCredentials,\n   478→    email: Optional[str] = None,\n   479→    admin_user: User = Depends(require_role(UserRole.ADMIN))\n   480→) -> Dict[str, Any]:\n   481→    """"""Register new user (admin only).""""""\n   482→    if auth_manager is None:\n   483→        raise HTTPException(503, ""Authentication service not ready"")\n   484→    \n   485→    try:\n   486→        user = await auth_manager.create_user(\n   487→            username=credentials.username,\n   488→            password=credentials.password,\n   489→            email=email,\n   490→            role=UserRole.USER\n   491→        )\n   492→        \n   493→        return {\n   494→            ""message"": ""User created successfully"",\n   495→            ""user"": {\n   496→                ""id"": user.id,\n   497→                ""username"": user.username,\n   498→                ""email"": user.email,\n   499→                ""role"": user.role.value\n   500→            }\n   501→        }\n   502→    except Exception as e:\n   503→        logger.error(f""User registration failed: {e}"")\n   504→        raise HTTPException(400, str(e))\n   505→\n   506→# Security management endpoints\n   507→\n   508→@app.get(""/api/v1/security/events"")\n   509→async def get_security_events(\n   510→    limit: int = 100,\n   511→    admin_user: User = Depends(require_role(UserRole.ADMIN))\n   512→) -> Dict[str, Any]:\n   513→    """"""Get security events (admin only).""""""\n   514→    if auth_manager is None:\n   515→        raise HTTPException(503, ""Authentication service not ready"")\n   516→    \n   517→    try:\n   518→        events = await auth_manager.get_security_events(limit)\n   519→        return {\n   520→            ""events"": [\n   521→                {\n   522→                    ""id"": event.id,\n   523→                    ""event_type"": event.event_type,\n   524→                    ""user_id"": event.user_id,\n   525→                    ""username"": event.username,\n   526→                    ""ip_address"": event.ip_address,\n   527→                    ""timestamp"": event.timestamp.isoformat(),\n   528→                    ""details"": event.details,\n   529→                    ""severity"": event.severity\n   530→                }\n   531→                for event in events\n   532→            ]\n   533→        }\n   534→    except Exception as e:\n   535→        logger.error(f""Failed to get security events: {e}"")\n   536→        raise HTTPException(500, ""Failed to retrieve security events"")\n   537→\n   538→# Health check endpoints (public)\n   539→@app.get(""/health"")\n   540→async def health_check() -> Dict[str, Any]:\n   541→    """"""Basic health check endpoint.""""""\n   542→    return {""status"": ""healthy"", ""service"": ""mcp-server"", ""timestamp"": time.time()}\n   543→\n   544→@app.get(""/health/detailed"")\n   545→async def detailed_health_check() -> Dict[str, Any]:\n   546→    """"""Detailed health check endpoint.""""""\n   547→    try:\n   548→        overall_health = await health_checker.get_overall_health()\n   549→        component_results = await health_checker.check_all_components()\n   550→        \n   551→        return {\n   552→            ""status"": overall_health.status.value,\n   553→            ""message"": overall_health.message,\n   554→            ""timestamp"": overall_health.timestamp,\n   555→            ""details"": overall_health.details,\n   556→            ""components"": [\n   557→                {\n   558→                    ""component"": result.component,\n   559→                    ""status"": result.status.value,\n   560→                    ""message"": result.message,\n   561→                    ""details"": result.details\n   562→                }\n   563→                for result in component_results\n   564→            ]\n   565→        }\n   566→    except Exception as e:\n   567→        logger.error(f""Health check failed: {e}"", exc_info=True)\n   568→        return {\n   569→            ""status"": ""unhealthy"",\n   570→            ""message"": f""Health check failed: {str(e)}"",\n   571→            ""timestamp"": time.time()\n   572→        }\n   573→\n   574→@app.get(""/health/{component}"")\n   575→async def component_health_check(component: str) -> Dict[str, Any]:\n   576→    """"""Health check for a specific component.""""""\n   577→    try:\n   578→        result = await health_checker.check_component(component)\n   579→        return {\n   580→            ""component"": result.component,\n   581→            ""status"": result.status.value,\n   582→            ""message"": result.message,\n   583→            ""timestamp"": result.timestamp,\n   584→            ""details"": result.details\n   585→        }\n   586→    except Exception as e:\n   587→        logger.error(f""Component health check failed for {component}: {e}"", exc_info=True)\n   588→        raise HTTPException(500, f""Health check failed: {str(e)}"")\n   589→\n   590→# Metrics endpoints\n   591→@app.get(""/metrics"", response_class=PlainTextResponse)\n   592→def get_metrics() -> str:\n   593→    """"""Prometheus metrics endpoint.""""""\n   594→    try:\n   595→        return metrics_collector.get_metrics()\n   596→    except Exception as e:\n   597→        logger.error(f""Failed to get metrics: {e}"", exc_info=True)\n   598→        raise HTTPException(500, f""Failed to get metrics: {str(e)}"")\n   599→\n   600→@app.get(""/metrics"")\n   601→def get_prometheus_metrics() -> Response:\n   602→    """"""Prometheus metrics endpoint.""""""\n   603→    try:\n   604→        prometheus_exporter = get_prometheus_exporter()\n   605→        \n   606→        # Update build info\n   607→        prometheus_exporter.set_build_info(\n   608→            version=""1.0.0"",\n   609→            commit=os.getenv(""GIT_COMMIT"", ""unknown""),\n   610→            build_time=os.getenv(""BUILD_TIME"", ""unknown"")\n   611→        )\n   612→        \n   613→        # Update system metrics\n   614→        import psutil\n   615→        process = psutil.Process()\n   616→        memory_info = process.memory_info()\n   617→        prometheus_exporter.set_memory_usage(memory_info.rss, memory_info.vms)\n   618→        prometheus_exporter.set_cpu_usage(process.cpu_percent())\n   619→        prometheus_exporter.set_active_threads(threading.active_count())\n   620→        \n   621→        # Update plugin metrics\n   622→        if plugin_loader:\n   623→            stats = plugin_loader.get_statistics()\n   624→            for lang, plugin in plugin_loader.get_active_plugins().items():\n   625→                prometheus_exporter.plugin_status.labels(\n   626→                    plugin=plugin.__class__.__name__,\n   627→                    language=lang\n   628→                ).set(1)\n   629→        \n   630→        # Update file watcher metrics\n   631→        if file_watcher:\n   632→            # This would need to be implemented in FileWatcher\n   633→            # prometheus_exporter.set_files_watched(file_watcher.get_watched_count())\n   634→            pass\n   635→        \n   636→        # Generate metrics\n   637→        metrics = prometheus_exporter.generate_metrics()\n   638→        return Response(\n   639→            content=metrics,\n   640→            media_type=prometheus_exporter.get_content_type()\n   641→        )\n   642→    except Exception as e:\n   643→        logger.error(f""Failed to generate Prometheus metrics: {e}"", exc_info=True)\n   644→        raise HTTPException(500, f""Failed to generate metrics: {str(e)}"")\n   645→\n   646→@app.get(""/metrics/json"")\n   647→def get_metrics_json(current_user: User = Depends(require_permission(Permission.READ))) -> Dict[str, Any]:\n   648→    """"""JSON metrics endpoint for programmatic access.""""""\n   649→    try:\n   650→        families = metrics_collector.get_metric_families()\n   651→        stats = metrics_collector.get_stats()\n   652→        \n   653→        return {\n   654→            ""timestamp"": time.time(),\n   655→            ""collector_stats"": stats,\n   656→            ""metric_families"": families\n   657→        }\n   658→    except Exception as e:\n   659→        logger.error(f""Failed to get JSON metrics: {e}"", exc_info=True)\n   660→        raise HTTPException(500, f""Failed to get metrics: {str(e)}"")\n   661→\n   662→@app.get(""/symbol"", response_model=SymbolDef | None)\n   663→async def symbol(\n   664→    symbol: str,\n   665→    current_user: User = Depends(require_permission(Permission.READ))\n   666→):\n   667→    if dispatcher is None:\n   668→        logger.error(""Symbol lookup attempted but dispatcher not ready"")\n   669→        raise HTTPException(503, ""Dispatcher not ready"")\n   670→    \n   671→    start_time = time.time()\n   672→    try:\n   673→        logger.debug(f""Looking up symbol: {symbol} for user: {current_user.username}"")\n   674→        \n   675→        # Try cache first if query cache is available\n   676→        cached_result = None\n   677→        if query_cache and query_cache.config.enabled:\n   678→            cached_result = await query_cache.get_cached_result(\n   679→                QueryType.SYMBOL_LOOKUP,\n   680→                symbol=symbol\n   681→            )\n   682→        \n   683→        if cached_result is not None:\n   684→            logger.debug(f""Found cached symbol: {symbol}"")\n   685→            duration = time.time() - start_time\n   686→            business_metrics.record_search_performed(\n   687→                query=symbol,\n   688→                semantic=False,\n   689→                results_count=1,\n   690→                duration=duration\n   691→            )\n   692→            return cached_result\n   693→        \n   694→        # Record symbol lookup metrics\n   695→        with metrics_collector.time_function(""symbol_lookup""):\n   696→            result = dispatcher.lookup(symbol)\n   697→        \n   698→        # Cache the result if available\n   699→        if query_cache and query_cache.config.enabled and result:\n   700→            await query_cache.cache_result(\n   701→                QueryType.SYMBOL_LOOKUP,\n   702→                result,\n   703→                symbol=symbol\n   704→            )\n   705→        \n   706→        # Record business metrics\n   707→        duration = time.time() - start_time\n   708→        business_metrics.record_search_performed(\n   709→            query=symbol,\n   710→            semantic=False,\n   711→            results_count=1 if result else 0,\n   712→            duration=duration\n   713→        )\n   714→        \n   715→        if result:\n   716→            logger.debug(f""Found symbol: {symbol}"")\n   717→        else:\n   718→            logger.debug(f""Symbol not found: {symbol}"")\n   719→        return result\n   720→    except Exception as e:\n   721→        duration = time.time() - start_time\n   722→        business_metrics.record_search_performed(\n   723→            query=symbol,\n   724→            semantic=False,\n   725→            results_count=0,\n   726→            duration=duration\n   727→        )\n   728→        logger.error(f""Error looking up symbol \'{symbol}\': {e}"", exc_info=True)\n   729→        raise HTTPException(500, f""Internal error during symbol lookup: {str(e)}"")\n   730→\n   731→@app.get(""/search"", response_model=list[SearchResult])\n   732→async def search(\n   733→    q: str,\n   734→    semantic: bool = False,\n   735→    limit: int = 20,\n   736→    mode: str = ""auto"",  # ""auto"", ""hybrid"", ""bm25"", ""semantic"", ""fuzzy"", ""classic""\n   737→    language: Optional[str] = None,\n   738→    file_filter: Optional[str] = None,\n   739→    current_user: User = Depends(require_permission(Permission.READ))\n   740→):\n   741→    """"""Search with support for multiple modes including hybrid search.\n   742→    \n   743→    Args:\n   744→        q: Search query\n   745→        semantic: Whether to use semantic search (for backward compatibility)\n   746→        limit: Maximum number of results\n   747→        mode: Search mode - ""auto"" (default), ""hybrid"", ""bm25"", ""semantic"", ""fuzzy"", or ""classic""\n   748→        language: Filter by programming language\n   749→        file_filter: Filter by file path pattern\n   750→        current_user: Authenticated user\n   751→    """"""\n   752→    if dispatcher is None and mode == ""classic"":\n   753→        logger.error(""Search attempted but dispatcher not ready"")\n   754→        raise HTTPException(503, ""Dispatcher not ready"")\n   755→    \n   756→    start_time = time.time()\n   757→    try:\n   758→        # Determine effective search mode\n   759→        effective_mode = mode\n   760→        if mode == ""auto"":\n   761→            # Auto mode: use hybrid if available, otherwise fall back\n   762→            if hybrid_search is not None:\n   763→                effective_mode = ""hybrid""\n   764→            elif semantic and hasattr(dispatcher, \'search\'):\n   765→                effective_mode = ""classic""\n   766→            else:\n   767→                effective_mode = ""bm25"" if bm25_indexer else ""classic""\n   768→        \n   769→        logger.debug(f""Searching for: \'{q}\' (mode={effective_mode}, limit={limit}, language={language}) for user: {current_user.username}"")\n   770→        \n   771→        # Build filters\n   772→        filters = {}\n   773→        if language:\n   774→            filters[\'language\'] = language\n   775→        if file_filter:\n   776→            filters[\'file_filter\'] = file_filter\n   777→        \n   778→        # Try cache first if query cache is available\n   779→        cache_key_parts = [q, effective_mode, str(limit)]\n   780→        if filters:\n   781→            cache_key_parts.extend([f""{k}:{v}"" for k, v in sorted(filters.items())])\n   782→        \n   783→        cached_results = None\n   784→        if query_cache and query_cache.config.enabled:\n   785→            query_type = QueryType.SEMANTIC_SEARCH if effective_mode == ""semantic"" else QueryType.SEARCH\n   786→            cached_results = await query_cache.get_cached_result(\n   787→                query_type,\n   788→                q=q,\n   789→                semantic=(effective_mode == ""semantic""),\n   790→                limit=limit\n   791→            )\n   792→        \n   793→        if cached_results is not None:\n   794→            logger.debug(f""Found cached search results for: \'{q}\' ({len(cached_results)} results)"")\n   795→            duration = time.time() - start_time\n   796→            business_metrics.record_search_performed(\n   797→                query=q,\n   798→                semantic=(effective_mode == ""semantic""),\n   799→                results_count=len(cached_results),\n   800→                duration=duration\n   801→            )\n   802→            return cached_results\n   803→        \n   804→        # Perform search based on mode\n   805→        results = []\n   806→        \n   807→        if effective_mode == ""hybrid"" and hybrid_search:\n   808→            # Use hybrid search\n   809→            with metrics_collector.time_function(""search"", labels={""mode"": ""hybrid""}):\n   810→                hybrid_results = await hybrid_search.search(\n   811→                    query=q,\n   812→                    filters=filters,\n   813→                    limit=limit\n   814→                )\n   815→                # Convert to SearchResult format\n   816→                for r in hybrid_results:\n   817→                    results.append(SearchResult(\n   818→                        file_path=r[\'filepath\'],\n   819→                        snippet=r[\'snippet\'],\n   820→                        score=r[\'score\']\n   821→                    ))\n   822→        \n   823→        elif effective_mode == ""bm25"" and bm25_indexer:\n   824→            # Direct BM25 search\n   825→            with metrics_collector.time_function(""search"", labels={""mode"": ""bm25""}):\n   826→                bm25_results = bm25_indexer.search(q, limit=limit, **filters)\n   827→                for r in bm25_results:\n   828→                    results.append(SearchResult(\n   829→                        file_path=r[\'filepath\'],\n   830→                        snippet=r.get(\'snippet\', \'\'),\n   831→                        score=r[\'score\']\n   832→                    ))\n   833→        \n   834→        elif effective_mode == ""fuzzy"" and fuzzy_indexer:\n   835→            # Direct fuzzy search\n   836→            with metrics_collector.time_function(""search"", labels={""mode"": ""fuzzy""}):\n   837→                fuzzy_results = fuzzy_indexer.search_fuzzy(q, max_results=limit)\n   838→                for r in fuzzy_results:\n   839→                    results.append(SearchResult(\n   840→                        file_path=r.get(\'file_path\', \'\'),\n   841→                        snippet=r.get(\'context\', \'\'),\n   842→                        score=r.get(\'score\', 0.0)\n   843→                    ))\n   844→        \n   845→        elif effective_mode == ""semantic"":\n   846→            # Use classic dispatcher with semantic=True\n   847→            if dispatcher:\n   848→                with metrics_collector.time_function(""search"", labels={""mode"": ""semantic""}):\n   849→                    results = list(dispatcher.search(q, semantic=True, limit=limit))\n   850→            else:\n   851→                raise HTTPException(\n   852→                    503, \n   853→                    detail={\n   854→                        ""error"": ""Semantic search not available"",\n   855→                        ""reason"": ""Missing Voyage AI API key configuration"",\n   856→                        ""setup"": {\n   857→                            ""method_1_mcp_json"": [\n   858→                                ""Configure in .mcp.json (recommended for Claude Code):"",\n   859→                                ""{"",\n   860→                                \'  ""mcpServers"": {\',\n   861→                                \'    ""code-index-mcp"": {\',\n   862→                                \'      ""command"": ""uvicorn"",\',\n   863→                                \'      ""args"": [""mcp_server.gateway:app""],\',\n   864→                                \'      ""env"": {\',\n   865→                                \'        ""VOYAGE_AI_API_KEY"": ""your-key-here"",\',\n   866→                                \'        ""SEMANTIC_SEARCH_ENABLED"": ""true""\',\n   867→                                \'      }\',\n   868→                                \'    }\',\n   869→                                \'  }\',\n   870→                                \'}\'\n   871→                            ],\n   872→                            ""method_2_cli"": [\n   873→                                ""Or use Claude Code CLI:"",\n   874→                                ""claude mcp add code-index-mcp -e VOYAGE_AI_API_KEY=your_key -e SEMANTIC_SEARCH_ENABLED=true -- uvicorn mcp_server.gateway:app""\n   875→                            ],\n   876→                            ""method_3_env"": [\n   877→                                ""Or set environment variables:"",\n   878→                                ""export VOYAGE_AI_API_KEY=your_key"",\n   879→                                ""export SEMANTIC_SEARCH_ENABLED=true""\n   880→                            ],\n   881→                            ""method_4_dotenv"": [\n   882→                                ""Or add to .env file:"",\n   883→                                ""VOYAGE_AI_API_KEY=your_key"",\n   884→                                ""SEMANTIC_SEARCH_ENABLED=true""\n   885→                            ],\n   886→                            ""get_api_key"": ""Get your API key from: https://www.voyageai.com/"",\n   887→                            ""alternative"": ""Use mode=\'hybrid\' or mode=\'bm25\' for keyword-based search""\n   888→                        }\n   889→                    }\n   890→                )\n   891→        \n   892→        else:\n   893→            # Classic search through dispatcher\n   894→            if dispatcher:\n   895→                with metrics_collector.time_function(""search"", labels={""mode"": ""classic""}):\n   896→                    results = list(dispatcher.search(q, semantic=False, limit=limit))\n   897→            else:\n   898→                raise HTTPException(503, ""Classic search not available"")\n   899→        \n   900→        # Cache the results if available\n   901→        if query_cache and query_cache.config.enabled and results:\n   902→            query_type = QueryType.SEMANTIC_SEARCH if effective_mode == ""semantic"" else QueryType.SEARCH\n   903→            await query_cache.cache_result(\n   904→                query_type,\n   905→                results,\n   906→                q=q,\n   907→                semantic=(effective_mode == ""semantic""),\n   908→                limit=limit\n   909→            )\n   910→        \n   911→        # Record business metrics\n   912→        duration = time.time() - start_time\n   913→        business_metrics.record_search_performed(\n   914→            query=q,\n   915→            semantic=(effective_mode == ""semantic""),\n   916→            results_count=len(results),\n   917→            duration=duration\n   918→        )\n   919→        \n   920→        logger.debug(f""Search returned {len(results)} results using {effective_mode} mode"")\n   921→        return results\n   922→    except Exception as e:\n   923→        duration = time.time() - start_time\n   924→        business_metrics.record_search_performed(\n   925→            query=q,\n   926→            semantic=semantic,\n   927→            results_count=0,\n   928→            duration=duration\n   929→        )\n   930→        logger.error(f""Error during search for \'{q}\': {e}"", exc_info=True)\n   931→        raise HTTPException(500, f""Internal error during search: {str(e)}"")\n   932→\n   933→@app.get(""/search/capabilities"")\n   934→async def get_search_capabilities() -> Dict[str, Any]:\n   935→    """"""Get available search capabilities and configuration guidance.""""""\n   936→    voyage_key = os.environ.get(\'VOYAGE_API_KEY\') or os.environ.get(\'VOYAGE_AI_API_KEY\')\n   937→    semantic_enabled = os.environ.get(\'SEMANTIC_SEARCH_ENABLED\', \'false\').lower() == \'true\'\n   938→    \n   939→    return {\n   940→        ""available_modes"": {\n   941→            ""bm25"": bm25_indexer is not None,\n   942→            ""fuzzy"": fuzzy_indexer is not None,\n   943→            ""semantic"": semantic_indexer is not None,\n   944→            ""hybrid"": hybrid_search is not None,\n   945→            ""classic"": dispatcher is not None\n   946→        },\n   947→        ""semantic_config"": {\n   948→            ""enabled"": semantic_indexer is not None,\n   949→            ""api_key_configured"": bool(voyage_key),\n   950→            ""semantic_enabled_flag"": semantic_enabled,\n   951→            ""status"": ""ready"" if semantic_indexer else ""not_configured""\n   952→        },\n   953→        ""configuration_guide"": {\n   954→            ""mcp_json_example"": {\n   955→                ""description"": ""Add to .mcp.json for Claude Code (recommended)"",\n   956→                ""config"": {\n   957→                    ""mcpServers"": {\n   958→                        ""code-index-mcp"": {\n   959→                            ""command"": ""uvicorn"",\n   960→                            ""args"": [""mcp_server.gateway:app""],\n   961→                            ""env"": {\n   962→                                ""VOYAGE_AI_API_KEY"": ""your-key-here"",\n   963→                                ""SEMANTIC_SEARCH_ENABLED"": ""true""\n   964→                            }\n   965→                        }\n   966→                    }\n   967→                }\n   968→            },\n   969→            ""cli_command"": ""claude mcp add code-index-mcp -e VOYAGE_AI_API_KEY=key -e SEMANTIC_SEARCH_ENABLED=true -- uvicorn mcp_server.gateway:app"",\n   970→            ""env_file"": ""Add to .env: VOYAGE_AI_API_KEY=key and SEMANTIC_SEARCH_ENABLED=true"",\n   971→            ""get_api_key"": ""https://www.voyageai.com/""\n   972→        }\n   973→    }\n   974→\n   975→@app.get(""/status"")\n   976→async def status(current_user: User = Depends(require_permission(Permission.READ))) -> Dict[str, Any]:\n   977→    """"""Returns server status including plugin information and statistics.""""""\n   978→    if dispatcher is None:\n   979→        return {\n   980→            ""status"": ""error"",\n   981→            ""plugins"": 0,\n   982→            ""indexed_files"": {""total"": 0, ""by_language"": {}},\n   983→            ""version"": ""0.1.0"",\n   984→            ""message"": ""Dispatcher not initialized""\n   985→        }\n   986→    \n   987→    try:\n   988→        # Try cache first if query cache is available\n   989→        cached_status = None\n   990→        if query_cache and query_cache.config.enabled:\n   991→            cached_status = await query_cache.get_cached_result(\n   992→                QueryType.PROJECT_STATUS\n   993→            )\n   994→        \n   995→        if cached_status is not None:\n   996→            return cached_status\n   997→        \n   998→        # Get plugin count\n   999→        plugin_count = len(dispatcher._plugins) if hasattr(dispatcher, \'_plugins\') else 0\n  1000→        \n  1001→        # Get indexed files statistics\n  1002→        indexed_stats = {""total"": 0, ""by_language"": {}}\n  1003→        if hasattr(dispatcher, \'get_statistics\'):\n  1004→            indexed_stats = dispatcher.get_statistics()\n  1005→        elif hasattr(dispatcher, \'_plugins\'):\n  1006→            # Calculate basic statistics from plugins\n  1007→            for plugin in dispatcher._plugins:\n  1008→                if hasattr(plugin, \'get_indexed_count\'):\n  1009→                    count = plugin.get_indexed_count()\n  1010→                    indexed_stats[""total""] += count\n  1011→                    lang = getattr(plugin, \'language\', getattr(plugin, \'lang\', \'unknown\'))\n  1012→                    indexed_stats[""by_language""][lang] = count\n  1013→        \n  1014→        # Add database statistics if available\n  1015→        db_stats = {}\n  1016→        if sqlite_store:\n  1017→            db_stats = sqlite_store.get_statistics()\n  1018→        \n  1019→        # Add cache statistics if available\n  1020→        cache_stats = {}\n  1021→        if cache_manager:\n  1022→            try:\n  1023→                cache_metrics = await cache_manager.get_metrics()\n  1024→                cache_stats = {\n  1025→                    ""hit_rate"": cache_metrics.hit_rate,\n  1026→                    ""entries"": cache_metrics.entries_count,\n  1027→                    ""memory_usage_mb"": cache_metrics.memory_usage_mb\n  1028→                }\n  1029→            except Exception as e:\n  1030→                logger.warning(f""Failed to get cache stats: {e}"")\n  1031→        \n  1032→        status_data = {\n  1033→            ""status"": ""operational"",\n  1034→            ""plugins"": plugin_count,\n  1035→            ""indexed_files"": indexed_stats,\n  1036→            ""database"": db_stats,\n  1037→            ""cache"": cache_stats,\n  1038→            ""search_capabilities"": [],\n  1039→            ""version"": ""0.1.0""\n  1040→        }\n  1041→        \n  1042→        # Add search capabilities\n  1043→        if bm25_indexer:\n  1044→            status_data[""search_capabilities""].append(""bm25"")\n  1045→        if fuzzy_indexer:\n  1046→            status_data[""search_capabilities""].append(""fuzzy"") \n  1047→        if hasattr(app.state, \'semantic_indexer\') and app.state.semantic_indexer:\n  1048→            status_data[""search_capabilities""].append(""semantic"")\n  1049→        if hybrid_search:\n  1050→            status_data[""search_capabilities""].append(""hybrid"")\n  1051→        \n  1052→        # Cache the status\n  1053→        if query_cache and query_cache.config.enabled:\n  1054→            await query_cache.cache_result(\n  1055→                QueryType.PROJECT_STATUS,\n  1056→                status_data\n  1057→            )\n  1058→        \n  1059→        return status_data\n  1060→    except Exception as e:\n  1061→        logger.error(f""Error getting server status: {e}"", exc_info=True)\n  1062→        return {\n  1063→            ""status"": ""error"",\n  1064→            ""plugins"": 0,\n  1065→            ""indexed_files"": {""total"": 0, ""by_language"": {}},\n  1066→            ""version"": ""0.1.0"",\n  1067→            ""message"": str(e)\n  1068→        }\n  1069→\n  1070→@app.get(""/plugins"")\n  1071→def plugins(current_user: User = Depends(require_permission(Permission.READ))) -> List[Dict[str, Any]]:\n  1072→    """"""Returns list of loaded plugins with their information.""""""\n  1073→    if plugin_manager is None:\n  1074→        logger.error(""Plugin list requested but plugin manager not ready"")\n  1075→        raise HTTPException(503, ""Plugin manager not ready"")\n  1076→    \n  1077→    try:\n  1078→        plugin_list = []\n  1079→        plugin_infos = plugin_manager._registry.list_plugins()\n  1080→        plugin_status = plugin_manager.get_plugin_status()\n  1081→        \n  1082→        for info in plugin_infos:\n  1083→            status = plugin_status.get(info.name, {})\n  1084→            plugin_data = {\n  1085→                ""name"": info.name,\n  1086→                ""version"": info.version,\n  1087→                ""description"": info.description,\n  1088→                ""author"": info.author,\n  1089→                ""language"": info.language,\n  1090→                ""file_extensions"": info.file_extensions,\n  1091→                ""state"": status.get(\'state\', \'unknown\'),\n  1092→                ""enabled"": status.get(\'enabled\', False)\n  1093→            }\n  1094→            plugin_list.append(plugin_data)\n  1095→        \n  1096→        logger.debug(f""Returning {len(plugin_list)} plugins"")\n  1097→        return plugin_list\n  1098→    except Exception as e:\n  1099→        logger.error(f""Error getting plugin list: {e}"", exc_info=True)\n  1100→        raise HTTPException(500, f""Internal error getting plugins: {str(e)}"")\n  1101→\n  1102→@app.post(""/reindex"")\n  1103→async def reindex(\n  1104→    path: Optional[str] = None,\n  1105→    current_user: User = Depends(require_permission(Permission.EXECUTE))\n  1106→) -> Dict[str, str]:\n  1107→    """"""Triggers manual reindexing of files.\n  1108→    \n  1109→    Args:\n  1110→        path: Optional specific directory path to reindex. If not provided,\n  1111→              reindexes all configured paths.\n  1112→    \n  1113→    Returns:\n  1114→        Task status information.\n  1115→    """"""\n  1116→    if dispatcher is None:\n  1117→        logger.error(""Reindex requested but dispatcher not ready"")\n  1118→        raise HTTPException(503, ""Dispatcher not ready"")\n  1119→    \n  1120→    try:\n  1121→        logger.info(f""Manual reindex requested for path: {path or \'all\'}"")\n  1122→        # Since dispatcher has index_file method, we can use it for reindexing\n  1123→        if path:\n  1124→            # Reindex specific path\n  1125→            target_path = Path(path)\n  1126→            if not target_path.exists():\n  1127→                raise HTTPException(404, f""Path not found: {path}"")\n  1128→            \n  1129→            indexed_count = 0\n  1130→            if target_path.is_file():\n  1131→                # Single file\n  1132→                dispatcher.index_file(target_path)\n  1133→                indexed_count = 1\n  1134→            else:\n  1135→                # Directory - find all supported files\n  1136→                for file_path in target_path.rglob(""*""):\n  1137→                    if file_path.is_file():\n  1138→                        try:\n  1139→                            # Check if any plugin supports this file\n  1140→                            for plugin in dispatcher._plugins:\n  1141→                                if plugin.supports(file_path):\n  1142→                                    dispatcher.index_file(file_path)\n  1143→                                    indexed_count += 1\n  1144→                                    break\n  1145→                        except Exception as e:\n  1146→                            # Log but continue with other files\n  1147→                            logger.warning(f""Failed to index {file_path}: {e}"")\n  1148→            \n  1149→            logger.info(f""Successfully reindexed {indexed_count} files in {path}"")\n  1150→            return {\n  1151→                ""status"": ""completed"",\n  1152→                ""message"": f""Reindexed {indexed_count} files in {path}""\n  1153→            }\n  1154→        else:\n  1155→            # Reindex all supported files\n  1156→            indexed_count = 0\n  1157→            indexed_by_type = {}\n  1158→            \n  1159→            # Find all files and check if any plugin supports them\n  1160→            for file_path in Path(""."").rglob(""*""):\n  1161→                if file_path.is_file():\n  1162→                    try:\n  1163→                        # Check if any plugin supports this file\n  1164→                        for plugin in dispatcher._plugins:\n  1165→                            if plugin.supports(file_path):\n  1166→                                dispatcher.index_file(file_path)\n  1167→                                indexed_count += 1\n  1168→                                \n  1169→                                # Track by file type\n  1170→                                suffix = file_path.suffix.lower()\n  1171→                                indexed_by_type[suffix] = indexed_by_type.get(suffix, 0) + 1\n  1172→                                break\n  1173→                    except Exception as e:\n  1174→                        # Log but continue with other files\n  1175→                        logger.warning(f""Failed to index {file_path}: {e}"")\n  1176→            \n  1177→            # Build summary message\n  1178→            type_summary = "", "".join([f""{count} {ext} files"" for ext, count in indexed_by_type.items()])\n  1179→            logger.info(f""Successfully reindexed {indexed_count} files: {type_summary}"")\n  1180→            return {\n  1181→                ""status"": ""completed"",\n  1182→                ""message"": f""Reindexed {indexed_count} files ({type_summary})""\n  1183→            }\n  1184→    except HTTPException:\n  1185→        raise\n  1186→    except Exception as e:\n  1187→        logger.error(f""Reindexing failed: {e}"", exc_info=True)\n  1188→        raise HTTPException(500, f""Reindexing failed: {str(e)}"")\n  1189→\n  1190→@app.post(""/plugins/{plugin_name}/reload"")\n  1191→async def reload_plugin(\n  1192→    plugin_name: str,\n  1193→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1194→) -> Dict[str, str]:\n  1195→    """"""Reload a specific plugin.\n  1196→    \n  1197→    Args:\n  1198→        plugin_name: Name of the plugin to reload\n  1199→        \n  1200→    Returns:\n  1201→        Status message\n  1202→    """"""\n  1203→    if plugin_manager is None:\n  1204→        logger.error(""Plugin reload requested but plugin manager not ready"")\n  1205→        raise HTTPException(503, ""Plugin manager not ready"")\n  1206→    \n  1207→    try:\n  1208→        plugin_manager.reload_plugin(plugin_name)\n  1209→        return {\n  1210→            ""status"": ""success"",\n  1211→            ""message"": f""Plugin \'{plugin_name}\' reloaded successfully""\n  1212→        }\n  1213→    except Exception as e:\n  1214→        logger.error(f""Failed to reload plugin \'{plugin_name}\': {e}"", exc_info=True)\n  1215→        raise HTTPException(500, f""Failed to reload plugin: {str(e)}"")\n  1216→\n  1217→@app.post(""/plugins/{plugin_name}/enable"")\n  1218→async def enable_plugin(\n  1219→    plugin_name: str,\n  1220→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1221→) -> Dict[str, str]:\n  1222→    """"""Enable a disabled plugin.\n  1223→    \n  1224→    Args:\n  1225→        plugin_name: Name of the plugin to enable\n  1226→        \n  1227→    Returns:\n  1228→        Status message\n  1229→    """"""\n  1230→    if plugin_manager is None:\n  1231→        logger.error(""Plugin enable requested but plugin manager not ready"")\n  1232→        raise HTTPException(503, ""Plugin manager not ready"")\n  1233→    \n  1234→    try:\n  1235→        plugin_manager.enable_plugin(plugin_name)\n  1236→        # Recreate dispatcher with updated plugins\n  1237→        active_plugins = plugin_manager.get_active_plugins()\n  1238→        global dispatcher\n  1239→        dispatcher = Dispatcher(list(active_plugins.values()))\n  1240→        \n  1241→        return {\n  1242→            ""status"": ""success"",\n  1243→            ""message"": f""Plugin \'{plugin_name}\' enabled successfully""\n  1244→        }\n  1245→    except Exception as e:\n  1246→        logger.error(f""Failed to enable plugin \'{plugin_name}\': {e}"", exc_info=True)\n  1247→        raise HTTPException(500, f""Failed to enable plugin: {str(e)}"")\n  1248→\n  1249→@app.post(""/plugins/{plugin_name}/disable"")\n  1250→async def disable_plugin(\n  1251→    plugin_name: str,\n  1252→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1253→) -> Dict[str, str]:\n  1254→    """"""Disable an active plugin.\n  1255→    \n  1256→    Args:\n  1257→        plugin_name: Name of the plugin to disable\n  1258→        \n  1259→    Returns:\n  1260→        Status message\n  1261→    """"""\n  1262→    if plugin_manager is None:\n  1263→        logger.error(""Plugin disable requested but plugin manager not ready"")\n  1264→        raise HTTPException(503, ""Plugin manager not ready"")\n  1265→    \n  1266→    try:\n  1267→        plugin_manager.disable_plugin(plugin_name)\n  1268→        # Recreate dispatcher with updated plugins\n  1269→        active_plugins = plugin_manager.get_active_plugins()\n  1270→        global dispatcher\n  1271→        dispatcher = Dispatcher(list(active_plugins.values()))\n  1272→        \n  1273→        return {\n  1274→            ""status"": ""success"",\n  1275→            ""message"": f""Plugin \'{plugin_name}\' disabled successfully""\n  1276→        }\n  1277→    except Exception as e:\n  1278→        logger.error(f""Failed to disable plugin \'{plugin_name}\': {e}"", exc_info=True)\n  1279→        raise HTTPException(500, f""Failed to disable plugin: {str(e)}"")\n  1280→\n  1281→# Cache management endpoints\n  1282→\n  1283→@app.get(""/cache/stats"")\n  1284→async def get_cache_stats(\n  1285→    current_user: User = Depends(require_permission(Permission.READ))\n  1286→) -> Dict[str, Any]:\n  1287→    """"""Get cache statistics and performance metrics.""""""\n  1288→    if not cache_manager:\n  1289→        raise HTTPException(503, ""Cache manager not ready"")\n  1290→    \n  1291→    try:\n  1292→        cache_metrics = await cache_manager.get_metrics()\n  1293→        backend_stats = await cache_manager.get_backend_stats()\n  1294→        \n  1295→        stats = {\n  1296→            ""cache_metrics"": {\n  1297→                ""hits"": cache_metrics.hits,\n  1298→                ""misses"": cache_metrics.misses,\n  1299→                ""sets"": cache_metrics.sets,\n  1300→                ""deletes"": cache_metrics.deletes,\n  1301→                ""hit_rate"": cache_metrics.hit_rate,\n  1302→                ""avg_response_time_ms"": cache_metrics.avg_response_time_ms,\n  1303→                ""entries_count"": cache_metrics.entries_count,\n  1304→                ""memory_usage_mb"": cache_metrics.memory_usage_mb\n  1305→            },\n  1306→            ""backend_stats"": backend_stats\n  1307→        }\n  1308→        \n  1309→        # Add query cache stats if available\n  1310→        if query_cache:\n  1311→            query_stats = await query_cache.get_cache_stats()\n  1312→            stats[""query_cache""] = query_stats\n  1313→        \n  1314→        return stats\n  1315→    except Exception as e:\n  1316→        logger.error(f""Failed to get cache stats: {e}"")\n  1317→        raise HTTPException(500, f""Failed to get cache statistics: {str(e)}"")\n  1318→\n  1319→@app.post(""/cache/clear"")\n  1320→async def clear_cache(\n  1321→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1322→) -> Dict[str, Any]:\n  1323→    """"""Clear all cache entries (admin only).""""""\n  1324→    if not cache_manager:\n  1325→        raise HTTPException(503, ""Cache manager not ready"")\n  1326→    \n  1327→    try:\n  1328→        count = await cache_manager.clear()\n  1329→        logger.info(f""Cache cleared by admin user {current_user.username}: {count} entries"")\n  1330→        \n  1331→        return {\n  1332→            ""status"": ""success"",\n  1333→            ""message"": f""Cleared {count} cache entries"",\n  1334→            ""cleared_entries"": count\n  1335→        }\n  1336→    except Exception as e:\n  1337→        logger.error(f""Failed to clear cache: {e}"")\n  1338→        raise HTTPException(500, f""Failed to clear cache: {str(e)}"")\n  1339→\n  1340→@app.post(""/cache/invalidate"")\n  1341→async def invalidate_cache_by_tags(\n  1342→    tags: List[str],\n  1343→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1344→) -> Dict[str, Any]:\n  1345→    """"""Invalidate cache entries by tags (admin only).""""""\n  1346→    if not cache_manager:\n  1347→        raise HTTPException(503, ""Cache manager not ready"")\n  1348→    \n  1349→    try:\n  1350→        tag_set = set(tags)\n  1351→        count = await cache_manager.invalidate_by_tags(tag_set)\n  1352→        logger.info(f""Cache invalidated by admin user {current_user.username}: {count} entries with tags {tags}"")\n  1353→        \n  1354→        return {\n  1355→            ""status"": ""success"",\n  1356→            ""message"": f""Invalidated {count} cache entries with tags {tags}"",\n  1357→            ""invalidated_entries"": count,\n  1358→            ""tags"": tags\n  1359→        }\n  1360→    except Exception as e:\n  1361→        logger.error(f""Failed to invalidate cache by tags: {e}"")\n  1362→        raise HTTPException(500, f""Failed to invalidate cache: {str(e)}"")\n  1363→\n  1364→@app.post(""/cache/invalidate/files"")\n  1365→async def invalidate_cache_by_files(\n  1366→    file_paths: List[str],\n  1367→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1368→) -> Dict[str, Any]:\n  1369→    """"""Invalidate cache entries that depend on specific files (admin only).""""""\n  1370→    if not query_cache:\n  1371→        raise HTTPException(503, ""Query cache not ready"")\n  1372→    \n  1373→    try:\n  1374→        total_count = 0\n  1375→        for file_path in file_paths:\n  1376→            count = await query_cache.invalidate_file_queries(file_path)\n  1377→            total_count += count\n  1378→        \n  1379→        logger.info(f""File-based cache invalidation by admin user {current_user.username}: {total_count} entries for {len(file_paths)} files"")\n  1380→        \n  1381→        return {\n  1382→            ""status"": ""success"",\n  1383→            ""message"": f""Invalidated {total_count} cache entries for {len(file_paths)} files"",\n  1384→            ""invalidated_entries"": total_count,\n  1385→            ""files"": file_paths\n  1386→        }\n  1387→    except Exception as e:\n  1388→        logger.error(f""Failed to invalidate cache by files: {e}"")\n  1389→        raise HTTPException(500, f""Failed to invalidate cache by files: {str(e)}"")\n  1390→\n  1391→@app.post(""/cache/invalidate/semantic"")\n  1392→async def invalidate_semantic_cache(\n  1393→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1394→) -> Dict[str, Any]:\n  1395→    """"""Invalidate all semantic search cache entries (admin only).""""""\n  1396→    if not query_cache:\n  1397→        raise HTTPException(503, ""Query cache not ready"")\n  1398→    \n  1399→    try:\n  1400→        count = await query_cache.invalidate_semantic_queries()\n  1401→        logger.info(f""Semantic cache invalidated by admin user {current_user.username}: {count} entries"")\n  1402→        \n  1403→        return {\n  1404→            ""status"": ""success"",\n  1405→            ""message"": f""Invalidated {count} semantic search cache entries"",\n  1406→            ""invalidated_entries"": count\n  1407→        }\n  1408→    except Exception as e:\n  1409→        logger.error(f""Failed to invalidate semantic cache: {e}"")\n  1410→        raise HTTPException(500, f""Failed to invalidate semantic cache: {str(e)}"")\n  1411→\n  1412→@app.post(""/cache/warm"")\n  1413→async def warm_cache(\n  1414→    keys: List[str],\n  1415→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1416→) -> Dict[str, Any]:\n  1417→    """"""Warm cache with predefined keys (admin only).""""""\n  1418→    if not cache_manager:\n  1419→        raise HTTPException(503, ""Cache manager not ready"")\n  1420→    \n  1421→    try:\n  1422→        # Simple factory function for warming - would need more sophisticated logic in production\n  1423→        async def factory(key: str):\n  1424→            # This is a placeholder - in real implementation would depend on key type\n  1425→            return f""warmed_value_for_{key}""\n  1426→        \n  1427→        count = await cache_manager.warm_cache(keys, factory)\n  1428→        logger.info(f""Cache warmed by admin user {current_user.username}: {count} entries"")\n  1429→        \n  1430→        return {\n  1431→            ""status"": ""success"",\n  1432→            ""message"": f""Warmed {count} cache entries"",\n  1433→            ""warmed_entries"": count,\n  1434→            ""requested_keys"": len(keys)\n  1435→        }\n  1436→    except Exception as e:\n  1437→        logger.error(f""Failed to warm cache: {e}"")\n  1438→        raise HTTPException(500, f""Failed to warm cache: {str(e)}"")\n  1439→\n  1440→@app.post(""/cache/cleanup"")\n  1441→async def cleanup_cache(\n  1442→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1443→) -> Dict[str, Any]:\n  1444→    """"""Manually trigger cache cleanup (admin only).""""""\n  1445→    if not cache_manager:\n  1446→        raise HTTPException(503, ""Cache manager not ready"")\n  1447→    \n  1448→    try:\n  1449→        count = await cache_manager.cleanup()\n  1450→        logger.info(f""Cache cleanup triggered by admin user {current_user.username}: {count} entries cleaned"")\n  1451→        \n  1452→        return {\n  1453→            ""status"": ""success"",\n  1454→            ""message"": f""Cleaned up {count} expired cache entries"",\n  1455→            ""cleaned_entries"": count\n  1456→        }\n  1457→    except Exception as e:\n  1458→        logger.error(f""Failed to cleanup cache: {e}"")\n  1459→        raise HTTPException(500, f""Failed to cleanup cache: {str(e)}"")\n  1460→\n  1461→# Hybrid Search endpoints\n  1462→\n  1463→@app.get(""/search/config"")\n  1464→async def get_search_config(\n  1465→    current_user: User = Depends(require_permission(Permission.READ))\n  1466→) -> Dict[str, Any]:\n  1467→    """"""Get current hybrid search configuration.""""""\n  1468→    if not hybrid_search:\n  1469→        raise HTTPException(503, ""Hybrid search not available"")\n  1470→    \n  1471→    config = hybrid_search.config\n  1472→    return {\n  1473→        ""weights"": {\n  1474→            ""bm25"": config.bm25_weight,\n  1475→            ""semantic"": config.semantic_weight,\n  1476→            ""fuzzy"": config.fuzzy_weight\n  1477→        },\n  1478→        ""enabled_methods"": {\n  1479→            ""bm25"": config.enable_bm25,\n  1480→            ""semantic"": config.enable_semantic,\n  1481→            ""fuzzy"": config.enable_fuzzy\n  1482→        },\n  1483→        ""rrf_k"": config.rrf_k,\n  1484→        ""parallel_execution"": config.parallel_execution,\n  1485→        ""cache_results"": config.cache_results,\n  1486→        ""limits"": {\n  1487→            ""individual_limit"": config.individual_limit,\n  1488→            ""final_limit"": config.final_limit\n  1489→        }\n  1490→    }\n  1491→\n  1492→@app.put(""/search/config/weights"")\n  1493→async def update_search_weights(\n  1494→    bm25: Optional[float] = None,\n  1495→    semantic: Optional[float] = None,\n  1496→    fuzzy: Optional[float] = None,\n  1497→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1498→) -> Dict[str, Any]:\n  1499→    """"""Update hybrid search weights (admin only).\n  1500→    \n  1501→    Weights will be normalized to sum to 1.0.\n  1502→    """"""\n  1503→    if not hybrid_search:\n  1504→        raise HTTPException(503, ""Hybrid search not available"")\n  1505→    \n  1506→    try:\n  1507→        hybrid_search.set_weights(bm25=bm25, semantic=semantic, fuzzy=fuzzy)\n  1508→        \n  1509→        # Get updated config\n  1510→        config = hybrid_search.config\n  1511→        \n  1512→        logger.info(f""Search weights updated by {current_user.username}: ""\n  1513→                   f""BM25={config.bm25_weight:.3f}, ""\n  1514→                   f""Semantic={config.semantic_weight:.3f}, ""\n  1515→                   f""Fuzzy={config.fuzzy_weight:.3f}"")\n  1516→        \n  1517→        return {\n  1518→            ""status"": ""success"",\n  1519→            ""weights"": {\n  1520→                ""bm25"": config.bm25_weight,\n  1521→                ""semantic"": config.semantic_weight,\n  1522→                ""fuzzy"": config.fuzzy_weight\n  1523→            }\n  1524→        }\n  1525→    except Exception as e:\n  1526→        logger.error(f""Failed to update search weights: {e}"")\n  1527→        raise HTTPException(500, f""Failed to update weights: {str(e)}"")\n  1528→\n  1529→@app.put(""/search/config/methods"")\n  1530→async def toggle_search_methods(\n  1531→    bm25: Optional[bool] = None,\n  1532→    semantic: Optional[bool] = None,\n  1533→    fuzzy: Optional[bool] = None,\n  1534→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1535→) -> Dict[str, Any]:\n  1536→    """"""Enable or disable search methods (admin only).""""""\n  1537→    if not hybrid_search:\n  1538→        raise HTTPException(503, ""Hybrid search not available"")\n  1539→    \n  1540→    try:\n  1541→        hybrid_search.enable_methods(bm25=bm25, semantic=semantic, fuzzy=fuzzy)\n  1542→        \n  1543→        # Get updated config\n  1544→        config = hybrid_search.config\n  1545→        \n  1546→        logger.info(f""Search methods updated by {current_user.username}: ""\n  1547→                   f""BM25={config.enable_bm25}, ""\n  1548→                   f""Semantic={config.enable_semantic}, ""\n  1549→                   f""Fuzzy={config.enable_fuzzy}"")\n  1550→        \n  1551→        return {\n  1552→            ""status"": ""success"",\n  1553→            ""enabled_methods"": {\n  1554→                ""bm25"": config.enable_bm25,\n  1555→                ""semantic"": config.enable_semantic,\n  1556→                ""fuzzy"": config.enable_fuzzy\n  1557→            }\n  1558→        }\n  1559→    except Exception as e:\n  1560→        logger.error(f""Failed to update search methods: {e}"")\n  1561→        raise HTTPException(500, f""Failed to update methods: {str(e)}"")\n  1562→\n  1563→@app.get(""/search/statistics"")\n  1564→async def get_search_statistics(\n  1565→    current_user: User = Depends(require_permission(Permission.READ))\n  1566→) -> Dict[str, Any]:\n  1567→    """"""Get search statistics and performance metrics.""""""\n  1568→    stats = {}\n  1569→    \n  1570→    # Hybrid search statistics\n  1571→    if hybrid_search:\n  1572→        stats[""hybrid_search""] = hybrid_search.get_statistics()\n  1573→    \n  1574→    # BM25 statistics\n  1575→    if bm25_indexer:\n  1576→        stats[""bm25""] = bm25_indexer.get_statistics()\n  1577→    \n  1578→    # Fuzzy search statistics\n  1579→    if fuzzy_indexer and hasattr(fuzzy_indexer, \'get_statistics\'):\n  1580→        stats[""fuzzy""] = fuzzy_indexer.get_statistics()\n  1581→    \n  1582→    # Add general search metrics from business metrics\n  1583→    if business_metrics:\n  1584→        search_metrics = business_metrics.get_search_metrics()\n  1585→        stats[""general""] = {\n  1586→            ""total_searches"": search_metrics.get(""total_searches"", 0),\n  1587→            ""average_response_time_ms"": search_metrics.get(""avg_response_time"", 0),\n  1588→            ""search_success_rate"": search_metrics.get(""success_rate"", 0)\n  1589→        }\n  1590→    \n  1591→    return stats\n  1592→\n  1593→@app.post(""/search/optimize"")\n  1594→async def optimize_search_indexes(\n  1595→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1596→) -> Dict[str, Any]:\n  1597→    """"""Optimize search indexes for better performance (admin only).""""""\n  1598→    results = {}\n  1599→    \n  1600→    try:\n  1601→        # Optimize BM25 indexes\n  1602→        if bm25_indexer:\n  1603→            bm25_indexer.optimize()\n  1604→            results[""bm25""] = ""optimized""\n  1605→            logger.info(""BM25 indexes optimized"")\n  1606→        \n  1607→        # Optimize FTS5 tables in SQLite\n  1608→        if sqlite_store:\n  1609→            sqlite_store.optimize_fts_tables()\n  1610→            results[""fts5""] = ""optimized""\n  1611→            logger.info(""FTS5 tables optimized"")\n  1612→        \n  1613→        # Clear hybrid search cache\n  1614→        if hybrid_search:\n  1615→            hybrid_search.clear_cache()\n  1616→            results[""hybrid_cache""] = ""cleared""\n  1617→            logger.info(""Hybrid search cache cleared"")\n  1618→        \n  1619→        logger.info(f""Search indexes optimized by {current_user.username}"")\n  1620→        \n  1621→        return {\n  1622→            ""status"": ""success"",\n  1623→            ""message"": ""Search indexes optimized successfully"",\n  1624→            ""results"": results\n  1625→        }\n  1626→    except Exception as e:\n  1627→        logger.error(f""Failed to optimize search indexes: {e}"")\n  1628→        raise HTTPException(500, f""Failed to optimize indexes: {str(e)}"")\n  1629→\n  1630→@app.get(""/search/term/{term}/stats"")\n  1631→async def get_term_statistics(\n  1632→    term: str,\n  1633→    current_user: User = Depends(require_permission(Permission.READ))\n  1634→) -> Dict[str, Any]:\n  1635→    """"""Get statistics for a specific search term.""""""\n  1636→    stats = {}\n  1637→    \n  1638→    try:\n  1639→        # BM25 term statistics\n  1640→        if bm25_indexer:\n  1641→            stats[""bm25""] = bm25_indexer.get_term_statistics(term)\n  1642→        \n  1643→        # SQLite FTS5 statistics\n  1644→        if sqlite_store:\n  1645→            stats[""fts5""] = sqlite_store.get_bm25_term_statistics(term)\n  1646→        \n  1647→        return {\n  1648→            ""term"": term,\n  1649→            ""statistics"": stats\n  1650→        }\n  1651→    except Exception as e:\n  1652→        logger.error(f""Failed to get term statistics: {e}"")\n  1653→        raise HTTPException(500, f""Failed to get term statistics: {str(e)}"")\n  1654→\n  1655→@app.post(""/search/rebuild"")\n  1656→async def rebuild_search_indexes(\n  1657→    index_type: str = ""all"",  # ""all"", ""bm25"", ""fuzzy"", ""semantic""\n  1658→    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1659→) -> Dict[str, Any]:\n  1660→    """"""Rebuild search indexes (admin only).""""""\n  1661→    if index_type not in [""all"", ""bm25"", ""fuzzy"", ""semantic""]:\n  1662→        raise HTTPException(400, ""Invalid index_type. Must be \'all\', \'bm25\', \'fuzzy\', or \'semantic\'"")\n  1663→    \n  1664→    results = {}\n  1665→    \n  1666→    try:\n  1667→        if index_type in [""all"", ""bm25""] and bm25_indexer:\n  1668→            bm25_indexer.rebuild()\n  1669→            results[""bm25""] = ""rebuilt""\n  1670→            logger.info(""BM25 index rebuilt"")\n  1671→        \n  1672→        if index_type in [""all"", ""fuzzy""] and fuzzy_indexer:\n  1673→            fuzzy_indexer.clear()\n  1674→            # Re-index all files\n  1675→            if sqlite_store:\n  1676→                files = sqlite_store.get_all_files()\n  1677→                for file_info in files:\n  1678→                    try:\n  1679→                        with open(file_info[""path""], \'r\', encoding=\'utf-8\') as f:\n  1680→                            content = f.read()\n  1681→                        fuzzy_indexer.add_file(file_info[""path""], content)\n  1682→                    except Exception as e:\n  1683→                        logger.warning(f""Failed to re-index {file_info[\'path\']}: {e}"")\n  1684→            results[""fuzzy""] = ""rebuilt""\n  1685→            logger.info(""Fuzzy index rebuilt"")\n  1686→        \n  1687→        if index_type in [""all"", ""semantic""]:\n  1688→            # Semantic index rebuild would go here if available\n  1689→            if hasattr(hybrid_search, \'semantic_indexer\') and hybrid_search.semantic_indexer:\n  1690→                results[""semantic""] = ""rebuild_not_implemented""\n  1691→            else:\n  1692→                results[""semantic""] = ""not_available""\n  1693→        \n  1694→        logger.info(f""Search indexes rebuilt by {current_user.username}: {index_type}"")\n  1695→        \n  1696→        return {\n  1697→            ""status"": ""success"",\n  1698→            ""message"": f""Search indexes rebuilt successfully"",\n  1699→            ""index_type"": index_type,\n  1700→            ""results"": results\n  1701→        }\n  1702→    except Exception as e:\n  1703→        logger.error(f""Failed to rebuild search indexes: {e}"")\n  1704→        raise HTTPException(500, f""Failed to rebuild indexes: {str(e)}"")\n  1705→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,native,0,True,8,0,0,0,8,0,0,0,237,237,59242,33236,237,0.5,0.25,0,,,,,transcript_response
a9f87083-f603-4b60-a151-8cabc58f385f,2025-06-26T07:40:39.873Z,transcript_a9f87083-f603-4b60-a151-8cabc58f385f,"{'type': 'tool_result', 'content': ""Claude requested permissions to use Bash, but you haven't granted it yet."", 'is_error': True, 'tool_use_id': 'toolu_01YG8snCckKRLCx49m96r8Br'}",transcript_query,unknown,native,0,True,8,0,0,0,8,0,0,0,119,119,92478,899,119,0.5,0.25,0,,,,,transcript_response
a9f87083-f603-4b60-a151-8cabc58f385f,2025-06-26T07:40:44.416Z,transcript_a9f87083-f603-4b60-a151-8cabc58f385f,"{'type': 'tool_result', 'content': ""Claude requested permissions to use Bash, but you haven't granted it yet."", 'is_error': True, 'tool_use_id': 'toolu_01ENroNedxpTy7utd5SxohmL'}",transcript_query,unknown,native,0,True,8,0,0,0,8,0,0,0,203,203,93377,154,203,0.5,0.25,0,,,,,transcript_response
a9f87083-f603-4b60-a151-8cabc58f385f,2025-06-26T07:40:50.439Z,transcript_a9f87083-f603-4b60-a151-8cabc58f385f,"{'tool_use_id': 'toolu_01JGCnbxWBSvMPKDQqxbCfvj', 'type': 'tool_result', 'content': '     1→""""""\n     2→Shared Interfaces for MCP Server\n     3→\n     4→Cross-cutting interfaces and common types used across all modules.\n     5→These interfaces define the contracts for logging, metrics, configuration,\n     6→caching, events, and other shared concerns.\n     7→""""""\n     8→\n     9→from abc import ABC, abstractmethod\n    10→from typing import Any, Dict, List, Optional, Callable, ContextManager, Awaitable, TypeVar, Generic\n    11→from dataclasses import dataclass\n    12→from datetime import datetime\n    13→from enum import Enum\n    14→\n    15→T = TypeVar(\'T\')\n    16→\n    17→# ========================================\n    18→# Common Enums\n    19→# ========================================\n    20→\n    21→class LogLevel(Enum):\n    22→    """"""Log levels for the logging system""""""\n    23→    DEBUG = ""debug""\n    24→    INFO = ""info""\n    25→    WARNING = ""warning""\n    26→    ERROR = ""error""\n    27→    CRITICAL = ""critical""\n    28→\n    29→class IndexStatus(Enum):\n    30→    """"""Status of indexing operations""""""\n    31→    PENDING = ""pending""\n    32→    IN_PROGRESS = ""in_progress""\n    33→    COMPLETED = ""completed""\n    34→    FAILED = ""failed""\n    35→    CANCELLED = ""cancelled""\n    36→\n    37→class PluginStatus(Enum):\n    38→    """"""Status of plugin lifecycle""""""\n    39→    UNLOADED = ""unloaded""\n    40→    LOADING = ""loading""\n    41→    READY = ""ready""\n    42→    ERROR = ""error""\n    43→    DISABLED = ""disabled""\n    44→\n    45→# ========================================\n    46→# Common Data Types\n    47→# ========================================\n    48→\n    49→@dataclass\n    50→class Event:\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,native,0,True,8,0,0,0,8,0,0,0,2957,2957,93531,2060,2957,0.5,0.25,0,,,,,transcript_response
1ec7365b-990d-4e16-bf1f-f88a8cc6dbc3,2025-06-26T07:31:01.648Z,transcript_1ec7365b-990d-4e16-bf1f-f88a8cc6dbc3,Find the BM25Indexer class definition,transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,1,1,0,13916,1,0.5,0.25,0,,,,,transcript_response
1ec7365b-990d-4e16-bf1f-f88a8cc6dbc3,2025-06-26T07:31:05.406Z,transcript_1ec7365b-990d-4e16-bf1f-f88a8cc6dbc3,"{'tool_use_id': 'toolu_016X3dpUCt4W1Sgkub16uyKX', 'type': 'tool_result', 'content': 'Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/bm25_indexer.py'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,2,2,13916,226,2,0.5,0.25,0,,,,,transcript_response
be840494-fb6b-4efd-85e3-87a494a7de4b,2025-06-26T07:36:17.724Z,transcript_be840494-fb6b-4efd-85e3-87a494a7de4b,Find the BM25Indexer class definition,transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,58,58,13916,0,58,0.5,0.25,0,,,,,transcript_response
be840494-fb6b-4efd-85e3-87a494a7de4b,2025-06-26T07:36:20.463Z,transcript_be840494-fb6b-4efd-85e3-87a494a7de4b,"{'tool_use_id': 'toolu_011t9wysiLJAso7JJFU7rd7i', 'type': 'tool_result', 'content': 'Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/bm25_indexer.py'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,1,1,14142,0,1,0.5,0.25,0,,,,,transcript_response
21a1dd51-144a-415a-8e34-3b8b29c0aa7a,2025-06-26T07:36:25.893Z,transcript_21a1dd51-144a-415a-8e34-3b8b29c0aa7a,Find the EnhancedDispatcher class and show its main methods,transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,1,1,13920,0,1,0.5,0.25,0,,,,,transcript_response
21a1dd51-144a-415a-8e34-3b8b29c0aa7a,2025-06-26T07:36:29.226Z,transcript_21a1dd51-144a-415a-8e34-3b8b29c0aa7a,"{'tool_use_id': 'toolu_01CPDdcQTg9yPX1aqgw5k5Bf', 'type': 'tool_result', 'content': 'Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/level4/path_management.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/path_management_architecture.md'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,27,27,13920,447,27,0.5,0.25,0,,,,,transcript_response
21a1dd51-144a-415a-8e34-3b8b29c0aa7a,2025-06-26T07:36:33.312Z,transcript_21a1dd51-144a-415a-8e34-3b8b29c0aa7a,"{'tool_use_id': 'toolu_01MtQhGfGGjTgcyobqwUMrXZ', 'type': 'tool_result', 'content': '     1→""""""Enhanced dispatcher with dynamic plugin loading via PluginFactory.""""""\n     2→from pathlib import Path\n     3→from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4→import logging\n     5→import hashlib\n     6→import time\n     7→import re\n     8→from datetime import datetime\n     9→import os\n    10→\n    11→from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12→from ..plugins.plugin_factory import PluginFactory\n    13→from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14→from ..storage.sqlite_store import SQLiteStore\n    15→from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16→from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17→# Note: We\'ve removed ignore pattern checks to allow indexing ALL files\n    18→# Filtering happens only during export via SecureIndexExporter\n    19→# from ..core.ignore_patterns import get_ignore_manager\n    20→\n    21→logger = logging.getLogger(__name__)\n    22→\n    23→\n    24→class EnhancedDispatcher:\n    25→    """"""Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.""""""\n    26→    \n    27→    # Document query patterns - common documentation search terms\n    28→    DOCUMENT_QUERY_PATTERNS = [\n    29→        r\'\\b(how\\s+to|howto)\\b\',\n    30→        r\'\\b(getting\\s+started|get\\s+started)\\b\',\n    31→        r\'\\b(installation|install|setup)\\b\',\n    32→        r\'\\b(configuration|configure|config)\\b\',\n    33→        r\'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b\',\n    34→        r\'\\b(tutorial|guide|walkthrough)\\b\',\n    35→        r\'\\b(example|sample|snippet)\\b\',\n    36→        r\'\\b(readme|documentation|docs)\\b\',\n    37→        r\'\\b(usage|use\\s+case|using)\\b\',\n    38→        r\'\\b(reference|manual)\\b\',\n    39→        r\'\\b(faq|frequently\\s+asked)\\b\',\n    40→        r\'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b\',\n    41→        r\'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b\',\n    42→        r\'\\b(architecture|design|overview)\\b\',\n    43→        r\'\\b(changelog|release\\s+notes|migration)\\b\'\n    44→    ]\n    45→    \n    46→    # Documentation file patterns\n    47→    DOCUMENTATION_FILE_PATTERNS = [\n    48→        r\'readme(\\.\\w+)?$\',\n    49→        r\'changelog(\\.\\w+)?$\',\n    50→        r\'contributing(\\.\\w+)?$\',\n    51→        r\'license(\\.\\w+)?$\',\n    52→        r\'install(\\.\\w+)?$\',\n    53→        r\'setup(\\.\\w+)?$\',\n    54→        r\'guide(\\.\\w+)?$\',\n    55→        r\'tutorial(\\.\\w+)?$\',\n    56→        r\'\\.md$\',\n    57→        r\'\\.rst$\',\n    58→        r\'\\.txt$\',\n    59→        r\'docs?/\',\n    60→        r\'documentation/\'\n    61→    ]\n    62→    \n    63→    def __init__(self, \n    64→                 plugins: Optional[List[IPlugin]] = None,\n    65→                 sqlite_store: Optional[SQLiteStore] = None,\n    66→                 enable_advanced_features: bool = True,\n    67→                 use_plugin_factory: bool = True,\n    68→                 lazy_load: bool = True,\n    69→                 semantic_search_enabled: bool = True):\n    70→        """"""Initialize the enhanced dispatcher.\n    71→        \n    72→        Args:\n    73→            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74→            sqlite_store: SQLite store for plugin persistence\n    75→            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76→            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77→            lazy_load: Whether to lazy-load plugins on demand\n    78→            semantic_search_enabled: Whether to enable semantic search in plugins\n    79→        """"""\n    80→        self._sqlite_store = sqlite_store\n    81→        self._enable_advanced = enable_advanced_features\n    82→        self._use_factory = use_plugin_factory\n    83→        self._lazy_load = lazy_load\n    84→        self._semantic_enabled = semantic_search_enabled\n    85→        \n    86→        # Plugin storage\n    87→        self._plugins: List[IPlugin] = []\n    88→        self._by_lang: Dict[str, IPlugin] = {}\n    89→        self._loaded_languages: set[str] = set()\n    90→        \n    91→        # Cache for file hashes to avoid re-indexing unchanged files\n    92→        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93→        \n    94→        # Advanced components\n    95→        if self._enable_advanced:\n    96→            self._file_matcher = FileTypeMatcher()\n    97→            self._router = PluginRouter(self._file_matcher)\n    98→            self._aggregator = ResultAggregator()\n    99→        \n   100→        # Performance tracking\n   101→        self._operation_stats = {\n   102→            \'searches\': 0,\n   103→            \'lookups\': 0,\n   104→            \'indexings\': 0,\n   105→            \'total_time\': 0.0,\n   106→            \'plugins_loaded\': 0\n   107→        }\n   108→        \n   109→        # Initialize plugins\n   110→        if plugins:\n   111→            # Use provided plugins (backward compatibility)\n   112→            self._plugins = plugins\n   113→            self._by_lang = {p.lang: p for p in plugins}\n   114→            for plugin in plugins:\n   115→                self._loaded_languages.add(getattr(plugin, \'lang\', \'unknown\'))\n   116→            if self._enable_advanced:\n   117→                self._register_plugins_with_router()\n   118→        elif use_plugin_factory and not lazy_load:\n   119→            # Load all plugins immediately\n   120→            self._load_all_plugins()\n   121→        # If lazy_load is True, plugins will be loaded on demand\n   122→        \n   123→        # Compile document query patterns for performance\n   124→        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125→        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126→        \n   127→        logger.info(f""Enhanced dispatcher initialized with {len(self._plugins)} plugins"")\n   128→    \n   129→    def _load_all_plugins(self):\n   130→        """"""Load all available plugins using PluginFactory.""""""\n   131→        logger.info(""Loading all available plugins..."")\n   132→        \n   133→        all_plugins = PluginFactory.create_all_plugins(\n   134→            sqlite_store=self._sqlite_store,\n   135→            enable_semantic=self._semantic_enabled\n   136→        )\n   137→        \n   138→        for lang, plugin in all_plugins.items():\n   139→            self._plugins.append(plugin)\n   140→            self._by_lang[lang] = plugin\n   141→            self._loaded_languages.add(lang)\n   142→            self._operation_stats[\'plugins_loaded\'] += 1\n   143→        \n   144→        if self._enable_advanced:\n   145→            self._register_plugins_with_router()\n   146→        \n   147→        logger.info(f""Loaded {len(all_plugins)} plugins: {\', \'.join(sorted(all_plugins.keys()))}"")\n   148→    \n   149→    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150→        """"""Ensure a plugin for the given language is loaded.\n   151→        \n   152→        Args:\n   153→            language: Language code (e.g., \'python\', \'go\')\n   154→            \n   155→        Returns:\n   156→            Plugin instance or None if not available\n   157→        """"""\n   158→        # Normalize language\n   159→        language = language.lower().replace(\'-\', \'_\')\n   160→        \n   161→        # Check if already loaded\n   162→        if language in self._by_lang:\n   163→            return self._by_lang[language]\n   164→        \n   165→        # If not using factory or already tried to load, return None\n   166→        if not self._use_factory or language in self._loaded_languages:\n   167→            return None\n   168→        \n   169→        # Try to load the plugin\n   170→        try:\n   171→            logger.info(f""Lazy loading plugin for {language}"")\n   172→            plugin = PluginFactory.create_plugin(\n   173→                language,\n   174→                sqlite_store=self._sqlite_store,\n   175→                enable_semantic=self._semantic_enabled\n   176→            )\n   177→            \n   178→            # Add to collections\n   179→            self._plugins.append(plugin)\n   180→            self._by_lang[language] = plugin\n   181→            self._loaded_languages.add(language)\n   182→            self._operation_stats[\'plugins_loaded\'] += 1\n   183→            \n   184→            # Register with router if needed\n   185→            if self._enable_advanced:\n   186→                capabilities = self._detect_plugin_capabilities(plugin)\n   187→                self._router.register_plugin(plugin, capabilities)\n   188→            \n   189→            logger.info(f""Successfully loaded {language} plugin"")\n   190→            return plugin\n   191→            \n   192→        except ValueError as e:\n   193→            logger.warning(f""No plugin available for {language}: {e}"")\n   194→            self._loaded_languages.add(language)  # Mark as attempted\n   195→            return None\n   196→        except Exception as e:\n   197→            logger.error(f""Error loading plugin for {language}: {e}"")\n   198→            self._loaded_languages.add(language)  # Mark as attempted\n   199→            return None\n   200→    \n   201→    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202→        """"""Ensure a plugin is loaded for the given file.\n   203→        \n   204→        Args:\n   205→            path: File path\n   206→            \n   207→        Returns:\n   208→            Plugin instance or None if not available\n   209→        """"""\n   210→        # Get language from file extension\n   211→        extension = path.suffix.lower()\n   212→        language = get_language_by_extension(extension)\n   213→        \n   214→        if language:\n   215→            return self._ensure_plugin_loaded(language)\n   216→        \n   217→        # Fallback: try all loaded plugins\n   218→        for plugin in self._plugins:\n   219→            if plugin.supports(path):\n   220→                return plugin\n   221→        \n   222→        return None\n   223→    \n   224→    def _register_plugins_with_router(self):\n   225→        """"""Register plugins with the router and assign capabilities.""""""\n   226→        for plugin in self._plugins:\n   227→            # Determine capabilities based on plugin type/language\n   228→            capabilities = self._detect_plugin_capabilities(plugin)\n   229→            self._router.register_plugin(plugin, capabilities)\n   230→    \n   231→    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232→        """"""Detect capabilities for a plugin based on its language and features.""""""\n   233→        capabilities = []\n   234→        lang = getattr(plugin, \'lang\', \'unknown\')\n   235→        \n   236→        # Base capabilities all plugins have\n   237→        capabilities.append(PluginCapability(\n   238→            \'syntax_analysis\', \'1.0\', f\'{lang} syntax analysis\', \n   239→            priority=70, metadata={\'language\': lang}\n   240→        ))\n   241→        \n   242→        capabilities.append(PluginCapability(\n   243→            \'code_search\', \'1.0\', f\'{lang} code search\', \n   244→            priority=80, metadata={\'language\': lang}\n   245→        ))\n   246→        \n   247→        # Check for semantic search capability\n   248→        if hasattr(plugin, \'_enable_semantic\') and plugin._enable_semantic:\n   249→            capabilities.append(PluginCapability(\n   250→                \'semantic_search\', \'1.0\', f\'{lang} semantic search\', \n   251→                priority=90, metadata={\'language\': lang}\n   252→            ))\n   253→        \n   254→        # Language-specific capabilities\n   255→        if lang == \'python\':\n   256→            capabilities.extend([\n   257→                PluginCapability(\'refactoring\', \'1.0\', \'Python refactoring support\', 75),\n   258→                PluginCapability(\'type_analysis\', \'1.0\', \'Python type analysis\', 85)\n   259→            ])\n   260→        elif lang in [\'javascript\', \'typescript\']:\n   261→            capabilities.extend([\n   262→                PluginCapability(\'linting\', \'1.0\', \'JavaScript/TypeScript linting\', 85),\n   263→                PluginCapability(\'bundling_analysis\', \'1.0\', \'Module bundling analysis\', 70),\n   264→                PluginCapability(\'framework_support\', \'1.0\', \'Framework-specific support\', 75)\n   265→            ])\n   266→        elif lang in [\'c\', \'cpp\']:\n   267→            capabilities.extend([\n   268→                PluginCapability(\'compilation_analysis\', \'1.0\', \'Compilation analysis\', 80),\n   269→                PluginCapability(\'memory_analysis\', \'1.0\', \'Memory usage analysis\', 70),\n   270→                PluginCapability(\'performance_profiling\', \'1.0\', \'Performance profiling\', 75)\n   271→            ])\n   272→        elif lang in [\'go\', \'rust\']:\n   273→            capabilities.extend([\n   274→                PluginCapability(\'package_analysis\', \'1.0\', f\'{lang} package analysis\', 80),\n   275→                PluginCapability(\'concurrency_analysis\', \'1.0\', f\'{lang} concurrency analysis\', 75)\n   276→            ])\n   277→        elif lang in [\'java\', \'kotlin\', \'scala\']:\n   278→            capabilities.extend([\n   279→                PluginCapability(\'jvm_analysis\', \'1.0\', \'JVM bytecode analysis\', 75),\n   280→                PluginCapability(\'build_tool_integration\', \'1.0\', \'Build tool integration\', 70)\n   281→            ])\n   282→        \n   283→        return capabilities\n   284→    \n   285→    @property\n   286→    def plugins(self):\n   287→        """"""Get the dictionary of loaded plugins by language.""""""\n   288→        return self._by_lang\n   289→    \n   290→    @property\n   291→    def supported_languages(self) -> List[str]:\n   292→        """"""Get list of all supported languages (loaded and available).""""""\n   293→        if self._use_factory:\n   294→            return PluginFactory.get_supported_languages()\n   295→        else:\n   296→            return list(self._by_lang.keys())\n   297→    \n   298→    def _match_plugin(self, path: Path) -> IPlugin:\n   299→        """"""Match a plugin for the given file path.""""""\n   300→        # Ensure plugin is loaded if using lazy loading\n   301→        if self._lazy_load and self._use_factory:\n   302→            plugin = self._ensure_plugin_for_file(path)\n   303→            if plugin:\n   304→                return plugin\n   305→        \n   306→        # Use advanced routing if available\n   307→        if self._enable_advanced and self._router:\n   308→            route_result = self._router.get_best_plugin(path)\n   309→            if route_result:\n   310→                return route_result.plugin\n   311→        \n   312→        # Fallback to basic matching\n   313→        for p in self._plugins:\n   314→            if p.supports(path):\n   315→                return p\n   316→        \n   317→        raise RuntimeError(f""No plugin found for {path}"")\n   318→    \n   319→    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320→        """"""Get all plugins that can handle a file with confidence scores.""""""\n   321→        # Ensure plugin is loaded if using lazy loading\n   322→        if self._lazy_load and self._use_factory:\n   323→            self._ensure_plugin_for_file(path)\n   324→        \n   325→        if self._enable_advanced and self._router:\n   326→            route_results = self._router.route_file(path)\n   327→            return [(result.plugin, result.confidence) for result in route_results]\n   328→        else:\n   329→            # Basic fallback\n   330→            matching_plugins = []\n   331→            for plugin in self._plugins:\n   332→                if plugin.supports(path):\n   333→                    matching_plugins.append((plugin, 1.0))\n   334→            return matching_plugins\n   335→    \n   336→    def lookup(self, symbol: str) -> SymbolDef | None:\n   337→        """"""Look up symbol definition across all plugins.""""""\n   338→        start_time = time.time()\n   339→        \n   340→        try:\n   341→            # For symbol lookup, we may need to search across all languages\n   342→            # Load all plugins if using lazy loading\n   343→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344→                self._load_all_plugins()\n   345→            \n   346→            if self._enable_advanced and self._aggregator:\n   347→                # Use advanced aggregation\n   348→                definitions_by_plugin = {}\n   349→                for plugin in self._plugins:\n   350→                    try:\n   351→                        definition = plugin.getDefinition(symbol)\n   352→                        definitions_by_plugin[plugin] = definition\n   353→                    except Exception as e:\n   354→                        logger.warning(f""Plugin {plugin.lang} failed to get definition for {symbol}: {e}"")\n   355→                        definitions_by_plugin[plugin] = None\n   356→                \n   357→                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358→                \n   359→                self._operation_stats[\'lookups\'] += 1\n   360→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   361→                \n   362→                return result\n   363→            else:\n   364→                # Fallback to basic lookup\n   365→                for p in self._plugins:\n   366→                    res = p.getDefinition(symbol)\n   367→                    if res:\n   368→                        self._operation_stats[\'lookups\'] += 1\n   369→                        self._operation_stats[\'total_time\'] += time.time() - start_time\n   370→                        return res\n   371→                return None\n   372→                \n   373→        except Exception as e:\n   374→            logger.error(f""Error in symbol lookup for {symbol}: {e}"", exc_info=True)\n   375→            return None\n   376→    \n   377→    def _is_document_query(self, query: str) -> bool:\n   378→        """"""Check if the query is looking for documentation.\n   379→        \n   380→        Args:\n   381→            query: Search query string\n   382→            \n   383→        Returns:\n   384→            True if this appears to be a documentation query\n   385→        """"""\n   386→        query_lower = query.lower()\n   387→        \n   388→        # Check against document query patterns\n   389→        for pattern in self._compiled_doc_patterns:\n   390→            if pattern.search(query_lower):\n   391→                return True\n   392→        \n   393→        # Check for question words at the beginning\n   394→        question_starters = [\'how\', \'what\', \'where\', \'when\', \'why\', \'can\', \'is\', \'does\', \'should\']\n   395→        first_word = query_lower.split()[0] if query_lower.split() else \'\'\n   396→        if first_word in question_starters:\n   397→            return True\n   398→        \n   399→        return False\n   400→    \n   401→    def _expand_document_query(self, query: str) -> List[str]:\n   402→        """"""Expand a document query with related terms for better search coverage.\n   403→        \n   404→        Args:\n   405→            query: Original search query\n   406→            \n   407→        Returns:\n   408→            List of expanded query variations\n   409→        """"""\n   410→        expanded_queries = [query]  # Always include original\n   411→        query_lower = query.lower()\n   412→        \n   413→        # Common expansions for documentation queries\n   414→        expansions = {\n   415→            \'install\': [\'installation\', \'setup\', \'getting started\', \'requirements\'],\n   416→            \'config\': [\'configuration\', \'configure\', \'settings\', \'options\', \'parameters\'],\n   417→            \'api\': [\'api documentation\', \'api reference\', \'endpoint\', \'method\'],\n   418→            \'how to\': [\'tutorial\', \'guide\', \'example\', \'usage\'],\n   419→            \'example\': [\'sample\', \'snippet\', \'demo\', \'code example\'],\n   420→            \'error\': [\'troubleshoot\', \'debug\', \'issue\', \'problem\', \'fix\'],\n   421→            \'getting started\': [\'quickstart\', \'tutorial\', \'introduction\', \'setup\'],\n   422→            \'guide\': [\'tutorial\', \'documentation\', \'walkthrough\', \'how to\'],\n   423→            \'usage\': [\'how to use\', \'example\', \'api\', \'reference\']\n   424→        }\n   425→        \n   426→        # Apply expansions\n   427→        for term, related_terms in expansions.items():\n   428→            if term in query_lower:\n   429→                for related in related_terms:\n   430→                    # Replace the term with related term\n   431→                    expanded = query_lower.replace(term, related)\n   432→                    if expanded != query_lower and expanded not in expanded_queries:\n   433→                        expanded_queries.append(expanded)\n   434→                \n   435→                # Also add queries with additional terms\n   436→                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437→                    expanded = f""{query} {related}""\n   438→                    if expanded not in expanded_queries:\n   439→                        expanded_queries.append(expanded)\n   440→        \n   441→        # Add file-specific searches for common documentation files\n   442→        if self._is_document_query(query):\n   443→            # Extract the main topic from the query\n   444→            topic_words = []\n   445→            for word in query.lower().split():\n   446→                if word not in [\'how\', \'to\', \'the\', \'a\', \'an\', \'is\', \'are\', \'what\', \'where\', \'when\']:\n   447→                    topic_words.append(word)\n   448→            \n   449→            if topic_words:\n   450→                topic = \' \'.join(topic_words[:2])  # Use first two topic words\n   451→                expanded_queries.extend([\n   452→                    f""README {topic}"",\n   453→                    f""{topic} documentation"",\n   454→                    f""{topic} docs"",\n   455→                    f""{topic} guide""\n   456→                ])\n   457→        \n   458→        # Remove duplicates while preserving order\n   459→        seen = set()\n   460→        unique_queries = []\n   461→        for q in expanded_queries:\n   462→            if q not in seen:\n   463→                seen.add(q)\n   464→                unique_queries.append(q)\n   465→        \n   466→        return unique_queries[:10]  # Limit to 10 queries max\n   467→    \n   468→    def _is_documentation_file(self, file_path: str) -> bool:\n   469→        """"""Check if a file path is likely a documentation file.\n   470→        \n   471→        Args:\n   472→            file_path: Path to check\n   473→            \n   474→        Returns:\n   475→            True if this appears to be a documentation file\n   476→        """"""\n   477→        path_lower = file_path.lower()\n   478→        \n   479→        for pattern in self._compiled_file_patterns:\n   480→            if pattern.search(path_lower):\n   481→                return True\n   482→        \n   483→        return False\n   484→    \n   485→    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486→        """"""Adjust ranking to prioritize documentation files for document queries.\n   487→        \n   488→        Args:\n   489→            query: Original search query\n   490→            results: List of aggregated results\n   491→            \n   492→        Returns:\n   493→            Re-ranked results with documentation prioritized\n   494→        """"""\n   495→        if not self._is_document_query(query):\n   496→            return results\n   497→        \n   498→        # Separate documentation and code results\n   499→        doc_results = []\n   500→        code_results = []\n   501→        \n   502→        for result in results:\n   503→            if self._is_documentation_file(result.primary_result.get(\'file\', \'\')):\n   504→                # Boost documentation files for document queries\n   505→                result.rank_score *= 1.5\n   506→                result.metadata[\'doc_boost\'] = True\n   507→                doc_results.append(result)\n   508→            else:\n   509→                code_results.append(result)\n   510→        \n   511→        # Sort each group by rank score\n   512→        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513→        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514→        \n   515→        # Combine with documentation files first\n   516→        return doc_results + code_results\n   517→    \n   518→    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519→        """"""Search for code and documentation across all plugins.""""""\n   520→        start_time = time.time()\n   521→        \n   522→        try:\n   523→            # For search, we may need to search across all languages\n   524→            # Load all plugins if using lazy loading\n   525→            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526→                self._load_all_plugins()\n   527→            \n   528→            # Detect if this is a document query\n   529→            is_doc_query = self._is_document_query(query)\n   530→            \n   531→            # Expand query if it\'s a document query\n   532→            queries = [query]\n   533→            if is_doc_query:\n   534→                queries = self._expand_document_query(query)\n   535→                logger.info(f""Expanded document query \'{query}\' to {len(queries)} variations"")\n   536→                # Force semantic search for natural language queries\n   537→                semantic = True\n   538→            \n   539→            if self._enable_advanced and self._aggregator:\n   540→                # Use advanced aggregation\n   541→                all_results_by_plugin = {}\n   542→                opts = {""semantic"": semantic, ""limit"": limit * 2 if is_doc_query else limit}\n   543→                \n   544→                # Search with all query variations\n   545→                for search_query in queries:\n   546→                    for plugin in self._plugins:\n   547→                        try:\n   548→                            results = list(plugin.search(search_query, opts))\n   549→                            if results:\n   550→                                if plugin not in all_results_by_plugin:\n   551→                                    all_results_by_plugin[plugin] = []\n   552→                                all_results_by_plugin[plugin].extend(results)\n   553→                        except Exception as e:\n   554→                            logger.warning(f""Plugin {plugin.lang} failed to search for {search_query}: {e}"")\n   555→                \n   556→                # Deduplicate results per plugin\n   557→                for plugin, results in all_results_by_plugin.items():\n   558→                    seen = set()\n   559→                    unique_results = []\n   560→                    for result in results:\n   561→                        key = f""{result[\'file\']}:{result[\'line\']}""\n   562→                        if key not in seen:\n   563→                            seen.add(key)\n   564→                            unique_results.append(result)\n   565→                    all_results_by_plugin[plugin] = unique_results\n   566→                \n   567→                # Configure aggregator for document queries\n   568→                if is_doc_query and self._enable_advanced:\n   569→                    # Adjust ranking criteria for documentation\n   570→                    doc_criteria = RankingCriteria(\n   571→                        relevance_weight=0.5,      # Increase relevance weight\n   572→                        confidence_weight=0.2,     # Reduce confidence weight\n   573→                        frequency_weight=0.2,      # Keep frequency weight\n   574→                        recency_weight=0.1,        # Keep recency weight\n   575→                        prefer_exact_matches=False,  # Natural language doesn\'t need exact matches\n   576→                        boost_multiple_sources=True,\n   577→                        boost_common_extensions=True\n   578→                    )\n   579→                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580→                \n   581→                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582→                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583→                )\n   584→                \n   585→                # Adjust ranking for document queries\n   586→                if is_doc_query:\n   587→                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588→                \n   589→                # Apply final limit\n   590→                if limit and len(aggregated_results) > limit:\n   591→                    aggregated_results = aggregated_results[:limit]\n   592→                \n   593→                logger.debug(f""Search aggregation stats: {stats.total_results} total, ""\n   594→                           f""{stats.unique_results} unique, {stats.plugins_used} plugins used, ""\n   595→                           f""document_query={is_doc_query}"")\n   596→                \n   597→                self._operation_stats[\'searches\'] += 1\n   598→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   599→                \n   600→                # Yield primary results from aggregated results\n   601→                for aggregated in aggregated_results:\n   602→                    yield aggregated.primary_result\n   603→            else:\n   604→                # Fallback to basic search\n   605→                # Detect if this is a document query\n   606→                is_doc_query = self._is_document_query(query)\n   607→                \n   608→                # Expand query if it\'s a document query\n   609→                queries = [query]\n   610→                if is_doc_query:\n   611→                    queries = self._expand_document_query(query)\n   612→                    semantic = True  # Force semantic search for natural language\n   613→                \n   614→                opts = {""semantic"": semantic, ""limit"": limit}\n   615→                all_results = []\n   616→                \n   617→                # Search with all query variations\n   618→                for search_query in queries:\n   619→                    for p in self._plugins:\n   620→                        try:\n   621→                            for result in p.search(search_query, opts):\n   622→                                all_results.append(result)\n   623→                        except Exception as e:\n   624→                            logger.warning(f""Plugin {p.lang} failed to search for {search_query}: {e}"")\n   625→                \n   626→                # Deduplicate results\n   627→                seen = set()\n   628→                unique_results = []\n   629→                for result in all_results:\n   630→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   631→                    if key not in seen:\n   632→                        seen.add(key)\n   633→                        unique_results.append(result)\n   634→                \n   635→                # Sort by score if available\n   636→                unique_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   637→                \n   638→                # Prioritize documentation files for document queries\n   639→                if is_doc_query:\n   640→                    doc_results = []\n   641→                    code_results = []\n   642→                    for result in unique_results:\n   643→                        if self._is_documentation_file(result.get(\'file\', \'\')):\n   644→                            doc_results.append(result)\n   645→                        else:\n   646→                            code_results.append(result)\n   647→                    unique_results = doc_results + code_results\n   648→                \n   649→                # Apply limit\n   650→                count = 0\n   651→                for result in unique_results:\n   652→                    if limit and count >= limit:\n   653→                        break\n   654→                    yield result\n   655→                    count += 1\n   656→                \n   657→                self._operation_stats[\'searches\'] += 1\n   658→                self._operation_stats[\'total_time\'] += time.time() - start_time\n   659→                \n   660→        except Exception as e:\n   661→            logger.error(f""Error in search for {query}: {e}"", exc_info=True)\n   662→    \n   663→    def index_file(self, path: Path) -> None:\n   664→        """"""Index a single file if it has changed.""""""\n   665→        try:\n   666→            # Ensure path is absolute to avoid relative/absolute path issues\n   667→            path = path.resolve()\n   668→            \n   669→            # Find the appropriate plugin\n   670→            plugin = self._match_plugin(path)\n   671→            \n   672→            # Read file content\n   673→            try:\n   674→                content = path.read_text(encoding=\'utf-8\')\n   675→            except UnicodeDecodeError:\n   676→                # Try with different encodings\n   677→                try:\n   678→                    content = path.read_text(encoding=\'latin-1\')\n   679→                except Exception as e:\n   680→                    logger.error(f""Failed to read {path}: {e}"")\n   681→                    return\n   682→            \n   683→            # Check if we need to re-index (simplified for now)\n   684→            # TODO: Implement proper caching logic\n   685→            \n   686→            # Index the file\n   687→            start_time = time.time()\n   688→            logger.info(f""Indexing {path} with {plugin.lang} plugin"")\n   689→            shard = plugin.indexFile(path, content)\n   690→            \n   691→            # Record performance if advanced features enabled\n   692→            if self._enable_advanced and self._router:\n   693→                execution_time = time.time() - start_time\n   694→                self._router.record_performance(plugin, execution_time)\n   695→            \n   696→            self._operation_stats[\'indexings\'] += 1\n   697→            self._operation_stats[\'total_time\'] += time.time() - start_time\n   698→            \n   699→            logger.info(f""Successfully indexed {path}: {len(shard.get(\'symbols\', []))} symbols found"")\n   700→            \n   701→        except RuntimeError as e:\n   702→            # No plugin found for this file type\n   703→            logger.debug(f""No plugin for {path}: {e}"")\n   704→        except Exception as e:\n   705→            logger.error(f""Error indexing {path}: {e}"", exc_info=True)\n   706→    \n   707→    def get_statistics(self) -> dict:\n   708→        """"""Get comprehensive statistics across all plugins and components.""""""\n   709→        stats = {\n   710→            ""total_plugins"": len(self._plugins),\n   711→            ""loaded_languages"": sorted(list(self._loaded_languages)),\n   712→            ""supported_languages"": len(self.supported_languages),\n   713→            ""operations"": self._operation_stats.copy()\n   714→        }\n   715→        \n   716→        # Add language breakdown\n   717→        stats[""by_language""] = {}\n   718→        for lang, plugin in self._by_lang.items():\n   719→            plugin_info = {\n   720→                ""loaded"": True,\n   721→                ""class"": plugin.__class__.__name__\n   722→            }\n   723→            if hasattr(plugin, \'get_indexed_count\'):\n   724→                plugin_info[""indexed_files""] = plugin.get_indexed_count()\n   725→            stats[""by_language""][lang] = plugin_info\n   726→        \n   727→        return stats\n   728→    \n   729→    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730→        """"""\n   731→        Index all files in a directory, respecting ignore patterns.\n   732→        \n   733→        Args:\n   734→            directory: Directory to index\n   735→            recursive: Whether to index subdirectories\n   736→            \n   737→        Returns:\n   738→            Statistics about indexed files\n   739→        """"""\n   740→        logger.info(f""Indexing directory: {directory} (recursive={recursive})"")\n   741→        \n   742→        # Note: We don\'t use ignore patterns during indexing\n   743→        # ALL files are indexed for local search capability\n   744→        # Filtering happens only during export/sharing\n   745→        \n   746→        # Get all supported extensions\n   747→        supported_extensions = get_all_extensions()\n   748→        \n   749→        stats = {\n   750→            ""total_files"": 0,\n   751→            ""indexed_files"": 0,\n   752→            ""ignored_files"": 0,\n   753→            ""failed_files"": 0,\n   754→            ""by_language"": {}\n   755→        }\n   756→        \n   757→        # Walk directory\n   758→        if recursive:\n   759→            file_iterator = directory.rglob(""*"")\n   760→        else:\n   761→            file_iterator = directory.glob(""*"")\n   762→            \n   763→        for path in file_iterator:\n   764→            if not path.is_file():\n   765→                continue\n   766→                \n   767→            stats[""total_files""] += 1\n   768→            \n   769→            # NOTE: We index ALL files locally, including gitignored ones\n   770→            # Filtering happens only during export/sharing\n   771→            # This allows local search of .env, secrets, etc.\n   772→                \n   773→            # Try to find a plugin that supports this file\n   774→            # This allows us to index ALL files, including .env, .key, etc.\n   775→            try:\n   776→                # First try to match by extension\n   777→                if path.suffix in supported_extensions:\n   778→                    self.index_file(path)\n   779→                    stats[""indexed_files""] += 1\n   780→                # For files without recognized extensions, try each plugin\'s supports() method\n   781→                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782→                else:\n   783→                    matched = False\n   784→                    for plugin in self._plugins:\n   785→                        if plugin.supports(path):\n   786→                            self.index_file(path)\n   787→                            stats[""indexed_files""] += 1\n   788→                            matched = True\n   789→                            break\n   790→                    \n   791→                    # If no plugin matched but we want to index everything,\n   792→                    # we could add a fallback here to index as plaintext\n   793→                    # For now, we\'ll skip unmatched files\n   794→                    if not matched:\n   795→                        logger.debug(f""No plugin found for {path}"")\n   796→                \n   797→                # Track by language\n   798→                language = get_language_by_extension(path.suffix)\n   799→                if language:\n   800→                    stats[""by_language""][language] = stats[""by_language""].get(language, 0) + 1\n   801→                    \n   802→            except Exception as e:\n   803→                logger.error(f""Failed to index {path}: {e}"")\n   804→                stats[""failed_files""] += 1\n   805→                \n   806→        logger.info(f""Directory indexing complete: {stats[\'indexed_files\']} indexed, ""\n   807→                   f""{stats[\'ignored_files\']} ignored, {stats[\'failed_files\']} failed"")\n   808→        \n   809→        return stats\n   810→    \n   811→    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812→        """"""Search specifically across documentation files.\n   813→        \n   814→        Args:\n   815→            topic: Topic to search for (e.g., ""installation"", ""configuration"")\n   816→            doc_types: Optional list of document types to search (e.g., [""readme"", ""guide"", ""api""])\n   817→            limit: Maximum number of results\n   818→            \n   819→        Returns:\n   820→            Search results from documentation files\n   821→        """"""\n   822→        # Default document types if not specified\n   823→        if doc_types is None:\n   824→            doc_types = [""readme"", ""documentation"", ""guide"", ""tutorial"", ""api"", ""changelog"", ""contributing""]\n   825→        \n   826→        # Build search queries for different document types\n   827→        queries = []\n   828→        for doc_type in doc_types:\n   829→            queries.extend([\n   830→                f""{doc_type} {topic}"",\n   831→                f""{topic} {doc_type}"",\n   832→                f""{topic} in {doc_type}""\n   833→            ])\n   834→        \n   835→        # Also search for the topic in common doc filenames\n   836→        queries.extend([\n   837→            f""README {topic}"",\n   838→            f""CONTRIBUTING {topic}"",\n   839→            f""docs {topic}"",\n   840→            f""documentation {topic}""\n   841→        ])\n   842→        \n   843→        # Deduplicate queries\n   844→        queries = list(dict.fromkeys(queries))\n   845→        \n   846→        logger.info(f""Cross-document search for \'{topic}\' with {len(queries)} query variations"")\n   847→        \n   848→        # Use the enhanced search with document-specific handling\n   849→        all_results = []\n   850→        seen = set()\n   851→        \n   852→        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853→            for result in self.search(query, semantic=True, limit=limit):\n   854→                # Only include documentation files\n   855→                if self._is_documentation_file(result.get(\'file\', \'\')):\n   856→                    key = f""{result[\'file\']}:{result[\'line\']}""\n   857→                    if key not in seen:\n   858→                        seen.add(key)\n   859→                        all_results.append(result)\n   860→        \n   861→        # Sort by relevance (score) and return top results\n   862→        all_results.sort(key=lambda r: r.get(\'score\', 0.5) or 0.5, reverse=True)\n   863→        \n   864→        count = 0\n   865→        for result in all_results:\n   866→            if count >= limit:\n   867→                break\n   868→            yield result\n   869→            count += 1\n   870→    \n   871→    def health_check(self) -> Dict[str, Any]:\n   872→        """"""Perform a health check on all components.""""""\n   873→        health = {\n   874→            \'status\': \'healthy\',\n   875→            \'components\': {\n   876→                \'dispatcher\': {\n   877→                    \'status\': \'healthy\',\n   878→                    \'plugins_loaded\': len(self._plugins),\n   879→                    \'languages_supported\': len(self.supported_languages),\n   880→                    \'factory_enabled\': self._use_factory,\n   881→                    \'lazy_loading\': self._lazy_load\n   882→                }\n   883→            },\n   884→            \'plugins\': {},\n   885→            \'errors\': []\n   886→        }\n   887→        \n   888→        # Check plugin health\n   889→        for lang, plugin in self._by_lang.items():\n   890→            try:\n   891→                plugin_health = {\n   892→                    \'status\': \'healthy\',\n   893→                    \'class\': plugin.__class__.__name__,\n   894→                    \'semantic_enabled\': getattr(plugin, \'_enable_semantic\', False)\n   895→                }\n   896→                if hasattr(plugin, \'get_indexed_count\'):\n   897→                    plugin_health[\'indexed_files\'] = plugin.get_indexed_count()\n   898→            except Exception as e:\n   899→                plugin_health = {\n   900→                    \'status\': \'error\',\n   901→                    \'error\': str(e)\n   902→                }\n   903→                health[\'errors\'].append(f""Plugin {lang}: {str(e)}"")\n   904→            \n   905→            health[\'plugins\'][lang] = plugin_health\n   906→        \n   907→        # Determine overall health\n   908→        if len(health[\'errors\']) > 0:\n   909→            health[\'status\'] = \'degraded\' if len(health[\'errors\']) < 3 else \'unhealthy\'\n   910→        \n   911→        return health\n   912→    \n   913→    def remove_file(self, path: Union[Path, str]) -> None:\n   914→        """"""Remove a file from all indexes.\n   915→        \n   916→        Args:\n   917→            path: File path to remove\n   918→        """"""\n   919→        path = Path(path).resolve()\n   920→        logger.info(f""Removing file from index: {path}"")\n   921→        \n   922→        try:\n   923→            # Remove from SQLite if available\n   924→            if self._sqlite_store:\n   925→                from ..core.path_resolver import PathResolver\n   926→                path_resolver = PathResolver()\n   927→                try:\n   928→                    relative_path = path_resolver.normalize_path(path)\n   929→                    # Get repository ID - for now assume 1\n   930→                    # TODO: Properly detect repository\n   931→                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932→                except Exception as e:\n   933→                    logger.error(f""Error removing from SQLite: {e}"")\n   934→            \n   935→            # Remove from semantic index if available\n   936→            try:\n   937→                plugin = self._match_plugin(path)\n   938→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   939→                    plugin._indexer.remove_file(path)\n   940→                    logger.info(f""Removed from semantic index: {path}"")\n   941→            except Exception as e:\n   942→                logger.warning(f""Error removing from semantic index: {e}"")\n   943→            \n   944→            # Update statistics\n   945→            self._operation_stats[\'deletions\'] = self._operation_stats.get(\'deletions\', 0) + 1\n   946→            \n   947→        except Exception as e:\n   948→            logger.error(f""Error removing file {path}: {e}"", exc_info=True)\n   949→    \n   950→    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951→        """"""Move a file in all indexes.\n   952→        \n   953→        Args:\n   954→            old_path: Original file path\n   955→            new_path: New file path\n   956→            content_hash: Optional content hash to verify unchanged content\n   957→        """"""\n   958→        old_path = Path(old_path).resolve()\n   959→        new_path = Path(new_path).resolve()\n   960→        logger.info(f""Moving file in index: {old_path} -> {new_path}"")\n   961→        \n   962→        try:\n   963→            # Move in SQLite if available\n   964→            if self._sqlite_store:\n   965→                from ..core.path_resolver import PathResolver\n   966→                path_resolver = PathResolver()\n   967→                try:\n   968→                    old_relative = path_resolver.normalize_path(old_path)\n   969→                    new_relative = path_resolver.normalize_path(new_path)\n   970→                    # Get repository ID - for now assume 1\n   971→                    # TODO: Properly detect repository\n   972→                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973→                except Exception as e:\n   974→                    logger.error(f""Error moving in SQLite: {e}"")\n   975→            \n   976→            # Move in semantic index if available\n   977→            try:\n   978→                plugin = self._match_plugin(new_path)\n   979→                if plugin and hasattr(plugin, \'_indexer\') and plugin._indexer:\n   980→                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981→                    logger.info(f""Moved in semantic index: {old_path} -> {new_path}"")\n   982→            except Exception as e:\n   983→                logger.warning(f""Error moving in semantic index: {e}"")\n   984→            \n   985→            # Update statistics\n   986→            self._operation_stats[\'moves\'] = self._operation_stats.get(\'moves\', 0) + 1\n   987→            \n   988→        except Exception as e:\n   989→            logger.error(f""Error moving file {old_path} -> {new_path}: {e}"", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,1,1,14367,13323,1,0.5,0.25,0,,,,,transcript_response
bce49748-dbd5-4f39-8bef-d4ed666a4850,2025-06-26T07:32:40.779Z,transcript_bce49748-dbd5-4f39-8bef-d4ed666a4850,Find the BM25Indexer class definition,transcript_query,unknown,native,0,True,4,0,0,0,4,0,0,0,58,58,13916,0,58,0.5,0.25,0,,,,,transcript_response
bce49748-dbd5-4f39-8bef-d4ed666a4850,2025-06-26T07:32:43.685Z,transcript_bce49748-dbd5-4f39-8bef-d4ed666a4850,"{'tool_use_id': 'toolu_01FJfEB7KEmYNe7Pkss7F6iC', 'type': 'tool_result', 'content': 'Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/bm25_indexer.py'}",transcript_query,unknown,native,0,True,7,0,0,0,7,0,0,0,1,1,14142,0,1,0.5,0.25,0,,,,,transcript_response
1750923060,2025-06-26T07:31:00.864352,symbol_001,Find the BM25Indexer class definition,symbol_queries,simple,mcp,8027.41,True,9,0,7,15,31,3,3,5,3,7,0,0,38.75,0.5,0.25,0,,,,,success
1750923060,2025-06-26T07:31:00.864990,symbol_001,Find the BM25Indexer class definition,symbol_queries,simple,native,7132.414,True,9,0,4,9,19,2,1,3,2,4,0,0,23.75,0.5,0.25,0,,,,,success
1750923060,2025-06-26T07:31:08.895373,symbol_002,Find the EnhancedDispatcher class and show its main methods,symbol_queries,medium,mcp,22183.313,True,14,0,106,213,426,53,42,71,53,106,0,0,532.5,0.5,0.25,0,,,,,success
1750923060,2025-06-26T07:31:08.895432,symbol_002,Find the EnhancedDispatcher class and show its main methods,symbol_queries,medium,native,23155.100000000002,True,14,0,115,230,461,57,46,76,57,115,0,0,576.25,0.5,0.25,0,,,,,success
1750923377,2025-06-26T07:36:17.083057,symbol_001,Find the BM25Indexer class definition,symbol_queries,simple,mcp,8195.618,True,9,0,7,15,31,3,3,5,3,7,0,0,38.75,0.5,0.25,0,,,,,success
1750923377,2025-06-26T07:36:17.083543,symbol_001,Find the BM25Indexer class definition,symbol_queries,simple,native,6191.5019999999995,True,9,0,4,9,19,2,1,3,2,4,0,0,23.75,0.5,0.25,0,,,,,success
1750923377,2025-06-26T07:36:25.281056,symbol_002,Find the EnhancedDispatcher class and show its main methods,symbol_queries,medium,mcp,21137.219999999998,True,14,0,126,253,507,63,50,84,63,126,0,0,633.75,0.5,0.25,0,,,,,success
1750923377,2025-06-26T07:36:25.281110,symbol_002,Find the EnhancedDispatcher class and show its main methods,symbol_queries,medium,native,18932.867000000002,True,14,0,98,196,392,49,39,65,49,98,0,0,490.0,0.5,0.25,0,,,,,success
1750923377,2025-06-26T07:36:46.420898,content_001,Find all functions that use asyncio,content_queries,simple,mcp,120085.959,False,8,0,0,0,1,0,0,0,0,0,0,0,1.25,0.5,0.25,0,,,,,failure
1750923377,2025-06-26T07:36:46.420949,content_001,Find all functions that use asyncio,content_queries,simple,native,120086.18,False,8,0,0,0,1,0,0,0,0,0,0,0,1.25,0.5,0.25,0,,,,,failure
1750923551,2025-06-26T07:39:11.442461,symbol_001,Find the BM25Indexer class definition,symbol_queries,simple,mcp,6746.713,True,9,0,7,15,31,3,3,5,3,7,0,0,38.75,0.5,0.25,0,,,,,success
1750923551,2025-06-26T07:39:11.443389,symbol_001,Find the BM25Indexer class definition,symbol_queries,simple,native,13552.759999999998,True,9,0,9,18,37,4,3,6,4,9,0,0,46.25,0.5,0.25,0,,,,,success
1750923551,2025-06-26T07:39:24.998742,symbol_002,Find the EnhancedDispatcher class and show its main methods,symbol_queries,medium,mcp,20918.44,True,14,0,117,235,471,58,47,78,58,117,0,0,588.75,0.5,0.25,0,,,,,success
1750923551,2025-06-26T07:39:24.998789,symbol_002,Find the EnhancedDispatcher class and show its main methods,symbol_queries,medium,native,21368.458,True,14,0,122,245,490,61,49,81,61,122,0,0,612.5,0.5,0.25,0,,,,,success
1750923551,2025-06-26T07:39:46.369653,content_001,Find all functions that use asyncio,content_queries,simple,mcp,120088.21399999999,False,8,0,0,0,1,0,0,0,0,0,0,0,1.25,0.5,0.25,0,,,,,failure
1750923551,2025-06-26T07:39:46.369696,content_001,Find all functions that use asyncio,content_queries,simple,native,119996.527,False,8,0,0,0,1,0,0,0,0,0,0,0,1.25,0.5,0.25,0,,,,,failure
1750923160,2025-06-26T07:32:40.149536,symbol_001,Find the BM25Indexer class definition,symbol_queries,simple,mcp,11638.653,True,9,0,8,16,32,4,3,5,4,8,0,0,40.0,0.5,0.25,0,,,,,success
1750923160,2025-06-26T07:32:40.149739,symbol_001,Find the BM25Indexer class definition,symbol_queries,simple,native,6298.263999999999,True,9,0,4,9,19,2,1,3,2,4,0,0,23.75,0.5,0.25,0,,,,,success
1750923160,2025-06-26T07:32:51.789971,symbol_002,Find the EnhancedDispatcher class and show its main methods,symbol_queries,medium,mcp,23412.188000000002,True,14,0,153,306,613,76,61,102,76,153,0,0,766.25,0.5,0.25,0,,,,,success
1750923160,2025-06-26T07:32:51.790023,symbol_002,Find the EnhancedDispatcher class and show its main methods,symbol_queries,medium,native,55664.453,True,14,0,50,101,203,25,20,33,25,50,0,0,253.75,0.5,0.25,0,,,,,success
1750923160,2025-06-26T07:33:47.457369,content_001,Find all functions that use asyncio,content_queries,simple,mcp,120026.60299999999,False,8,0,0,0,1,0,0,0,0,0,0,0,1.25,0.5,0.25,0,,,,,failure
1750923160,2025-06-26T07:33:47.457416,content_001,Find all functions that use asyncio,content_queries,simple,native,120082.291,False,8,0,0,0,1,0,0,0,0,0,0,0,1.25,0.5,0.25,0,,,,,failure
