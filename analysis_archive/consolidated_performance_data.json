[
  {
    "session_id": "badee9bd-4114-482e-94e0-25c4201bb901",
    "test_date": "2025-06-26T07:33:48.096Z",
    "query_id": "transcript_badee9bd-4114-482e-94e0-25c4201bb901",
    "query_text": "Find all functions that use asyncio",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13160,
    "cache_creation_tokens": 742,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "badee9bd-4114-482e-94e0-25c4201bb901",
    "test_date": "2025-06-26T07:33:52.115Z",
    "query_id": "transcript_badee9bd-4114-482e-94e0-25c4201bb901",
    "query_text": [
      {
        "tool_use_id": "toolu_01BxnWrP5deruNHCMqotmat2",
        "type": "tool_result",
        "content": "Found 72 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/conftest.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_developer_workflows.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_repository_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_semantic_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_plugins_direct.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/populate_bm25_quick.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_reranking_working.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/docs/development/TESTING-GUIDE.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/prometheus_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/watchdog.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/file_watcher.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/grpc_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/MCP.md"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13902,
    "cache_creation_tokens": 5047,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "badee9bd-4114-482e-94e0-25c4201bb901",
    "test_date": "2025-06-26T07:33:57.310Z",
    "query_id": "transcript_badee9bd-4114-482e-94e0-25c4201bb901",
    "query_text": [
      {
        "tool_use_id": "toolu_011MwrmpEc2gEDUqiR1hvcxY",
        "type": "tool_result",
        "content": "Found 76 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_python_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_section_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/security/auth_manager.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/security/security_middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/api_gateway_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/cache_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/dispatcher_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/indexing_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/metrics_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/security_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/shared_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/storage_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_runner.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/benchmark_reranking_comparison.py"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 2,
    "output_tokens_total": 2,
    "cache_read_tokens": 18949,
    "cache_creation_tokens": 2687,
    "tokens_per_result": 2,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "975ed141-04b0-4979-9ed2-38231876b9a9",
    "test_date": "2025-06-26T07:32:40.807Z",
    "query_id": "transcript_975ed141-04b0-4979-9ed2-38231876b9a9",
    "query_text": "Find the BM25Indexer class definition",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 58,
    "output_tokens_total": 58,
    "cache_read_tokens": 13905,
    "cache_creation_tokens": 0,
    "tokens_per_result": 58,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "975ed141-04b0-4979-9ed2-38231876b9a9",
    "test_date": "2025-06-26T07:32:44.051Z",
    "query_id": "transcript_975ed141-04b0-4979-9ed2-38231876b9a9",
    "query_text": [
      {
        "tool_use_id": "toolu_018qBJwnF7aT19i4gE5eks5W",
        "type": "tool_result",
        "content": "Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/bm25_indexer.py"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 25,
    "output_tokens_total": 25,
    "cache_read_tokens": 14133,
    "cache_creation_tokens": 0,
    "tokens_per_result": 25,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "975ed141-04b0-4979-9ed2-38231876b9a9",
    "test_date": "2025-06-26T07:32:48.004Z",
    "query_id": "transcript_975ed141-04b0-4979-9ed2-38231876b9a9",
    "query_text": [
      {
        "tool_use_id": "toolu_01RWxLRPbTPa7yLUcxF8Ahe3",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"\n     2\u2192BM25 Indexer using SQLite FTS5 for full-text search.\n     3\u2192\n     4\u2192This module provides BM25-based full-text search indexing using SQLite's FTS5 \n     5\u2192extension, which includes built-in BM25 ranking algorithms.\n     6\u2192\"\"\"\n     7\u2192\n     8\u2192import sqlite3\n     9\u2192import logging\n    10\u2192import hashlib\n    11\u2192import json\n    12\u2192from typing import List, Dict, Any, Optional, Tuple\n    13\u2192from datetime import datetime\n    14\u2192from pathlib import Path\n    15\u2192from contextlib import contextmanager\n    16\u2192\n    17\u2192from ..storage.sqlite_store import SQLiteStore\n    18\u2192# Interface definition inline for now\n    19\u2192from abc import ABC, abstractmethod\n    20\u2192\n    21\u2192logger = logging.getLogger(__name__)\n    22\u2192\n    23\u2192# Define IIndexer interface inline\n    24\u2192class IIndexer(ABC):\n    25\u2192    @abstractmethod\n    26\u2192    def index_documents(self, documents: List[Dict[str, Any]]) -> bool:\n    27\u2192        pass\n    28\u2192    \n    29\u2192    @abstractmethod\n    30\u2192    def search(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:\n    31\u2192        pass\n    32\u2192\n    33\u2192\n    34\u2192class BM25Indexer(IIndexer):\n    35\u2192    \"\"\"\n    36\u2192    BM25-based full-text search indexer using SQLite FTS5.\n    37\u2192    \n    38\u2192    This indexer provides:\n    39\u2192    - Efficient full-text search using inverted indexes\n    40\u2192    - Built-in BM25 ranking algorithm\n    41\u2192    - Support for phrase searches, prefix searches, and boolean operators\n    42\u2192    - Integration with existing SQLite storage\n    43\u2192    \"\"\"\n    44\u2192    \n    45\u2192    def __init__(self, storage: SQLiteStore, table_name: str = \"bm25_content\"):\n    46\u2192        \"\"\"\n    47\u2192        Initialize the BM25 indexer.\n    48\u2192        \n    49\u2192        Args:\n    50\u2192            storage: SQLite storage instance\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 14133,
    "cache_creation_tokens": 728,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "f7e3333c-51c3-40d3-a43a-b4b7a573f5aa",
    "test_date": "2025-06-26T07:36:17.714Z",
    "query_id": "transcript_f7e3333c-51c3-40d3-a43a-b4b7a573f5aa",
    "query_text": "Find the BM25Indexer class definition",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 58,
    "output_tokens_total": 58,
    "cache_read_tokens": 13905,
    "cache_creation_tokens": 0,
    "tokens_per_result": 58,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "f7e3333c-51c3-40d3-a43a-b4b7a573f5aa",
    "test_date": "2025-06-26T07:36:21.835Z",
    "query_id": "transcript_f7e3333c-51c3-40d3-a43a-b4b7a573f5aa",
    "query_text": [
      {
        "tool_use_id": "toolu_01KLEcZQyms98jyEaZwfeqyQ",
        "type": "tool_result",
        "content": "Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/bm25_indexer.py"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 14133,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "0be9b10a-8143-4183-be2c-4956b3822221",
    "test_date": "2025-06-26T07:31:32.678Z",
    "query_id": "transcript_0be9b10a-8143-4183-be2c-4956b3822221",
    "query_text": "Locate the SQLiteStore class and explain its search method",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13160,
    "cache_creation_tokens": 747,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "0be9b10a-8143-4183-be2c-4956b3822221",
    "test_date": "2025-06-26T07:31:36.781Z",
    "query_id": "transcript_0be9b10a-8143-4183-be2c-4956b3822221",
    "query_text": "Search for the SQLiteStore class definition in the codebase. Look for files that contain \"class SQLiteStore\" or similar patterns. Return the file path and location where this class is defined.",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 81,
    "output_tokens_total": 81,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 10750,
    "tokens_per_result": 81,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "0be9b10a-8143-4183-be2c-4956b3822221",
    "test_date": "2025-06-26T07:31:41.222Z",
    "query_id": "transcript_0be9b10a-8143-4183-be2c-4956b3822221",
    "query_text": [
      {
        "tool_use_id": "toolu_01Cusb5WCkMw7KXUpdnkbXR2",
        "type": "tool_result",
        "content": "Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/storage/sqlite_store.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/path_management_architecture.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/indexer_actual.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/storage_actual.puml"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 105,
    "output_tokens_total": 105,
    "cache_read_tokens": 10750,
    "cache_creation_tokens": 235,
    "tokens_per_result": 105,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "0be9b10a-8143-4183-be2c-4956b3822221",
    "test_date": "2025-06-26T07:31:44.671Z",
    "query_id": "transcript_0be9b10a-8143-4183-be2c-4956b3822221",
    "query_text": [
      {
        "tool_use_id": "toolu_01UosLuHspyRcapQjBtnW7ku",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"\n     2\u2192SQLite-based persistence layer for the MCP Server.\n     3\u2192\n     4\u2192This module provides a local storage implementation using SQLite with FTS5\n     5\u2192for efficient full-text search capabilities.\n     6\u2192\"\"\"\n     7\u2192\n     8\u2192import sqlite3\n     9\u2192import json\n    10\u2192import logging\n    11\u2192from typing import List, Dict, Any, Optional, Tuple, Union\n    12\u2192from datetime import datetime\n    13\u2192from pathlib import Path\n    14\u2192from contextlib import contextmanager\n    15\u2192\n    16\u2192from ..core.path_resolver import PathResolver\n    17\u2192\n    18\u2192logger = logging.getLogger(__name__)\n    19\u2192\n    20\u2192\n    21\u2192class SQLiteStore:\n    22\u2192    \"\"\"SQLite-based storage implementation with FTS5 support.\"\"\"\n    23\u2192    \n    24\u2192    def __init__(self, db_path: str = \"code_index.db\", path_resolver: Optional[PathResolver] = None):\n    25\u2192        \"\"\"\n    26\u2192        Initialize the SQLite store.\n    27\u2192        \n    28\u2192        Args:\n    29\u2192            db_path: Path to the SQLite database file\n    30\u2192            path_resolver: PathResolver instance for path management\n    31\u2192        \"\"\"\n    32\u2192        self.db_path = db_path\n    33\u2192        self.path_resolver = path_resolver or PathResolver()\n    34\u2192        self._init_database()\n    35\u2192        self._run_migrations()\n    36\u2192    \n    37\u2192    def _init_database(self):\n    38\u2192        \"\"\"Initialize database and create schema if needed.\"\"\"\n    39\u2192        with self._get_connection() as conn:\n    40\u2192            # Enable foreign keys\n    41\u2192            conn.execute(\"PRAGMA foreign_keys = ON\")\n    42\u2192            \n    43\u2192            # Enable FTS5 if available\n    44\u2192            self._check_fts5_support(conn)\n    45\u2192            \n    46\u2192            # Check if schema exists\n    47\u2192            cursor = conn.execute(\n    48\u2192                \"SELECT name FROM sqlite_master WHERE type='table' AND name='schema_version'\"\n    49\u2192            )\n    50\u2192            if not cursor.fetchone():\n    51\u2192                self._init_schema(conn)\n    52\u2192                logger.info(f\"Initialized database schema at {self.db_path}\")\n    53\u2192            else:\n    54\u2192                logger.info(f\"Using existing database at {self.db_path}\")\n    55\u2192    \n    56\u2192    def _run_migrations(self):\n    57\u2192        \"\"\"Run any pending database migrations.\"\"\"\n    58\u2192        migrations_dir = Path(__file__).parent / \"migrations\"\n    59\u2192        if not migrations_dir.exists():\n    60\u2192            return\n    61\u2192        \n    62\u2192        with self._get_connection() as conn:\n    63\u2192            # Get current schema version\n    64\u2192            try:\n    65\u2192                cursor = conn.execute(\"SELECT MAX(version) FROM schema_version\")\n    66\u2192                current_version = cursor.fetchone()[0] or 0\n    67\u2192            except sqlite3.OperationalError:\n    68\u2192                current_version = 0\n    69\u2192            \n    70\u2192            # Run migrations\n    71\u2192            for migration_file in sorted(migrations_dir.glob(\"*.sql\")):\n    72\u2192                # Extract version from filename (e.g., \"002_relative_paths.sql\" -> 2)\n    73\u2192                try:\n    74\u2192                    version = int(migration_file.stem.split('_')[0])\n    75\u2192                except (ValueError, IndexError):\n    76\u2192                    continue\n    77\u2192                \n    78\u2192                if version > current_version:\n    79\u2192                    logger.info(f\"Running migration {migration_file.name}\")\n    80\u2192                    with open(migration_file, 'r') as f:\n    81\u2192                        conn.executescript(f.read())\n    82\u2192                    logger.info(f\"Completed migration to version {version}\")\n    83\u2192    \n    84\u2192    @contextmanager\n    85\u2192    def _get_connection(self):\n    86\u2192        \"\"\"Get a database connection with proper error handling.\"\"\"\n    87\u2192        conn = sqlite3.connect(self.db_path)\n    88\u2192        conn.row_factory = sqlite3.Row\n    89\u2192        try:\n    90\u2192            yield conn\n    91\u2192            conn.commit()\n    92\u2192        except Exception:\n    93\u2192            conn.rollback()\n    94\u2192            raise\n    95\u2192        finally:\n    96\u2192            conn.close()\n    97\u2192    \n    98\u2192    def _check_fts5_support(self, conn: sqlite3.Connection) -> bool:\n    99\u2192        \"\"\"Check if FTS5 is supported in this SQLite build.\"\"\"\n   100\u2192        try:\n   101\u2192            cursor = conn.execute(\"PRAGMA compile_options;\")\n   102\u2192            options = [row[0] for row in cursor]\n   103\u2192            fts5_enabled = any('ENABLE_FTS5' in option for option in options)\n   104\u2192            if fts5_enabled:\n   105\u2192                logger.info(\"FTS5 support confirmed\")\n   106\u2192            else:\n   107\u2192                logger.warning(\"FTS5 may not be available in this SQLite build\")\n   108\u2192            return fts5_enabled\n   109\u2192        except Exception as e:\n   110\u2192            logger.warning(f\"Could not check FTS5 support: {e}\")\n   111\u2192            return False\n   112\u2192    \n   113\u2192    def _init_schema(self, conn: sqlite3.Connection):\n   114\u2192        \"\"\"Initialize the database schema.\"\"\"\n   115\u2192        # Create core tables\n   116\u2192        conn.executescript(\"\"\"\n   117\u2192            -- Schema Version\n   118\u2192            CREATE TABLE IF NOT EXISTS schema_version (\n   119\u2192                version INTEGER PRIMARY KEY,\n   120\u2192                applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   121\u2192                description TEXT\n   122\u2192            );\n   123\u2192            \n   124\u2192            -- Repositories\n   125\u2192            CREATE TABLE IF NOT EXISTS repositories (\n   126\u2192                id INTEGER PRIMARY KEY,\n   127\u2192                path TEXT UNIQUE NOT NULL,\n   128\u2192                name TEXT NOT NULL,\n   129\u2192                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   130\u2192                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   131\u2192                metadata JSON\n   132\u2192            );\n   133\u2192            \n   134\u2192            -- Files\n   135\u2192            CREATE TABLE IF NOT EXISTS files (\n   136\u2192                id INTEGER PRIMARY KEY,\n   137\u2192                repository_id INTEGER NOT NULL,\n   138\u2192                path TEXT NOT NULL,\n   139\u2192                relative_path TEXT NOT NULL,\n   140\u2192                language TEXT,\n   141\u2192                size INTEGER,\n   142\u2192                hash TEXT,\n   143\u2192                last_modified TIMESTAMP,\n   144\u2192                indexed_at TIMESTAMP,\n   145\u2192                metadata JSON,\n   146\u2192                FOREIGN KEY (repository_id) REFERENCES repositories(id),\n   147\u2192                UNIQUE(repository_id, path)\n   148\u2192            );\n   149\u2192            \n   150\u2192            CREATE INDEX IF NOT EXISTS idx_files_language ON files(language);\n   151\u2192            CREATE INDEX IF NOT EXISTS idx_files_hash ON files(hash);\n   152\u2192            \n   153\u2192            -- Symbols\n   154\u2192            CREATE TABLE IF NOT EXISTS symbols (\n   155\u2192                id INTEGER PRIMARY KEY,\n   156\u2192                file_id INTEGER NOT NULL,\n   157\u2192                name TEXT NOT NULL,\n   158\u2192                kind TEXT NOT NULL,\n   159\u2192                line_start INTEGER NOT NULL,\n   160\u2192                line_end INTEGER NOT NULL,\n   161\u2192                column_start INTEGER,\n   162\u2192                column_end INTEGER,\n   163\u2192                signature TEXT,\n   164\u2192                documentation TEXT,\n   165\u2192                metadata JSON,\n   166\u2192                FOREIGN KEY (file_id) REFERENCES files(id)\n   167\u2192            );\n   168\u2192            \n   169\u2192            CREATE INDEX IF NOT EXISTS idx_symbols_name ON symbols(name);\n   170\u2192            CREATE INDEX IF NOT EXISTS idx_symbols_kind ON symbols(kind);\n   171\u2192            CREATE INDEX IF NOT EXISTS idx_symbols_file ON symbols(file_id);\n   172\u2192            \n   173\u2192            -- Imports\n   174\u2192            CREATE TABLE IF NOT EXISTS imports (\n   175\u2192                id INTEGER PRIMARY KEY,\n   176\u2192                file_id INTEGER NOT NULL,\n   177\u2192                imported_path TEXT NOT NULL,\n   178\u2192                imported_name TEXT,\n   179\u2192                alias TEXT,\n   180\u2192                line_number INTEGER,\n   181\u2192                is_relative BOOLEAN,\n   182\u2192                metadata JSON,\n   183\u2192                FOREIGN KEY (file_id) REFERENCES files(id)\n   184\u2192            );\n   185\u2192            \n   186\u2192            CREATE INDEX IF NOT EXISTS idx_imports_file ON imports(file_id);\n   187\u2192            CREATE INDEX IF NOT EXISTS idx_imports_path ON imports(imported_path);\n   188\u2192            \n   189\u2192            -- References (using symbol_references to avoid keyword conflict)\n   190\u2192            CREATE TABLE IF NOT EXISTS symbol_references (\n   191\u2192                id INTEGER PRIMARY KEY,\n   192\u2192                symbol_id INTEGER NOT NULL,\n   193\u2192                file_id INTEGER NOT NULL,\n   194\u2192                line_number INTEGER NOT NULL,\n   195\u2192                column_number INTEGER,\n   196\u2192                reference_kind TEXT,\n   197\u2192                metadata JSON,\n   198\u2192                FOREIGN KEY (symbol_id) REFERENCES symbols(id),\n   199\u2192                FOREIGN KEY (file_id) REFERENCES files(id)\n   200\u2192            );\n   201\u2192            \n   202\u2192            CREATE INDEX IF NOT EXISTS idx_references_symbol ON symbol_references(symbol_id);\n   203\u2192            CREATE INDEX IF NOT EXISTS idx_references_file ON symbol_references(file_id);\n   204\u2192            \n   205\u2192            -- Full-Text Search tables\n   206\u2192            CREATE VIRTUAL TABLE IF NOT EXISTS fts_symbols USING fts5(\n   207\u2192                name,\n   208\u2192                documentation,\n   209\u2192                content=symbols,\n   210\u2192                content_rowid=id\n   211\u2192            );\n   212\u2192            \n   213\u2192            CREATE VIRTUAL TABLE IF NOT EXISTS fts_code USING fts5(\n   214\u2192                content,\n   215\u2192                file_id UNINDEXED\n   216\u2192            );\n   217\u2192            \n   218\u2192            -- Fuzzy Search\n   219\u2192            CREATE TABLE IF NOT EXISTS symbol_trigrams (\n   220\u2192                symbol_id INTEGER NOT NULL,\n   221\u2192                trigram TEXT NOT NULL,\n   222\u2192                FOREIGN KEY (symbol_id) REFERENCES symbols(id)\n   223\u2192            );\n   224\u2192            \n   225\u2192            CREATE INDEX IF NOT EXISTS idx_trigrams ON symbol_trigrams(trigram);\n   226\u2192            \n   227\u2192            -- Embeddings\n   228\u2192            CREATE TABLE IF NOT EXISTS embeddings (\n   229\u2192                id INTEGER PRIMARY KEY,\n   230\u2192                file_id INTEGER,\n   231\u2192                symbol_id INTEGER,\n   232\u2192                chunk_start INTEGER,\n   233\u2192                chunk_end INTEGER,\n   234\u2192                embedding BLOB NOT NULL,\n   235\u2192                model_version TEXT NOT NULL,\n   236\u2192                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   237\u2192                metadata JSON,\n   238\u2192                FOREIGN KEY (file_id) REFERENCES files(id),\n   239\u2192                FOREIGN KEY (symbol_id) REFERENCES symbols(id)\n   240\u2192            );\n   241\u2192            \n   242\u2192            CREATE INDEX IF NOT EXISTS idx_embeddings_file ON embeddings(file_id);\n   243\u2192            CREATE INDEX IF NOT EXISTS idx_embeddings_symbol ON embeddings(symbol_id);\n   244\u2192            \n   245\u2192            -- Cache Tables\n   246\u2192            CREATE TABLE IF NOT EXISTS query_cache (\n   247\u2192                query_hash TEXT PRIMARY KEY,\n   248\u2192                query_text TEXT NOT NULL,\n   249\u2192                result JSON NOT NULL,\n   250\u2192                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   251\u2192                expires_at TIMESTAMP NOT NULL,\n   252\u2192                hit_count INTEGER DEFAULT 0\n   253\u2192            );\n   254\u2192            \n   255\u2192            CREATE INDEX IF NOT EXISTS idx_cache_expires ON query_cache(expires_at);\n   256\u2192            \n   257\u2192            CREATE TABLE IF NOT EXISTS parse_cache (\n   258\u2192                file_hash TEXT PRIMARY KEY,\n   259\u2192                ast JSON NOT NULL,\n   260\u2192                parser_version TEXT NOT NULL,\n   261\u2192                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n   262\u2192            );\n   263\u2192            \n   264\u2192            -- Migration Log\n   265\u2192            CREATE TABLE IF NOT EXISTS migrations (\n   266\u2192                id INTEGER PRIMARY KEY,\n   267\u2192                version_from INTEGER,\n   268\u2192                version_to INTEGER,\n   269\u2192                executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   270\u2192                duration_ms INTEGER,\n   271\u2192                status TEXT\n   272\u2192            );\n   273\u2192            \n   274\u2192            -- Index Configuration\n   275\u2192            CREATE TABLE IF NOT EXISTS index_config (\n   276\u2192                id INTEGER PRIMARY KEY,\n   277\u2192                config_key TEXT UNIQUE NOT NULL,\n   278\u2192                config_value TEXT NOT NULL,\n   279\u2192                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   280\u2192                description TEXT\n   281\u2192            );\n   282\u2192            \n   283\u2192            -- Insert initial index configuration\n   284\u2192            INSERT OR IGNORE INTO index_config (config_key, config_value, description) VALUES\n   285\u2192                ('embedding_model', 'voyage-code-3', 'Current embedding model used for vector search'),\n   286\u2192                ('model_dimension', '1024', 'Embedding vector dimension'),\n   287\u2192                ('distance_metric', 'cosine', 'Distance metric for vector similarity'),\n   288\u2192                ('index_version', '1.0', 'Index schema version');\n   289\u2192            \n   290\u2192            -- Insert initial schema version\n   291\u2192            INSERT INTO schema_version (version, description) \n   292\u2192            VALUES (1, 'Initial schema creation');\n   293\u2192        \"\"\")\n   294\u2192        \n   295\u2192        # Create triggers for FTS\n   296\u2192        conn.executescript(\"\"\"\n   297\u2192            -- Triggers to keep FTS in sync with symbols table\n   298\u2192            CREATE TRIGGER IF NOT EXISTS symbols_ai AFTER INSERT ON symbols\n   299\u2192            BEGIN\n   300\u2192                INSERT INTO fts_symbols(rowid, name, documentation)\n   301\u2192                VALUES (new.id, new.name, new.documentation);\n   302\u2192            END;\n   303\u2192            \n   304\u2192            CREATE TRIGGER IF NOT EXISTS symbols_ad AFTER DELETE ON symbols\n   305\u2192            BEGIN\n   306\u2192                DELETE FROM fts_symbols WHERE rowid = old.id;\n   307\u2192            END;\n   308\u2192            \n   309\u2192            CREATE TRIGGER IF NOT EXISTS symbols_au AFTER UPDATE ON symbols\n   310\u2192            BEGIN\n   311\u2192                UPDATE fts_symbols \n   312\u2192                SET name = new.name, documentation = new.documentation\n   313\u2192                WHERE rowid = new.id;\n   314\u2192            END;\n   315\u2192        \"\"\")\n   316\u2192    \n   317\u2192    # Repository operations\n   318\u2192    def create_repository(self, path: str, name: str, metadata: Optional[Dict] = None) -> int:\n   319\u2192        \"\"\"Create a new repository entry.\"\"\"\n   320\u2192        with self._get_connection() as conn:\n   321\u2192            cursor = conn.execute(\n   322\u2192                \"\"\"INSERT INTO repositories (path, name, metadata) \n   323\u2192                   VALUES (?, ?, ?)\n   324\u2192                   ON CONFLICT(path) DO UPDATE SET \n   325\u2192                   name=excluded.name, \n   326\u2192                   metadata=excluded.metadata,\n   327\u2192                   updated_at=CURRENT_TIMESTAMP\"\"\",\n   328\u2192                (path, name, json.dumps(metadata or {}))\n   329\u2192            )\n   330\u2192            if cursor.lastrowid:\n   331\u2192                return cursor.lastrowid\n   332\u2192            else:\n   333\u2192                # If lastrowid is None, it means we updated an existing row\n   334\u2192                # Get the id of the existing repository\n   335\u2192                cursor = conn.execute(\"SELECT id FROM repositories WHERE path = ?\", (path,))\n   336\u2192                return cursor.fetchone()[0]\n   337\u2192    \n   338\u2192    def get_repository(self, path: str) -> Optional[Dict]:\n   339\u2192        \"\"\"Get repository by path.\"\"\"\n   340\u2192        with self._get_connection() as conn:\n   341\u2192            cursor = conn.execute(\n   342\u2192                \"SELECT * FROM repositories WHERE path = ?\", (path,)\n   343\u2192            )\n   344\u2192            row = cursor.fetchone()\n   345\u2192            return dict(row) if row else None\n   346\u2192    \n   347\u2192    # File operations\n   348\u2192    def store_file(self, repository_id: int, file_path: Union[str, Path],\n   349\u2192                   language: Optional[str] = None, size: Optional[int] = None,\n   350\u2192                   metadata: Optional[Dict] = None) -> int:\n   351\u2192        \"\"\"Store file information using relative paths and content hashes.\"\"\"\n   352\u2192        # Convert to Path object and normalize\n   353\u2192        path = Path(file_path)\n   354\u2192        \n   355\u2192        # Compute relative path\n   356\u2192        relative_path = self.path_resolver.normalize_path(path)\n   357\u2192        \n   358\u2192        # Compute content hash\n   359\u2192        content_hash = self.path_resolver.compute_content_hash(path)\n   360\u2192        \n   361\u2192        with self._get_connection() as conn:\n   362\u2192            # Check if file already exists with same content hash\n   363\u2192            existing = self.get_file_by_content_hash(content_hash, repository_id)\n   364\u2192            if existing and existing['relative_path'] != relative_path:\n   365\u2192                # File moved - record the move\n   366\u2192                self.move_file(existing['relative_path'], relative_path, repository_id, content_hash)\n   367\u2192                return existing['id']\n   368\u2192            \n   369\u2192            # Store using relative path as primary identifier\n   370\u2192            cursor = conn.execute(\n   371\u2192                \"\"\"INSERT INTO files \n   372\u2192                   (repository_id, path, relative_path, language, size, hash, content_hash,\n   373\u2192                    last_modified, indexed_at, metadata, is_deleted)\n   374\u2192                   VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, ?, FALSE)\n   375\u2192                   ON CONFLICT(repository_id, relative_path) DO UPDATE SET\n   376\u2192                   path=excluded.path,\n   377\u2192                   language=excluded.language,\n   378\u2192                   size=excluded.size,\n   379\u2192                   hash=excluded.hash,\n   380\u2192                   content_hash=excluded.content_hash,\n   381\u2192                   last_modified=CURRENT_TIMESTAMP,\n   382\u2192                   indexed_at=CURRENT_TIMESTAMP,\n   383\u2192                   metadata=excluded.metadata,\n   384\u2192                   is_deleted=FALSE,\n   385\u2192                   deleted_at=NULL\"\"\",\n   386\u2192                (repository_id, str(path), relative_path, language, size, \n   387\u2192                 self.path_resolver.compute_file_hash(path), content_hash,\n   388\u2192                 json.dumps(metadata or {}))\n   389\u2192            )\n   390\u2192            if cursor.lastrowid:\n   391\u2192                return cursor.lastrowid\n   392\u2192            else:\n   393\u2192                # Get the id of the existing file\n   394\u2192                cursor = conn.execute(\n   395\u2192                    \"SELECT id FROM files WHERE repository_id = ? AND relative_path = ?\", \n   396\u2192                    (repository_id, relative_path)\n   397\u2192                )\n   398\u2192                return cursor.fetchone()[0]\n   399\u2192    \n   400\u2192    def get_file(self, file_path: Union[str, Path], repository_id: Optional[int] = None) -> Optional[Dict]:\n   401\u2192        \"\"\"Get file by path (relative or absolute).\"\"\"\n   402\u2192        # Try to normalize path if it's absolute\n   403\u2192        try:\n   404\u2192            relative_path = self.path_resolver.normalize_path(file_path)\n   405\u2192        except ValueError:\n   406\u2192            # Path might already be relative\n   407\u2192            relative_path = str(file_path).replace('\\\\', '/')\n   408\u2192        \n   409\u2192        with self._get_connection() as conn:\n   410\u2192            if repository_id:\n   411\u2192                cursor = conn.execute(\n   412\u2192                    \"SELECT * FROM files WHERE relative_path = ? AND repository_id = ? AND is_deleted = FALSE\",\n   413\u2192                    (relative_path, repository_id)\n   414\u2192                )\n   415\u2192            else:\n   416\u2192                cursor = conn.execute(\n   417\u2192                    \"SELECT * FROM files WHERE relative_path = ? AND is_deleted = FALSE\", \n   418\u2192                    (relative_path,)\n   419\u2192                )\n   420\u2192            row = cursor.fetchone()\n   421\u2192            return dict(row) if row else None\n   422\u2192    \n   423\u2192    # Symbol operations\n   424\u2192    def store_symbol(self, file_id: int, name: str, kind: str,\n   425\u2192                     line_start: int, line_end: int,\n   426\u2192                     column_start: Optional[int] = None,\n   427\u2192                     column_end: Optional[int] = None,\n   428\u2192                     signature: Optional[str] = None,\n   429\u2192                     documentation: Optional[str] = None,\n   430\u2192                     metadata: Optional[Dict] = None) -> int:\n   431\u2192        \"\"\"Store a symbol definition.\"\"\"\n   432\u2192        with self._get_connection() as conn:\n   433\u2192            cursor = conn.execute(\n   434\u2192                \"\"\"INSERT INTO symbols \n   435\u2192                   (file_id, name, kind, line_start, line_end, column_start, \n   436\u2192                    column_end, signature, documentation, metadata)\n   437\u2192                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n   438\u2192                (file_id, name, kind, line_start, line_end, column_start,\n   439\u2192                 column_end, signature, documentation, json.dumps(metadata or {}))\n   440\u2192            )\n   441\u2192            symbol_id = cursor.lastrowid\n   442\u2192            \n   443\u2192            # Generate and store trigrams for fuzzy search\n   444\u2192            self._store_trigrams(conn, symbol_id, name)\n   445\u2192            \n   446\u2192            return symbol_id\n   447\u2192    \n   448\u2192    def _store_trigrams(self, conn: sqlite3.Connection, symbol_id: int, name: str):\n   449\u2192        \"\"\"Generate and store trigrams for a symbol name.\"\"\"\n   450\u2192        # Generate trigrams\n   451\u2192        trigrams = set()\n   452\u2192        padded_name = f\"  {name}  \"  # Pad with spaces for edge trigrams\n   453\u2192        for i in range(len(padded_name) - 2):\n   454\u2192            trigram = padded_name[i:i+3].lower()\n   455\u2192            trigrams.add(trigram)\n   456\u2192        \n   457\u2192        # Store trigrams\n   458\u2192        for trigram in trigrams:\n   459\u2192            conn.execute(\n   460\u2192                \"INSERT INTO symbol_trigrams (symbol_id, trigram) VALUES (?, ?)\",\n   461\u2192                (symbol_id, trigram)\n   462\u2192            )\n   463\u2192    \n   464\u2192    def get_symbol(self, name: str, kind: Optional[str] = None) -> List[Dict]:\n   465\u2192        \"\"\"Get symbols by name and optionally kind.\"\"\"\n   466\u2192        with self._get_connection() as conn:\n   467\u2192            if kind:\n   468\u2192                cursor = conn.execute(\n   469\u2192                    \"\"\"SELECT s.*, f.path as file_path \n   470\u2192                       FROM symbols s\n   471\u2192                       JOIN files f ON s.file_id = f.id\n   472\u2192                       WHERE s.name = ? AND s.kind = ?\"\"\",\n   473\u2192                    (name, kind)\n   474\u2192                )\n   475\u2192            else:\n   476\u2192                cursor = conn.execute(\n   477\u2192                    \"\"\"SELECT s.*, f.path as file_path \n   478\u2192                       FROM symbols s\n   479\u2192                       JOIN files f ON s.file_id = f.id\n   480\u2192                       WHERE s.name = ?\"\"\",\n   481\u2192                    (name,)\n   482\u2192                )\n   483\u2192            return [dict(row) for row in cursor.fetchall()]\n   484\u2192    \n   485\u2192    # Reference operations\n   486\u2192    def store_reference(self, symbol_id: int, file_id: int,\n   487\u2192                       line_number: int, column_number: Optional[int] = None,\n   488\u2192                       reference_kind: Optional[str] = None,\n   489\u2192                       metadata: Optional[Dict] = None) -> int:\n   490\u2192        \"\"\"Store a symbol reference.\"\"\"\n   491\u2192        with self._get_connection() as conn:\n   492\u2192            cursor = conn.execute(\n   493\u2192                \"\"\"INSERT INTO symbol_references \n   494\u2192                   (symbol_id, file_id, line_number, column_number, \n   495\u2192                    reference_kind, metadata)\n   496\u2192                   VALUES (?, ?, ?, ?, ?, ?)\"\"\",\n   497\u2192                (symbol_id, file_id, line_number, column_number,\n   498\u2192                 reference_kind, json.dumps(metadata or {}))\n   499\u2192            )\n   500\u2192            return cursor.lastrowid\n   501\u2192    \n   502\u2192    def get_references(self, symbol_id: int) -> List[Dict]:\n   503\u2192        \"\"\"Get all references to a symbol.\"\"\"\n   504\u2192        with self._get_connection() as conn:\n   505\u2192            cursor = conn.execute(\n   506\u2192                \"\"\"SELECT r.*, f.path as file_path\n   507\u2192                   FROM symbol_references r\n   508\u2192                   JOIN files f ON r.file_id = f.id\n   509\u2192                   WHERE r.symbol_id = ?\"\"\",\n   510\u2192                (symbol_id,)\n   511\u2192            )\n   512\u2192            return [dict(row) for row in cursor.fetchall()]\n   513\u2192    \n   514\u2192    # Search operations\n   515\u2192    def search_symbols_fuzzy(self, query: str, limit: int = 20) -> List[Dict]:\n   516\u2192        \"\"\"\n   517\u2192        Fuzzy search for symbols using trigrams.\n   518\u2192        \n   519\u2192        Args:\n   520\u2192            query: Search query\n   521\u2192            limit: Maximum number of results\n   522\u2192            \n   523\u2192        Returns:\n   524\u2192            List of matching symbols with relevance scores\n   525\u2192        \"\"\"\n   526\u2192        with self._get_connection() as conn:\n   527\u2192            # Generate trigrams for the query\n   528\u2192            query_trigrams = set()\n   529\u2192            padded_query = f\"  {query}  \"\n   530\u2192            for i in range(len(padded_query) - 2):\n   531\u2192                trigram = padded_query[i:i+3].lower()\n   532\u2192                query_trigrams.add(trigram)\n   533\u2192            \n   534\u2192            if not query_trigrams:\n   535\u2192                return []\n   536\u2192            \n   537\u2192            # Build query with trigram matching\n   538\u2192            placeholders = ','.join(['?'] * len(query_trigrams))\n   539\u2192            \n   540\u2192            cursor = conn.execute(f\"\"\"\n   541\u2192                SELECT s.*, f.path as file_path,\n   542\u2192                       COUNT(DISTINCT st.trigram) as matches,\n   543\u2192                       COUNT(DISTINCT st.trigram) * 1.0 / ? as score\n   544\u2192                FROM symbols s\n   545\u2192                JOIN files f ON s.file_id = f.id\n   546\u2192                JOIN symbol_trigrams st ON s.id = st.symbol_id\n   547\u2192                WHERE st.trigram IN ({placeholders})\n   548\u2192                GROUP BY s.id\n   549\u2192                ORDER BY score DESC, s.name\n   550\u2192                LIMIT ?\n   551\u2192            \"\"\", [len(query_trigrams)] + list(query_trigrams) + [limit])\n   552\u2192            \n   553\u2192            return [dict(row) for row in cursor.fetchall()]\n   554\u2192    \n   555\u2192    def search_symbols_fts(self, query: str, limit: int = 20) -> List[Dict]:\n   556\u2192        \"\"\"\n   557\u2192        Full-text search for symbols.\n   558\u2192        \n   559\u2192        Args:\n   560\u2192            query: Search query\n   561\u2192            limit: Maximum number of results\n   562\u2192            \n   563\u2192        Returns:\n   564\u2192            List of matching symbols\n   565\u2192        \"\"\"\n   566\u2192        with self._get_connection() as conn:\n   567\u2192            cursor = conn.execute(\n   568\u2192                \"\"\"SELECT s.*, f.path as file_path\n   569\u2192                   FROM symbols s\n   570\u2192                   JOIN files f ON s.file_id = f.id\n   571\u2192                   JOIN fts_symbols ON s.id = fts_symbols.rowid\n   572\u2192                   WHERE fts_symbols MATCH ?\n   573\u2192                   ORDER BY rank\n   574\u2192                   LIMIT ?\"\"\",\n   575\u2192                (query, limit)\n   576\u2192            )\n   577\u2192            return [dict(row) for row in cursor.fetchall()]\n   578\u2192    \n   579\u2192    def search_code_fts(self, query: str, limit: int = 20) -> List[Dict]:\n   580\u2192        \"\"\"\n   581\u2192        Full-text search in code content.\n   582\u2192        \n   583\u2192        Args:\n   584\u2192            query: Search query\n   585\u2192            limit: Maximum number of results\n   586\u2192            \n   587\u2192        Returns:\n   588\u2192            List of matching code snippets\n   589\u2192        \"\"\"\n   590\u2192        with self._get_connection() as conn:\n   591\u2192            cursor = conn.execute(\n   592\u2192                \"\"\"SELECT fts.*, f.path as file_path\n   593\u2192                   FROM fts_code fts\n   594\u2192                   JOIN files f ON fts.file_id = f.id\n   595\u2192                   WHERE fts_code MATCH ?\n   596\u2192                   ORDER BY rank\n   597\u2192                   LIMIT ?\"\"\",\n   598\u2192                (query, limit)\n   599\u2192            )\n   600\u2192            return [dict(row) for row in cursor.fetchall()]\n   601\u2192    \n   602\u2192    # Index operations for fuzzy_indexer.py integration\n   603\u2192    def persist_fuzzy_index(self, index_data: Dict[str, List[Tuple[str, Any]]]):\n   604\u2192        \"\"\"\n   605\u2192        Persist fuzzy index data to database.\n   606\u2192        \n   607\u2192        Args:\n   608\u2192            index_data: Dictionary mapping trigrams to list of (item, metadata) tuples\n   609\u2192        \"\"\"\n   610\u2192        with self._get_connection() as conn:\n   611\u2192            # Clear existing fuzzy index (for simplicity; in production, use incremental updates)\n   612\u2192            conn.execute(\"DELETE FROM symbol_trigrams\")\n   613\u2192            \n   614\u2192            # Store all symbols and their trigrams\n   615\u2192            for trigram, items in index_data.items():\n   616\u2192                for item, metadata in items:\n   617\u2192                    # For now, assume item is a symbol name and metadata contains file info\n   618\u2192                    # This is a simplified implementation\n   619\u2192                    if isinstance(metadata, dict) and 'file_id' in metadata:\n   620\u2192                        conn.execute(\n   621\u2192                            \"INSERT INTO symbol_trigrams (symbol_id, trigram) VALUES (?, ?)\",\n   622\u2192                            (metadata.get('symbol_id', 0), trigram)\n   623\u2192                        )\n   624\u2192    \n   625\u2192    def load_fuzzy_index(self) -> Dict[str, List[Tuple[str, Any]]]:\n   626\u2192        \"\"\"\n   627\u2192        Load fuzzy index data from database.\n   628\u2192        \n   629\u2192        Returns:\n   630\u2192            Dictionary mapping trigrams to list of (item, metadata) tuples\n   631\u2192        \"\"\"\n   632\u2192        index_data = {}\n   633\u2192        \n   634\u2192        with self._get_connection() as conn:\n   635\u2192            cursor = conn.execute(\n   636\u2192                \"\"\"SELECT st.trigram, s.name, s.id, s.file_id, f.path\n   637\u2192                   FROM symbol_trigrams st\n   638\u2192                   JOIN symbols s ON st.symbol_id = s.id\n   639\u2192                   JOIN files f ON s.file_id = f.id\"\"\"\n   640\u2192            )\n   641\u2192            \n   642\u2192            for row in cursor:\n   643\u2192                trigram = row['trigram']\n   644\u2192                if trigram not in index_data:\n   645\u2192                    index_data[trigram] = []\n   646\u2192                \n   647\u2192                # Store symbol name with metadata\n   648\u2192                metadata = {\n   649\u2192                    'symbol_id': row['id'],\n   650\u2192                    'file_id': row['file_id'],\n   651\u2192                    'file_path': row['path']\n   652\u2192                }\n   653\u2192                index_data[trigram].append((row['name'], metadata))\n   654\u2192        \n   655\u2192        return index_data\n   656\u2192    \n   657\u2192    # Utility methods\n   658\u2192    def clear_cache(self):\n   659\u2192        \"\"\"Clear expired cache entries.\"\"\"\n   660\u2192        with self._get_connection() as conn:\n   661\u2192            conn.execute(\n   662\u2192                \"DELETE FROM query_cache WHERE expires_at < CURRENT_TIMESTAMP\"\n   663\u2192            )\n   664\u2192    \n   665\u2192    def get_statistics(self) -> Dict[str, int]:\n   666\u2192        \"\"\"Get database statistics.\"\"\"\n   667\u2192        stats = {}\n   668\u2192        with self._get_connection() as conn:\n   669\u2192            tables = ['repositories', 'files', 'symbols', 'symbol_references', 'imports']\n   670\u2192            for table in tables:\n   671\u2192                cursor = conn.execute(f\"SELECT COUNT(*) FROM {table}\")\n   672\u2192                stats[table] = cursor.fetchone()[0]\n   673\u2192        return stats\n   674\u2192    \n   675\u2192    # Enhanced FTS5 search methods for BM25\n   676\u2192    \n   677\u2192    def search_bm25(self, query: str, table: str = 'fts_code', \n   678\u2192                    limit: int = 20, offset: int = 0,\n   679\u2192                    columns: Optional[List[str]] = None) -> List[Dict[str, Any]]:\n   680\u2192        \"\"\"\n   681\u2192        Perform BM25 search using FTS5.\n   682\u2192        \n   683\u2192        Args:\n   684\u2192            query: FTS5 query string\n   685\u2192            table: FTS table to search\n   686\u2192            limit: Maximum results\n   687\u2192            offset: Result offset for pagination\n   688\u2192            columns: Specific columns to return\n   689\u2192            \n   690\u2192        Returns:\n   691\u2192            List of search results with BM25 scores\n   692\u2192        \"\"\"\n   693\u2192        with self._get_connection() as conn:\n   694\u2192            # Default columns if not specified\n   695\u2192            if not columns:\n   696\u2192                columns = ['*', f'bm25({table}) as score']\n   697\u2192            else:\n   698\u2192                columns = columns + [f'bm25({table}) as score']\n   699\u2192            \n   700\u2192            columns_str = ', '.join(columns)\n   701\u2192            \n   702\u2192            cursor = conn.execute(f\"\"\"\n   703\u2192                SELECT {columns_str}\n   704\u2192                FROM {table}\n   705\u2192                WHERE {table} MATCH ?\n   706\u2192                ORDER BY bm25({table})\n   707\u2192                LIMIT ? OFFSET ?\n   708\u2192            \"\"\", (query, limit, offset))\n   709\u2192            \n   710\u2192            results = []\n   711\u2192            for row in cursor:\n   712\u2192                results.append(dict(row))\n   713\u2192            \n   714\u2192            return results\n   715\u2192    \n   716\u2192    def search_bm25_with_snippets(self, query: str, table: str = 'fts_code',\n   717\u2192                                  content_column: int = 0,\n   718\u2192                                  limit: int = 20) -> List[Dict[str, Any]]:\n   719\u2192        \"\"\"\n   720\u2192        Search with BM25 and return snippets.\n   721\u2192        \n   722\u2192        Args:\n   723\u2192            query: FTS5 query string\n   724\u2192            table: FTS table to search\n   725\u2192            content_column: Column index for snippet extraction\n   726\u2192            limit: Maximum results\n   727\u2192            \n   728\u2192        Returns:\n   729\u2192            List of results with snippets\n   730\u2192        \"\"\"\n   731\u2192        with self._get_connection() as conn:\n   732\u2192            cursor = conn.execute(f\"\"\"\n   733\u2192                SELECT \n   734\u2192                    *,\n   735\u2192                    snippet({table}, {content_column}, '<mark>', '</mark>', '...', 32) as snippet,\n   736\u2192                    bm25({table}) as score\n   737\u2192                FROM {table}\n   738\u2192                WHERE {table} MATCH ?\n   739\u2192                ORDER BY bm25({table})\n   740\u2192                LIMIT ?\n   741\u2192            \"\"\", (query, limit))\n   742\u2192            \n   743\u2192            results = []\n   744\u2192            for row in cursor:\n   745\u2192                results.append(dict(row))\n   746\u2192            \n   747\u2192            return results\n   748\u2192    \n   749\u2192    def search_bm25_with_highlight(self, query: str, table: str = 'fts_code',\n   750\u2192                                   highlight_column: int = 0,\n   751\u2192                                   limit: int = 20) -> List[Dict[str, Any]]:\n   752\u2192        \"\"\"\n   753\u2192        Search with BM25 and return highlighted content.\n   754\u2192        \n   755\u2192        Args:\n   756\u2192            query: FTS5 query string\n   757\u2192            table: FTS table to search\n   758\u2192            highlight_column: Column index for highlighting\n   759\u2192            limit: Maximum results\n   760\u2192            \n   761\u2192        Returns:\n   762\u2192            List of results with highlighted content\n   763\u2192        \"\"\"\n   764\u2192        with self._get_connection() as conn:\n   765\u2192            cursor = conn.execute(f\"\"\"\n   766\u2192                SELECT \n   767\u2192                    *,\n   768\u2192                    highlight({table}, {highlight_column}, '<b>', '</b>') as highlighted,\n   769\u2192                    bm25({table}) as score\n   770\u2192                FROM {table}\n   771\u2192                WHERE {table} MATCH ?\n   772\u2192                ORDER BY bm25({table})\n   773\u2192                LIMIT ?\n   774\u2192            \"\"\", (query, limit))\n   775\u2192            \n   776\u2192            results = []\n   777\u2192            for row in cursor:\n   778\u2192                results.append(dict(row))\n   779\u2192            \n   780\u2192            return results\n   781\u2192    \n   782\u2192    def get_bm25_term_statistics(self, term: str, table: str = 'fts_code') -> Dict[str, Any]:\n   783\u2192        \"\"\"\n   784\u2192        Get term statistics for BM25 tuning.\n   785\u2192        \n   786\u2192        Args:\n   787\u2192            term: Term to analyze\n   788\u2192            table: FTS table to search\n   789\u2192            \n   790\u2192        Returns:\n   791\u2192            Dictionary with term statistics\n   792\u2192        \"\"\"\n   793\u2192        with self._get_connection() as conn:\n   794\u2192            # Get document frequency\n   795\u2192            cursor = conn.execute(f\"\"\"\n   796\u2192                SELECT COUNT(*) FROM {table}\n   797\u2192                WHERE {table} MATCH ?\n   798\u2192            \"\"\", (term,))\n   799\u2192            doc_freq = cursor.fetchone()[0]\n   800\u2192            \n   801\u2192            # Get total documents\n   802\u2192            cursor = conn.execute(f\"SELECT COUNT(*) FROM {table}\")\n   803\u2192            total_docs = cursor.fetchone()[0]\n   804\u2192            \n   805\u2192            # Get average document length (approximation)\n   806\u2192            cursor = conn.execute(f\"\"\"\n   807\u2192                SELECT AVG(LENGTH(content)) FROM {table}\n   808\u2192            \"\"\")\n   809\u2192            avg_doc_length = cursor.fetchone()[0] or 0\n   810\u2192            \n   811\u2192            return {\n   812\u2192                'term': term,\n   813\u2192                'document_frequency': doc_freq,\n   814\u2192                'total_documents': total_docs,\n   815\u2192                'idf': self._calculate_idf(doc_freq, total_docs),\n   816\u2192                'average_document_length': avg_doc_length\n   817\u2192            }\n   818\u2192    \n   819\u2192    def _calculate_idf(self, doc_freq: int, total_docs: int) -> float:\n   820\u2192        \"\"\"Calculate Inverse Document Frequency.\"\"\"\n   821\u2192        import math\n   822\u2192        if doc_freq == 0:\n   823\u2192            return 0\n   824\u2192        return math.log((total_docs - doc_freq + 0.5) / (doc_freq + 0.5))\n   825\u2192    \n   826\u2192    def optimize_fts_tables(self):\n   827\u2192        \"\"\"Optimize all FTS5 tables for better performance.\"\"\"\n   828\u2192        with self._get_connection() as conn:\n   829\u2192            # Get all FTS5 tables\n   830\u2192            cursor = conn.execute(\"\"\"\n   831\u2192                SELECT name FROM sqlite_master \n   832\u2192                WHERE type='table' AND sql LIKE '%USING fts5%'\n   833\u2192            \"\"\")\n   834\u2192            \n   835\u2192            fts_tables = [row[0] for row in cursor]\n   836\u2192            \n   837\u2192            for table in fts_tables:\n   838\u2192                try:\n   839\u2192                    conn.execute(f\"INSERT INTO {table}({table}) VALUES('optimize')\")\n   840\u2192                    logger.info(f\"Optimized FTS5 table: {table}\")\n   841\u2192                except Exception as e:\n   842\u2192                    logger.warning(f\"Could not optimize {table}: {e}\")\n   843\u2192    \n   844\u2192    def rebuild_fts_tables(self):\n   845\u2192        \"\"\"Rebuild all FTS5 tables.\"\"\"\n   846\u2192        with self._get_connection() as conn:\n   847\u2192            # Get all FTS5 tables\n   848\u2192            cursor = conn.execute(\"\"\"\n   849\u2192                SELECT name FROM sqlite_master \n   850\u2192                WHERE type='table' AND sql LIKE '%USING fts5%'\n   851\u2192            \"\"\")\n   852\u2192            \n   853\u2192            fts_tables = [row[0] for row in cursor]\n   854\u2192            \n   855\u2192            for table in fts_tables:\n   856\u2192                try:\n   857\u2192                    conn.execute(f\"INSERT INTO {table}({table}) VALUES('rebuild')\")\n   858\u2192                    logger.info(f\"Rebuilt FTS5 table: {table}\")\n   859\u2192                except Exception as e:\n   860\u2192                    logger.warning(f\"Could not rebuild {table}: {e}\")\n   861\u2192    \n   862\u2192    # New file operation methods for path management\n   863\u2192    def get_file_by_content_hash(self, content_hash: str, repository_id: int) -> Optional[Dict]:\n   864\u2192        \"\"\"Get file by content hash.\"\"\"\n   865\u2192        with self._get_connection() as conn:\n   866\u2192            cursor = conn.execute(\n   867\u2192                \"\"\"SELECT * FROM files \n   868\u2192                   WHERE content_hash = ? AND repository_id = ? AND is_deleted = FALSE\n   869\u2192                   ORDER BY indexed_at DESC LIMIT 1\"\"\",\n   870\u2192                (content_hash, repository_id)\n   871\u2192            )\n   872\u2192            row = cursor.fetchone()\n   873\u2192            return dict(row) if row else None\n   874\u2192    \n   875\u2192    def mark_file_deleted(self, relative_path: str, repository_id: int):\n   876\u2192        \"\"\"Mark a file as deleted (soft delete).\"\"\"\n   877\u2192        with self._get_connection() as conn:\n   878\u2192            conn.execute(\n   879\u2192                \"\"\"UPDATE files \n   880\u2192                   SET is_deleted = TRUE, deleted_at = CURRENT_TIMESTAMP\n   881\u2192                   WHERE relative_path = ? AND repository_id = ?\"\"\",\n   882\u2192                (relative_path, repository_id)\n   883\u2192            )\n   884\u2192            logger.info(f\"Marked file as deleted: {relative_path}\")\n   885\u2192    \n   886\u2192    def remove_file(self, relative_path: str, repository_id: int):\n   887\u2192        \"\"\"Remove file and all associated data (hard delete).\"\"\"\n   888\u2192        with self._get_connection() as conn:\n   889\u2192            # Get file ID first\n   890\u2192            cursor = conn.execute(\n   891\u2192                \"SELECT id FROM files WHERE relative_path = ? AND repository_id = ?\",\n   892\u2192                (relative_path, repository_id)\n   893\u2192            )\n   894\u2192            row = cursor.fetchone()\n   895\u2192            if not row:\n   896\u2192                logger.warning(f\"File not found for removal: {relative_path}\")\n   897\u2192                return\n   898\u2192            \n   899\u2192            file_id = row[0]\n   900\u2192            \n   901\u2192            # Delete all associated data (cascade should handle most)\n   902\u2192            conn.execute(\"DELETE FROM symbol_references WHERE file_id = ?\", (file_id,))\n   903\u2192            conn.execute(\"DELETE FROM imports WHERE file_id = ?\", (file_id,))\n   904\u2192            conn.execute(\"DELETE FROM embeddings WHERE file_id = ?\", (file_id,))\n   905\u2192            conn.execute(\"DELETE FROM fts_code WHERE file_id = ?\", (file_id,))\n   906\u2192            conn.execute(\"DELETE FROM symbols WHERE file_id = ?\", (file_id,))\n   907\u2192            conn.execute(\"DELETE FROM files WHERE id = ?\", (file_id,))\n   908\u2192            \n   909\u2192            logger.info(f\"Removed file and all associated data: {relative_path}\")\n   910\u2192    \n   911\u2192    def move_file(self, old_path: str, new_path: str, repository_id: int, content_hash: str):\n   912\u2192        \"\"\"Record a file move operation.\"\"\"\n   913\u2192        with self._get_connection() as conn:\n   914\u2192            # Update the file path\n   915\u2192            conn.execute(\n   916\u2192                \"\"\"UPDATE files \n   917\u2192                   SET relative_path = ?, path = ?\n   918\u2192                   WHERE relative_path = ? AND repository_id = ?\"\"\",\n   919\u2192                (new_path, new_path, old_path, repository_id)\n   920\u2192            )\n   921\u2192            \n   922\u2192            # Record the move\n   923\u2192            conn.execute(\n   924\u2192                \"\"\"INSERT INTO file_moves \n   925\u2192                   (repository_id, old_relative_path, new_relative_path, content_hash, move_type)\n   926\u2192                   VALUES (?, ?, ?, ?, ?)\"\"\",\n   927\u2192                (repository_id, old_path, new_path, content_hash, 'rename')\n   928\u2192            )\n   929\u2192            \n   930\u2192            logger.info(f\"Recorded file move: {old_path} -> {new_path}\")\n   931\u2192    \n   932\u2192    def cleanup_deleted_files(self, days_old: int = 30):\n   933\u2192        \"\"\"Clean up files marked as deleted for more than specified days.\"\"\"\n   934\u2192        with self._get_connection() as conn:\n   935\u2192            cursor = conn.execute(\n   936\u2192                \"\"\"SELECT COUNT(*) FROM files \n   937\u2192                   WHERE is_deleted = TRUE \n   938\u2192                   AND deleted_at < datetime('now', '-' || ? || ' days')\"\"\",\n   939\u2192                (days_old,)\n   940\u2192            )\n   941\u2192            count = cursor.fetchone()[0]\n   942\u2192            \n   943\u2192            if count > 0:\n   944\u2192                # Get file IDs for cleanup\n   945\u2192                cursor = conn.execute(\n   946\u2192                    \"\"\"SELECT id, relative_path FROM files \n   947\u2192                       WHERE is_deleted = TRUE \n   948\u2192                       AND deleted_at < datetime('now', '-' || ? || ' days')\"\"\",\n   949\u2192                    (days_old,)\n   950\u2192                )\n   951\u2192                \n   952\u2192                for row in cursor:\n   953\u2192                    file_id, path = row\n   954\u2192                    # Use remove_file for thorough cleanup\n   955\u2192                    self.remove_file(path, repository_id=None)\n   956\u2192                \n   957\u2192                logger.info(f\"Cleaned up {count} deleted files older than {days_old} days\")\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 366,
    "output_tokens_total": 366,
    "cache_read_tokens": 10985,
    "cache_creation_tokens": 12413,
    "tokens_per_result": 366,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "0be9b10a-8143-4183-be2c-4956b3822221",
    "test_date": "2025-06-26T07:31:55.627Z",
    "query_id": "transcript_0be9b10a-8143-4183-be2c-4956b3822221",
    "query_text": [
      {
        "tool_use_id": "toolu_01JLJyGaGgmaMuwcEKSaza17",
        "type": "tool_result",
        "content": [
          {
            "type": "text",
            "text": "Based on my search of the codebase, I found the **SQLiteStore** class definition in the following location:\n\n**File Path:** `/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/storage/sqlite_store.py`\n\n**Class Definition Location:** Line 21\n\nThe class is defined as:\n```python\nclass SQLiteStore:\n    \"\"\"SQLite-based storage implementation with FTS5 support.\"\"\"\n```\n\n**Key Details:**\n- The SQLiteStore class begins at line 21 in the file\n- It's a comprehensive SQLite-based persistence layer for the MCP Server\n- The class includes full-text search capabilities using SQLite's FTS5 extension\n- It provides methods for managing repositories, files, symbols, references, and search operations\n- The class has 957 lines of code with extensive functionality for code indexing and storage\n\n**Additional Files Mentioning SQLiteStore:**\nThe search also found references to SQLiteStore in these documentation/architecture files:\n- `/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/path_management_architecture.md`\n- `/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/indexer_actual.puml`\n- `/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/storage_actual.puml`\n\nThe main implementation is in the `sqlite_store.py` file at the path specified above."
          }
        ]
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 6,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 6,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13907,
    "cache_creation_tokens": 1000,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "fc4165fc-a1eb-45da-99d7-83d1b18b5743",
    "test_date": "2025-06-26T07:39:47.003Z",
    "query_id": "transcript_fc4165fc-a1eb-45da-99d7-83d1b18b5743",
    "query_text": "Find all functions that use asyncio",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13902,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "fc4165fc-a1eb-45da-99d7-83d1b18b5743",
    "test_date": "2025-06-26T07:39:50.112Z",
    "query_id": "transcript_fc4165fc-a1eb-45da-99d7-83d1b18b5743",
    "query_text": [
      {
        "tool_use_id": "toolu_01Tq8yDeNT3sdv3pFDCQ1Y23",
        "type": "tool_result",
        "content": "Found 78 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/conftest.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_developer_workflows.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_repository_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_semantic_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_plugins_direct.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/populate_bm25_quick.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/pytest.ini\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/requirements-production.txt\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/requirements.txt\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_reranking_working.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/docs/development/TESTING-GUIDE.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/codex-setup.sh\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/prometheus_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/watchdog.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/file_watcher.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/grpc_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/MCP.md"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13902,
    "cache_creation_tokens": 5381,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "fc4165fc-a1eb-45da-99d7-83d1b18b5743",
    "test_date": "2025-06-26T07:39:53.803Z",
    "query_id": "transcript_fc4165fc-a1eb-45da-99d7-83d1b18b5743",
    "query_text": [
      {
        "tool_use_id": "toolu_01KstMUeVNDUTpQNu9C9UaWk",
        "type": "tool_result",
        "content": "Found 76 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_python_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_section_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/security/auth_manager.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/security/security_middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/api_gateway_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/cache_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/dispatcher_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/indexing_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/metrics_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/security_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/shared_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/interfaces/storage_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_runner.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/benchmark_reranking_comparison.py"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 19283,
    "cache_creation_tokens": 2687,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "73b4d92d-1822-486e-a91b-1bead8ef1fed",
    "test_date": "2025-06-26T07:39:12.090Z",
    "query_id": "transcript_73b4d92d-1822-486e-a91b-1bead8ef1fed",
    "query_text": "Find the BM25Indexer class definition",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 58,
    "output_tokens_total": 58,
    "cache_read_tokens": 13905,
    "cache_creation_tokens": 0,
    "tokens_per_result": 58,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "73b4d92d-1822-486e-a91b-1bead8ef1fed",
    "test_date": "2025-06-26T07:39:15.071Z",
    "query_id": "transcript_73b4d92d-1822-486e-a91b-1bead8ef1fed",
    "query_text": [
      {
        "tool_use_id": "toolu_01MvPjVajmr66G4Tb7Zs7iVN",
        "type": "tool_result",
        "content": "Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/bm25_indexer.py"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 54,
    "output_tokens_total": 54,
    "cache_read_tokens": 14133,
    "cache_creation_tokens": 0,
    "tokens_per_result": 54,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "b3c51fa0-9c9d-4a32-8cbd-fa54e2104fdc",
    "test_date": "2025-06-26T07:32:52.428Z",
    "query_id": "transcript_b3c51fa0-9c9d-4a32-8cbd-fa54e2104fdc",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13909,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "b3c51fa0-9c9d-4a32-8cbd-fa54e2104fdc",
    "test_date": "2025-06-26T07:32:56.023Z",
    "query_id": "transcript_b3c51fa0-9c9d-4a32-8cbd-fa54e2104fdc",
    "query_text": [
      {
        "tool_use_id": "toolu_016exHaADieaLmnyDL7Yy2sa",
        "type": "tool_result",
        "content": "Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/path_management_architecture.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/path_management.puml"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 27,
    "output_tokens_total": 27,
    "cache_read_tokens": 14364,
    "cache_creation_tokens": 0,
    "tokens_per_result": 27,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "b3c51fa0-9c9d-4a32-8cbd-fa54e2104fdc",
    "test_date": "2025-06-26T07:33:00.280Z",
    "query_id": "transcript_b3c51fa0-9c9d-4a32-8cbd-fa54e2104fdc",
    "query_text": [
      {
        "tool_use_id": "toolu_01YWtUxDN4QzWBEe9gnETM8r",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"Enhanced dispatcher with dynamic plugin loading via PluginFactory.\"\"\"\n     2\u2192from pathlib import Path\n     3\u2192from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4\u2192import logging\n     5\u2192import hashlib\n     6\u2192import time\n     7\u2192import re\n     8\u2192from datetime import datetime\n     9\u2192import os\n    10\u2192\n    11\u2192from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12\u2192from ..plugins.plugin_factory import PluginFactory\n    13\u2192from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14\u2192from ..storage.sqlite_store import SQLiteStore\n    15\u2192from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16\u2192from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17\u2192# Note: We've removed ignore pattern checks to allow indexing ALL files\n    18\u2192# Filtering happens only during export via SecureIndexExporter\n    19\u2192# from ..core.ignore_patterns import get_ignore_manager\n    20\u2192\n    21\u2192logger = logging.getLogger(__name__)\n    22\u2192\n    23\u2192\n    24\u2192class EnhancedDispatcher:\n    25\u2192    \"\"\"Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.\"\"\"\n    26\u2192    \n    27\u2192    # Document query patterns - common documentation search terms\n    28\u2192    DOCUMENT_QUERY_PATTERNS = [\n    29\u2192        r'\\b(how\\s+to|howto)\\b',\n    30\u2192        r'\\b(getting\\s+started|get\\s+started)\\b',\n    31\u2192        r'\\b(installation|install|setup)\\b',\n    32\u2192        r'\\b(configuration|configure|config)\\b',\n    33\u2192        r'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b',\n    34\u2192        r'\\b(tutorial|guide|walkthrough)\\b',\n    35\u2192        r'\\b(example|sample|snippet)\\b',\n    36\u2192        r'\\b(readme|documentation|docs)\\b',\n    37\u2192        r'\\b(usage|use\\s+case|using)\\b',\n    38\u2192        r'\\b(reference|manual)\\b',\n    39\u2192        r'\\b(faq|frequently\\s+asked)\\b',\n    40\u2192        r'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b',\n    41\u2192        r'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b',\n    42\u2192        r'\\b(architecture|design|overview)\\b',\n    43\u2192        r'\\b(changelog|release\\s+notes|migration)\\b'\n    44\u2192    ]\n    45\u2192    \n    46\u2192    # Documentation file patterns\n    47\u2192    DOCUMENTATION_FILE_PATTERNS = [\n    48\u2192        r'readme(\\.\\w+)?$',\n    49\u2192        r'changelog(\\.\\w+)?$',\n    50\u2192        r'contributing(\\.\\w+)?$',\n    51\u2192        r'license(\\.\\w+)?$',\n    52\u2192        r'install(\\.\\w+)?$',\n    53\u2192        r'setup(\\.\\w+)?$',\n    54\u2192        r'guide(\\.\\w+)?$',\n    55\u2192        r'tutorial(\\.\\w+)?$',\n    56\u2192        r'\\.md$',\n    57\u2192        r'\\.rst$',\n    58\u2192        r'\\.txt$',\n    59\u2192        r'docs?/',\n    60\u2192        r'documentation/'\n    61\u2192    ]\n    62\u2192    \n    63\u2192    def __init__(self, \n    64\u2192                 plugins: Optional[List[IPlugin]] = None,\n    65\u2192                 sqlite_store: Optional[SQLiteStore] = None,\n    66\u2192                 enable_advanced_features: bool = True,\n    67\u2192                 use_plugin_factory: bool = True,\n    68\u2192                 lazy_load: bool = True,\n    69\u2192                 semantic_search_enabled: bool = True):\n    70\u2192        \"\"\"Initialize the enhanced dispatcher.\n    71\u2192        \n    72\u2192        Args:\n    73\u2192            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74\u2192            sqlite_store: SQLite store for plugin persistence\n    75\u2192            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76\u2192            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77\u2192            lazy_load: Whether to lazy-load plugins on demand\n    78\u2192            semantic_search_enabled: Whether to enable semantic search in plugins\n    79\u2192        \"\"\"\n    80\u2192        self._sqlite_store = sqlite_store\n    81\u2192        self._enable_advanced = enable_advanced_features\n    82\u2192        self._use_factory = use_plugin_factory\n    83\u2192        self._lazy_load = lazy_load\n    84\u2192        self._semantic_enabled = semantic_search_enabled\n    85\u2192        \n    86\u2192        # Plugin storage\n    87\u2192        self._plugins: List[IPlugin] = []\n    88\u2192        self._by_lang: Dict[str, IPlugin] = {}\n    89\u2192        self._loaded_languages: set[str] = set()\n    90\u2192        \n    91\u2192        # Cache for file hashes to avoid re-indexing unchanged files\n    92\u2192        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93\u2192        \n    94\u2192        # Advanced components\n    95\u2192        if self._enable_advanced:\n    96\u2192            self._file_matcher = FileTypeMatcher()\n    97\u2192            self._router = PluginRouter(self._file_matcher)\n    98\u2192            self._aggregator = ResultAggregator()\n    99\u2192        \n   100\u2192        # Performance tracking\n   101\u2192        self._operation_stats = {\n   102\u2192            'searches': 0,\n   103\u2192            'lookups': 0,\n   104\u2192            'indexings': 0,\n   105\u2192            'total_time': 0.0,\n   106\u2192            'plugins_loaded': 0\n   107\u2192        }\n   108\u2192        \n   109\u2192        # Initialize plugins\n   110\u2192        if plugins:\n   111\u2192            # Use provided plugins (backward compatibility)\n   112\u2192            self._plugins = plugins\n   113\u2192            self._by_lang = {p.lang: p for p in plugins}\n   114\u2192            for plugin in plugins:\n   115\u2192                self._loaded_languages.add(getattr(plugin, 'lang', 'unknown'))\n   116\u2192            if self._enable_advanced:\n   117\u2192                self._register_plugins_with_router()\n   118\u2192        elif use_plugin_factory and not lazy_load:\n   119\u2192            # Load all plugins immediately\n   120\u2192            self._load_all_plugins()\n   121\u2192        # If lazy_load is True, plugins will be loaded on demand\n   122\u2192        \n   123\u2192        # Compile document query patterns for performance\n   124\u2192        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125\u2192        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126\u2192        \n   127\u2192        logger.info(f\"Enhanced dispatcher initialized with {len(self._plugins)} plugins\")\n   128\u2192    \n   129\u2192    def _load_all_plugins(self):\n   130\u2192        \"\"\"Load all available plugins using PluginFactory.\"\"\"\n   131\u2192        logger.info(\"Loading all available plugins...\")\n   132\u2192        \n   133\u2192        all_plugins = PluginFactory.create_all_plugins(\n   134\u2192            sqlite_store=self._sqlite_store,\n   135\u2192            enable_semantic=self._semantic_enabled\n   136\u2192        )\n   137\u2192        \n   138\u2192        for lang, plugin in all_plugins.items():\n   139\u2192            self._plugins.append(plugin)\n   140\u2192            self._by_lang[lang] = plugin\n   141\u2192            self._loaded_languages.add(lang)\n   142\u2192            self._operation_stats['plugins_loaded'] += 1\n   143\u2192        \n   144\u2192        if self._enable_advanced:\n   145\u2192            self._register_plugins_with_router()\n   146\u2192        \n   147\u2192        logger.info(f\"Loaded {len(all_plugins)} plugins: {', '.join(sorted(all_plugins.keys()))}\")\n   148\u2192    \n   149\u2192    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150\u2192        \"\"\"Ensure a plugin for the given language is loaded.\n   151\u2192        \n   152\u2192        Args:\n   153\u2192            language: Language code (e.g., 'python', 'go')\n   154\u2192            \n   155\u2192        Returns:\n   156\u2192            Plugin instance or None if not available\n   157\u2192        \"\"\"\n   158\u2192        # Normalize language\n   159\u2192        language = language.lower().replace('-', '_')\n   160\u2192        \n   161\u2192        # Check if already loaded\n   162\u2192        if language in self._by_lang:\n   163\u2192            return self._by_lang[language]\n   164\u2192        \n   165\u2192        # If not using factory or already tried to load, return None\n   166\u2192        if not self._use_factory or language in self._loaded_languages:\n   167\u2192            return None\n   168\u2192        \n   169\u2192        # Try to load the plugin\n   170\u2192        try:\n   171\u2192            logger.info(f\"Lazy loading plugin for {language}\")\n   172\u2192            plugin = PluginFactory.create_plugin(\n   173\u2192                language,\n   174\u2192                sqlite_store=self._sqlite_store,\n   175\u2192                enable_semantic=self._semantic_enabled\n   176\u2192            )\n   177\u2192            \n   178\u2192            # Add to collections\n   179\u2192            self._plugins.append(plugin)\n   180\u2192            self._by_lang[language] = plugin\n   181\u2192            self._loaded_languages.add(language)\n   182\u2192            self._operation_stats['plugins_loaded'] += 1\n   183\u2192            \n   184\u2192            # Register with router if needed\n   185\u2192            if self._enable_advanced:\n   186\u2192                capabilities = self._detect_plugin_capabilities(plugin)\n   187\u2192                self._router.register_plugin(plugin, capabilities)\n   188\u2192            \n   189\u2192            logger.info(f\"Successfully loaded {language} plugin\")\n   190\u2192            return plugin\n   191\u2192            \n   192\u2192        except ValueError as e:\n   193\u2192            logger.warning(f\"No plugin available for {language}: {e}\")\n   194\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   195\u2192            return None\n   196\u2192        except Exception as e:\n   197\u2192            logger.error(f\"Error loading plugin for {language}: {e}\")\n   198\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   199\u2192            return None\n   200\u2192    \n   201\u2192    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202\u2192        \"\"\"Ensure a plugin is loaded for the given file.\n   203\u2192        \n   204\u2192        Args:\n   205\u2192            path: File path\n   206\u2192            \n   207\u2192        Returns:\n   208\u2192            Plugin instance or None if not available\n   209\u2192        \"\"\"\n   210\u2192        # Get language from file extension\n   211\u2192        extension = path.suffix.lower()\n   212\u2192        language = get_language_by_extension(extension)\n   213\u2192        \n   214\u2192        if language:\n   215\u2192            return self._ensure_plugin_loaded(language)\n   216\u2192        \n   217\u2192        # Fallback: try all loaded plugins\n   218\u2192        for plugin in self._plugins:\n   219\u2192            if plugin.supports(path):\n   220\u2192                return plugin\n   221\u2192        \n   222\u2192        return None\n   223\u2192    \n   224\u2192    def _register_plugins_with_router(self):\n   225\u2192        \"\"\"Register plugins with the router and assign capabilities.\"\"\"\n   226\u2192        for plugin in self._plugins:\n   227\u2192            # Determine capabilities based on plugin type/language\n   228\u2192            capabilities = self._detect_plugin_capabilities(plugin)\n   229\u2192            self._router.register_plugin(plugin, capabilities)\n   230\u2192    \n   231\u2192    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232\u2192        \"\"\"Detect capabilities for a plugin based on its language and features.\"\"\"\n   233\u2192        capabilities = []\n   234\u2192        lang = getattr(plugin, 'lang', 'unknown')\n   235\u2192        \n   236\u2192        # Base capabilities all plugins have\n   237\u2192        capabilities.append(PluginCapability(\n   238\u2192            'syntax_analysis', '1.0', f'{lang} syntax analysis', \n   239\u2192            priority=70, metadata={'language': lang}\n   240\u2192        ))\n   241\u2192        \n   242\u2192        capabilities.append(PluginCapability(\n   243\u2192            'code_search', '1.0', f'{lang} code search', \n   244\u2192            priority=80, metadata={'language': lang}\n   245\u2192        ))\n   246\u2192        \n   247\u2192        # Check for semantic search capability\n   248\u2192        if hasattr(plugin, '_enable_semantic') and plugin._enable_semantic:\n   249\u2192            capabilities.append(PluginCapability(\n   250\u2192                'semantic_search', '1.0', f'{lang} semantic search', \n   251\u2192                priority=90, metadata={'language': lang}\n   252\u2192            ))\n   253\u2192        \n   254\u2192        # Language-specific capabilities\n   255\u2192        if lang == 'python':\n   256\u2192            capabilities.extend([\n   257\u2192                PluginCapability('refactoring', '1.0', 'Python refactoring support', 75),\n   258\u2192                PluginCapability('type_analysis', '1.0', 'Python type analysis', 85)\n   259\u2192            ])\n   260\u2192        elif lang in ['javascript', 'typescript']:\n   261\u2192            capabilities.extend([\n   262\u2192                PluginCapability('linting', '1.0', 'JavaScript/TypeScript linting', 85),\n   263\u2192                PluginCapability('bundling_analysis', '1.0', 'Module bundling analysis', 70),\n   264\u2192                PluginCapability('framework_support', '1.0', 'Framework-specific support', 75)\n   265\u2192            ])\n   266\u2192        elif lang in ['c', 'cpp']:\n   267\u2192            capabilities.extend([\n   268\u2192                PluginCapability('compilation_analysis', '1.0', 'Compilation analysis', 80),\n   269\u2192                PluginCapability('memory_analysis', '1.0', 'Memory usage analysis', 70),\n   270\u2192                PluginCapability('performance_profiling', '1.0', 'Performance profiling', 75)\n   271\u2192            ])\n   272\u2192        elif lang in ['go', 'rust']:\n   273\u2192            capabilities.extend([\n   274\u2192                PluginCapability('package_analysis', '1.0', f'{lang} package analysis', 80),\n   275\u2192                PluginCapability('concurrency_analysis', '1.0', f'{lang} concurrency analysis', 75)\n   276\u2192            ])\n   277\u2192        elif lang in ['java', 'kotlin', 'scala']:\n   278\u2192            capabilities.extend([\n   279\u2192                PluginCapability('jvm_analysis', '1.0', 'JVM bytecode analysis', 75),\n   280\u2192                PluginCapability('build_tool_integration', '1.0', 'Build tool integration', 70)\n   281\u2192            ])\n   282\u2192        \n   283\u2192        return capabilities\n   284\u2192    \n   285\u2192    @property\n   286\u2192    def plugins(self):\n   287\u2192        \"\"\"Get the dictionary of loaded plugins by language.\"\"\"\n   288\u2192        return self._by_lang\n   289\u2192    \n   290\u2192    @property\n   291\u2192    def supported_languages(self) -> List[str]:\n   292\u2192        \"\"\"Get list of all supported languages (loaded and available).\"\"\"\n   293\u2192        if self._use_factory:\n   294\u2192            return PluginFactory.get_supported_languages()\n   295\u2192        else:\n   296\u2192            return list(self._by_lang.keys())\n   297\u2192    \n   298\u2192    def _match_plugin(self, path: Path) -> IPlugin:\n   299\u2192        \"\"\"Match a plugin for the given file path.\"\"\"\n   300\u2192        # Ensure plugin is loaded if using lazy loading\n   301\u2192        if self._lazy_load and self._use_factory:\n   302\u2192            plugin = self._ensure_plugin_for_file(path)\n   303\u2192            if plugin:\n   304\u2192                return plugin\n   305\u2192        \n   306\u2192        # Use advanced routing if available\n   307\u2192        if self._enable_advanced and self._router:\n   308\u2192            route_result = self._router.get_best_plugin(path)\n   309\u2192            if route_result:\n   310\u2192                return route_result.plugin\n   311\u2192        \n   312\u2192        # Fallback to basic matching\n   313\u2192        for p in self._plugins:\n   314\u2192            if p.supports(path):\n   315\u2192                return p\n   316\u2192        \n   317\u2192        raise RuntimeError(f\"No plugin found for {path}\")\n   318\u2192    \n   319\u2192    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320\u2192        \"\"\"Get all plugins that can handle a file with confidence scores.\"\"\"\n   321\u2192        # Ensure plugin is loaded if using lazy loading\n   322\u2192        if self._lazy_load and self._use_factory:\n   323\u2192            self._ensure_plugin_for_file(path)\n   324\u2192        \n   325\u2192        if self._enable_advanced and self._router:\n   326\u2192            route_results = self._router.route_file(path)\n   327\u2192            return [(result.plugin, result.confidence) for result in route_results]\n   328\u2192        else:\n   329\u2192            # Basic fallback\n   330\u2192            matching_plugins = []\n   331\u2192            for plugin in self._plugins:\n   332\u2192                if plugin.supports(path):\n   333\u2192                    matching_plugins.append((plugin, 1.0))\n   334\u2192            return matching_plugins\n   335\u2192    \n   336\u2192    def lookup(self, symbol: str) -> SymbolDef | None:\n   337\u2192        \"\"\"Look up symbol definition across all plugins.\"\"\"\n   338\u2192        start_time = time.time()\n   339\u2192        \n   340\u2192        try:\n   341\u2192            # For symbol lookup, we may need to search across all languages\n   342\u2192            # Load all plugins if using lazy loading\n   343\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344\u2192                self._load_all_plugins()\n   345\u2192            \n   346\u2192            if self._enable_advanced and self._aggregator:\n   347\u2192                # Use advanced aggregation\n   348\u2192                definitions_by_plugin = {}\n   349\u2192                for plugin in self._plugins:\n   350\u2192                    try:\n   351\u2192                        definition = plugin.getDefinition(symbol)\n   352\u2192                        definitions_by_plugin[plugin] = definition\n   353\u2192                    except Exception as e:\n   354\u2192                        logger.warning(f\"Plugin {plugin.lang} failed to get definition for {symbol}: {e}\")\n   355\u2192                        definitions_by_plugin[plugin] = None\n   356\u2192                \n   357\u2192                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358\u2192                \n   359\u2192                self._operation_stats['lookups'] += 1\n   360\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   361\u2192                \n   362\u2192                return result\n   363\u2192            else:\n   364\u2192                # Fallback to basic lookup\n   365\u2192                for p in self._plugins:\n   366\u2192                    res = p.getDefinition(symbol)\n   367\u2192                    if res:\n   368\u2192                        self._operation_stats['lookups'] += 1\n   369\u2192                        self._operation_stats['total_time'] += time.time() - start_time\n   370\u2192                        return res\n   371\u2192                return None\n   372\u2192                \n   373\u2192        except Exception as e:\n   374\u2192            logger.error(f\"Error in symbol lookup for {symbol}: {e}\", exc_info=True)\n   375\u2192            return None\n   376\u2192    \n   377\u2192    def _is_document_query(self, query: str) -> bool:\n   378\u2192        \"\"\"Check if the query is looking for documentation.\n   379\u2192        \n   380\u2192        Args:\n   381\u2192            query: Search query string\n   382\u2192            \n   383\u2192        Returns:\n   384\u2192            True if this appears to be a documentation query\n   385\u2192        \"\"\"\n   386\u2192        query_lower = query.lower()\n   387\u2192        \n   388\u2192        # Check against document query patterns\n   389\u2192        for pattern in self._compiled_doc_patterns:\n   390\u2192            if pattern.search(query_lower):\n   391\u2192                return True\n   392\u2192        \n   393\u2192        # Check for question words at the beginning\n   394\u2192        question_starters = ['how', 'what', 'where', 'when', 'why', 'can', 'is', 'does', 'should']\n   395\u2192        first_word = query_lower.split()[0] if query_lower.split() else ''\n   396\u2192        if first_word in question_starters:\n   397\u2192            return True\n   398\u2192        \n   399\u2192        return False\n   400\u2192    \n   401\u2192    def _expand_document_query(self, query: str) -> List[str]:\n   402\u2192        \"\"\"Expand a document query with related terms for better search coverage.\n   403\u2192        \n   404\u2192        Args:\n   405\u2192            query: Original search query\n   406\u2192            \n   407\u2192        Returns:\n   408\u2192            List of expanded query variations\n   409\u2192        \"\"\"\n   410\u2192        expanded_queries = [query]  # Always include original\n   411\u2192        query_lower = query.lower()\n   412\u2192        \n   413\u2192        # Common expansions for documentation queries\n   414\u2192        expansions = {\n   415\u2192            'install': ['installation', 'setup', 'getting started', 'requirements'],\n   416\u2192            'config': ['configuration', 'configure', 'settings', 'options', 'parameters'],\n   417\u2192            'api': ['api documentation', 'api reference', 'endpoint', 'method'],\n   418\u2192            'how to': ['tutorial', 'guide', 'example', 'usage'],\n   419\u2192            'example': ['sample', 'snippet', 'demo', 'code example'],\n   420\u2192            'error': ['troubleshoot', 'debug', 'issue', 'problem', 'fix'],\n   421\u2192            'getting started': ['quickstart', 'tutorial', 'introduction', 'setup'],\n   422\u2192            'guide': ['tutorial', 'documentation', 'walkthrough', 'how to'],\n   423\u2192            'usage': ['how to use', 'example', 'api', 'reference']\n   424\u2192        }\n   425\u2192        \n   426\u2192        # Apply expansions\n   427\u2192        for term, related_terms in expansions.items():\n   428\u2192            if term in query_lower:\n   429\u2192                for related in related_terms:\n   430\u2192                    # Replace the term with related term\n   431\u2192                    expanded = query_lower.replace(term, related)\n   432\u2192                    if expanded != query_lower and expanded not in expanded_queries:\n   433\u2192                        expanded_queries.append(expanded)\n   434\u2192                \n   435\u2192                # Also add queries with additional terms\n   436\u2192                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437\u2192                    expanded = f\"{query} {related}\"\n   438\u2192                    if expanded not in expanded_queries:\n   439\u2192                        expanded_queries.append(expanded)\n   440\u2192        \n   441\u2192        # Add file-specific searches for common documentation files\n   442\u2192        if self._is_document_query(query):\n   443\u2192            # Extract the main topic from the query\n   444\u2192            topic_words = []\n   445\u2192            for word in query.lower().split():\n   446\u2192                if word not in ['how', 'to', 'the', 'a', 'an', 'is', 'are', 'what', 'where', 'when']:\n   447\u2192                    topic_words.append(word)\n   448\u2192            \n   449\u2192            if topic_words:\n   450\u2192                topic = ' '.join(topic_words[:2])  # Use first two topic words\n   451\u2192                expanded_queries.extend([\n   452\u2192                    f\"README {topic}\",\n   453\u2192                    f\"{topic} documentation\",\n   454\u2192                    f\"{topic} docs\",\n   455\u2192                    f\"{topic} guide\"\n   456\u2192                ])\n   457\u2192        \n   458\u2192        # Remove duplicates while preserving order\n   459\u2192        seen = set()\n   460\u2192        unique_queries = []\n   461\u2192        for q in expanded_queries:\n   462\u2192            if q not in seen:\n   463\u2192                seen.add(q)\n   464\u2192                unique_queries.append(q)\n   465\u2192        \n   466\u2192        return unique_queries[:10]  # Limit to 10 queries max\n   467\u2192    \n   468\u2192    def _is_documentation_file(self, file_path: str) -> bool:\n   469\u2192        \"\"\"Check if a file path is likely a documentation file.\n   470\u2192        \n   471\u2192        Args:\n   472\u2192            file_path: Path to check\n   473\u2192            \n   474\u2192        Returns:\n   475\u2192            True if this appears to be a documentation file\n   476\u2192        \"\"\"\n   477\u2192        path_lower = file_path.lower()\n   478\u2192        \n   479\u2192        for pattern in self._compiled_file_patterns:\n   480\u2192            if pattern.search(path_lower):\n   481\u2192                return True\n   482\u2192        \n   483\u2192        return False\n   484\u2192    \n   485\u2192    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486\u2192        \"\"\"Adjust ranking to prioritize documentation files for document queries.\n   487\u2192        \n   488\u2192        Args:\n   489\u2192            query: Original search query\n   490\u2192            results: List of aggregated results\n   491\u2192            \n   492\u2192        Returns:\n   493\u2192            Re-ranked results with documentation prioritized\n   494\u2192        \"\"\"\n   495\u2192        if not self._is_document_query(query):\n   496\u2192            return results\n   497\u2192        \n   498\u2192        # Separate documentation and code results\n   499\u2192        doc_results = []\n   500\u2192        code_results = []\n   501\u2192        \n   502\u2192        for result in results:\n   503\u2192            if self._is_documentation_file(result.primary_result.get('file', '')):\n   504\u2192                # Boost documentation files for document queries\n   505\u2192                result.rank_score *= 1.5\n   506\u2192                result.metadata['doc_boost'] = True\n   507\u2192                doc_results.append(result)\n   508\u2192            else:\n   509\u2192                code_results.append(result)\n   510\u2192        \n   511\u2192        # Sort each group by rank score\n   512\u2192        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513\u2192        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514\u2192        \n   515\u2192        # Combine with documentation files first\n   516\u2192        return doc_results + code_results\n   517\u2192    \n   518\u2192    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519\u2192        \"\"\"Search for code and documentation across all plugins.\"\"\"\n   520\u2192        start_time = time.time()\n   521\u2192        \n   522\u2192        try:\n   523\u2192            # For search, we may need to search across all languages\n   524\u2192            # Load all plugins if using lazy loading\n   525\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526\u2192                self._load_all_plugins()\n   527\u2192            \n   528\u2192            # Detect if this is a document query\n   529\u2192            is_doc_query = self._is_document_query(query)\n   530\u2192            \n   531\u2192            # Expand query if it's a document query\n   532\u2192            queries = [query]\n   533\u2192            if is_doc_query:\n   534\u2192                queries = self._expand_document_query(query)\n   535\u2192                logger.info(f\"Expanded document query '{query}' to {len(queries)} variations\")\n   536\u2192                # Force semantic search for natural language queries\n   537\u2192                semantic = True\n   538\u2192            \n   539\u2192            if self._enable_advanced and self._aggregator:\n   540\u2192                # Use advanced aggregation\n   541\u2192                all_results_by_plugin = {}\n   542\u2192                opts = {\"semantic\": semantic, \"limit\": limit * 2 if is_doc_query else limit}\n   543\u2192                \n   544\u2192                # Search with all query variations\n   545\u2192                for search_query in queries:\n   546\u2192                    for plugin in self._plugins:\n   547\u2192                        try:\n   548\u2192                            results = list(plugin.search(search_query, opts))\n   549\u2192                            if results:\n   550\u2192                                if plugin not in all_results_by_plugin:\n   551\u2192                                    all_results_by_plugin[plugin] = []\n   552\u2192                                all_results_by_plugin[plugin].extend(results)\n   553\u2192                        except Exception as e:\n   554\u2192                            logger.warning(f\"Plugin {plugin.lang} failed to search for {search_query}: {e}\")\n   555\u2192                \n   556\u2192                # Deduplicate results per plugin\n   557\u2192                for plugin, results in all_results_by_plugin.items():\n   558\u2192                    seen = set()\n   559\u2192                    unique_results = []\n   560\u2192                    for result in results:\n   561\u2192                        key = f\"{result['file']}:{result['line']}\"\n   562\u2192                        if key not in seen:\n   563\u2192                            seen.add(key)\n   564\u2192                            unique_results.append(result)\n   565\u2192                    all_results_by_plugin[plugin] = unique_results\n   566\u2192                \n   567\u2192                # Configure aggregator for document queries\n   568\u2192                if is_doc_query and self._enable_advanced:\n   569\u2192                    # Adjust ranking criteria for documentation\n   570\u2192                    doc_criteria = RankingCriteria(\n   571\u2192                        relevance_weight=0.5,      # Increase relevance weight\n   572\u2192                        confidence_weight=0.2,     # Reduce confidence weight\n   573\u2192                        frequency_weight=0.2,      # Keep frequency weight\n   574\u2192                        recency_weight=0.1,        # Keep recency weight\n   575\u2192                        prefer_exact_matches=False,  # Natural language doesn't need exact matches\n   576\u2192                        boost_multiple_sources=True,\n   577\u2192                        boost_common_extensions=True\n   578\u2192                    )\n   579\u2192                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580\u2192                \n   581\u2192                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582\u2192                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583\u2192                )\n   584\u2192                \n   585\u2192                # Adjust ranking for document queries\n   586\u2192                if is_doc_query:\n   587\u2192                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588\u2192                \n   589\u2192                # Apply final limit\n   590\u2192                if limit and len(aggregated_results) > limit:\n   591\u2192                    aggregated_results = aggregated_results[:limit]\n   592\u2192                \n   593\u2192                logger.debug(f\"Search aggregation stats: {stats.total_results} total, \"\n   594\u2192                           f\"{stats.unique_results} unique, {stats.plugins_used} plugins used, \"\n   595\u2192                           f\"document_query={is_doc_query}\")\n   596\u2192                \n   597\u2192                self._operation_stats['searches'] += 1\n   598\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   599\u2192                \n   600\u2192                # Yield primary results from aggregated results\n   601\u2192                for aggregated in aggregated_results:\n   602\u2192                    yield aggregated.primary_result\n   603\u2192            else:\n   604\u2192                # Fallback to basic search\n   605\u2192                # Detect if this is a document query\n   606\u2192                is_doc_query = self._is_document_query(query)\n   607\u2192                \n   608\u2192                # Expand query if it's a document query\n   609\u2192                queries = [query]\n   610\u2192                if is_doc_query:\n   611\u2192                    queries = self._expand_document_query(query)\n   612\u2192                    semantic = True  # Force semantic search for natural language\n   613\u2192                \n   614\u2192                opts = {\"semantic\": semantic, \"limit\": limit}\n   615\u2192                all_results = []\n   616\u2192                \n   617\u2192                # Search with all query variations\n   618\u2192                for search_query in queries:\n   619\u2192                    for p in self._plugins:\n   620\u2192                        try:\n   621\u2192                            for result in p.search(search_query, opts):\n   622\u2192                                all_results.append(result)\n   623\u2192                        except Exception as e:\n   624\u2192                            logger.warning(f\"Plugin {p.lang} failed to search for {search_query}: {e}\")\n   625\u2192                \n   626\u2192                # Deduplicate results\n   627\u2192                seen = set()\n   628\u2192                unique_results = []\n   629\u2192                for result in all_results:\n   630\u2192                    key = f\"{result['file']}:{result['line']}\"\n   631\u2192                    if key not in seen:\n   632\u2192                        seen.add(key)\n   633\u2192                        unique_results.append(result)\n   634\u2192                \n   635\u2192                # Sort by score if available\n   636\u2192                unique_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   637\u2192                \n   638\u2192                # Prioritize documentation files for document queries\n   639\u2192                if is_doc_query:\n   640\u2192                    doc_results = []\n   641\u2192                    code_results = []\n   642\u2192                    for result in unique_results:\n   643\u2192                        if self._is_documentation_file(result.get('file', '')):\n   644\u2192                            doc_results.append(result)\n   645\u2192                        else:\n   646\u2192                            code_results.append(result)\n   647\u2192                    unique_results = doc_results + code_results\n   648\u2192                \n   649\u2192                # Apply limit\n   650\u2192                count = 0\n   651\u2192                for result in unique_results:\n   652\u2192                    if limit and count >= limit:\n   653\u2192                        break\n   654\u2192                    yield result\n   655\u2192                    count += 1\n   656\u2192                \n   657\u2192                self._operation_stats['searches'] += 1\n   658\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   659\u2192                \n   660\u2192        except Exception as e:\n   661\u2192            logger.error(f\"Error in search for {query}: {e}\", exc_info=True)\n   662\u2192    \n   663\u2192    def index_file(self, path: Path) -> None:\n   664\u2192        \"\"\"Index a single file if it has changed.\"\"\"\n   665\u2192        try:\n   666\u2192            # Ensure path is absolute to avoid relative/absolute path issues\n   667\u2192            path = path.resolve()\n   668\u2192            \n   669\u2192            # Find the appropriate plugin\n   670\u2192            plugin = self._match_plugin(path)\n   671\u2192            \n   672\u2192            # Read file content\n   673\u2192            try:\n   674\u2192                content = path.read_text(encoding='utf-8')\n   675\u2192            except UnicodeDecodeError:\n   676\u2192                # Try with different encodings\n   677\u2192                try:\n   678\u2192                    content = path.read_text(encoding='latin-1')\n   679\u2192                except Exception as e:\n   680\u2192                    logger.error(f\"Failed to read {path}: {e}\")\n   681\u2192                    return\n   682\u2192            \n   683\u2192            # Check if we need to re-index (simplified for now)\n   684\u2192            # TODO: Implement proper caching logic\n   685\u2192            \n   686\u2192            # Index the file\n   687\u2192            start_time = time.time()\n   688\u2192            logger.info(f\"Indexing {path} with {plugin.lang} plugin\")\n   689\u2192            shard = plugin.indexFile(path, content)\n   690\u2192            \n   691\u2192            # Record performance if advanced features enabled\n   692\u2192            if self._enable_advanced and self._router:\n   693\u2192                execution_time = time.time() - start_time\n   694\u2192                self._router.record_performance(plugin, execution_time)\n   695\u2192            \n   696\u2192            self._operation_stats['indexings'] += 1\n   697\u2192            self._operation_stats['total_time'] += time.time() - start_time\n   698\u2192            \n   699\u2192            logger.info(f\"Successfully indexed {path}: {len(shard.get('symbols', []))} symbols found\")\n   700\u2192            \n   701\u2192        except RuntimeError as e:\n   702\u2192            # No plugin found for this file type\n   703\u2192            logger.debug(f\"No plugin for {path}: {e}\")\n   704\u2192        except Exception as e:\n   705\u2192            logger.error(f\"Error indexing {path}: {e}\", exc_info=True)\n   706\u2192    \n   707\u2192    def get_statistics(self) -> dict:\n   708\u2192        \"\"\"Get comprehensive statistics across all plugins and components.\"\"\"\n   709\u2192        stats = {\n   710\u2192            \"total_plugins\": len(self._plugins),\n   711\u2192            \"loaded_languages\": sorted(list(self._loaded_languages)),\n   712\u2192            \"supported_languages\": len(self.supported_languages),\n   713\u2192            \"operations\": self._operation_stats.copy()\n   714\u2192        }\n   715\u2192        \n   716\u2192        # Add language breakdown\n   717\u2192        stats[\"by_language\"] = {}\n   718\u2192        for lang, plugin in self._by_lang.items():\n   719\u2192            plugin_info = {\n   720\u2192                \"loaded\": True,\n   721\u2192                \"class\": plugin.__class__.__name__\n   722\u2192            }\n   723\u2192            if hasattr(plugin, 'get_indexed_count'):\n   724\u2192                plugin_info[\"indexed_files\"] = plugin.get_indexed_count()\n   725\u2192            stats[\"by_language\"][lang] = plugin_info\n   726\u2192        \n   727\u2192        return stats\n   728\u2192    \n   729\u2192    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730\u2192        \"\"\"\n   731\u2192        Index all files in a directory, respecting ignore patterns.\n   732\u2192        \n   733\u2192        Args:\n   734\u2192            directory: Directory to index\n   735\u2192            recursive: Whether to index subdirectories\n   736\u2192            \n   737\u2192        Returns:\n   738\u2192            Statistics about indexed files\n   739\u2192        \"\"\"\n   740\u2192        logger.info(f\"Indexing directory: {directory} (recursive={recursive})\")\n   741\u2192        \n   742\u2192        # Note: We don't use ignore patterns during indexing\n   743\u2192        # ALL files are indexed for local search capability\n   744\u2192        # Filtering happens only during export/sharing\n   745\u2192        \n   746\u2192        # Get all supported extensions\n   747\u2192        supported_extensions = get_all_extensions()\n   748\u2192        \n   749\u2192        stats = {\n   750\u2192            \"total_files\": 0,\n   751\u2192            \"indexed_files\": 0,\n   752\u2192            \"ignored_files\": 0,\n   753\u2192            \"failed_files\": 0,\n   754\u2192            \"by_language\": {}\n   755\u2192        }\n   756\u2192        \n   757\u2192        # Walk directory\n   758\u2192        if recursive:\n   759\u2192            file_iterator = directory.rglob(\"*\")\n   760\u2192        else:\n   761\u2192            file_iterator = directory.glob(\"*\")\n   762\u2192            \n   763\u2192        for path in file_iterator:\n   764\u2192            if not path.is_file():\n   765\u2192                continue\n   766\u2192                \n   767\u2192            stats[\"total_files\"] += 1\n   768\u2192            \n   769\u2192            # NOTE: We index ALL files locally, including gitignored ones\n   770\u2192            # Filtering happens only during export/sharing\n   771\u2192            # This allows local search of .env, secrets, etc.\n   772\u2192                \n   773\u2192            # Try to find a plugin that supports this file\n   774\u2192            # This allows us to index ALL files, including .env, .key, etc.\n   775\u2192            try:\n   776\u2192                # First try to match by extension\n   777\u2192                if path.suffix in supported_extensions:\n   778\u2192                    self.index_file(path)\n   779\u2192                    stats[\"indexed_files\"] += 1\n   780\u2192                # For files without recognized extensions, try each plugin's supports() method\n   781\u2192                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782\u2192                else:\n   783\u2192                    matched = False\n   784\u2192                    for plugin in self._plugins:\n   785\u2192                        if plugin.supports(path):\n   786\u2192                            self.index_file(path)\n   787\u2192                            stats[\"indexed_files\"] += 1\n   788\u2192                            matched = True\n   789\u2192                            break\n   790\u2192                    \n   791\u2192                    # If no plugin matched but we want to index everything,\n   792\u2192                    # we could add a fallback here to index as plaintext\n   793\u2192                    # For now, we'll skip unmatched files\n   794\u2192                    if not matched:\n   795\u2192                        logger.debug(f\"No plugin found for {path}\")\n   796\u2192                \n   797\u2192                # Track by language\n   798\u2192                language = get_language_by_extension(path.suffix)\n   799\u2192                if language:\n   800\u2192                    stats[\"by_language\"][language] = stats[\"by_language\"].get(language, 0) + 1\n   801\u2192                    \n   802\u2192            except Exception as e:\n   803\u2192                logger.error(f\"Failed to index {path}: {e}\")\n   804\u2192                stats[\"failed_files\"] += 1\n   805\u2192                \n   806\u2192        logger.info(f\"Directory indexing complete: {stats['indexed_files']} indexed, \"\n   807\u2192                   f\"{stats['ignored_files']} ignored, {stats['failed_files']} failed\")\n   808\u2192        \n   809\u2192        return stats\n   810\u2192    \n   811\u2192    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812\u2192        \"\"\"Search specifically across documentation files.\n   813\u2192        \n   814\u2192        Args:\n   815\u2192            topic: Topic to search for (e.g., \"installation\", \"configuration\")\n   816\u2192            doc_types: Optional list of document types to search (e.g., [\"readme\", \"guide\", \"api\"])\n   817\u2192            limit: Maximum number of results\n   818\u2192            \n   819\u2192        Returns:\n   820\u2192            Search results from documentation files\n   821\u2192        \"\"\"\n   822\u2192        # Default document types if not specified\n   823\u2192        if doc_types is None:\n   824\u2192            doc_types = [\"readme\", \"documentation\", \"guide\", \"tutorial\", \"api\", \"changelog\", \"contributing\"]\n   825\u2192        \n   826\u2192        # Build search queries for different document types\n   827\u2192        queries = []\n   828\u2192        for doc_type in doc_types:\n   829\u2192            queries.extend([\n   830\u2192                f\"{doc_type} {topic}\",\n   831\u2192                f\"{topic} {doc_type}\",\n   832\u2192                f\"{topic} in {doc_type}\"\n   833\u2192            ])\n   834\u2192        \n   835\u2192        # Also search for the topic in common doc filenames\n   836\u2192        queries.extend([\n   837\u2192            f\"README {topic}\",\n   838\u2192            f\"CONTRIBUTING {topic}\",\n   839\u2192            f\"docs {topic}\",\n   840\u2192            f\"documentation {topic}\"\n   841\u2192        ])\n   842\u2192        \n   843\u2192        # Deduplicate queries\n   844\u2192        queries = list(dict.fromkeys(queries))\n   845\u2192        \n   846\u2192        logger.info(f\"Cross-document search for '{topic}' with {len(queries)} query variations\")\n   847\u2192        \n   848\u2192        # Use the enhanced search with document-specific handling\n   849\u2192        all_results = []\n   850\u2192        seen = set()\n   851\u2192        \n   852\u2192        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853\u2192            for result in self.search(query, semantic=True, limit=limit):\n   854\u2192                # Only include documentation files\n   855\u2192                if self._is_documentation_file(result.get('file', '')):\n   856\u2192                    key = f\"{result['file']}:{result['line']}\"\n   857\u2192                    if key not in seen:\n   858\u2192                        seen.add(key)\n   859\u2192                        all_results.append(result)\n   860\u2192        \n   861\u2192        # Sort by relevance (score) and return top results\n   862\u2192        all_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   863\u2192        \n   864\u2192        count = 0\n   865\u2192        for result in all_results:\n   866\u2192            if count >= limit:\n   867\u2192                break\n   868\u2192            yield result\n   869\u2192            count += 1\n   870\u2192    \n   871\u2192    def health_check(self) -> Dict[str, Any]:\n   872\u2192        \"\"\"Perform a health check on all components.\"\"\"\n   873\u2192        health = {\n   874\u2192            'status': 'healthy',\n   875\u2192            'components': {\n   876\u2192                'dispatcher': {\n   877\u2192                    'status': 'healthy',\n   878\u2192                    'plugins_loaded': len(self._plugins),\n   879\u2192                    'languages_supported': len(self.supported_languages),\n   880\u2192                    'factory_enabled': self._use_factory,\n   881\u2192                    'lazy_loading': self._lazy_load\n   882\u2192                }\n   883\u2192            },\n   884\u2192            'plugins': {},\n   885\u2192            'errors': []\n   886\u2192        }\n   887\u2192        \n   888\u2192        # Check plugin health\n   889\u2192        for lang, plugin in self._by_lang.items():\n   890\u2192            try:\n   891\u2192                plugin_health = {\n   892\u2192                    'status': 'healthy',\n   893\u2192                    'class': plugin.__class__.__name__,\n   894\u2192                    'semantic_enabled': getattr(plugin, '_enable_semantic', False)\n   895\u2192                }\n   896\u2192                if hasattr(plugin, 'get_indexed_count'):\n   897\u2192                    plugin_health['indexed_files'] = plugin.get_indexed_count()\n   898\u2192            except Exception as e:\n   899\u2192                plugin_health = {\n   900\u2192                    'status': 'error',\n   901\u2192                    'error': str(e)\n   902\u2192                }\n   903\u2192                health['errors'].append(f\"Plugin {lang}: {str(e)}\")\n   904\u2192            \n   905\u2192            health['plugins'][lang] = plugin_health\n   906\u2192        \n   907\u2192        # Determine overall health\n   908\u2192        if len(health['errors']) > 0:\n   909\u2192            health['status'] = 'degraded' if len(health['errors']) < 3 else 'unhealthy'\n   910\u2192        \n   911\u2192        return health\n   912\u2192    \n   913\u2192    def remove_file(self, path: Union[Path, str]) -> None:\n   914\u2192        \"\"\"Remove a file from all indexes.\n   915\u2192        \n   916\u2192        Args:\n   917\u2192            path: File path to remove\n   918\u2192        \"\"\"\n   919\u2192        path = Path(path).resolve()\n   920\u2192        logger.info(f\"Removing file from index: {path}\")\n   921\u2192        \n   922\u2192        try:\n   923\u2192            # Remove from SQLite if available\n   924\u2192            if self._sqlite_store:\n   925\u2192                from ..core.path_resolver import PathResolver\n   926\u2192                path_resolver = PathResolver()\n   927\u2192                try:\n   928\u2192                    relative_path = path_resolver.normalize_path(path)\n   929\u2192                    # Get repository ID - for now assume 1\n   930\u2192                    # TODO: Properly detect repository\n   931\u2192                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932\u2192                except Exception as e:\n   933\u2192                    logger.error(f\"Error removing from SQLite: {e}\")\n   934\u2192            \n   935\u2192            # Remove from semantic index if available\n   936\u2192            try:\n   937\u2192                plugin = self._match_plugin(path)\n   938\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   939\u2192                    plugin._indexer.remove_file(path)\n   940\u2192                    logger.info(f\"Removed from semantic index: {path}\")\n   941\u2192            except Exception as e:\n   942\u2192                logger.warning(f\"Error removing from semantic index: {e}\")\n   943\u2192            \n   944\u2192            # Update statistics\n   945\u2192            self._operation_stats['deletions'] = self._operation_stats.get('deletions', 0) + 1\n   946\u2192            \n   947\u2192        except Exception as e:\n   948\u2192            logger.error(f\"Error removing file {path}: {e}\", exc_info=True)\n   949\u2192    \n   950\u2192    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951\u2192        \"\"\"Move a file in all indexes.\n   952\u2192        \n   953\u2192        Args:\n   954\u2192            old_path: Original file path\n   955\u2192            new_path: New file path\n   956\u2192            content_hash: Optional content hash to verify unchanged content\n   957\u2192        \"\"\"\n   958\u2192        old_path = Path(old_path).resolve()\n   959\u2192        new_path = Path(new_path).resolve()\n   960\u2192        logger.info(f\"Moving file in index: {old_path} -> {new_path}\")\n   961\u2192        \n   962\u2192        try:\n   963\u2192            # Move in SQLite if available\n   964\u2192            if self._sqlite_store:\n   965\u2192                from ..core.path_resolver import PathResolver\n   966\u2192                path_resolver = PathResolver()\n   967\u2192                try:\n   968\u2192                    old_relative = path_resolver.normalize_path(old_path)\n   969\u2192                    new_relative = path_resolver.normalize_path(new_path)\n   970\u2192                    # Get repository ID - for now assume 1\n   971\u2192                    # TODO: Properly detect repository\n   972\u2192                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973\u2192                except Exception as e:\n   974\u2192                    logger.error(f\"Error moving in SQLite: {e}\")\n   975\u2192            \n   976\u2192            # Move in semantic index if available\n   977\u2192            try:\n   978\u2192                plugin = self._match_plugin(new_path)\n   979\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   980\u2192                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981\u2192                    logger.info(f\"Moved in semantic index: {old_path} -> {new_path}\")\n   982\u2192            except Exception as e:\n   983\u2192                logger.warning(f\"Error moving in semantic index: {e}\")\n   984\u2192            \n   985\u2192            # Update statistics\n   986\u2192            self._operation_stats['moves'] = self._operation_stats.get('moves', 0) + 1\n   987\u2192            \n   988\u2192        except Exception as e:\n   989\u2192            logger.error(f\"Error moving file {old_path} -> {new_path}: {e}\", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 27688,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "09502d6f-29cc-4822-b126-71a41e248439",
    "test_date": "2025-06-26T07:39:25.627Z",
    "query_id": "transcript_09502d6f-29cc-4822-b126-71a41e248439",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13909,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "09502d6f-29cc-4822-b126-71a41e248439",
    "test_date": "2025-06-26T07:39:28.899Z",
    "query_id": "transcript_09502d6f-29cc-4822-b126-71a41e248439",
    "query_text": [
      {
        "tool_use_id": "toolu_01Vo8bZb6youMntNGm5unGhc",
        "type": "tool_result",
        "content": "Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/path_management_architecture.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/path_management.puml"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 29,
    "output_tokens_total": 29,
    "cache_read_tokens": 13909,
    "cache_creation_tokens": 457,
    "tokens_per_result": 29,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "09502d6f-29cc-4822-b126-71a41e248439",
    "test_date": "2025-06-26T07:39:32.568Z",
    "query_id": "transcript_09502d6f-29cc-4822-b126-71a41e248439",
    "query_text": [
      {
        "tool_use_id": "toolu_0119njWJTbS4dNHVKw9JiRgr",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"Enhanced dispatcher with dynamic plugin loading via PluginFactory.\"\"\"\n     2\u2192from pathlib import Path\n     3\u2192from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4\u2192import logging\n     5\u2192import hashlib\n     6\u2192import time\n     7\u2192import re\n     8\u2192from datetime import datetime\n     9\u2192import os\n    10\u2192\n    11\u2192from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12\u2192from ..plugins.plugin_factory import PluginFactory\n    13\u2192from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14\u2192from ..storage.sqlite_store import SQLiteStore\n    15\u2192from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16\u2192from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17\u2192# Note: We've removed ignore pattern checks to allow indexing ALL files\n    18\u2192# Filtering happens only during export via SecureIndexExporter\n    19\u2192# from ..core.ignore_patterns import get_ignore_manager\n    20\u2192\n    21\u2192logger = logging.getLogger(__name__)\n    22\u2192\n    23\u2192\n    24\u2192class EnhancedDispatcher:\n    25\u2192    \"\"\"Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.\"\"\"\n    26\u2192    \n    27\u2192    # Document query patterns - common documentation search terms\n    28\u2192    DOCUMENT_QUERY_PATTERNS = [\n    29\u2192        r'\\b(how\\s+to|howto)\\b',\n    30\u2192        r'\\b(getting\\s+started|get\\s+started)\\b',\n    31\u2192        r'\\b(installation|install|setup)\\b',\n    32\u2192        r'\\b(configuration|configure|config)\\b',\n    33\u2192        r'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b',\n    34\u2192        r'\\b(tutorial|guide|walkthrough)\\b',\n    35\u2192        r'\\b(example|sample|snippet)\\b',\n    36\u2192        r'\\b(readme|documentation|docs)\\b',\n    37\u2192        r'\\b(usage|use\\s+case|using)\\b',\n    38\u2192        r'\\b(reference|manual)\\b',\n    39\u2192        r'\\b(faq|frequently\\s+asked)\\b',\n    40\u2192        r'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b',\n    41\u2192        r'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b',\n    42\u2192        r'\\b(architecture|design|overview)\\b',\n    43\u2192        r'\\b(changelog|release\\s+notes|migration)\\b'\n    44\u2192    ]\n    45\u2192    \n    46\u2192    # Documentation file patterns\n    47\u2192    DOCUMENTATION_FILE_PATTERNS = [\n    48\u2192        r'readme(\\.\\w+)?$',\n    49\u2192        r'changelog(\\.\\w+)?$',\n    50\u2192        r'contributing(\\.\\w+)?$',\n    51\u2192        r'license(\\.\\w+)?$',\n    52\u2192        r'install(\\.\\w+)?$',\n    53\u2192        r'setup(\\.\\w+)?$',\n    54\u2192        r'guide(\\.\\w+)?$',\n    55\u2192        r'tutorial(\\.\\w+)?$',\n    56\u2192        r'\\.md$',\n    57\u2192        r'\\.rst$',\n    58\u2192        r'\\.txt$',\n    59\u2192        r'docs?/',\n    60\u2192        r'documentation/'\n    61\u2192    ]\n    62\u2192    \n    63\u2192    def __init__(self, \n    64\u2192                 plugins: Optional[List[IPlugin]] = None,\n    65\u2192                 sqlite_store: Optional[SQLiteStore] = None,\n    66\u2192                 enable_advanced_features: bool = True,\n    67\u2192                 use_plugin_factory: bool = True,\n    68\u2192                 lazy_load: bool = True,\n    69\u2192                 semantic_search_enabled: bool = True):\n    70\u2192        \"\"\"Initialize the enhanced dispatcher.\n    71\u2192        \n    72\u2192        Args:\n    73\u2192            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74\u2192            sqlite_store: SQLite store for plugin persistence\n    75\u2192            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76\u2192            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77\u2192            lazy_load: Whether to lazy-load plugins on demand\n    78\u2192            semantic_search_enabled: Whether to enable semantic search in plugins\n    79\u2192        \"\"\"\n    80\u2192        self._sqlite_store = sqlite_store\n    81\u2192        self._enable_advanced = enable_advanced_features\n    82\u2192        self._use_factory = use_plugin_factory\n    83\u2192        self._lazy_load = lazy_load\n    84\u2192        self._semantic_enabled = semantic_search_enabled\n    85\u2192        \n    86\u2192        # Plugin storage\n    87\u2192        self._plugins: List[IPlugin] = []\n    88\u2192        self._by_lang: Dict[str, IPlugin] = {}\n    89\u2192        self._loaded_languages: set[str] = set()\n    90\u2192        \n    91\u2192        # Cache for file hashes to avoid re-indexing unchanged files\n    92\u2192        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93\u2192        \n    94\u2192        # Advanced components\n    95\u2192        if self._enable_advanced:\n    96\u2192            self._file_matcher = FileTypeMatcher()\n    97\u2192            self._router = PluginRouter(self._file_matcher)\n    98\u2192            self._aggregator = ResultAggregator()\n    99\u2192        \n   100\u2192        # Performance tracking\n   101\u2192        self._operation_stats = {\n   102\u2192            'searches': 0,\n   103\u2192            'lookups': 0,\n   104\u2192            'indexings': 0,\n   105\u2192            'total_time': 0.0,\n   106\u2192            'plugins_loaded': 0\n   107\u2192        }\n   108\u2192        \n   109\u2192        # Initialize plugins\n   110\u2192        if plugins:\n   111\u2192            # Use provided plugins (backward compatibility)\n   112\u2192            self._plugins = plugins\n   113\u2192            self._by_lang = {p.lang: p for p in plugins}\n   114\u2192            for plugin in plugins:\n   115\u2192                self._loaded_languages.add(getattr(plugin, 'lang', 'unknown'))\n   116\u2192            if self._enable_advanced:\n   117\u2192                self._register_plugins_with_router()\n   118\u2192        elif use_plugin_factory and not lazy_load:\n   119\u2192            # Load all plugins immediately\n   120\u2192            self._load_all_plugins()\n   121\u2192        # If lazy_load is True, plugins will be loaded on demand\n   122\u2192        \n   123\u2192        # Compile document query patterns for performance\n   124\u2192        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125\u2192        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126\u2192        \n   127\u2192        logger.info(f\"Enhanced dispatcher initialized with {len(self._plugins)} plugins\")\n   128\u2192    \n   129\u2192    def _load_all_plugins(self):\n   130\u2192        \"\"\"Load all available plugins using PluginFactory.\"\"\"\n   131\u2192        logger.info(\"Loading all available plugins...\")\n   132\u2192        \n   133\u2192        all_plugins = PluginFactory.create_all_plugins(\n   134\u2192            sqlite_store=self._sqlite_store,\n   135\u2192            enable_semantic=self._semantic_enabled\n   136\u2192        )\n   137\u2192        \n   138\u2192        for lang, plugin in all_plugins.items():\n   139\u2192            self._plugins.append(plugin)\n   140\u2192            self._by_lang[lang] = plugin\n   141\u2192            self._loaded_languages.add(lang)\n   142\u2192            self._operation_stats['plugins_loaded'] += 1\n   143\u2192        \n   144\u2192        if self._enable_advanced:\n   145\u2192            self._register_plugins_with_router()\n   146\u2192        \n   147\u2192        logger.info(f\"Loaded {len(all_plugins)} plugins: {', '.join(sorted(all_plugins.keys()))}\")\n   148\u2192    \n   149\u2192    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150\u2192        \"\"\"Ensure a plugin for the given language is loaded.\n   151\u2192        \n   152\u2192        Args:\n   153\u2192            language: Language code (e.g., 'python', 'go')\n   154\u2192            \n   155\u2192        Returns:\n   156\u2192            Plugin instance or None if not available\n   157\u2192        \"\"\"\n   158\u2192        # Normalize language\n   159\u2192        language = language.lower().replace('-', '_')\n   160\u2192        \n   161\u2192        # Check if already loaded\n   162\u2192        if language in self._by_lang:\n   163\u2192            return self._by_lang[language]\n   164\u2192        \n   165\u2192        # If not using factory or already tried to load, return None\n   166\u2192        if not self._use_factory or language in self._loaded_languages:\n   167\u2192            return None\n   168\u2192        \n   169\u2192        # Try to load the plugin\n   170\u2192        try:\n   171\u2192            logger.info(f\"Lazy loading plugin for {language}\")\n   172\u2192            plugin = PluginFactory.create_plugin(\n   173\u2192                language,\n   174\u2192                sqlite_store=self._sqlite_store,\n   175\u2192                enable_semantic=self._semantic_enabled\n   176\u2192            )\n   177\u2192            \n   178\u2192            # Add to collections\n   179\u2192            self._plugins.append(plugin)\n   180\u2192            self._by_lang[language] = plugin\n   181\u2192            self._loaded_languages.add(language)\n   182\u2192            self._operation_stats['plugins_loaded'] += 1\n   183\u2192            \n   184\u2192            # Register with router if needed\n   185\u2192            if self._enable_advanced:\n   186\u2192                capabilities = self._detect_plugin_capabilities(plugin)\n   187\u2192                self._router.register_plugin(plugin, capabilities)\n   188\u2192            \n   189\u2192            logger.info(f\"Successfully loaded {language} plugin\")\n   190\u2192            return plugin\n   191\u2192            \n   192\u2192        except ValueError as e:\n   193\u2192            logger.warning(f\"No plugin available for {language}: {e}\")\n   194\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   195\u2192            return None\n   196\u2192        except Exception as e:\n   197\u2192            logger.error(f\"Error loading plugin for {language}: {e}\")\n   198\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   199\u2192            return None\n   200\u2192    \n   201\u2192    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202\u2192        \"\"\"Ensure a plugin is loaded for the given file.\n   203\u2192        \n   204\u2192        Args:\n   205\u2192            path: File path\n   206\u2192            \n   207\u2192        Returns:\n   208\u2192            Plugin instance or None if not available\n   209\u2192        \"\"\"\n   210\u2192        # Get language from file extension\n   211\u2192        extension = path.suffix.lower()\n   212\u2192        language = get_language_by_extension(extension)\n   213\u2192        \n   214\u2192        if language:\n   215\u2192            return self._ensure_plugin_loaded(language)\n   216\u2192        \n   217\u2192        # Fallback: try all loaded plugins\n   218\u2192        for plugin in self._plugins:\n   219\u2192            if plugin.supports(path):\n   220\u2192                return plugin\n   221\u2192        \n   222\u2192        return None\n   223\u2192    \n   224\u2192    def _register_plugins_with_router(self):\n   225\u2192        \"\"\"Register plugins with the router and assign capabilities.\"\"\"\n   226\u2192        for plugin in self._plugins:\n   227\u2192            # Determine capabilities based on plugin type/language\n   228\u2192            capabilities = self._detect_plugin_capabilities(plugin)\n   229\u2192            self._router.register_plugin(plugin, capabilities)\n   230\u2192    \n   231\u2192    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232\u2192        \"\"\"Detect capabilities for a plugin based on its language and features.\"\"\"\n   233\u2192        capabilities = []\n   234\u2192        lang = getattr(plugin, 'lang', 'unknown')\n   235\u2192        \n   236\u2192        # Base capabilities all plugins have\n   237\u2192        capabilities.append(PluginCapability(\n   238\u2192            'syntax_analysis', '1.0', f'{lang} syntax analysis', \n   239\u2192            priority=70, metadata={'language': lang}\n   240\u2192        ))\n   241\u2192        \n   242\u2192        capabilities.append(PluginCapability(\n   243\u2192            'code_search', '1.0', f'{lang} code search', \n   244\u2192            priority=80, metadata={'language': lang}\n   245\u2192        ))\n   246\u2192        \n   247\u2192        # Check for semantic search capability\n   248\u2192        if hasattr(plugin, '_enable_semantic') and plugin._enable_semantic:\n   249\u2192            capabilities.append(PluginCapability(\n   250\u2192                'semantic_search', '1.0', f'{lang} semantic search', \n   251\u2192                priority=90, metadata={'language': lang}\n   252\u2192            ))\n   253\u2192        \n   254\u2192        # Language-specific capabilities\n   255\u2192        if lang == 'python':\n   256\u2192            capabilities.extend([\n   257\u2192                PluginCapability('refactoring', '1.0', 'Python refactoring support', 75),\n   258\u2192                PluginCapability('type_analysis', '1.0', 'Python type analysis', 85)\n   259\u2192            ])\n   260\u2192        elif lang in ['javascript', 'typescript']:\n   261\u2192            capabilities.extend([\n   262\u2192                PluginCapability('linting', '1.0', 'JavaScript/TypeScript linting', 85),\n   263\u2192                PluginCapability('bundling_analysis', '1.0', 'Module bundling analysis', 70),\n   264\u2192                PluginCapability('framework_support', '1.0', 'Framework-specific support', 75)\n   265\u2192            ])\n   266\u2192        elif lang in ['c', 'cpp']:\n   267\u2192            capabilities.extend([\n   268\u2192                PluginCapability('compilation_analysis', '1.0', 'Compilation analysis', 80),\n   269\u2192                PluginCapability('memory_analysis', '1.0', 'Memory usage analysis', 70),\n   270\u2192                PluginCapability('performance_profiling', '1.0', 'Performance profiling', 75)\n   271\u2192            ])\n   272\u2192        elif lang in ['go', 'rust']:\n   273\u2192            capabilities.extend([\n   274\u2192                PluginCapability('package_analysis', '1.0', f'{lang} package analysis', 80),\n   275\u2192                PluginCapability('concurrency_analysis', '1.0', f'{lang} concurrency analysis', 75)\n   276\u2192            ])\n   277\u2192        elif lang in ['java', 'kotlin', 'scala']:\n   278\u2192            capabilities.extend([\n   279\u2192                PluginCapability('jvm_analysis', '1.0', 'JVM bytecode analysis', 75),\n   280\u2192                PluginCapability('build_tool_integration', '1.0', 'Build tool integration', 70)\n   281\u2192            ])\n   282\u2192        \n   283\u2192        return capabilities\n   284\u2192    \n   285\u2192    @property\n   286\u2192    def plugins(self):\n   287\u2192        \"\"\"Get the dictionary of loaded plugins by language.\"\"\"\n   288\u2192        return self._by_lang\n   289\u2192    \n   290\u2192    @property\n   291\u2192    def supported_languages(self) -> List[str]:\n   292\u2192        \"\"\"Get list of all supported languages (loaded and available).\"\"\"\n   293\u2192        if self._use_factory:\n   294\u2192            return PluginFactory.get_supported_languages()\n   295\u2192        else:\n   296\u2192            return list(self._by_lang.keys())\n   297\u2192    \n   298\u2192    def _match_plugin(self, path: Path) -> IPlugin:\n   299\u2192        \"\"\"Match a plugin for the given file path.\"\"\"\n   300\u2192        # Ensure plugin is loaded if using lazy loading\n   301\u2192        if self._lazy_load and self._use_factory:\n   302\u2192            plugin = self._ensure_plugin_for_file(path)\n   303\u2192            if plugin:\n   304\u2192                return plugin\n   305\u2192        \n   306\u2192        # Use advanced routing if available\n   307\u2192        if self._enable_advanced and self._router:\n   308\u2192            route_result = self._router.get_best_plugin(path)\n   309\u2192            if route_result:\n   310\u2192                return route_result.plugin\n   311\u2192        \n   312\u2192        # Fallback to basic matching\n   313\u2192        for p in self._plugins:\n   314\u2192            if p.supports(path):\n   315\u2192                return p\n   316\u2192        \n   317\u2192        raise RuntimeError(f\"No plugin found for {path}\")\n   318\u2192    \n   319\u2192    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320\u2192        \"\"\"Get all plugins that can handle a file with confidence scores.\"\"\"\n   321\u2192        # Ensure plugin is loaded if using lazy loading\n   322\u2192        if self._lazy_load and self._use_factory:\n   323\u2192            self._ensure_plugin_for_file(path)\n   324\u2192        \n   325\u2192        if self._enable_advanced and self._router:\n   326\u2192            route_results = self._router.route_file(path)\n   327\u2192            return [(result.plugin, result.confidence) for result in route_results]\n   328\u2192        else:\n   329\u2192            # Basic fallback\n   330\u2192            matching_plugins = []\n   331\u2192            for plugin in self._plugins:\n   332\u2192                if plugin.supports(path):\n   333\u2192                    matching_plugins.append((plugin, 1.0))\n   334\u2192            return matching_plugins\n   335\u2192    \n   336\u2192    def lookup(self, symbol: str) -> SymbolDef | None:\n   337\u2192        \"\"\"Look up symbol definition across all plugins.\"\"\"\n   338\u2192        start_time = time.time()\n   339\u2192        \n   340\u2192        try:\n   341\u2192            # For symbol lookup, we may need to search across all languages\n   342\u2192            # Load all plugins if using lazy loading\n   343\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344\u2192                self._load_all_plugins()\n   345\u2192            \n   346\u2192            if self._enable_advanced and self._aggregator:\n   347\u2192                # Use advanced aggregation\n   348\u2192                definitions_by_plugin = {}\n   349\u2192                for plugin in self._plugins:\n   350\u2192                    try:\n   351\u2192                        definition = plugin.getDefinition(symbol)\n   352\u2192                        definitions_by_plugin[plugin] = definition\n   353\u2192                    except Exception as e:\n   354\u2192                        logger.warning(f\"Plugin {plugin.lang} failed to get definition for {symbol}: {e}\")\n   355\u2192                        definitions_by_plugin[plugin] = None\n   356\u2192                \n   357\u2192                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358\u2192                \n   359\u2192                self._operation_stats['lookups'] += 1\n   360\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   361\u2192                \n   362\u2192                return result\n   363\u2192            else:\n   364\u2192                # Fallback to basic lookup\n   365\u2192                for p in self._plugins:\n   366\u2192                    res = p.getDefinition(symbol)\n   367\u2192                    if res:\n   368\u2192                        self._operation_stats['lookups'] += 1\n   369\u2192                        self._operation_stats['total_time'] += time.time() - start_time\n   370\u2192                        return res\n   371\u2192                return None\n   372\u2192                \n   373\u2192        except Exception as e:\n   374\u2192            logger.error(f\"Error in symbol lookup for {symbol}: {e}\", exc_info=True)\n   375\u2192            return None\n   376\u2192    \n   377\u2192    def _is_document_query(self, query: str) -> bool:\n   378\u2192        \"\"\"Check if the query is looking for documentation.\n   379\u2192        \n   380\u2192        Args:\n   381\u2192            query: Search query string\n   382\u2192            \n   383\u2192        Returns:\n   384\u2192            True if this appears to be a documentation query\n   385\u2192        \"\"\"\n   386\u2192        query_lower = query.lower()\n   387\u2192        \n   388\u2192        # Check against document query patterns\n   389\u2192        for pattern in self._compiled_doc_patterns:\n   390\u2192            if pattern.search(query_lower):\n   391\u2192                return True\n   392\u2192        \n   393\u2192        # Check for question words at the beginning\n   394\u2192        question_starters = ['how', 'what', 'where', 'when', 'why', 'can', 'is', 'does', 'should']\n   395\u2192        first_word = query_lower.split()[0] if query_lower.split() else ''\n   396\u2192        if first_word in question_starters:\n   397\u2192            return True\n   398\u2192        \n   399\u2192        return False\n   400\u2192    \n   401\u2192    def _expand_document_query(self, query: str) -> List[str]:\n   402\u2192        \"\"\"Expand a document query with related terms for better search coverage.\n   403\u2192        \n   404\u2192        Args:\n   405\u2192            query: Original search query\n   406\u2192            \n   407\u2192        Returns:\n   408\u2192            List of expanded query variations\n   409\u2192        \"\"\"\n   410\u2192        expanded_queries = [query]  # Always include original\n   411\u2192        query_lower = query.lower()\n   412\u2192        \n   413\u2192        # Common expansions for documentation queries\n   414\u2192        expansions = {\n   415\u2192            'install': ['installation', 'setup', 'getting started', 'requirements'],\n   416\u2192            'config': ['configuration', 'configure', 'settings', 'options', 'parameters'],\n   417\u2192            'api': ['api documentation', 'api reference', 'endpoint', 'method'],\n   418\u2192            'how to': ['tutorial', 'guide', 'example', 'usage'],\n   419\u2192            'example': ['sample', 'snippet', 'demo', 'code example'],\n   420\u2192            'error': ['troubleshoot', 'debug', 'issue', 'problem', 'fix'],\n   421\u2192            'getting started': ['quickstart', 'tutorial', 'introduction', 'setup'],\n   422\u2192            'guide': ['tutorial', 'documentation', 'walkthrough', 'how to'],\n   423\u2192            'usage': ['how to use', 'example', 'api', 'reference']\n   424\u2192        }\n   425\u2192        \n   426\u2192        # Apply expansions\n   427\u2192        for term, related_terms in expansions.items():\n   428\u2192            if term in query_lower:\n   429\u2192                for related in related_terms:\n   430\u2192                    # Replace the term with related term\n   431\u2192                    expanded = query_lower.replace(term, related)\n   432\u2192                    if expanded != query_lower and expanded not in expanded_queries:\n   433\u2192                        expanded_queries.append(expanded)\n   434\u2192                \n   435\u2192                # Also add queries with additional terms\n   436\u2192                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437\u2192                    expanded = f\"{query} {related}\"\n   438\u2192                    if expanded not in expanded_queries:\n   439\u2192                        expanded_queries.append(expanded)\n   440\u2192        \n   441\u2192        # Add file-specific searches for common documentation files\n   442\u2192        if self._is_document_query(query):\n   443\u2192            # Extract the main topic from the query\n   444\u2192            topic_words = []\n   445\u2192            for word in query.lower().split():\n   446\u2192                if word not in ['how', 'to', 'the', 'a', 'an', 'is', 'are', 'what', 'where', 'when']:\n   447\u2192                    topic_words.append(word)\n   448\u2192            \n   449\u2192            if topic_words:\n   450\u2192                topic = ' '.join(topic_words[:2])  # Use first two topic words\n   451\u2192                expanded_queries.extend([\n   452\u2192                    f\"README {topic}\",\n   453\u2192                    f\"{topic} documentation\",\n   454\u2192                    f\"{topic} docs\",\n   455\u2192                    f\"{topic} guide\"\n   456\u2192                ])\n   457\u2192        \n   458\u2192        # Remove duplicates while preserving order\n   459\u2192        seen = set()\n   460\u2192        unique_queries = []\n   461\u2192        for q in expanded_queries:\n   462\u2192            if q not in seen:\n   463\u2192                seen.add(q)\n   464\u2192                unique_queries.append(q)\n   465\u2192        \n   466\u2192        return unique_queries[:10]  # Limit to 10 queries max\n   467\u2192    \n   468\u2192    def _is_documentation_file(self, file_path: str) -> bool:\n   469\u2192        \"\"\"Check if a file path is likely a documentation file.\n   470\u2192        \n   471\u2192        Args:\n   472\u2192            file_path: Path to check\n   473\u2192            \n   474\u2192        Returns:\n   475\u2192            True if this appears to be a documentation file\n   476\u2192        \"\"\"\n   477\u2192        path_lower = file_path.lower()\n   478\u2192        \n   479\u2192        for pattern in self._compiled_file_patterns:\n   480\u2192            if pattern.search(path_lower):\n   481\u2192                return True\n   482\u2192        \n   483\u2192        return False\n   484\u2192    \n   485\u2192    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486\u2192        \"\"\"Adjust ranking to prioritize documentation files for document queries.\n   487\u2192        \n   488\u2192        Args:\n   489\u2192            query: Original search query\n   490\u2192            results: List of aggregated results\n   491\u2192            \n   492\u2192        Returns:\n   493\u2192            Re-ranked results with documentation prioritized\n   494\u2192        \"\"\"\n   495\u2192        if not self._is_document_query(query):\n   496\u2192            return results\n   497\u2192        \n   498\u2192        # Separate documentation and code results\n   499\u2192        doc_results = []\n   500\u2192        code_results = []\n   501\u2192        \n   502\u2192        for result in results:\n   503\u2192            if self._is_documentation_file(result.primary_result.get('file', '')):\n   504\u2192                # Boost documentation files for document queries\n   505\u2192                result.rank_score *= 1.5\n   506\u2192                result.metadata['doc_boost'] = True\n   507\u2192                doc_results.append(result)\n   508\u2192            else:\n   509\u2192                code_results.append(result)\n   510\u2192        \n   511\u2192        # Sort each group by rank score\n   512\u2192        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513\u2192        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514\u2192        \n   515\u2192        # Combine with documentation files first\n   516\u2192        return doc_results + code_results\n   517\u2192    \n   518\u2192    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519\u2192        \"\"\"Search for code and documentation across all plugins.\"\"\"\n   520\u2192        start_time = time.time()\n   521\u2192        \n   522\u2192        try:\n   523\u2192            # For search, we may need to search across all languages\n   524\u2192            # Load all plugins if using lazy loading\n   525\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526\u2192                self._load_all_plugins()\n   527\u2192            \n   528\u2192            # Detect if this is a document query\n   529\u2192            is_doc_query = self._is_document_query(query)\n   530\u2192            \n   531\u2192            # Expand query if it's a document query\n   532\u2192            queries = [query]\n   533\u2192            if is_doc_query:\n   534\u2192                queries = self._expand_document_query(query)\n   535\u2192                logger.info(f\"Expanded document query '{query}' to {len(queries)} variations\")\n   536\u2192                # Force semantic search for natural language queries\n   537\u2192                semantic = True\n   538\u2192            \n   539\u2192            if self._enable_advanced and self._aggregator:\n   540\u2192                # Use advanced aggregation\n   541\u2192                all_results_by_plugin = {}\n   542\u2192                opts = {\"semantic\": semantic, \"limit\": limit * 2 if is_doc_query else limit}\n   543\u2192                \n   544\u2192                # Search with all query variations\n   545\u2192                for search_query in queries:\n   546\u2192                    for plugin in self._plugins:\n   547\u2192                        try:\n   548\u2192                            results = list(plugin.search(search_query, opts))\n   549\u2192                            if results:\n   550\u2192                                if plugin not in all_results_by_plugin:\n   551\u2192                                    all_results_by_plugin[plugin] = []\n   552\u2192                                all_results_by_plugin[plugin].extend(results)\n   553\u2192                        except Exception as e:\n   554\u2192                            logger.warning(f\"Plugin {plugin.lang} failed to search for {search_query}: {e}\")\n   555\u2192                \n   556\u2192                # Deduplicate results per plugin\n   557\u2192                for plugin, results in all_results_by_plugin.items():\n   558\u2192                    seen = set()\n   559\u2192                    unique_results = []\n   560\u2192                    for result in results:\n   561\u2192                        key = f\"{result['file']}:{result['line']}\"\n   562\u2192                        if key not in seen:\n   563\u2192                            seen.add(key)\n   564\u2192                            unique_results.append(result)\n   565\u2192                    all_results_by_plugin[plugin] = unique_results\n   566\u2192                \n   567\u2192                # Configure aggregator for document queries\n   568\u2192                if is_doc_query and self._enable_advanced:\n   569\u2192                    # Adjust ranking criteria for documentation\n   570\u2192                    doc_criteria = RankingCriteria(\n   571\u2192                        relevance_weight=0.5,      # Increase relevance weight\n   572\u2192                        confidence_weight=0.2,     # Reduce confidence weight\n   573\u2192                        frequency_weight=0.2,      # Keep frequency weight\n   574\u2192                        recency_weight=0.1,        # Keep recency weight\n   575\u2192                        prefer_exact_matches=False,  # Natural language doesn't need exact matches\n   576\u2192                        boost_multiple_sources=True,\n   577\u2192                        boost_common_extensions=True\n   578\u2192                    )\n   579\u2192                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580\u2192                \n   581\u2192                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582\u2192                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583\u2192                )\n   584\u2192                \n   585\u2192                # Adjust ranking for document queries\n   586\u2192                if is_doc_query:\n   587\u2192                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588\u2192                \n   589\u2192                # Apply final limit\n   590\u2192                if limit and len(aggregated_results) > limit:\n   591\u2192                    aggregated_results = aggregated_results[:limit]\n   592\u2192                \n   593\u2192                logger.debug(f\"Search aggregation stats: {stats.total_results} total, \"\n   594\u2192                           f\"{stats.unique_results} unique, {stats.plugins_used} plugins used, \"\n   595\u2192                           f\"document_query={is_doc_query}\")\n   596\u2192                \n   597\u2192                self._operation_stats['searches'] += 1\n   598\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   599\u2192                \n   600\u2192                # Yield primary results from aggregated results\n   601\u2192                for aggregated in aggregated_results:\n   602\u2192                    yield aggregated.primary_result\n   603\u2192            else:\n   604\u2192                # Fallback to basic search\n   605\u2192                # Detect if this is a document query\n   606\u2192                is_doc_query = self._is_document_query(query)\n   607\u2192                \n   608\u2192                # Expand query if it's a document query\n   609\u2192                queries = [query]\n   610\u2192                if is_doc_query:\n   611\u2192                    queries = self._expand_document_query(query)\n   612\u2192                    semantic = True  # Force semantic search for natural language\n   613\u2192                \n   614\u2192                opts = {\"semantic\": semantic, \"limit\": limit}\n   615\u2192                all_results = []\n   616\u2192                \n   617\u2192                # Search with all query variations\n   618\u2192                for search_query in queries:\n   619\u2192                    for p in self._plugins:\n   620\u2192                        try:\n   621\u2192                            for result in p.search(search_query, opts):\n   622\u2192                                all_results.append(result)\n   623\u2192                        except Exception as e:\n   624\u2192                            logger.warning(f\"Plugin {p.lang} failed to search for {search_query}: {e}\")\n   625\u2192                \n   626\u2192                # Deduplicate results\n   627\u2192                seen = set()\n   628\u2192                unique_results = []\n   629\u2192                for result in all_results:\n   630\u2192                    key = f\"{result['file']}:{result['line']}\"\n   631\u2192                    if key not in seen:\n   632\u2192                        seen.add(key)\n   633\u2192                        unique_results.append(result)\n   634\u2192                \n   635\u2192                # Sort by score if available\n   636\u2192                unique_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   637\u2192                \n   638\u2192                # Prioritize documentation files for document queries\n   639\u2192                if is_doc_query:\n   640\u2192                    doc_results = []\n   641\u2192                    code_results = []\n   642\u2192                    for result in unique_results:\n   643\u2192                        if self._is_documentation_file(result.get('file', '')):\n   644\u2192                            doc_results.append(result)\n   645\u2192                        else:\n   646\u2192                            code_results.append(result)\n   647\u2192                    unique_results = doc_results + code_results\n   648\u2192                \n   649\u2192                # Apply limit\n   650\u2192                count = 0\n   651\u2192                for result in unique_results:\n   652\u2192                    if limit and count >= limit:\n   653\u2192                        break\n   654\u2192                    yield result\n   655\u2192                    count += 1\n   656\u2192                \n   657\u2192                self._operation_stats['searches'] += 1\n   658\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   659\u2192                \n   660\u2192        except Exception as e:\n   661\u2192            logger.error(f\"Error in search for {query}: {e}\", exc_info=True)\n   662\u2192    \n   663\u2192    def index_file(self, path: Path) -> None:\n   664\u2192        \"\"\"Index a single file if it has changed.\"\"\"\n   665\u2192        try:\n   666\u2192            # Ensure path is absolute to avoid relative/absolute path issues\n   667\u2192            path = path.resolve()\n   668\u2192            \n   669\u2192            # Find the appropriate plugin\n   670\u2192            plugin = self._match_plugin(path)\n   671\u2192            \n   672\u2192            # Read file content\n   673\u2192            try:\n   674\u2192                content = path.read_text(encoding='utf-8')\n   675\u2192            except UnicodeDecodeError:\n   676\u2192                # Try with different encodings\n   677\u2192                try:\n   678\u2192                    content = path.read_text(encoding='latin-1')\n   679\u2192                except Exception as e:\n   680\u2192                    logger.error(f\"Failed to read {path}: {e}\")\n   681\u2192                    return\n   682\u2192            \n   683\u2192            # Check if we need to re-index (simplified for now)\n   684\u2192            # TODO: Implement proper caching logic\n   685\u2192            \n   686\u2192            # Index the file\n   687\u2192            start_time = time.time()\n   688\u2192            logger.info(f\"Indexing {path} with {plugin.lang} plugin\")\n   689\u2192            shard = plugin.indexFile(path, content)\n   690\u2192            \n   691\u2192            # Record performance if advanced features enabled\n   692\u2192            if self._enable_advanced and self._router:\n   693\u2192                execution_time = time.time() - start_time\n   694\u2192                self._router.record_performance(plugin, execution_time)\n   695\u2192            \n   696\u2192            self._operation_stats['indexings'] += 1\n   697\u2192            self._operation_stats['total_time'] += time.time() - start_time\n   698\u2192            \n   699\u2192            logger.info(f\"Successfully indexed {path}: {len(shard.get('symbols', []))} symbols found\")\n   700\u2192            \n   701\u2192        except RuntimeError as e:\n   702\u2192            # No plugin found for this file type\n   703\u2192            logger.debug(f\"No plugin for {path}: {e}\")\n   704\u2192        except Exception as e:\n   705\u2192            logger.error(f\"Error indexing {path}: {e}\", exc_info=True)\n   706\u2192    \n   707\u2192    def get_statistics(self) -> dict:\n   708\u2192        \"\"\"Get comprehensive statistics across all plugins and components.\"\"\"\n   709\u2192        stats = {\n   710\u2192            \"total_plugins\": len(self._plugins),\n   711\u2192            \"loaded_languages\": sorted(list(self._loaded_languages)),\n   712\u2192            \"supported_languages\": len(self.supported_languages),\n   713\u2192            \"operations\": self._operation_stats.copy()\n   714\u2192        }\n   715\u2192        \n   716\u2192        # Add language breakdown\n   717\u2192        stats[\"by_language\"] = {}\n   718\u2192        for lang, plugin in self._by_lang.items():\n   719\u2192            plugin_info = {\n   720\u2192                \"loaded\": True,\n   721\u2192                \"class\": plugin.__class__.__name__\n   722\u2192            }\n   723\u2192            if hasattr(plugin, 'get_indexed_count'):\n   724\u2192                plugin_info[\"indexed_files\"] = plugin.get_indexed_count()\n   725\u2192            stats[\"by_language\"][lang] = plugin_info\n   726\u2192        \n   727\u2192        return stats\n   728\u2192    \n   729\u2192    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730\u2192        \"\"\"\n   731\u2192        Index all files in a directory, respecting ignore patterns.\n   732\u2192        \n   733\u2192        Args:\n   734\u2192            directory: Directory to index\n   735\u2192            recursive: Whether to index subdirectories\n   736\u2192            \n   737\u2192        Returns:\n   738\u2192            Statistics about indexed files\n   739\u2192        \"\"\"\n   740\u2192        logger.info(f\"Indexing directory: {directory} (recursive={recursive})\")\n   741\u2192        \n   742\u2192        # Note: We don't use ignore patterns during indexing\n   743\u2192        # ALL files are indexed for local search capability\n   744\u2192        # Filtering happens only during export/sharing\n   745\u2192        \n   746\u2192        # Get all supported extensions\n   747\u2192        supported_extensions = get_all_extensions()\n   748\u2192        \n   749\u2192        stats = {\n   750\u2192            \"total_files\": 0,\n   751\u2192            \"indexed_files\": 0,\n   752\u2192            \"ignored_files\": 0,\n   753\u2192            \"failed_files\": 0,\n   754\u2192            \"by_language\": {}\n   755\u2192        }\n   756\u2192        \n   757\u2192        # Walk directory\n   758\u2192        if recursive:\n   759\u2192            file_iterator = directory.rglob(\"*\")\n   760\u2192        else:\n   761\u2192            file_iterator = directory.glob(\"*\")\n   762\u2192            \n   763\u2192        for path in file_iterator:\n   764\u2192            if not path.is_file():\n   765\u2192                continue\n   766\u2192                \n   767\u2192            stats[\"total_files\"] += 1\n   768\u2192            \n   769\u2192            # NOTE: We index ALL files locally, including gitignored ones\n   770\u2192            # Filtering happens only during export/sharing\n   771\u2192            # This allows local search of .env, secrets, etc.\n   772\u2192                \n   773\u2192            # Try to find a plugin that supports this file\n   774\u2192            # This allows us to index ALL files, including .env, .key, etc.\n   775\u2192            try:\n   776\u2192                # First try to match by extension\n   777\u2192                if path.suffix in supported_extensions:\n   778\u2192                    self.index_file(path)\n   779\u2192                    stats[\"indexed_files\"] += 1\n   780\u2192                # For files without recognized extensions, try each plugin's supports() method\n   781\u2192                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782\u2192                else:\n   783\u2192                    matched = False\n   784\u2192                    for plugin in self._plugins:\n   785\u2192                        if plugin.supports(path):\n   786\u2192                            self.index_file(path)\n   787\u2192                            stats[\"indexed_files\"] += 1\n   788\u2192                            matched = True\n   789\u2192                            break\n   790\u2192                    \n   791\u2192                    # If no plugin matched but we want to index everything,\n   792\u2192                    # we could add a fallback here to index as plaintext\n   793\u2192                    # For now, we'll skip unmatched files\n   794\u2192                    if not matched:\n   795\u2192                        logger.debug(f\"No plugin found for {path}\")\n   796\u2192                \n   797\u2192                # Track by language\n   798\u2192                language = get_language_by_extension(path.suffix)\n   799\u2192                if language:\n   800\u2192                    stats[\"by_language\"][language] = stats[\"by_language\"].get(language, 0) + 1\n   801\u2192                    \n   802\u2192            except Exception as e:\n   803\u2192                logger.error(f\"Failed to index {path}: {e}\")\n   804\u2192                stats[\"failed_files\"] += 1\n   805\u2192                \n   806\u2192        logger.info(f\"Directory indexing complete: {stats['indexed_files']} indexed, \"\n   807\u2192                   f\"{stats['ignored_files']} ignored, {stats['failed_files']} failed\")\n   808\u2192        \n   809\u2192        return stats\n   810\u2192    \n   811\u2192    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812\u2192        \"\"\"Search specifically across documentation files.\n   813\u2192        \n   814\u2192        Args:\n   815\u2192            topic: Topic to search for (e.g., \"installation\", \"configuration\")\n   816\u2192            doc_types: Optional list of document types to search (e.g., [\"readme\", \"guide\", \"api\"])\n   817\u2192            limit: Maximum number of results\n   818\u2192            \n   819\u2192        Returns:\n   820\u2192            Search results from documentation files\n   821\u2192        \"\"\"\n   822\u2192        # Default document types if not specified\n   823\u2192        if doc_types is None:\n   824\u2192            doc_types = [\"readme\", \"documentation\", \"guide\", \"tutorial\", \"api\", \"changelog\", \"contributing\"]\n   825\u2192        \n   826\u2192        # Build search queries for different document types\n   827\u2192        queries = []\n   828\u2192        for doc_type in doc_types:\n   829\u2192            queries.extend([\n   830\u2192                f\"{doc_type} {topic}\",\n   831\u2192                f\"{topic} {doc_type}\",\n   832\u2192                f\"{topic} in {doc_type}\"\n   833\u2192            ])\n   834\u2192        \n   835\u2192        # Also search for the topic in common doc filenames\n   836\u2192        queries.extend([\n   837\u2192            f\"README {topic}\",\n   838\u2192            f\"CONTRIBUTING {topic}\",\n   839\u2192            f\"docs {topic}\",\n   840\u2192            f\"documentation {topic}\"\n   841\u2192        ])\n   842\u2192        \n   843\u2192        # Deduplicate queries\n   844\u2192        queries = list(dict.fromkeys(queries))\n   845\u2192        \n   846\u2192        logger.info(f\"Cross-document search for '{topic}' with {len(queries)} query variations\")\n   847\u2192        \n   848\u2192        # Use the enhanced search with document-specific handling\n   849\u2192        all_results = []\n   850\u2192        seen = set()\n   851\u2192        \n   852\u2192        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853\u2192            for result in self.search(query, semantic=True, limit=limit):\n   854\u2192                # Only include documentation files\n   855\u2192                if self._is_documentation_file(result.get('file', '')):\n   856\u2192                    key = f\"{result['file']}:{result['line']}\"\n   857\u2192                    if key not in seen:\n   858\u2192                        seen.add(key)\n   859\u2192                        all_results.append(result)\n   860\u2192        \n   861\u2192        # Sort by relevance (score) and return top results\n   862\u2192        all_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   863\u2192        \n   864\u2192        count = 0\n   865\u2192        for result in all_results:\n   866\u2192            if count >= limit:\n   867\u2192                break\n   868\u2192            yield result\n   869\u2192            count += 1\n   870\u2192    \n   871\u2192    def health_check(self) -> Dict[str, Any]:\n   872\u2192        \"\"\"Perform a health check on all components.\"\"\"\n   873\u2192        health = {\n   874\u2192            'status': 'healthy',\n   875\u2192            'components': {\n   876\u2192                'dispatcher': {\n   877\u2192                    'status': 'healthy',\n   878\u2192                    'plugins_loaded': len(self._plugins),\n   879\u2192                    'languages_supported': len(self.supported_languages),\n   880\u2192                    'factory_enabled': self._use_factory,\n   881\u2192                    'lazy_loading': self._lazy_load\n   882\u2192                }\n   883\u2192            },\n   884\u2192            'plugins': {},\n   885\u2192            'errors': []\n   886\u2192        }\n   887\u2192        \n   888\u2192        # Check plugin health\n   889\u2192        for lang, plugin in self._by_lang.items():\n   890\u2192            try:\n   891\u2192                plugin_health = {\n   892\u2192                    'status': 'healthy',\n   893\u2192                    'class': plugin.__class__.__name__,\n   894\u2192                    'semantic_enabled': getattr(plugin, '_enable_semantic', False)\n   895\u2192                }\n   896\u2192                if hasattr(plugin, 'get_indexed_count'):\n   897\u2192                    plugin_health['indexed_files'] = plugin.get_indexed_count()\n   898\u2192            except Exception as e:\n   899\u2192                plugin_health = {\n   900\u2192                    'status': 'error',\n   901\u2192                    'error': str(e)\n   902\u2192                }\n   903\u2192                health['errors'].append(f\"Plugin {lang}: {str(e)}\")\n   904\u2192            \n   905\u2192            health['plugins'][lang] = plugin_health\n   906\u2192        \n   907\u2192        # Determine overall health\n   908\u2192        if len(health['errors']) > 0:\n   909\u2192            health['status'] = 'degraded' if len(health['errors']) < 3 else 'unhealthy'\n   910\u2192        \n   911\u2192        return health\n   912\u2192    \n   913\u2192    def remove_file(self, path: Union[Path, str]) -> None:\n   914\u2192        \"\"\"Remove a file from all indexes.\n   915\u2192        \n   916\u2192        Args:\n   917\u2192            path: File path to remove\n   918\u2192        \"\"\"\n   919\u2192        path = Path(path).resolve()\n   920\u2192        logger.info(f\"Removing file from index: {path}\")\n   921\u2192        \n   922\u2192        try:\n   923\u2192            # Remove from SQLite if available\n   924\u2192            if self._sqlite_store:\n   925\u2192                from ..core.path_resolver import PathResolver\n   926\u2192                path_resolver = PathResolver()\n   927\u2192                try:\n   928\u2192                    relative_path = path_resolver.normalize_path(path)\n   929\u2192                    # Get repository ID - for now assume 1\n   930\u2192                    # TODO: Properly detect repository\n   931\u2192                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932\u2192                except Exception as e:\n   933\u2192                    logger.error(f\"Error removing from SQLite: {e}\")\n   934\u2192            \n   935\u2192            # Remove from semantic index if available\n   936\u2192            try:\n   937\u2192                plugin = self._match_plugin(path)\n   938\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   939\u2192                    plugin._indexer.remove_file(path)\n   940\u2192                    logger.info(f\"Removed from semantic index: {path}\")\n   941\u2192            except Exception as e:\n   942\u2192                logger.warning(f\"Error removing from semantic index: {e}\")\n   943\u2192            \n   944\u2192            # Update statistics\n   945\u2192            self._operation_stats['deletions'] = self._operation_stats.get('deletions', 0) + 1\n   946\u2192            \n   947\u2192        except Exception as e:\n   948\u2192            logger.error(f\"Error removing file {path}: {e}\", exc_info=True)\n   949\u2192    \n   950\u2192    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951\u2192        \"\"\"Move a file in all indexes.\n   952\u2192        \n   953\u2192        Args:\n   954\u2192            old_path: Original file path\n   955\u2192            new_path: New file path\n   956\u2192            content_hash: Optional content hash to verify unchanged content\n   957\u2192        \"\"\"\n   958\u2192        old_path = Path(old_path).resolve()\n   959\u2192        new_path = Path(new_path).resolve()\n   960\u2192        logger.info(f\"Moving file in index: {old_path} -> {new_path}\")\n   961\u2192        \n   962\u2192        try:\n   963\u2192            # Move in SQLite if available\n   964\u2192            if self._sqlite_store:\n   965\u2192                from ..core.path_resolver import PathResolver\n   966\u2192                path_resolver = PathResolver()\n   967\u2192                try:\n   968\u2192                    old_relative = path_resolver.normalize_path(old_path)\n   969\u2192                    new_relative = path_resolver.normalize_path(new_path)\n   970\u2192                    # Get repository ID - for now assume 1\n   971\u2192                    # TODO: Properly detect repository\n   972\u2192                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973\u2192                except Exception as e:\n   974\u2192                    logger.error(f\"Error moving in SQLite: {e}\")\n   975\u2192            \n   976\u2192            # Move in semantic index if available\n   977\u2192            try:\n   978\u2192                plugin = self._match_plugin(new_path)\n   979\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   980\u2192                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981\u2192                    logger.info(f\"Moved in semantic index: {old_path} -> {new_path}\")\n   982\u2192            except Exception as e:\n   983\u2192                logger.warning(f\"Error moving in semantic index: {e}\")\n   984\u2192            \n   985\u2192            # Update statistics\n   986\u2192            self._operation_stats['moves'] = self._operation_stats.get('moves', 0) + 1\n   987\u2192            \n   988\u2192        except Exception as e:\n   989\u2192            logger.error(f\"Error moving file {old_path} -> {new_path}: {e}\", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 14366,
    "cache_creation_tokens": 13324,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "0eae0e31-aa5c-4b13-83d0-28bb4143fd22",
    "test_date": "2025-06-26T07:31:01.656Z",
    "query_id": "transcript_0eae0e31-aa5c-4b13-83d0-28bb4143fd22",
    "query_text": "Find the BM25Indexer class definition",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 13905,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "0eae0e31-aa5c-4b13-83d0-28bb4143fd22",
    "test_date": "2025-06-26T07:31:05.854Z",
    "query_id": "transcript_0eae0e31-aa5c-4b13-83d0-28bb4143fd22",
    "query_text": [
      {
        "tool_use_id": "toolu_01SjDWcufiuQoWCxEHUFnfiR",
        "type": "tool_result",
        "content": "Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/bm25_indexer.py"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13905,
    "cache_creation_tokens": 228,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "e514801b-3ad0-4d1c-9f08-c28e03f0b0a9",
    "test_date": "2025-06-26T07:31:09.557Z",
    "query_id": "transcript_e514801b-3ad0-4d1c-9f08-c28e03f0b0a9",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13160,
    "cache_creation_tokens": 749,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "e514801b-3ad0-4d1c-9f08-c28e03f0b0a9",
    "test_date": "2025-06-26T07:31:12.704Z",
    "query_id": "transcript_e514801b-3ad0-4d1c-9f08-c28e03f0b0a9",
    "query_text": [
      {
        "tool_use_id": "toolu_013RYNe4zYErbjT6yFJfJHP3",
        "type": "tool_result",
        "content": "Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/path_management_architecture.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/path_management.puml"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 29,
    "output_tokens_total": 29,
    "cache_read_tokens": 13909,
    "cache_creation_tokens": 455,
    "tokens_per_result": 29,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "e514801b-3ad0-4d1c-9f08-c28e03f0b0a9",
    "test_date": "2025-06-26T07:31:16.894Z",
    "query_id": "transcript_e514801b-3ad0-4d1c-9f08-c28e03f0b0a9",
    "query_text": [
      {
        "tool_use_id": "toolu_01WBt7AwPtsSUCPSJqPhzqRa",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"Enhanced dispatcher with dynamic plugin loading via PluginFactory.\"\"\"\n     2\u2192from pathlib import Path\n     3\u2192from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4\u2192import logging\n     5\u2192import hashlib\n     6\u2192import time\n     7\u2192import re\n     8\u2192from datetime import datetime\n     9\u2192import os\n    10\u2192\n    11\u2192from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12\u2192from ..plugins.plugin_factory import PluginFactory\n    13\u2192from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14\u2192from ..storage.sqlite_store import SQLiteStore\n    15\u2192from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16\u2192from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17\u2192# Note: We've removed ignore pattern checks to allow indexing ALL files\n    18\u2192# Filtering happens only during export via SecureIndexExporter\n    19\u2192# from ..core.ignore_patterns import get_ignore_manager\n    20\u2192\n    21\u2192logger = logging.getLogger(__name__)\n    22\u2192\n    23\u2192\n    24\u2192class EnhancedDispatcher:\n    25\u2192    \"\"\"Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.\"\"\"\n    26\u2192    \n    27\u2192    # Document query patterns - common documentation search terms\n    28\u2192    DOCUMENT_QUERY_PATTERNS = [\n    29\u2192        r'\\b(how\\s+to|howto)\\b',\n    30\u2192        r'\\b(getting\\s+started|get\\s+started)\\b',\n    31\u2192        r'\\b(installation|install|setup)\\b',\n    32\u2192        r'\\b(configuration|configure|config)\\b',\n    33\u2192        r'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b',\n    34\u2192        r'\\b(tutorial|guide|walkthrough)\\b',\n    35\u2192        r'\\b(example|sample|snippet)\\b',\n    36\u2192        r'\\b(readme|documentation|docs)\\b',\n    37\u2192        r'\\b(usage|use\\s+case|using)\\b',\n    38\u2192        r'\\b(reference|manual)\\b',\n    39\u2192        r'\\b(faq|frequently\\s+asked)\\b',\n    40\u2192        r'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b',\n    41\u2192        r'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b',\n    42\u2192        r'\\b(architecture|design|overview)\\b',\n    43\u2192        r'\\b(changelog|release\\s+notes|migration)\\b'\n    44\u2192    ]\n    45\u2192    \n    46\u2192    # Documentation file patterns\n    47\u2192    DOCUMENTATION_FILE_PATTERNS = [\n    48\u2192        r'readme(\\.\\w+)?$',\n    49\u2192        r'changelog(\\.\\w+)?$',\n    50\u2192        r'contributing(\\.\\w+)?$',\n    51\u2192        r'license(\\.\\w+)?$',\n    52\u2192        r'install(\\.\\w+)?$',\n    53\u2192        r'setup(\\.\\w+)?$',\n    54\u2192        r'guide(\\.\\w+)?$',\n    55\u2192        r'tutorial(\\.\\w+)?$',\n    56\u2192        r'\\.md$',\n    57\u2192        r'\\.rst$',\n    58\u2192        r'\\.txt$',\n    59\u2192        r'docs?/',\n    60\u2192        r'documentation/'\n    61\u2192    ]\n    62\u2192    \n    63\u2192    def __init__(self, \n    64\u2192                 plugins: Optional[List[IPlugin]] = None,\n    65\u2192                 sqlite_store: Optional[SQLiteStore] = None,\n    66\u2192                 enable_advanced_features: bool = True,\n    67\u2192                 use_plugin_factory: bool = True,\n    68\u2192                 lazy_load: bool = True,\n    69\u2192                 semantic_search_enabled: bool = True):\n    70\u2192        \"\"\"Initialize the enhanced dispatcher.\n    71\u2192        \n    72\u2192        Args:\n    73\u2192            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74\u2192            sqlite_store: SQLite store for plugin persistence\n    75\u2192            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76\u2192            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77\u2192            lazy_load: Whether to lazy-load plugins on demand\n    78\u2192            semantic_search_enabled: Whether to enable semantic search in plugins\n    79\u2192        \"\"\"\n    80\u2192        self._sqlite_store = sqlite_store\n    81\u2192        self._enable_advanced = enable_advanced_features\n    82\u2192        self._use_factory = use_plugin_factory\n    83\u2192        self._lazy_load = lazy_load\n    84\u2192        self._semantic_enabled = semantic_search_enabled\n    85\u2192        \n    86\u2192        # Plugin storage\n    87\u2192        self._plugins: List[IPlugin] = []\n    88\u2192        self._by_lang: Dict[str, IPlugin] = {}\n    89\u2192        self._loaded_languages: set[str] = set()\n    90\u2192        \n    91\u2192        # Cache for file hashes to avoid re-indexing unchanged files\n    92\u2192        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93\u2192        \n    94\u2192        # Advanced components\n    95\u2192        if self._enable_advanced:\n    96\u2192            self._file_matcher = FileTypeMatcher()\n    97\u2192            self._router = PluginRouter(self._file_matcher)\n    98\u2192            self._aggregator = ResultAggregator()\n    99\u2192        \n   100\u2192        # Performance tracking\n   101\u2192        self._operation_stats = {\n   102\u2192            'searches': 0,\n   103\u2192            'lookups': 0,\n   104\u2192            'indexings': 0,\n   105\u2192            'total_time': 0.0,\n   106\u2192            'plugins_loaded': 0\n   107\u2192        }\n   108\u2192        \n   109\u2192        # Initialize plugins\n   110\u2192        if plugins:\n   111\u2192            # Use provided plugins (backward compatibility)\n   112\u2192            self._plugins = plugins\n   113\u2192            self._by_lang = {p.lang: p for p in plugins}\n   114\u2192            for plugin in plugins:\n   115\u2192                self._loaded_languages.add(getattr(plugin, 'lang', 'unknown'))\n   116\u2192            if self._enable_advanced:\n   117\u2192                self._register_plugins_with_router()\n   118\u2192        elif use_plugin_factory and not lazy_load:\n   119\u2192            # Load all plugins immediately\n   120\u2192            self._load_all_plugins()\n   121\u2192        # If lazy_load is True, plugins will be loaded on demand\n   122\u2192        \n   123\u2192        # Compile document query patterns for performance\n   124\u2192        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125\u2192        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126\u2192        \n   127\u2192        logger.info(f\"Enhanced dispatcher initialized with {len(self._plugins)} plugins\")\n   128\u2192    \n   129\u2192    def _load_all_plugins(self):\n   130\u2192        \"\"\"Load all available plugins using PluginFactory.\"\"\"\n   131\u2192        logger.info(\"Loading all available plugins...\")\n   132\u2192        \n   133\u2192        all_plugins = PluginFactory.create_all_plugins(\n   134\u2192            sqlite_store=self._sqlite_store,\n   135\u2192            enable_semantic=self._semantic_enabled\n   136\u2192        )\n   137\u2192        \n   138\u2192        for lang, plugin in all_plugins.items():\n   139\u2192            self._plugins.append(plugin)\n   140\u2192            self._by_lang[lang] = plugin\n   141\u2192            self._loaded_languages.add(lang)\n   142\u2192            self._operation_stats['plugins_loaded'] += 1\n   143\u2192        \n   144\u2192        if self._enable_advanced:\n   145\u2192            self._register_plugins_with_router()\n   146\u2192        \n   147\u2192        logger.info(f\"Loaded {len(all_plugins)} plugins: {', '.join(sorted(all_plugins.keys()))}\")\n   148\u2192    \n   149\u2192    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150\u2192        \"\"\"Ensure a plugin for the given language is loaded.\n   151\u2192        \n   152\u2192        Args:\n   153\u2192            language: Language code (e.g., 'python', 'go')\n   154\u2192            \n   155\u2192        Returns:\n   156\u2192            Plugin instance or None if not available\n   157\u2192        \"\"\"\n   158\u2192        # Normalize language\n   159\u2192        language = language.lower().replace('-', '_')\n   160\u2192        \n   161\u2192        # Check if already loaded\n   162\u2192        if language in self._by_lang:\n   163\u2192            return self._by_lang[language]\n   164\u2192        \n   165\u2192        # If not using factory or already tried to load, return None\n   166\u2192        if not self._use_factory or language in self._loaded_languages:\n   167\u2192            return None\n   168\u2192        \n   169\u2192        # Try to load the plugin\n   170\u2192        try:\n   171\u2192            logger.info(f\"Lazy loading plugin for {language}\")\n   172\u2192            plugin = PluginFactory.create_plugin(\n   173\u2192                language,\n   174\u2192                sqlite_store=self._sqlite_store,\n   175\u2192                enable_semantic=self._semantic_enabled\n   176\u2192            )\n   177\u2192            \n   178\u2192            # Add to collections\n   179\u2192            self._plugins.append(plugin)\n   180\u2192            self._by_lang[language] = plugin\n   181\u2192            self._loaded_languages.add(language)\n   182\u2192            self._operation_stats['plugins_loaded'] += 1\n   183\u2192            \n   184\u2192            # Register with router if needed\n   185\u2192            if self._enable_advanced:\n   186\u2192                capabilities = self._detect_plugin_capabilities(plugin)\n   187\u2192                self._router.register_plugin(plugin, capabilities)\n   188\u2192            \n   189\u2192            logger.info(f\"Successfully loaded {language} plugin\")\n   190\u2192            return plugin\n   191\u2192            \n   192\u2192        except ValueError as e:\n   193\u2192            logger.warning(f\"No plugin available for {language}: {e}\")\n   194\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   195\u2192            return None\n   196\u2192        except Exception as e:\n   197\u2192            logger.error(f\"Error loading plugin for {language}: {e}\")\n   198\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   199\u2192            return None\n   200\u2192    \n   201\u2192    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202\u2192        \"\"\"Ensure a plugin is loaded for the given file.\n   203\u2192        \n   204\u2192        Args:\n   205\u2192            path: File path\n   206\u2192            \n   207\u2192        Returns:\n   208\u2192            Plugin instance or None if not available\n   209\u2192        \"\"\"\n   210\u2192        # Get language from file extension\n   211\u2192        extension = path.suffix.lower()\n   212\u2192        language = get_language_by_extension(extension)\n   213\u2192        \n   214\u2192        if language:\n   215\u2192            return self._ensure_plugin_loaded(language)\n   216\u2192        \n   217\u2192        # Fallback: try all loaded plugins\n   218\u2192        for plugin in self._plugins:\n   219\u2192            if plugin.supports(path):\n   220\u2192                return plugin\n   221\u2192        \n   222\u2192        return None\n   223\u2192    \n   224\u2192    def _register_plugins_with_router(self):\n   225\u2192        \"\"\"Register plugins with the router and assign capabilities.\"\"\"\n   226\u2192        for plugin in self._plugins:\n   227\u2192            # Determine capabilities based on plugin type/language\n   228\u2192            capabilities = self._detect_plugin_capabilities(plugin)\n   229\u2192            self._router.register_plugin(plugin, capabilities)\n   230\u2192    \n   231\u2192    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232\u2192        \"\"\"Detect capabilities for a plugin based on its language and features.\"\"\"\n   233\u2192        capabilities = []\n   234\u2192        lang = getattr(plugin, 'lang', 'unknown')\n   235\u2192        \n   236\u2192        # Base capabilities all plugins have\n   237\u2192        capabilities.append(PluginCapability(\n   238\u2192            'syntax_analysis', '1.0', f'{lang} syntax analysis', \n   239\u2192            priority=70, metadata={'language': lang}\n   240\u2192        ))\n   241\u2192        \n   242\u2192        capabilities.append(PluginCapability(\n   243\u2192            'code_search', '1.0', f'{lang} code search', \n   244\u2192            priority=80, metadata={'language': lang}\n   245\u2192        ))\n   246\u2192        \n   247\u2192        # Check for semantic search capability\n   248\u2192        if hasattr(plugin, '_enable_semantic') and plugin._enable_semantic:\n   249\u2192            capabilities.append(PluginCapability(\n   250\u2192                'semantic_search', '1.0', f'{lang} semantic search', \n   251\u2192                priority=90, metadata={'language': lang}\n   252\u2192            ))\n   253\u2192        \n   254\u2192        # Language-specific capabilities\n   255\u2192        if lang == 'python':\n   256\u2192            capabilities.extend([\n   257\u2192                PluginCapability('refactoring', '1.0', 'Python refactoring support', 75),\n   258\u2192                PluginCapability('type_analysis', '1.0', 'Python type analysis', 85)\n   259\u2192            ])\n   260\u2192        elif lang in ['javascript', 'typescript']:\n   261\u2192            capabilities.extend([\n   262\u2192                PluginCapability('linting', '1.0', 'JavaScript/TypeScript linting', 85),\n   263\u2192                PluginCapability('bundling_analysis', '1.0', 'Module bundling analysis', 70),\n   264\u2192                PluginCapability('framework_support', '1.0', 'Framework-specific support', 75)\n   265\u2192            ])\n   266\u2192        elif lang in ['c', 'cpp']:\n   267\u2192            capabilities.extend([\n   268\u2192                PluginCapability('compilation_analysis', '1.0', 'Compilation analysis', 80),\n   269\u2192                PluginCapability('memory_analysis', '1.0', 'Memory usage analysis', 70),\n   270\u2192                PluginCapability('performance_profiling', '1.0', 'Performance profiling', 75)\n   271\u2192            ])\n   272\u2192        elif lang in ['go', 'rust']:\n   273\u2192            capabilities.extend([\n   274\u2192                PluginCapability('package_analysis', '1.0', f'{lang} package analysis', 80),\n   275\u2192                PluginCapability('concurrency_analysis', '1.0', f'{lang} concurrency analysis', 75)\n   276\u2192            ])\n   277\u2192        elif lang in ['java', 'kotlin', 'scala']:\n   278\u2192            capabilities.extend([\n   279\u2192                PluginCapability('jvm_analysis', '1.0', 'JVM bytecode analysis', 75),\n   280\u2192                PluginCapability('build_tool_integration', '1.0', 'Build tool integration', 70)\n   281\u2192            ])\n   282\u2192        \n   283\u2192        return capabilities\n   284\u2192    \n   285\u2192    @property\n   286\u2192    def plugins(self):\n   287\u2192        \"\"\"Get the dictionary of loaded plugins by language.\"\"\"\n   288\u2192        return self._by_lang\n   289\u2192    \n   290\u2192    @property\n   291\u2192    def supported_languages(self) -> List[str]:\n   292\u2192        \"\"\"Get list of all supported languages (loaded and available).\"\"\"\n   293\u2192        if self._use_factory:\n   294\u2192            return PluginFactory.get_supported_languages()\n   295\u2192        else:\n   296\u2192            return list(self._by_lang.keys())\n   297\u2192    \n   298\u2192    def _match_plugin(self, path: Path) -> IPlugin:\n   299\u2192        \"\"\"Match a plugin for the given file path.\"\"\"\n   300\u2192        # Ensure plugin is loaded if using lazy loading\n   301\u2192        if self._lazy_load and self._use_factory:\n   302\u2192            plugin = self._ensure_plugin_for_file(path)\n   303\u2192            if plugin:\n   304\u2192                return plugin\n   305\u2192        \n   306\u2192        # Use advanced routing if available\n   307\u2192        if self._enable_advanced and self._router:\n   308\u2192            route_result = self._router.get_best_plugin(path)\n   309\u2192            if route_result:\n   310\u2192                return route_result.plugin\n   311\u2192        \n   312\u2192        # Fallback to basic matching\n   313\u2192        for p in self._plugins:\n   314\u2192            if p.supports(path):\n   315\u2192                return p\n   316\u2192        \n   317\u2192        raise RuntimeError(f\"No plugin found for {path}\")\n   318\u2192    \n   319\u2192    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320\u2192        \"\"\"Get all plugins that can handle a file with confidence scores.\"\"\"\n   321\u2192        # Ensure plugin is loaded if using lazy loading\n   322\u2192        if self._lazy_load and self._use_factory:\n   323\u2192            self._ensure_plugin_for_file(path)\n   324\u2192        \n   325\u2192        if self._enable_advanced and self._router:\n   326\u2192            route_results = self._router.route_file(path)\n   327\u2192            return [(result.plugin, result.confidence) for result in route_results]\n   328\u2192        else:\n   329\u2192            # Basic fallback\n   330\u2192            matching_plugins = []\n   331\u2192            for plugin in self._plugins:\n   332\u2192                if plugin.supports(path):\n   333\u2192                    matching_plugins.append((plugin, 1.0))\n   334\u2192            return matching_plugins\n   335\u2192    \n   336\u2192    def lookup(self, symbol: str) -> SymbolDef | None:\n   337\u2192        \"\"\"Look up symbol definition across all plugins.\"\"\"\n   338\u2192        start_time = time.time()\n   339\u2192        \n   340\u2192        try:\n   341\u2192            # For symbol lookup, we may need to search across all languages\n   342\u2192            # Load all plugins if using lazy loading\n   343\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344\u2192                self._load_all_plugins()\n   345\u2192            \n   346\u2192            if self._enable_advanced and self._aggregator:\n   347\u2192                # Use advanced aggregation\n   348\u2192                definitions_by_plugin = {}\n   349\u2192                for plugin in self._plugins:\n   350\u2192                    try:\n   351\u2192                        definition = plugin.getDefinition(symbol)\n   352\u2192                        definitions_by_plugin[plugin] = definition\n   353\u2192                    except Exception as e:\n   354\u2192                        logger.warning(f\"Plugin {plugin.lang} failed to get definition for {symbol}: {e}\")\n   355\u2192                        definitions_by_plugin[plugin] = None\n   356\u2192                \n   357\u2192                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358\u2192                \n   359\u2192                self._operation_stats['lookups'] += 1\n   360\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   361\u2192                \n   362\u2192                return result\n   363\u2192            else:\n   364\u2192                # Fallback to basic lookup\n   365\u2192                for p in self._plugins:\n   366\u2192                    res = p.getDefinition(symbol)\n   367\u2192                    if res:\n   368\u2192                        self._operation_stats['lookups'] += 1\n   369\u2192                        self._operation_stats['total_time'] += time.time() - start_time\n   370\u2192                        return res\n   371\u2192                return None\n   372\u2192                \n   373\u2192        except Exception as e:\n   374\u2192            logger.error(f\"Error in symbol lookup for {symbol}: {e}\", exc_info=True)\n   375\u2192            return None\n   376\u2192    \n   377\u2192    def _is_document_query(self, query: str) -> bool:\n   378\u2192        \"\"\"Check if the query is looking for documentation.\n   379\u2192        \n   380\u2192        Args:\n   381\u2192            query: Search query string\n   382\u2192            \n   383\u2192        Returns:\n   384\u2192            True if this appears to be a documentation query\n   385\u2192        \"\"\"\n   386\u2192        query_lower = query.lower()\n   387\u2192        \n   388\u2192        # Check against document query patterns\n   389\u2192        for pattern in self._compiled_doc_patterns:\n   390\u2192            if pattern.search(query_lower):\n   391\u2192                return True\n   392\u2192        \n   393\u2192        # Check for question words at the beginning\n   394\u2192        question_starters = ['how', 'what', 'where', 'when', 'why', 'can', 'is', 'does', 'should']\n   395\u2192        first_word = query_lower.split()[0] if query_lower.split() else ''\n   396\u2192        if first_word in question_starters:\n   397\u2192            return True\n   398\u2192        \n   399\u2192        return False\n   400\u2192    \n   401\u2192    def _expand_document_query(self, query: str) -> List[str]:\n   402\u2192        \"\"\"Expand a document query with related terms for better search coverage.\n   403\u2192        \n   404\u2192        Args:\n   405\u2192            query: Original search query\n   406\u2192            \n   407\u2192        Returns:\n   408\u2192            List of expanded query variations\n   409\u2192        \"\"\"\n   410\u2192        expanded_queries = [query]  # Always include original\n   411\u2192        query_lower = query.lower()\n   412\u2192        \n   413\u2192        # Common expansions for documentation queries\n   414\u2192        expansions = {\n   415\u2192            'install': ['installation', 'setup', 'getting started', 'requirements'],\n   416\u2192            'config': ['configuration', 'configure', 'settings', 'options', 'parameters'],\n   417\u2192            'api': ['api documentation', 'api reference', 'endpoint', 'method'],\n   418\u2192            'how to': ['tutorial', 'guide', 'example', 'usage'],\n   419\u2192            'example': ['sample', 'snippet', 'demo', 'code example'],\n   420\u2192            'error': ['troubleshoot', 'debug', 'issue', 'problem', 'fix'],\n   421\u2192            'getting started': ['quickstart', 'tutorial', 'introduction', 'setup'],\n   422\u2192            'guide': ['tutorial', 'documentation', 'walkthrough', 'how to'],\n   423\u2192            'usage': ['how to use', 'example', 'api', 'reference']\n   424\u2192        }\n   425\u2192        \n   426\u2192        # Apply expansions\n   427\u2192        for term, related_terms in expansions.items():\n   428\u2192            if term in query_lower:\n   429\u2192                for related in related_terms:\n   430\u2192                    # Replace the term with related term\n   431\u2192                    expanded = query_lower.replace(term, related)\n   432\u2192                    if expanded != query_lower and expanded not in expanded_queries:\n   433\u2192                        expanded_queries.append(expanded)\n   434\u2192                \n   435\u2192                # Also add queries with additional terms\n   436\u2192                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437\u2192                    expanded = f\"{query} {related}\"\n   438\u2192                    if expanded not in expanded_queries:\n   439\u2192                        expanded_queries.append(expanded)\n   440\u2192        \n   441\u2192        # Add file-specific searches for common documentation files\n   442\u2192        if self._is_document_query(query):\n   443\u2192            # Extract the main topic from the query\n   444\u2192            topic_words = []\n   445\u2192            for word in query.lower().split():\n   446\u2192                if word not in ['how', 'to', 'the', 'a', 'an', 'is', 'are', 'what', 'where', 'when']:\n   447\u2192                    topic_words.append(word)\n   448\u2192            \n   449\u2192            if topic_words:\n   450\u2192                topic = ' '.join(topic_words[:2])  # Use first two topic words\n   451\u2192                expanded_queries.extend([\n   452\u2192                    f\"README {topic}\",\n   453\u2192                    f\"{topic} documentation\",\n   454\u2192                    f\"{topic} docs\",\n   455\u2192                    f\"{topic} guide\"\n   456\u2192                ])\n   457\u2192        \n   458\u2192        # Remove duplicates while preserving order\n   459\u2192        seen = set()\n   460\u2192        unique_queries = []\n   461\u2192        for q in expanded_queries:\n   462\u2192            if q not in seen:\n   463\u2192                seen.add(q)\n   464\u2192                unique_queries.append(q)\n   465\u2192        \n   466\u2192        return unique_queries[:10]  # Limit to 10 queries max\n   467\u2192    \n   468\u2192    def _is_documentation_file(self, file_path: str) -> bool:\n   469\u2192        \"\"\"Check if a file path is likely a documentation file.\n   470\u2192        \n   471\u2192        Args:\n   472\u2192            file_path: Path to check\n   473\u2192            \n   474\u2192        Returns:\n   475\u2192            True if this appears to be a documentation file\n   476\u2192        \"\"\"\n   477\u2192        path_lower = file_path.lower()\n   478\u2192        \n   479\u2192        for pattern in self._compiled_file_patterns:\n   480\u2192            if pattern.search(path_lower):\n   481\u2192                return True\n   482\u2192        \n   483\u2192        return False\n   484\u2192    \n   485\u2192    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486\u2192        \"\"\"Adjust ranking to prioritize documentation files for document queries.\n   487\u2192        \n   488\u2192        Args:\n   489\u2192            query: Original search query\n   490\u2192            results: List of aggregated results\n   491\u2192            \n   492\u2192        Returns:\n   493\u2192            Re-ranked results with documentation prioritized\n   494\u2192        \"\"\"\n   495\u2192        if not self._is_document_query(query):\n   496\u2192            return results\n   497\u2192        \n   498\u2192        # Separate documentation and code results\n   499\u2192        doc_results = []\n   500\u2192        code_results = []\n   501\u2192        \n   502\u2192        for result in results:\n   503\u2192            if self._is_documentation_file(result.primary_result.get('file', '')):\n   504\u2192                # Boost documentation files for document queries\n   505\u2192                result.rank_score *= 1.5\n   506\u2192                result.metadata['doc_boost'] = True\n   507\u2192                doc_results.append(result)\n   508\u2192            else:\n   509\u2192                code_results.append(result)\n   510\u2192        \n   511\u2192        # Sort each group by rank score\n   512\u2192        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513\u2192        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514\u2192        \n   515\u2192        # Combine with documentation files first\n   516\u2192        return doc_results + code_results\n   517\u2192    \n   518\u2192    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519\u2192        \"\"\"Search for code and documentation across all plugins.\"\"\"\n   520\u2192        start_time = time.time()\n   521\u2192        \n   522\u2192        try:\n   523\u2192            # For search, we may need to search across all languages\n   524\u2192            # Load all plugins if using lazy loading\n   525\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526\u2192                self._load_all_plugins()\n   527\u2192            \n   528\u2192            # Detect if this is a document query\n   529\u2192            is_doc_query = self._is_document_query(query)\n   530\u2192            \n   531\u2192            # Expand query if it's a document query\n   532\u2192            queries = [query]\n   533\u2192            if is_doc_query:\n   534\u2192                queries = self._expand_document_query(query)\n   535\u2192                logger.info(f\"Expanded document query '{query}' to {len(queries)} variations\")\n   536\u2192                # Force semantic search for natural language queries\n   537\u2192                semantic = True\n   538\u2192            \n   539\u2192            if self._enable_advanced and self._aggregator:\n   540\u2192                # Use advanced aggregation\n   541\u2192                all_results_by_plugin = {}\n   542\u2192                opts = {\"semantic\": semantic, \"limit\": limit * 2 if is_doc_query else limit}\n   543\u2192                \n   544\u2192                # Search with all query variations\n   545\u2192                for search_query in queries:\n   546\u2192                    for plugin in self._plugins:\n   547\u2192                        try:\n   548\u2192                            results = list(plugin.search(search_query, opts))\n   549\u2192                            if results:\n   550\u2192                                if plugin not in all_results_by_plugin:\n   551\u2192                                    all_results_by_plugin[plugin] = []\n   552\u2192                                all_results_by_plugin[plugin].extend(results)\n   553\u2192                        except Exception as e:\n   554\u2192                            logger.warning(f\"Plugin {plugin.lang} failed to search for {search_query}: {e}\")\n   555\u2192                \n   556\u2192                # Deduplicate results per plugin\n   557\u2192                for plugin, results in all_results_by_plugin.items():\n   558\u2192                    seen = set()\n   559\u2192                    unique_results = []\n   560\u2192                    for result in results:\n   561\u2192                        key = f\"{result['file']}:{result['line']}\"\n   562\u2192                        if key not in seen:\n   563\u2192                            seen.add(key)\n   564\u2192                            unique_results.append(result)\n   565\u2192                    all_results_by_plugin[plugin] = unique_results\n   566\u2192                \n   567\u2192                # Configure aggregator for document queries\n   568\u2192                if is_doc_query and self._enable_advanced:\n   569\u2192                    # Adjust ranking criteria for documentation\n   570\u2192                    doc_criteria = RankingCriteria(\n   571\u2192                        relevance_weight=0.5,      # Increase relevance weight\n   572\u2192                        confidence_weight=0.2,     # Reduce confidence weight\n   573\u2192                        frequency_weight=0.2,      # Keep frequency weight\n   574\u2192                        recency_weight=0.1,        # Keep recency weight\n   575\u2192                        prefer_exact_matches=False,  # Natural language doesn't need exact matches\n   576\u2192                        boost_multiple_sources=True,\n   577\u2192                        boost_common_extensions=True\n   578\u2192                    )\n   579\u2192                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580\u2192                \n   581\u2192                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582\u2192                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583\u2192                )\n   584\u2192                \n   585\u2192                # Adjust ranking for document queries\n   586\u2192                if is_doc_query:\n   587\u2192                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588\u2192                \n   589\u2192                # Apply final limit\n   590\u2192                if limit and len(aggregated_results) > limit:\n   591\u2192                    aggregated_results = aggregated_results[:limit]\n   592\u2192                \n   593\u2192                logger.debug(f\"Search aggregation stats: {stats.total_results} total, \"\n   594\u2192                           f\"{stats.unique_results} unique, {stats.plugins_used} plugins used, \"\n   595\u2192                           f\"document_query={is_doc_query}\")\n   596\u2192                \n   597\u2192                self._operation_stats['searches'] += 1\n   598\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   599\u2192                \n   600\u2192                # Yield primary results from aggregated results\n   601\u2192                for aggregated in aggregated_results:\n   602\u2192                    yield aggregated.primary_result\n   603\u2192            else:\n   604\u2192                # Fallback to basic search\n   605\u2192                # Detect if this is a document query\n   606\u2192                is_doc_query = self._is_document_query(query)\n   607\u2192                \n   608\u2192                # Expand query if it's a document query\n   609\u2192                queries = [query]\n   610\u2192                if is_doc_query:\n   611\u2192                    queries = self._expand_document_query(query)\n   612\u2192                    semantic = True  # Force semantic search for natural language\n   613\u2192                \n   614\u2192                opts = {\"semantic\": semantic, \"limit\": limit}\n   615\u2192                all_results = []\n   616\u2192                \n   617\u2192                # Search with all query variations\n   618\u2192                for search_query in queries:\n   619\u2192                    for p in self._plugins:\n   620\u2192                        try:\n   621\u2192                            for result in p.search(search_query, opts):\n   622\u2192                                all_results.append(result)\n   623\u2192                        except Exception as e:\n   624\u2192                            logger.warning(f\"Plugin {p.lang} failed to search for {search_query}: {e}\")\n   625\u2192                \n   626\u2192                # Deduplicate results\n   627\u2192                seen = set()\n   628\u2192                unique_results = []\n   629\u2192                for result in all_results:\n   630\u2192                    key = f\"{result['file']}:{result['line']}\"\n   631\u2192                    if key not in seen:\n   632\u2192                        seen.add(key)\n   633\u2192                        unique_results.append(result)\n   634\u2192                \n   635\u2192                # Sort by score if available\n   636\u2192                unique_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   637\u2192                \n   638\u2192                # Prioritize documentation files for document queries\n   639\u2192                if is_doc_query:\n   640\u2192                    doc_results = []\n   641\u2192                    code_results = []\n   642\u2192                    for result in unique_results:\n   643\u2192                        if self._is_documentation_file(result.get('file', '')):\n   644\u2192                            doc_results.append(result)\n   645\u2192                        else:\n   646\u2192                            code_results.append(result)\n   647\u2192                    unique_results = doc_results + code_results\n   648\u2192                \n   649\u2192                # Apply limit\n   650\u2192                count = 0\n   651\u2192                for result in unique_results:\n   652\u2192                    if limit and count >= limit:\n   653\u2192                        break\n   654\u2192                    yield result\n   655\u2192                    count += 1\n   656\u2192                \n   657\u2192                self._operation_stats['searches'] += 1\n   658\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   659\u2192                \n   660\u2192        except Exception as e:\n   661\u2192            logger.error(f\"Error in search for {query}: {e}\", exc_info=True)\n   662\u2192    \n   663\u2192    def index_file(self, path: Path) -> None:\n   664\u2192        \"\"\"Index a single file if it has changed.\"\"\"\n   665\u2192        try:\n   666\u2192            # Ensure path is absolute to avoid relative/absolute path issues\n   667\u2192            path = path.resolve()\n   668\u2192            \n   669\u2192            # Find the appropriate plugin\n   670\u2192            plugin = self._match_plugin(path)\n   671\u2192            \n   672\u2192            # Read file content\n   673\u2192            try:\n   674\u2192                content = path.read_text(encoding='utf-8')\n   675\u2192            except UnicodeDecodeError:\n   676\u2192                # Try with different encodings\n   677\u2192                try:\n   678\u2192                    content = path.read_text(encoding='latin-1')\n   679\u2192                except Exception as e:\n   680\u2192                    logger.error(f\"Failed to read {path}: {e}\")\n   681\u2192                    return\n   682\u2192            \n   683\u2192            # Check if we need to re-index (simplified for now)\n   684\u2192            # TODO: Implement proper caching logic\n   685\u2192            \n   686\u2192            # Index the file\n   687\u2192            start_time = time.time()\n   688\u2192            logger.info(f\"Indexing {path} with {plugin.lang} plugin\")\n   689\u2192            shard = plugin.indexFile(path, content)\n   690\u2192            \n   691\u2192            # Record performance if advanced features enabled\n   692\u2192            if self._enable_advanced and self._router:\n   693\u2192                execution_time = time.time() - start_time\n   694\u2192                self._router.record_performance(plugin, execution_time)\n   695\u2192            \n   696\u2192            self._operation_stats['indexings'] += 1\n   697\u2192            self._operation_stats['total_time'] += time.time() - start_time\n   698\u2192            \n   699\u2192            logger.info(f\"Successfully indexed {path}: {len(shard.get('symbols', []))} symbols found\")\n   700\u2192            \n   701\u2192        except RuntimeError as e:\n   702\u2192            # No plugin found for this file type\n   703\u2192            logger.debug(f\"No plugin for {path}: {e}\")\n   704\u2192        except Exception as e:\n   705\u2192            logger.error(f\"Error indexing {path}: {e}\", exc_info=True)\n   706\u2192    \n   707\u2192    def get_statistics(self) -> dict:\n   708\u2192        \"\"\"Get comprehensive statistics across all plugins and components.\"\"\"\n   709\u2192        stats = {\n   710\u2192            \"total_plugins\": len(self._plugins),\n   711\u2192            \"loaded_languages\": sorted(list(self._loaded_languages)),\n   712\u2192            \"supported_languages\": len(self.supported_languages),\n   713\u2192            \"operations\": self._operation_stats.copy()\n   714\u2192        }\n   715\u2192        \n   716\u2192        # Add language breakdown\n   717\u2192        stats[\"by_language\"] = {}\n   718\u2192        for lang, plugin in self._by_lang.items():\n   719\u2192            plugin_info = {\n   720\u2192                \"loaded\": True,\n   721\u2192                \"class\": plugin.__class__.__name__\n   722\u2192            }\n   723\u2192            if hasattr(plugin, 'get_indexed_count'):\n   724\u2192                plugin_info[\"indexed_files\"] = plugin.get_indexed_count()\n   725\u2192            stats[\"by_language\"][lang] = plugin_info\n   726\u2192        \n   727\u2192        return stats\n   728\u2192    \n   729\u2192    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730\u2192        \"\"\"\n   731\u2192        Index all files in a directory, respecting ignore patterns.\n   732\u2192        \n   733\u2192        Args:\n   734\u2192            directory: Directory to index\n   735\u2192            recursive: Whether to index subdirectories\n   736\u2192            \n   737\u2192        Returns:\n   738\u2192            Statistics about indexed files\n   739\u2192        \"\"\"\n   740\u2192        logger.info(f\"Indexing directory: {directory} (recursive={recursive})\")\n   741\u2192        \n   742\u2192        # Note: We don't use ignore patterns during indexing\n   743\u2192        # ALL files are indexed for local search capability\n   744\u2192        # Filtering happens only during export/sharing\n   745\u2192        \n   746\u2192        # Get all supported extensions\n   747\u2192        supported_extensions = get_all_extensions()\n   748\u2192        \n   749\u2192        stats = {\n   750\u2192            \"total_files\": 0,\n   751\u2192            \"indexed_files\": 0,\n   752\u2192            \"ignored_files\": 0,\n   753\u2192            \"failed_files\": 0,\n   754\u2192            \"by_language\": {}\n   755\u2192        }\n   756\u2192        \n   757\u2192        # Walk directory\n   758\u2192        if recursive:\n   759\u2192            file_iterator = directory.rglob(\"*\")\n   760\u2192        else:\n   761\u2192            file_iterator = directory.glob(\"*\")\n   762\u2192            \n   763\u2192        for path in file_iterator:\n   764\u2192            if not path.is_file():\n   765\u2192                continue\n   766\u2192                \n   767\u2192            stats[\"total_files\"] += 1\n   768\u2192            \n   769\u2192            # NOTE: We index ALL files locally, including gitignored ones\n   770\u2192            # Filtering happens only during export/sharing\n   771\u2192            # This allows local search of .env, secrets, etc.\n   772\u2192                \n   773\u2192            # Try to find a plugin that supports this file\n   774\u2192            # This allows us to index ALL files, including .env, .key, etc.\n   775\u2192            try:\n   776\u2192                # First try to match by extension\n   777\u2192                if path.suffix in supported_extensions:\n   778\u2192                    self.index_file(path)\n   779\u2192                    stats[\"indexed_files\"] += 1\n   780\u2192                # For files without recognized extensions, try each plugin's supports() method\n   781\u2192                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782\u2192                else:\n   783\u2192                    matched = False\n   784\u2192                    for plugin in self._plugins:\n   785\u2192                        if plugin.supports(path):\n   786\u2192                            self.index_file(path)\n   787\u2192                            stats[\"indexed_files\"] += 1\n   788\u2192                            matched = True\n   789\u2192                            break\n   790\u2192                    \n   791\u2192                    # If no plugin matched but we want to index everything,\n   792\u2192                    # we could add a fallback here to index as plaintext\n   793\u2192                    # For now, we'll skip unmatched files\n   794\u2192                    if not matched:\n   795\u2192                        logger.debug(f\"No plugin found for {path}\")\n   796\u2192                \n   797\u2192                # Track by language\n   798\u2192                language = get_language_by_extension(path.suffix)\n   799\u2192                if language:\n   800\u2192                    stats[\"by_language\"][language] = stats[\"by_language\"].get(language, 0) + 1\n   801\u2192                    \n   802\u2192            except Exception as e:\n   803\u2192                logger.error(f\"Failed to index {path}: {e}\")\n   804\u2192                stats[\"failed_files\"] += 1\n   805\u2192                \n   806\u2192        logger.info(f\"Directory indexing complete: {stats['indexed_files']} indexed, \"\n   807\u2192                   f\"{stats['ignored_files']} ignored, {stats['failed_files']} failed\")\n   808\u2192        \n   809\u2192        return stats\n   810\u2192    \n   811\u2192    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812\u2192        \"\"\"Search specifically across documentation files.\n   813\u2192        \n   814\u2192        Args:\n   815\u2192            topic: Topic to search for (e.g., \"installation\", \"configuration\")\n   816\u2192            doc_types: Optional list of document types to search (e.g., [\"readme\", \"guide\", \"api\"])\n   817\u2192            limit: Maximum number of results\n   818\u2192            \n   819\u2192        Returns:\n   820\u2192            Search results from documentation files\n   821\u2192        \"\"\"\n   822\u2192        # Default document types if not specified\n   823\u2192        if doc_types is None:\n   824\u2192            doc_types = [\"readme\", \"documentation\", \"guide\", \"tutorial\", \"api\", \"changelog\", \"contributing\"]\n   825\u2192        \n   826\u2192        # Build search queries for different document types\n   827\u2192        queries = []\n   828\u2192        for doc_type in doc_types:\n   829\u2192            queries.extend([\n   830\u2192                f\"{doc_type} {topic}\",\n   831\u2192                f\"{topic} {doc_type}\",\n   832\u2192                f\"{topic} in {doc_type}\"\n   833\u2192            ])\n   834\u2192        \n   835\u2192        # Also search for the topic in common doc filenames\n   836\u2192        queries.extend([\n   837\u2192            f\"README {topic}\",\n   838\u2192            f\"CONTRIBUTING {topic}\",\n   839\u2192            f\"docs {topic}\",\n   840\u2192            f\"documentation {topic}\"\n   841\u2192        ])\n   842\u2192        \n   843\u2192        # Deduplicate queries\n   844\u2192        queries = list(dict.fromkeys(queries))\n   845\u2192        \n   846\u2192        logger.info(f\"Cross-document search for '{topic}' with {len(queries)} query variations\")\n   847\u2192        \n   848\u2192        # Use the enhanced search with document-specific handling\n   849\u2192        all_results = []\n   850\u2192        seen = set()\n   851\u2192        \n   852\u2192        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853\u2192            for result in self.search(query, semantic=True, limit=limit):\n   854\u2192                # Only include documentation files\n   855\u2192                if self._is_documentation_file(result.get('file', '')):\n   856\u2192                    key = f\"{result['file']}:{result['line']}\"\n   857\u2192                    if key not in seen:\n   858\u2192                        seen.add(key)\n   859\u2192                        all_results.append(result)\n   860\u2192        \n   861\u2192        # Sort by relevance (score) and return top results\n   862\u2192        all_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   863\u2192        \n   864\u2192        count = 0\n   865\u2192        for result in all_results:\n   866\u2192            if count >= limit:\n   867\u2192                break\n   868\u2192            yield result\n   869\u2192            count += 1\n   870\u2192    \n   871\u2192    def health_check(self) -> Dict[str, Any]:\n   872\u2192        \"\"\"Perform a health check on all components.\"\"\"\n   873\u2192        health = {\n   874\u2192            'status': 'healthy',\n   875\u2192            'components': {\n   876\u2192                'dispatcher': {\n   877\u2192                    'status': 'healthy',\n   878\u2192                    'plugins_loaded': len(self._plugins),\n   879\u2192                    'languages_supported': len(self.supported_languages),\n   880\u2192                    'factory_enabled': self._use_factory,\n   881\u2192                    'lazy_loading': self._lazy_load\n   882\u2192                }\n   883\u2192            },\n   884\u2192            'plugins': {},\n   885\u2192            'errors': []\n   886\u2192        }\n   887\u2192        \n   888\u2192        # Check plugin health\n   889\u2192        for lang, plugin in self._by_lang.items():\n   890\u2192            try:\n   891\u2192                plugin_health = {\n   892\u2192                    'status': 'healthy',\n   893\u2192                    'class': plugin.__class__.__name__,\n   894\u2192                    'semantic_enabled': getattr(plugin, '_enable_semantic', False)\n   895\u2192                }\n   896\u2192                if hasattr(plugin, 'get_indexed_count'):\n   897\u2192                    plugin_health['indexed_files'] = plugin.get_indexed_count()\n   898\u2192            except Exception as e:\n   899\u2192                plugin_health = {\n   900\u2192                    'status': 'error',\n   901\u2192                    'error': str(e)\n   902\u2192                }\n   903\u2192                health['errors'].append(f\"Plugin {lang}: {str(e)}\")\n   904\u2192            \n   905\u2192            health['plugins'][lang] = plugin_health\n   906\u2192        \n   907\u2192        # Determine overall health\n   908\u2192        if len(health['errors']) > 0:\n   909\u2192            health['status'] = 'degraded' if len(health['errors']) < 3 else 'unhealthy'\n   910\u2192        \n   911\u2192        return health\n   912\u2192    \n   913\u2192    def remove_file(self, path: Union[Path, str]) -> None:\n   914\u2192        \"\"\"Remove a file from all indexes.\n   915\u2192        \n   916\u2192        Args:\n   917\u2192            path: File path to remove\n   918\u2192        \"\"\"\n   919\u2192        path = Path(path).resolve()\n   920\u2192        logger.info(f\"Removing file from index: {path}\")\n   921\u2192        \n   922\u2192        try:\n   923\u2192            # Remove from SQLite if available\n   924\u2192            if self._sqlite_store:\n   925\u2192                from ..core.path_resolver import PathResolver\n   926\u2192                path_resolver = PathResolver()\n   927\u2192                try:\n   928\u2192                    relative_path = path_resolver.normalize_path(path)\n   929\u2192                    # Get repository ID - for now assume 1\n   930\u2192                    # TODO: Properly detect repository\n   931\u2192                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932\u2192                except Exception as e:\n   933\u2192                    logger.error(f\"Error removing from SQLite: {e}\")\n   934\u2192            \n   935\u2192            # Remove from semantic index if available\n   936\u2192            try:\n   937\u2192                plugin = self._match_plugin(path)\n   938\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   939\u2192                    plugin._indexer.remove_file(path)\n   940\u2192                    logger.info(f\"Removed from semantic index: {path}\")\n   941\u2192            except Exception as e:\n   942\u2192                logger.warning(f\"Error removing from semantic index: {e}\")\n   943\u2192            \n   944\u2192            # Update statistics\n   945\u2192            self._operation_stats['deletions'] = self._operation_stats.get('deletions', 0) + 1\n   946\u2192            \n   947\u2192        except Exception as e:\n   948\u2192            logger.error(f\"Error removing file {path}: {e}\", exc_info=True)\n   949\u2192    \n   950\u2192    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951\u2192        \"\"\"Move a file in all indexes.\n   952\u2192        \n   953\u2192        Args:\n   954\u2192            old_path: Original file path\n   955\u2192            new_path: New file path\n   956\u2192            content_hash: Optional content hash to verify unchanged content\n   957\u2192        \"\"\"\n   958\u2192        old_path = Path(old_path).resolve()\n   959\u2192        new_path = Path(new_path).resolve()\n   960\u2192        logger.info(f\"Moving file in index: {old_path} -> {new_path}\")\n   961\u2192        \n   962\u2192        try:\n   963\u2192            # Move in SQLite if available\n   964\u2192            if self._sqlite_store:\n   965\u2192                from ..core.path_resolver import PathResolver\n   966\u2192                path_resolver = PathResolver()\n   967\u2192                try:\n   968\u2192                    old_relative = path_resolver.normalize_path(old_path)\n   969\u2192                    new_relative = path_resolver.normalize_path(new_path)\n   970\u2192                    # Get repository ID - for now assume 1\n   971\u2192                    # TODO: Properly detect repository\n   972\u2192                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973\u2192                except Exception as e:\n   974\u2192                    logger.error(f\"Error moving in SQLite: {e}\")\n   975\u2192            \n   976\u2192            # Move in semantic index if available\n   977\u2192            try:\n   978\u2192                plugin = self._match_plugin(new_path)\n   979\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   980\u2192                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981\u2192                    logger.info(f\"Moved in semantic index: {old_path} -> {new_path}\")\n   982\u2192            except Exception as e:\n   983\u2192                logger.warning(f\"Error moving in semantic index: {e}\")\n   984\u2192            \n   985\u2192            # Update statistics\n   986\u2192            self._operation_stats['moves'] = self._operation_stats.get('moves', 0) + 1\n   987\u2192            \n   988\u2192        except Exception as e:\n   989\u2192            logger.error(f\"Error moving file {old_path} -> {new_path}: {e}\", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 14364,
    "cache_creation_tokens": 13324,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "78758208-1282-42af-ac35-ffe5b8f3d015",
    "test_date": "2025-06-26T07:36:25.905Z",
    "query_id": "transcript_78758208-1282-42af-ac35-ffe5b8f3d015",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 3,
    "output_tokens_total": 3,
    "cache_read_tokens": 13909,
    "cache_creation_tokens": 0,
    "tokens_per_result": 3,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "78758208-1282-42af-ac35-ffe5b8f3d015",
    "test_date": "2025-06-26T07:36:28.940Z",
    "query_id": "transcript_78758208-1282-42af-ac35-ffe5b8f3d015",
    "query_text": [
      {
        "tool_use_id": "toolu_01D1TMX7fxfiKurBHf3kehbw",
        "type": "tool_result",
        "content": "Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/path_management_architecture.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/path_management.puml"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 27,
    "output_tokens_total": 27,
    "cache_read_tokens": 14364,
    "cache_creation_tokens": 0,
    "tokens_per_result": 27,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "78758208-1282-42af-ac35-ffe5b8f3d015",
    "test_date": "2025-06-26T07:36:32.890Z",
    "query_id": "transcript_78758208-1282-42af-ac35-ffe5b8f3d015",
    "query_text": [
      {
        "tool_use_id": "toolu_01LU7y2MWZcf6K9pxbxitKFQ",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"Enhanced dispatcher with dynamic plugin loading via PluginFactory.\"\"\"\n     2\u2192from pathlib import Path\n     3\u2192from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4\u2192import logging\n     5\u2192import hashlib\n     6\u2192import time\n     7\u2192import re\n     8\u2192from datetime import datetime\n     9\u2192import os\n    10\u2192\n    11\u2192from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12\u2192from ..plugins.plugin_factory import PluginFactory\n    13\u2192from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14\u2192from ..storage.sqlite_store import SQLiteStore\n    15\u2192from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16\u2192from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17\u2192# Note: We've removed ignore pattern checks to allow indexing ALL files\n    18\u2192# Filtering happens only during export via SecureIndexExporter\n    19\u2192# from ..core.ignore_patterns import get_ignore_manager\n    20\u2192\n    21\u2192logger = logging.getLogger(__name__)\n    22\u2192\n    23\u2192\n    24\u2192class EnhancedDispatcher:\n    25\u2192    \"\"\"Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.\"\"\"\n    26\u2192    \n    27\u2192    # Document query patterns - common documentation search terms\n    28\u2192    DOCUMENT_QUERY_PATTERNS = [\n    29\u2192        r'\\b(how\\s+to|howto)\\b',\n    30\u2192        r'\\b(getting\\s+started|get\\s+started)\\b',\n    31\u2192        r'\\b(installation|install|setup)\\b',\n    32\u2192        r'\\b(configuration|configure|config)\\b',\n    33\u2192        r'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b',\n    34\u2192        r'\\b(tutorial|guide|walkthrough)\\b',\n    35\u2192        r'\\b(example|sample|snippet)\\b',\n    36\u2192        r'\\b(readme|documentation|docs)\\b',\n    37\u2192        r'\\b(usage|use\\s+case|using)\\b',\n    38\u2192        r'\\b(reference|manual)\\b',\n    39\u2192        r'\\b(faq|frequently\\s+asked)\\b',\n    40\u2192        r'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b',\n    41\u2192        r'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b',\n    42\u2192        r'\\b(architecture|design|overview)\\b',\n    43\u2192        r'\\b(changelog|release\\s+notes|migration)\\b'\n    44\u2192    ]\n    45\u2192    \n    46\u2192    # Documentation file patterns\n    47\u2192    DOCUMENTATION_FILE_PATTERNS = [\n    48\u2192        r'readme(\\.\\w+)?$',\n    49\u2192        r'changelog(\\.\\w+)?$',\n    50\u2192        r'contributing(\\.\\w+)?$',\n    51\u2192        r'license(\\.\\w+)?$',\n    52\u2192        r'install(\\.\\w+)?$',\n    53\u2192        r'setup(\\.\\w+)?$',\n    54\u2192        r'guide(\\.\\w+)?$',\n    55\u2192        r'tutorial(\\.\\w+)?$',\n    56\u2192        r'\\.md$',\n    57\u2192        r'\\.rst$',\n    58\u2192        r'\\.txt$',\n    59\u2192        r'docs?/',\n    60\u2192        r'documentation/'\n    61\u2192    ]\n    62\u2192    \n    63\u2192    def __init__(self, \n    64\u2192                 plugins: Optional[List[IPlugin]] = None,\n    65\u2192                 sqlite_store: Optional[SQLiteStore] = None,\n    66\u2192                 enable_advanced_features: bool = True,\n    67\u2192                 use_plugin_factory: bool = True,\n    68\u2192                 lazy_load: bool = True,\n    69\u2192                 semantic_search_enabled: bool = True):\n    70\u2192        \"\"\"Initialize the enhanced dispatcher.\n    71\u2192        \n    72\u2192        Args:\n    73\u2192            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74\u2192            sqlite_store: SQLite store for plugin persistence\n    75\u2192            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76\u2192            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77\u2192            lazy_load: Whether to lazy-load plugins on demand\n    78\u2192            semantic_search_enabled: Whether to enable semantic search in plugins\n    79\u2192        \"\"\"\n    80\u2192        self._sqlite_store = sqlite_store\n    81\u2192        self._enable_advanced = enable_advanced_features\n    82\u2192        self._use_factory = use_plugin_factory\n    83\u2192        self._lazy_load = lazy_load\n    84\u2192        self._semantic_enabled = semantic_search_enabled\n    85\u2192        \n    86\u2192        # Plugin storage\n    87\u2192        self._plugins: List[IPlugin] = []\n    88\u2192        self._by_lang: Dict[str, IPlugin] = {}\n    89\u2192        self._loaded_languages: set[str] = set()\n    90\u2192        \n    91\u2192        # Cache for file hashes to avoid re-indexing unchanged files\n    92\u2192        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93\u2192        \n    94\u2192        # Advanced components\n    95\u2192        if self._enable_advanced:\n    96\u2192            self._file_matcher = FileTypeMatcher()\n    97\u2192            self._router = PluginRouter(self._file_matcher)\n    98\u2192            self._aggregator = ResultAggregator()\n    99\u2192        \n   100\u2192        # Performance tracking\n   101\u2192        self._operation_stats = {\n   102\u2192            'searches': 0,\n   103\u2192            'lookups': 0,\n   104\u2192            'indexings': 0,\n   105\u2192            'total_time': 0.0,\n   106\u2192            'plugins_loaded': 0\n   107\u2192        }\n   108\u2192        \n   109\u2192        # Initialize plugins\n   110\u2192        if plugins:\n   111\u2192            # Use provided plugins (backward compatibility)\n   112\u2192            self._plugins = plugins\n   113\u2192            self._by_lang = {p.lang: p for p in plugins}\n   114\u2192            for plugin in plugins:\n   115\u2192                self._loaded_languages.add(getattr(plugin, 'lang', 'unknown'))\n   116\u2192            if self._enable_advanced:\n   117\u2192                self._register_plugins_with_router()\n   118\u2192        elif use_plugin_factory and not lazy_load:\n   119\u2192            # Load all plugins immediately\n   120\u2192            self._load_all_plugins()\n   121\u2192        # If lazy_load is True, plugins will be loaded on demand\n   122\u2192        \n   123\u2192        # Compile document query patterns for performance\n   124\u2192        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125\u2192        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126\u2192        \n   127\u2192        logger.info(f\"Enhanced dispatcher initialized with {len(self._plugins)} plugins\")\n   128\u2192    \n   129\u2192    def _load_all_plugins(self):\n   130\u2192        \"\"\"Load all available plugins using PluginFactory.\"\"\"\n   131\u2192        logger.info(\"Loading all available plugins...\")\n   132\u2192        \n   133\u2192        all_plugins = PluginFactory.create_all_plugins(\n   134\u2192            sqlite_store=self._sqlite_store,\n   135\u2192            enable_semantic=self._semantic_enabled\n   136\u2192        )\n   137\u2192        \n   138\u2192        for lang, plugin in all_plugins.items():\n   139\u2192            self._plugins.append(plugin)\n   140\u2192            self._by_lang[lang] = plugin\n   141\u2192            self._loaded_languages.add(lang)\n   142\u2192            self._operation_stats['plugins_loaded'] += 1\n   143\u2192        \n   144\u2192        if self._enable_advanced:\n   145\u2192            self._register_plugins_with_router()\n   146\u2192        \n   147\u2192        logger.info(f\"Loaded {len(all_plugins)} plugins: {', '.join(sorted(all_plugins.keys()))}\")\n   148\u2192    \n   149\u2192    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150\u2192        \"\"\"Ensure a plugin for the given language is loaded.\n   151\u2192        \n   152\u2192        Args:\n   153\u2192            language: Language code (e.g., 'python', 'go')\n   154\u2192            \n   155\u2192        Returns:\n   156\u2192            Plugin instance or None if not available\n   157\u2192        \"\"\"\n   158\u2192        # Normalize language\n   159\u2192        language = language.lower().replace('-', '_')\n   160\u2192        \n   161\u2192        # Check if already loaded\n   162\u2192        if language in self._by_lang:\n   163\u2192            return self._by_lang[language]\n   164\u2192        \n   165\u2192        # If not using factory or already tried to load, return None\n   166\u2192        if not self._use_factory or language in self._loaded_languages:\n   167\u2192            return None\n   168\u2192        \n   169\u2192        # Try to load the plugin\n   170\u2192        try:\n   171\u2192            logger.info(f\"Lazy loading plugin for {language}\")\n   172\u2192            plugin = PluginFactory.create_plugin(\n   173\u2192                language,\n   174\u2192                sqlite_store=self._sqlite_store,\n   175\u2192                enable_semantic=self._semantic_enabled\n   176\u2192            )\n   177\u2192            \n   178\u2192            # Add to collections\n   179\u2192            self._plugins.append(plugin)\n   180\u2192            self._by_lang[language] = plugin\n   181\u2192            self._loaded_languages.add(language)\n   182\u2192            self._operation_stats['plugins_loaded'] += 1\n   183\u2192            \n   184\u2192            # Register with router if needed\n   185\u2192            if self._enable_advanced:\n   186\u2192                capabilities = self._detect_plugin_capabilities(plugin)\n   187\u2192                self._router.register_plugin(plugin, capabilities)\n   188\u2192            \n   189\u2192            logger.info(f\"Successfully loaded {language} plugin\")\n   190\u2192            return plugin\n   191\u2192            \n   192\u2192        except ValueError as e:\n   193\u2192            logger.warning(f\"No plugin available for {language}: {e}\")\n   194\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   195\u2192            return None\n   196\u2192        except Exception as e:\n   197\u2192            logger.error(f\"Error loading plugin for {language}: {e}\")\n   198\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   199\u2192            return None\n   200\u2192    \n   201\u2192    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202\u2192        \"\"\"Ensure a plugin is loaded for the given file.\n   203\u2192        \n   204\u2192        Args:\n   205\u2192            path: File path\n   206\u2192            \n   207\u2192        Returns:\n   208\u2192            Plugin instance or None if not available\n   209\u2192        \"\"\"\n   210\u2192        # Get language from file extension\n   211\u2192        extension = path.suffix.lower()\n   212\u2192        language = get_language_by_extension(extension)\n   213\u2192        \n   214\u2192        if language:\n   215\u2192            return self._ensure_plugin_loaded(language)\n   216\u2192        \n   217\u2192        # Fallback: try all loaded plugins\n   218\u2192        for plugin in self._plugins:\n   219\u2192            if plugin.supports(path):\n   220\u2192                return plugin\n   221\u2192        \n   222\u2192        return None\n   223\u2192    \n   224\u2192    def _register_plugins_with_router(self):\n   225\u2192        \"\"\"Register plugins with the router and assign capabilities.\"\"\"\n   226\u2192        for plugin in self._plugins:\n   227\u2192            # Determine capabilities based on plugin type/language\n   228\u2192            capabilities = self._detect_plugin_capabilities(plugin)\n   229\u2192            self._router.register_plugin(plugin, capabilities)\n   230\u2192    \n   231\u2192    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232\u2192        \"\"\"Detect capabilities for a plugin based on its language and features.\"\"\"\n   233\u2192        capabilities = []\n   234\u2192        lang = getattr(plugin, 'lang', 'unknown')\n   235\u2192        \n   236\u2192        # Base capabilities all plugins have\n   237\u2192        capabilities.append(PluginCapability(\n   238\u2192            'syntax_analysis', '1.0', f'{lang} syntax analysis', \n   239\u2192            priority=70, metadata={'language': lang}\n   240\u2192        ))\n   241\u2192        \n   242\u2192        capabilities.append(PluginCapability(\n   243\u2192            'code_search', '1.0', f'{lang} code search', \n   244\u2192            priority=80, metadata={'language': lang}\n   245\u2192        ))\n   246\u2192        \n   247\u2192        # Check for semantic search capability\n   248\u2192        if hasattr(plugin, '_enable_semantic') and plugin._enable_semantic:\n   249\u2192            capabilities.append(PluginCapability(\n   250\u2192                'semantic_search', '1.0', f'{lang} semantic search', \n   251\u2192                priority=90, metadata={'language': lang}\n   252\u2192            ))\n   253\u2192        \n   254\u2192        # Language-specific capabilities\n   255\u2192        if lang == 'python':\n   256\u2192            capabilities.extend([\n   257\u2192                PluginCapability('refactoring', '1.0', 'Python refactoring support', 75),\n   258\u2192                PluginCapability('type_analysis', '1.0', 'Python type analysis', 85)\n   259\u2192            ])\n   260\u2192        elif lang in ['javascript', 'typescript']:\n   261\u2192            capabilities.extend([\n   262\u2192                PluginCapability('linting', '1.0', 'JavaScript/TypeScript linting', 85),\n   263\u2192                PluginCapability('bundling_analysis', '1.0', 'Module bundling analysis', 70),\n   264\u2192                PluginCapability('framework_support', '1.0', 'Framework-specific support', 75)\n   265\u2192            ])\n   266\u2192        elif lang in ['c', 'cpp']:\n   267\u2192            capabilities.extend([\n   268\u2192                PluginCapability('compilation_analysis', '1.0', 'Compilation analysis', 80),\n   269\u2192                PluginCapability('memory_analysis', '1.0', 'Memory usage analysis', 70),\n   270\u2192                PluginCapability('performance_profiling', '1.0', 'Performance profiling', 75)\n   271\u2192            ])\n   272\u2192        elif lang in ['go', 'rust']:\n   273\u2192            capabilities.extend([\n   274\u2192                PluginCapability('package_analysis', '1.0', f'{lang} package analysis', 80),\n   275\u2192                PluginCapability('concurrency_analysis', '1.0', f'{lang} concurrency analysis', 75)\n   276\u2192            ])\n   277\u2192        elif lang in ['java', 'kotlin', 'scala']:\n   278\u2192            capabilities.extend([\n   279\u2192                PluginCapability('jvm_analysis', '1.0', 'JVM bytecode analysis', 75),\n   280\u2192                PluginCapability('build_tool_integration', '1.0', 'Build tool integration', 70)\n   281\u2192            ])\n   282\u2192        \n   283\u2192        return capabilities\n   284\u2192    \n   285\u2192    @property\n   286\u2192    def plugins(self):\n   287\u2192        \"\"\"Get the dictionary of loaded plugins by language.\"\"\"\n   288\u2192        return self._by_lang\n   289\u2192    \n   290\u2192    @property\n   291\u2192    def supported_languages(self) -> List[str]:\n   292\u2192        \"\"\"Get list of all supported languages (loaded and available).\"\"\"\n   293\u2192        if self._use_factory:\n   294\u2192            return PluginFactory.get_supported_languages()\n   295\u2192        else:\n   296\u2192            return list(self._by_lang.keys())\n   297\u2192    \n   298\u2192    def _match_plugin(self, path: Path) -> IPlugin:\n   299\u2192        \"\"\"Match a plugin for the given file path.\"\"\"\n   300\u2192        # Ensure plugin is loaded if using lazy loading\n   301\u2192        if self._lazy_load and self._use_factory:\n   302\u2192            plugin = self._ensure_plugin_for_file(path)\n   303\u2192            if plugin:\n   304\u2192                return plugin\n   305\u2192        \n   306\u2192        # Use advanced routing if available\n   307\u2192        if self._enable_advanced and self._router:\n   308\u2192            route_result = self._router.get_best_plugin(path)\n   309\u2192            if route_result:\n   310\u2192                return route_result.plugin\n   311\u2192        \n   312\u2192        # Fallback to basic matching\n   313\u2192        for p in self._plugins:\n   314\u2192            if p.supports(path):\n   315\u2192                return p\n   316\u2192        \n   317\u2192        raise RuntimeError(f\"No plugin found for {path}\")\n   318\u2192    \n   319\u2192    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320\u2192        \"\"\"Get all plugins that can handle a file with confidence scores.\"\"\"\n   321\u2192        # Ensure plugin is loaded if using lazy loading\n   322\u2192        if self._lazy_load and self._use_factory:\n   323\u2192            self._ensure_plugin_for_file(path)\n   324\u2192        \n   325\u2192        if self._enable_advanced and self._router:\n   326\u2192            route_results = self._router.route_file(path)\n   327\u2192            return [(result.plugin, result.confidence) for result in route_results]\n   328\u2192        else:\n   329\u2192            # Basic fallback\n   330\u2192            matching_plugins = []\n   331\u2192            for plugin in self._plugins:\n   332\u2192                if plugin.supports(path):\n   333\u2192                    matching_plugins.append((plugin, 1.0))\n   334\u2192            return matching_plugins\n   335\u2192    \n   336\u2192    def lookup(self, symbol: str) -> SymbolDef | None:\n   337\u2192        \"\"\"Look up symbol definition across all plugins.\"\"\"\n   338\u2192        start_time = time.time()\n   339\u2192        \n   340\u2192        try:\n   341\u2192            # For symbol lookup, we may need to search across all languages\n   342\u2192            # Load all plugins if using lazy loading\n   343\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344\u2192                self._load_all_plugins()\n   345\u2192            \n   346\u2192            if self._enable_advanced and self._aggregator:\n   347\u2192                # Use advanced aggregation\n   348\u2192                definitions_by_plugin = {}\n   349\u2192                for plugin in self._plugins:\n   350\u2192                    try:\n   351\u2192                        definition = plugin.getDefinition(symbol)\n   352\u2192                        definitions_by_plugin[plugin] = definition\n   353\u2192                    except Exception as e:\n   354\u2192                        logger.warning(f\"Plugin {plugin.lang} failed to get definition for {symbol}: {e}\")\n   355\u2192                        definitions_by_plugin[plugin] = None\n   356\u2192                \n   357\u2192                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358\u2192                \n   359\u2192                self._operation_stats['lookups'] += 1\n   360\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   361\u2192                \n   362\u2192                return result\n   363\u2192            else:\n   364\u2192                # Fallback to basic lookup\n   365\u2192                for p in self._plugins:\n   366\u2192                    res = p.getDefinition(symbol)\n   367\u2192                    if res:\n   368\u2192                        self._operation_stats['lookups'] += 1\n   369\u2192                        self._operation_stats['total_time'] += time.time() - start_time\n   370\u2192                        return res\n   371\u2192                return None\n   372\u2192                \n   373\u2192        except Exception as e:\n   374\u2192            logger.error(f\"Error in symbol lookup for {symbol}: {e}\", exc_info=True)\n   375\u2192            return None\n   376\u2192    \n   377\u2192    def _is_document_query(self, query: str) -> bool:\n   378\u2192        \"\"\"Check if the query is looking for documentation.\n   379\u2192        \n   380\u2192        Args:\n   381\u2192            query: Search query string\n   382\u2192            \n   383\u2192        Returns:\n   384\u2192            True if this appears to be a documentation query\n   385\u2192        \"\"\"\n   386\u2192        query_lower = query.lower()\n   387\u2192        \n   388\u2192        # Check against document query patterns\n   389\u2192        for pattern in self._compiled_doc_patterns:\n   390\u2192            if pattern.search(query_lower):\n   391\u2192                return True\n   392\u2192        \n   393\u2192        # Check for question words at the beginning\n   394\u2192        question_starters = ['how', 'what', 'where', 'when', 'why', 'can', 'is', 'does', 'should']\n   395\u2192        first_word = query_lower.split()[0] if query_lower.split() else ''\n   396\u2192        if first_word in question_starters:\n   397\u2192            return True\n   398\u2192        \n   399\u2192        return False\n   400\u2192    \n   401\u2192    def _expand_document_query(self, query: str) -> List[str]:\n   402\u2192        \"\"\"Expand a document query with related terms for better search coverage.\n   403\u2192        \n   404\u2192        Args:\n   405\u2192            query: Original search query\n   406\u2192            \n   407\u2192        Returns:\n   408\u2192            List of expanded query variations\n   409\u2192        \"\"\"\n   410\u2192        expanded_queries = [query]  # Always include original\n   411\u2192        query_lower = query.lower()\n   412\u2192        \n   413\u2192        # Common expansions for documentation queries\n   414\u2192        expansions = {\n   415\u2192            'install': ['installation', 'setup', 'getting started', 'requirements'],\n   416\u2192            'config': ['configuration', 'configure', 'settings', 'options', 'parameters'],\n   417\u2192            'api': ['api documentation', 'api reference', 'endpoint', 'method'],\n   418\u2192            'how to': ['tutorial', 'guide', 'example', 'usage'],\n   419\u2192            'example': ['sample', 'snippet', 'demo', 'code example'],\n   420\u2192            'error': ['troubleshoot', 'debug', 'issue', 'problem', 'fix'],\n   421\u2192            'getting started': ['quickstart', 'tutorial', 'introduction', 'setup'],\n   422\u2192            'guide': ['tutorial', 'documentation', 'walkthrough', 'how to'],\n   423\u2192            'usage': ['how to use', 'example', 'api', 'reference']\n   424\u2192        }\n   425\u2192        \n   426\u2192        # Apply expansions\n   427\u2192        for term, related_terms in expansions.items():\n   428\u2192            if term in query_lower:\n   429\u2192                for related in related_terms:\n   430\u2192                    # Replace the term with related term\n   431\u2192                    expanded = query_lower.replace(term, related)\n   432\u2192                    if expanded != query_lower and expanded not in expanded_queries:\n   433\u2192                        expanded_queries.append(expanded)\n   434\u2192                \n   435\u2192                # Also add queries with additional terms\n   436\u2192                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437\u2192                    expanded = f\"{query} {related}\"\n   438\u2192                    if expanded not in expanded_queries:\n   439\u2192                        expanded_queries.append(expanded)\n   440\u2192        \n   441\u2192        # Add file-specific searches for common documentation files\n   442\u2192        if self._is_document_query(query):\n   443\u2192            # Extract the main topic from the query\n   444\u2192            topic_words = []\n   445\u2192            for word in query.lower().split():\n   446\u2192                if word not in ['how', 'to', 'the', 'a', 'an', 'is', 'are', 'what', 'where', 'when']:\n   447\u2192                    topic_words.append(word)\n   448\u2192            \n   449\u2192            if topic_words:\n   450\u2192                topic = ' '.join(topic_words[:2])  # Use first two topic words\n   451\u2192                expanded_queries.extend([\n   452\u2192                    f\"README {topic}\",\n   453\u2192                    f\"{topic} documentation\",\n   454\u2192                    f\"{topic} docs\",\n   455\u2192                    f\"{topic} guide\"\n   456\u2192                ])\n   457\u2192        \n   458\u2192        # Remove duplicates while preserving order\n   459\u2192        seen = set()\n   460\u2192        unique_queries = []\n   461\u2192        for q in expanded_queries:\n   462\u2192            if q not in seen:\n   463\u2192                seen.add(q)\n   464\u2192                unique_queries.append(q)\n   465\u2192        \n   466\u2192        return unique_queries[:10]  # Limit to 10 queries max\n   467\u2192    \n   468\u2192    def _is_documentation_file(self, file_path: str) -> bool:\n   469\u2192        \"\"\"Check if a file path is likely a documentation file.\n   470\u2192        \n   471\u2192        Args:\n   472\u2192            file_path: Path to check\n   473\u2192            \n   474\u2192        Returns:\n   475\u2192            True if this appears to be a documentation file\n   476\u2192        \"\"\"\n   477\u2192        path_lower = file_path.lower()\n   478\u2192        \n   479\u2192        for pattern in self._compiled_file_patterns:\n   480\u2192            if pattern.search(path_lower):\n   481\u2192                return True\n   482\u2192        \n   483\u2192        return False\n   484\u2192    \n   485\u2192    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486\u2192        \"\"\"Adjust ranking to prioritize documentation files for document queries.\n   487\u2192        \n   488\u2192        Args:\n   489\u2192            query: Original search query\n   490\u2192            results: List of aggregated results\n   491\u2192            \n   492\u2192        Returns:\n   493\u2192            Re-ranked results with documentation prioritized\n   494\u2192        \"\"\"\n   495\u2192        if not self._is_document_query(query):\n   496\u2192            return results\n   497\u2192        \n   498\u2192        # Separate documentation and code results\n   499\u2192        doc_results = []\n   500\u2192        code_results = []\n   501\u2192        \n   502\u2192        for result in results:\n   503\u2192            if self._is_documentation_file(result.primary_result.get('file', '')):\n   504\u2192                # Boost documentation files for document queries\n   505\u2192                result.rank_score *= 1.5\n   506\u2192                result.metadata['doc_boost'] = True\n   507\u2192                doc_results.append(result)\n   508\u2192            else:\n   509\u2192                code_results.append(result)\n   510\u2192        \n   511\u2192        # Sort each group by rank score\n   512\u2192        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513\u2192        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514\u2192        \n   515\u2192        # Combine with documentation files first\n   516\u2192        return doc_results + code_results\n   517\u2192    \n   518\u2192    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519\u2192        \"\"\"Search for code and documentation across all plugins.\"\"\"\n   520\u2192        start_time = time.time()\n   521\u2192        \n   522\u2192        try:\n   523\u2192            # For search, we may need to search across all languages\n   524\u2192            # Load all plugins if using lazy loading\n   525\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526\u2192                self._load_all_plugins()\n   527\u2192            \n   528\u2192            # Detect if this is a document query\n   529\u2192            is_doc_query = self._is_document_query(query)\n   530\u2192            \n   531\u2192            # Expand query if it's a document query\n   532\u2192            queries = [query]\n   533\u2192            if is_doc_query:\n   534\u2192                queries = self._expand_document_query(query)\n   535\u2192                logger.info(f\"Expanded document query '{query}' to {len(queries)} variations\")\n   536\u2192                # Force semantic search for natural language queries\n   537\u2192                semantic = True\n   538\u2192            \n   539\u2192            if self._enable_advanced and self._aggregator:\n   540\u2192                # Use advanced aggregation\n   541\u2192                all_results_by_plugin = {}\n   542\u2192                opts = {\"semantic\": semantic, \"limit\": limit * 2 if is_doc_query else limit}\n   543\u2192                \n   544\u2192                # Search with all query variations\n   545\u2192                for search_query in queries:\n   546\u2192                    for plugin in self._plugins:\n   547\u2192                        try:\n   548\u2192                            results = list(plugin.search(search_query, opts))\n   549\u2192                            if results:\n   550\u2192                                if plugin not in all_results_by_plugin:\n   551\u2192                                    all_results_by_plugin[plugin] = []\n   552\u2192                                all_results_by_plugin[plugin].extend(results)\n   553\u2192                        except Exception as e:\n   554\u2192                            logger.warning(f\"Plugin {plugin.lang} failed to search for {search_query}: {e}\")\n   555\u2192                \n   556\u2192                # Deduplicate results per plugin\n   557\u2192                for plugin, results in all_results_by_plugin.items():\n   558\u2192                    seen = set()\n   559\u2192                    unique_results = []\n   560\u2192                    for result in results:\n   561\u2192                        key = f\"{result['file']}:{result['line']}\"\n   562\u2192                        if key not in seen:\n   563\u2192                            seen.add(key)\n   564\u2192                            unique_results.append(result)\n   565\u2192                    all_results_by_plugin[plugin] = unique_results\n   566\u2192                \n   567\u2192                # Configure aggregator for document queries\n   568\u2192                if is_doc_query and self._enable_advanced:\n   569\u2192                    # Adjust ranking criteria for documentation\n   570\u2192                    doc_criteria = RankingCriteria(\n   571\u2192                        relevance_weight=0.5,      # Increase relevance weight\n   572\u2192                        confidence_weight=0.2,     # Reduce confidence weight\n   573\u2192                        frequency_weight=0.2,      # Keep frequency weight\n   574\u2192                        recency_weight=0.1,        # Keep recency weight\n   575\u2192                        prefer_exact_matches=False,  # Natural language doesn't need exact matches\n   576\u2192                        boost_multiple_sources=True,\n   577\u2192                        boost_common_extensions=True\n   578\u2192                    )\n   579\u2192                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580\u2192                \n   581\u2192                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582\u2192                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583\u2192                )\n   584\u2192                \n   585\u2192                # Adjust ranking for document queries\n   586\u2192                if is_doc_query:\n   587\u2192                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588\u2192                \n   589\u2192                # Apply final limit\n   590\u2192                if limit and len(aggregated_results) > limit:\n   591\u2192                    aggregated_results = aggregated_results[:limit]\n   592\u2192                \n   593\u2192                logger.debug(f\"Search aggregation stats: {stats.total_results} total, \"\n   594\u2192                           f\"{stats.unique_results} unique, {stats.plugins_used} plugins used, \"\n   595\u2192                           f\"document_query={is_doc_query}\")\n   596\u2192                \n   597\u2192                self._operation_stats['searches'] += 1\n   598\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   599\u2192                \n   600\u2192                # Yield primary results from aggregated results\n   601\u2192                for aggregated in aggregated_results:\n   602\u2192                    yield aggregated.primary_result\n   603\u2192            else:\n   604\u2192                # Fallback to basic search\n   605\u2192                # Detect if this is a document query\n   606\u2192                is_doc_query = self._is_document_query(query)\n   607\u2192                \n   608\u2192                # Expand query if it's a document query\n   609\u2192                queries = [query]\n   610\u2192                if is_doc_query:\n   611\u2192                    queries = self._expand_document_query(query)\n   612\u2192                    semantic = True  # Force semantic search for natural language\n   613\u2192                \n   614\u2192                opts = {\"semantic\": semantic, \"limit\": limit}\n   615\u2192                all_results = []\n   616\u2192                \n   617\u2192                # Search with all query variations\n   618\u2192                for search_query in queries:\n   619\u2192                    for p in self._plugins:\n   620\u2192                        try:\n   621\u2192                            for result in p.search(search_query, opts):\n   622\u2192                                all_results.append(result)\n   623\u2192                        except Exception as e:\n   624\u2192                            logger.warning(f\"Plugin {p.lang} failed to search for {search_query}: {e}\")\n   625\u2192                \n   626\u2192                # Deduplicate results\n   627\u2192                seen = set()\n   628\u2192                unique_results = []\n   629\u2192                for result in all_results:\n   630\u2192                    key = f\"{result['file']}:{result['line']}\"\n   631\u2192                    if key not in seen:\n   632\u2192                        seen.add(key)\n   633\u2192                        unique_results.append(result)\n   634\u2192                \n   635\u2192                # Sort by score if available\n   636\u2192                unique_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   637\u2192                \n   638\u2192                # Prioritize documentation files for document queries\n   639\u2192                if is_doc_query:\n   640\u2192                    doc_results = []\n   641\u2192                    code_results = []\n   642\u2192                    for result in unique_results:\n   643\u2192                        if self._is_documentation_file(result.get('file', '')):\n   644\u2192                            doc_results.append(result)\n   645\u2192                        else:\n   646\u2192                            code_results.append(result)\n   647\u2192                    unique_results = doc_results + code_results\n   648\u2192                \n   649\u2192                # Apply limit\n   650\u2192                count = 0\n   651\u2192                for result in unique_results:\n   652\u2192                    if limit and count >= limit:\n   653\u2192                        break\n   654\u2192                    yield result\n   655\u2192                    count += 1\n   656\u2192                \n   657\u2192                self._operation_stats['searches'] += 1\n   658\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   659\u2192                \n   660\u2192        except Exception as e:\n   661\u2192            logger.error(f\"Error in search for {query}: {e}\", exc_info=True)\n   662\u2192    \n   663\u2192    def index_file(self, path: Path) -> None:\n   664\u2192        \"\"\"Index a single file if it has changed.\"\"\"\n   665\u2192        try:\n   666\u2192            # Ensure path is absolute to avoid relative/absolute path issues\n   667\u2192            path = path.resolve()\n   668\u2192            \n   669\u2192            # Find the appropriate plugin\n   670\u2192            plugin = self._match_plugin(path)\n   671\u2192            \n   672\u2192            # Read file content\n   673\u2192            try:\n   674\u2192                content = path.read_text(encoding='utf-8')\n   675\u2192            except UnicodeDecodeError:\n   676\u2192                # Try with different encodings\n   677\u2192                try:\n   678\u2192                    content = path.read_text(encoding='latin-1')\n   679\u2192                except Exception as e:\n   680\u2192                    logger.error(f\"Failed to read {path}: {e}\")\n   681\u2192                    return\n   682\u2192            \n   683\u2192            # Check if we need to re-index (simplified for now)\n   684\u2192            # TODO: Implement proper caching logic\n   685\u2192            \n   686\u2192            # Index the file\n   687\u2192            start_time = time.time()\n   688\u2192            logger.info(f\"Indexing {path} with {plugin.lang} plugin\")\n   689\u2192            shard = plugin.indexFile(path, content)\n   690\u2192            \n   691\u2192            # Record performance if advanced features enabled\n   692\u2192            if self._enable_advanced and self._router:\n   693\u2192                execution_time = time.time() - start_time\n   694\u2192                self._router.record_performance(plugin, execution_time)\n   695\u2192            \n   696\u2192            self._operation_stats['indexings'] += 1\n   697\u2192            self._operation_stats['total_time'] += time.time() - start_time\n   698\u2192            \n   699\u2192            logger.info(f\"Successfully indexed {path}: {len(shard.get('symbols', []))} symbols found\")\n   700\u2192            \n   701\u2192        except RuntimeError as e:\n   702\u2192            # No plugin found for this file type\n   703\u2192            logger.debug(f\"No plugin for {path}: {e}\")\n   704\u2192        except Exception as e:\n   705\u2192            logger.error(f\"Error indexing {path}: {e}\", exc_info=True)\n   706\u2192    \n   707\u2192    def get_statistics(self) -> dict:\n   708\u2192        \"\"\"Get comprehensive statistics across all plugins and components.\"\"\"\n   709\u2192        stats = {\n   710\u2192            \"total_plugins\": len(self._plugins),\n   711\u2192            \"loaded_languages\": sorted(list(self._loaded_languages)),\n   712\u2192            \"supported_languages\": len(self.supported_languages),\n   713\u2192            \"operations\": self._operation_stats.copy()\n   714\u2192        }\n   715\u2192        \n   716\u2192        # Add language breakdown\n   717\u2192        stats[\"by_language\"] = {}\n   718\u2192        for lang, plugin in self._by_lang.items():\n   719\u2192            plugin_info = {\n   720\u2192                \"loaded\": True,\n   721\u2192                \"class\": plugin.__class__.__name__\n   722\u2192            }\n   723\u2192            if hasattr(plugin, 'get_indexed_count'):\n   724\u2192                plugin_info[\"indexed_files\"] = plugin.get_indexed_count()\n   725\u2192            stats[\"by_language\"][lang] = plugin_info\n   726\u2192        \n   727\u2192        return stats\n   728\u2192    \n   729\u2192    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730\u2192        \"\"\"\n   731\u2192        Index all files in a directory, respecting ignore patterns.\n   732\u2192        \n   733\u2192        Args:\n   734\u2192            directory: Directory to index\n   735\u2192            recursive: Whether to index subdirectories\n   736\u2192            \n   737\u2192        Returns:\n   738\u2192            Statistics about indexed files\n   739\u2192        \"\"\"\n   740\u2192        logger.info(f\"Indexing directory: {directory} (recursive={recursive})\")\n   741\u2192        \n   742\u2192        # Note: We don't use ignore patterns during indexing\n   743\u2192        # ALL files are indexed for local search capability\n   744\u2192        # Filtering happens only during export/sharing\n   745\u2192        \n   746\u2192        # Get all supported extensions\n   747\u2192        supported_extensions = get_all_extensions()\n   748\u2192        \n   749\u2192        stats = {\n   750\u2192            \"total_files\": 0,\n   751\u2192            \"indexed_files\": 0,\n   752\u2192            \"ignored_files\": 0,\n   753\u2192            \"failed_files\": 0,\n   754\u2192            \"by_language\": {}\n   755\u2192        }\n   756\u2192        \n   757\u2192        # Walk directory\n   758\u2192        if recursive:\n   759\u2192            file_iterator = directory.rglob(\"*\")\n   760\u2192        else:\n   761\u2192            file_iterator = directory.glob(\"*\")\n   762\u2192            \n   763\u2192        for path in file_iterator:\n   764\u2192            if not path.is_file():\n   765\u2192                continue\n   766\u2192                \n   767\u2192            stats[\"total_files\"] += 1\n   768\u2192            \n   769\u2192            # NOTE: We index ALL files locally, including gitignored ones\n   770\u2192            # Filtering happens only during export/sharing\n   771\u2192            # This allows local search of .env, secrets, etc.\n   772\u2192                \n   773\u2192            # Try to find a plugin that supports this file\n   774\u2192            # This allows us to index ALL files, including .env, .key, etc.\n   775\u2192            try:\n   776\u2192                # First try to match by extension\n   777\u2192                if path.suffix in supported_extensions:\n   778\u2192                    self.index_file(path)\n   779\u2192                    stats[\"indexed_files\"] += 1\n   780\u2192                # For files without recognized extensions, try each plugin's supports() method\n   781\u2192                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782\u2192                else:\n   783\u2192                    matched = False\n   784\u2192                    for plugin in self._plugins:\n   785\u2192                        if plugin.supports(path):\n   786\u2192                            self.index_file(path)\n   787\u2192                            stats[\"indexed_files\"] += 1\n   788\u2192                            matched = True\n   789\u2192                            break\n   790\u2192                    \n   791\u2192                    # If no plugin matched but we want to index everything,\n   792\u2192                    # we could add a fallback here to index as plaintext\n   793\u2192                    # For now, we'll skip unmatched files\n   794\u2192                    if not matched:\n   795\u2192                        logger.debug(f\"No plugin found for {path}\")\n   796\u2192                \n   797\u2192                # Track by language\n   798\u2192                language = get_language_by_extension(path.suffix)\n   799\u2192                if language:\n   800\u2192                    stats[\"by_language\"][language] = stats[\"by_language\"].get(language, 0) + 1\n   801\u2192                    \n   802\u2192            except Exception as e:\n   803\u2192                logger.error(f\"Failed to index {path}: {e}\")\n   804\u2192                stats[\"failed_files\"] += 1\n   805\u2192                \n   806\u2192        logger.info(f\"Directory indexing complete: {stats['indexed_files']} indexed, \"\n   807\u2192                   f\"{stats['ignored_files']} ignored, {stats['failed_files']} failed\")\n   808\u2192        \n   809\u2192        return stats\n   810\u2192    \n   811\u2192    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812\u2192        \"\"\"Search specifically across documentation files.\n   813\u2192        \n   814\u2192        Args:\n   815\u2192            topic: Topic to search for (e.g., \"installation\", \"configuration\")\n   816\u2192            doc_types: Optional list of document types to search (e.g., [\"readme\", \"guide\", \"api\"])\n   817\u2192            limit: Maximum number of results\n   818\u2192            \n   819\u2192        Returns:\n   820\u2192            Search results from documentation files\n   821\u2192        \"\"\"\n   822\u2192        # Default document types if not specified\n   823\u2192        if doc_types is None:\n   824\u2192            doc_types = [\"readme\", \"documentation\", \"guide\", \"tutorial\", \"api\", \"changelog\", \"contributing\"]\n   825\u2192        \n   826\u2192        # Build search queries for different document types\n   827\u2192        queries = []\n   828\u2192        for doc_type in doc_types:\n   829\u2192            queries.extend([\n   830\u2192                f\"{doc_type} {topic}\",\n   831\u2192                f\"{topic} {doc_type}\",\n   832\u2192                f\"{topic} in {doc_type}\"\n   833\u2192            ])\n   834\u2192        \n   835\u2192        # Also search for the topic in common doc filenames\n   836\u2192        queries.extend([\n   837\u2192            f\"README {topic}\",\n   838\u2192            f\"CONTRIBUTING {topic}\",\n   839\u2192            f\"docs {topic}\",\n   840\u2192            f\"documentation {topic}\"\n   841\u2192        ])\n   842\u2192        \n   843\u2192        # Deduplicate queries\n   844\u2192        queries = list(dict.fromkeys(queries))\n   845\u2192        \n   846\u2192        logger.info(f\"Cross-document search for '{topic}' with {len(queries)} query variations\")\n   847\u2192        \n   848\u2192        # Use the enhanced search with document-specific handling\n   849\u2192        all_results = []\n   850\u2192        seen = set()\n   851\u2192        \n   852\u2192        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853\u2192            for result in self.search(query, semantic=True, limit=limit):\n   854\u2192                # Only include documentation files\n   855\u2192                if self._is_documentation_file(result.get('file', '')):\n   856\u2192                    key = f\"{result['file']}:{result['line']}\"\n   857\u2192                    if key not in seen:\n   858\u2192                        seen.add(key)\n   859\u2192                        all_results.append(result)\n   860\u2192        \n   861\u2192        # Sort by relevance (score) and return top results\n   862\u2192        all_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   863\u2192        \n   864\u2192        count = 0\n   865\u2192        for result in all_results:\n   866\u2192            if count >= limit:\n   867\u2192                break\n   868\u2192            yield result\n   869\u2192            count += 1\n   870\u2192    \n   871\u2192    def health_check(self) -> Dict[str, Any]:\n   872\u2192        \"\"\"Perform a health check on all components.\"\"\"\n   873\u2192        health = {\n   874\u2192            'status': 'healthy',\n   875\u2192            'components': {\n   876\u2192                'dispatcher': {\n   877\u2192                    'status': 'healthy',\n   878\u2192                    'plugins_loaded': len(self._plugins),\n   879\u2192                    'languages_supported': len(self.supported_languages),\n   880\u2192                    'factory_enabled': self._use_factory,\n   881\u2192                    'lazy_loading': self._lazy_load\n   882\u2192                }\n   883\u2192            },\n   884\u2192            'plugins': {},\n   885\u2192            'errors': []\n   886\u2192        }\n   887\u2192        \n   888\u2192        # Check plugin health\n   889\u2192        for lang, plugin in self._by_lang.items():\n   890\u2192            try:\n   891\u2192                plugin_health = {\n   892\u2192                    'status': 'healthy',\n   893\u2192                    'class': plugin.__class__.__name__,\n   894\u2192                    'semantic_enabled': getattr(plugin, '_enable_semantic', False)\n   895\u2192                }\n   896\u2192                if hasattr(plugin, 'get_indexed_count'):\n   897\u2192                    plugin_health['indexed_files'] = plugin.get_indexed_count()\n   898\u2192            except Exception as e:\n   899\u2192                plugin_health = {\n   900\u2192                    'status': 'error',\n   901\u2192                    'error': str(e)\n   902\u2192                }\n   903\u2192                health['errors'].append(f\"Plugin {lang}: {str(e)}\")\n   904\u2192            \n   905\u2192            health['plugins'][lang] = plugin_health\n   906\u2192        \n   907\u2192        # Determine overall health\n   908\u2192        if len(health['errors']) > 0:\n   909\u2192            health['status'] = 'degraded' if len(health['errors']) < 3 else 'unhealthy'\n   910\u2192        \n   911\u2192        return health\n   912\u2192    \n   913\u2192    def remove_file(self, path: Union[Path, str]) -> None:\n   914\u2192        \"\"\"Remove a file from all indexes.\n   915\u2192        \n   916\u2192        Args:\n   917\u2192            path: File path to remove\n   918\u2192        \"\"\"\n   919\u2192        path = Path(path).resolve()\n   920\u2192        logger.info(f\"Removing file from index: {path}\")\n   921\u2192        \n   922\u2192        try:\n   923\u2192            # Remove from SQLite if available\n   924\u2192            if self._sqlite_store:\n   925\u2192                from ..core.path_resolver import PathResolver\n   926\u2192                path_resolver = PathResolver()\n   927\u2192                try:\n   928\u2192                    relative_path = path_resolver.normalize_path(path)\n   929\u2192                    # Get repository ID - for now assume 1\n   930\u2192                    # TODO: Properly detect repository\n   931\u2192                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932\u2192                except Exception as e:\n   933\u2192                    logger.error(f\"Error removing from SQLite: {e}\")\n   934\u2192            \n   935\u2192            # Remove from semantic index if available\n   936\u2192            try:\n   937\u2192                plugin = self._match_plugin(path)\n   938\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   939\u2192                    plugin._indexer.remove_file(path)\n   940\u2192                    logger.info(f\"Removed from semantic index: {path}\")\n   941\u2192            except Exception as e:\n   942\u2192                logger.warning(f\"Error removing from semantic index: {e}\")\n   943\u2192            \n   944\u2192            # Update statistics\n   945\u2192            self._operation_stats['deletions'] = self._operation_stats.get('deletions', 0) + 1\n   946\u2192            \n   947\u2192        except Exception as e:\n   948\u2192            logger.error(f\"Error removing file {path}: {e}\", exc_info=True)\n   949\u2192    \n   950\u2192    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951\u2192        \"\"\"Move a file in all indexes.\n   952\u2192        \n   953\u2192        Args:\n   954\u2192            old_path: Original file path\n   955\u2192            new_path: New file path\n   956\u2192            content_hash: Optional content hash to verify unchanged content\n   957\u2192        \"\"\"\n   958\u2192        old_path = Path(old_path).resolve()\n   959\u2192        new_path = Path(new_path).resolve()\n   960\u2192        logger.info(f\"Moving file in index: {old_path} -> {new_path}\")\n   961\u2192        \n   962\u2192        try:\n   963\u2192            # Move in SQLite if available\n   964\u2192            if self._sqlite_store:\n   965\u2192                from ..core.path_resolver import PathResolver\n   966\u2192                path_resolver = PathResolver()\n   967\u2192                try:\n   968\u2192                    old_relative = path_resolver.normalize_path(old_path)\n   969\u2192                    new_relative = path_resolver.normalize_path(new_path)\n   970\u2192                    # Get repository ID - for now assume 1\n   971\u2192                    # TODO: Properly detect repository\n   972\u2192                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973\u2192                except Exception as e:\n   974\u2192                    logger.error(f\"Error moving in SQLite: {e}\")\n   975\u2192            \n   976\u2192            # Move in semantic index if available\n   977\u2192            try:\n   978\u2192                plugin = self._match_plugin(new_path)\n   979\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   980\u2192                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981\u2192                    logger.info(f\"Moved in semantic index: {old_path} -> {new_path}\")\n   982\u2192            except Exception as e:\n   983\u2192                logger.warning(f\"Error moving in semantic index: {e}\")\n   984\u2192            \n   985\u2192            # Update statistics\n   986\u2192            self._operation_stats['moves'] = self._operation_stats.get('moves', 0) + 1\n   987\u2192            \n   988\u2192        except Exception as e:\n   989\u2192            logger.error(f\"Error moving file {old_path} -> {new_path}: {e}\", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 27688,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "c3c8369b-4664-47f4-b3f9-7436700938d0",
    "test_date": "2025-06-26T07:36:47.054Z",
    "query_id": "transcript_c3c8369b-4664-47f4-b3f9-7436700938d0",
    "query_text": "Find all functions that use asyncio",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13902,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "c3c8369b-4664-47f4-b3f9-7436700938d0",
    "test_date": "2025-06-26T07:36:50.620Z",
    "query_id": "transcript_c3c8369b-4664-47f4-b3f9-7436700938d0",
    "query_text": [
      {
        "tool_use_id": "toolu_01AcEHuEH4mB9MzGjyLaSgkV",
        "type": "tool_result",
        "content": "Found 78 files\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/conftest.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_developer_workflows.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_repository_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_semantic_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_plugins_direct.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/populate_bm25_quick.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/pytest.ini\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/requirements-production.txt\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/requirements.txt\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/demo_reranking_working.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/docs/development/TESTING-GUIDE.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/codex-setup.sh\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/prometheus_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/watchdog.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/architecture/level4/file_watcher.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/grpc_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-mcp/ai_docs/MCP.md"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "mcp",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 33,
    "output_tokens_total": 33,
    "cache_read_tokens": 13902,
    "cache_creation_tokens": 5379,
    "tokens_per_result": 33,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "f86b4d58-381b-4fe7-9d8b-38906cd581f9",
    "test_date": "2025-06-26T07:32:52.426Z",
    "query_id": "transcript_f86b4d58-381b-4fe7-9d8b-38906cd581f9",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 122,
    "output_tokens_total": 122,
    "cache_read_tokens": 13920,
    "cache_creation_tokens": 0,
    "tokens_per_result": 122,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "f86b4d58-381b-4fe7-9d8b-38906cd581f9",
    "test_date": "2025-06-26T07:32:56.802Z",
    "query_id": "transcript_f86b4d58-381b-4fe7-9d8b-38906cd581f9",
    "query_text": "Search for the EnhancedDispatcher class in the codebase. Find the file that contains this class definition and identify its main methods. Return the file path, line numbers, and a list of the main methods with their signatures.",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 91,
    "output_tokens_total": 91,
    "cache_read_tokens": 9986,
    "cache_creation_tokens": 783,
    "tokens_per_result": 91,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "f86b4d58-381b-4fe7-9d8b-38906cd581f9",
    "test_date": "2025-06-26T07:33:00.558Z",
    "query_id": "transcript_f86b4d58-381b-4fe7-9d8b-38906cd581f9",
    "query_text": [
      {
        "tool_use_id": "toolu_01YFRY2V6iZSnN5beWPsuq6n",
        "type": "tool_result",
        "content": "Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/level4/path_management.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/path_management_architecture.md"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 106,
    "output_tokens_total": 106,
    "cache_read_tokens": 10769,
    "cache_creation_tokens": 236,
    "tokens_per_result": 106,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "f86b4d58-381b-4fe7-9d8b-38906cd581f9",
    "test_date": "2025-06-26T07:33:04.269Z",
    "query_id": "transcript_f86b4d58-381b-4fe7-9d8b-38906cd581f9",
    "query_text": [
      {
        "tool_use_id": "toolu_01UM3pix73LxqfJCVR9R39iB",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"Enhanced dispatcher with dynamic plugin loading via PluginFactory.\"\"\"\n     2\u2192from pathlib import Path\n     3\u2192from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4\u2192import logging\n     5\u2192import hashlib\n     6\u2192import time\n     7\u2192import re\n     8\u2192from datetime import datetime\n     9\u2192import os\n    10\u2192\n    11\u2192from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12\u2192from ..plugins.plugin_factory import PluginFactory\n    13\u2192from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14\u2192from ..storage.sqlite_store import SQLiteStore\n    15\u2192from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16\u2192from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17\u2192# Note: We've removed ignore pattern checks to allow indexing ALL files\n    18\u2192# Filtering happens only during export via SecureIndexExporter\n    19\u2192# from ..core.ignore_patterns import get_ignore_manager\n    20\u2192\n    21\u2192logger = logging.getLogger(__name__)\n    22\u2192\n    23\u2192\n    24\u2192class EnhancedDispatcher:\n    25\u2192    \"\"\"Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.\"\"\"\n    26\u2192    \n    27\u2192    # Document query patterns - common documentation search terms\n    28\u2192    DOCUMENT_QUERY_PATTERNS = [\n    29\u2192        r'\\b(how\\s+to|howto)\\b',\n    30\u2192        r'\\b(getting\\s+started|get\\s+started)\\b',\n    31\u2192        r'\\b(installation|install|setup)\\b',\n    32\u2192        r'\\b(configuration|configure|config)\\b',\n    33\u2192        r'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b',\n    34\u2192        r'\\b(tutorial|guide|walkthrough)\\b',\n    35\u2192        r'\\b(example|sample|snippet)\\b',\n    36\u2192        r'\\b(readme|documentation|docs)\\b',\n    37\u2192        r'\\b(usage|use\\s+case|using)\\b',\n    38\u2192        r'\\b(reference|manual)\\b',\n    39\u2192        r'\\b(faq|frequently\\s+asked)\\b',\n    40\u2192        r'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b',\n    41\u2192        r'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b',\n    42\u2192        r'\\b(architecture|design|overview)\\b',\n    43\u2192        r'\\b(changelog|release\\s+notes|migration)\\b'\n    44\u2192    ]\n    45\u2192    \n    46\u2192    # Documentation file patterns\n    47\u2192    DOCUMENTATION_FILE_PATTERNS = [\n    48\u2192        r'readme(\\.\\w+)?$',\n    49\u2192        r'changelog(\\.\\w+)?$',\n    50\u2192        r'contributing(\\.\\w+)?$',\n    51\u2192        r'license(\\.\\w+)?$',\n    52\u2192        r'install(\\.\\w+)?$',\n    53\u2192        r'setup(\\.\\w+)?$',\n    54\u2192        r'guide(\\.\\w+)?$',\n    55\u2192        r'tutorial(\\.\\w+)?$',\n    56\u2192        r'\\.md$',\n    57\u2192        r'\\.rst$',\n    58\u2192        r'\\.txt$',\n    59\u2192        r'docs?/',\n    60\u2192        r'documentation/'\n    61\u2192    ]\n    62\u2192    \n    63\u2192    def __init__(self, \n    64\u2192                 plugins: Optional[List[IPlugin]] = None,\n    65\u2192                 sqlite_store: Optional[SQLiteStore] = None,\n    66\u2192                 enable_advanced_features: bool = True,\n    67\u2192                 use_plugin_factory: bool = True,\n    68\u2192                 lazy_load: bool = True,\n    69\u2192                 semantic_search_enabled: bool = True):\n    70\u2192        \"\"\"Initialize the enhanced dispatcher.\n    71\u2192        \n    72\u2192        Args:\n    73\u2192            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74\u2192            sqlite_store: SQLite store for plugin persistence\n    75\u2192            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76\u2192            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77\u2192            lazy_load: Whether to lazy-load plugins on demand\n    78\u2192            semantic_search_enabled: Whether to enable semantic search in plugins\n    79\u2192        \"\"\"\n    80\u2192        self._sqlite_store = sqlite_store\n    81\u2192        self._enable_advanced = enable_advanced_features\n    82\u2192        self._use_factory = use_plugin_factory\n    83\u2192        self._lazy_load = lazy_load\n    84\u2192        self._semantic_enabled = semantic_search_enabled\n    85\u2192        \n    86\u2192        # Plugin storage\n    87\u2192        self._plugins: List[IPlugin] = []\n    88\u2192        self._by_lang: Dict[str, IPlugin] = {}\n    89\u2192        self._loaded_languages: set[str] = set()\n    90\u2192        \n    91\u2192        # Cache for file hashes to avoid re-indexing unchanged files\n    92\u2192        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93\u2192        \n    94\u2192        # Advanced components\n    95\u2192        if self._enable_advanced:\n    96\u2192            self._file_matcher = FileTypeMatcher()\n    97\u2192            self._router = PluginRouter(self._file_matcher)\n    98\u2192            self._aggregator = ResultAggregator()\n    99\u2192        \n   100\u2192        # Performance tracking\n   101\u2192        self._operation_stats = {\n   102\u2192            'searches': 0,\n   103\u2192            'lookups': 0,\n   104\u2192            'indexings': 0,\n   105\u2192            'total_time': 0.0,\n   106\u2192            'plugins_loaded': 0\n   107\u2192        }\n   108\u2192        \n   109\u2192        # Initialize plugins\n   110\u2192        if plugins:\n   111\u2192            # Use provided plugins (backward compatibility)\n   112\u2192            self._plugins = plugins\n   113\u2192            self._by_lang = {p.lang: p for p in plugins}\n   114\u2192            for plugin in plugins:\n   115\u2192                self._loaded_languages.add(getattr(plugin, 'lang', 'unknown'))\n   116\u2192            if self._enable_advanced:\n   117\u2192                self._register_plugins_with_router()\n   118\u2192        elif use_plugin_factory and not lazy_load:\n   119\u2192            # Load all plugins immediately\n   120\u2192            self._load_all_plugins()\n   121\u2192        # If lazy_load is True, plugins will be loaded on demand\n   122\u2192        \n   123\u2192        # Compile document query patterns for performance\n   124\u2192        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125\u2192        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126\u2192        \n   127\u2192        logger.info(f\"Enhanced dispatcher initialized with {len(self._plugins)} plugins\")\n   128\u2192    \n   129\u2192    def _load_all_plugins(self):\n   130\u2192        \"\"\"Load all available plugins using PluginFactory.\"\"\"\n   131\u2192        logger.info(\"Loading all available plugins...\")\n   132\u2192        \n   133\u2192        all_plugins = PluginFactory.create_all_plugins(\n   134\u2192            sqlite_store=self._sqlite_store,\n   135\u2192            enable_semantic=self._semantic_enabled\n   136\u2192        )\n   137\u2192        \n   138\u2192        for lang, plugin in all_plugins.items():\n   139\u2192            self._plugins.append(plugin)\n   140\u2192            self._by_lang[lang] = plugin\n   141\u2192            self._loaded_languages.add(lang)\n   142\u2192            self._operation_stats['plugins_loaded'] += 1\n   143\u2192        \n   144\u2192        if self._enable_advanced:\n   145\u2192            self._register_plugins_with_router()\n   146\u2192        \n   147\u2192        logger.info(f\"Loaded {len(all_plugins)} plugins: {', '.join(sorted(all_plugins.keys()))}\")\n   148\u2192    \n   149\u2192    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150\u2192        \"\"\"Ensure a plugin for the given language is loaded.\n   151\u2192        \n   152\u2192        Args:\n   153\u2192            language: Language code (e.g., 'python', 'go')\n   154\u2192            \n   155\u2192        Returns:\n   156\u2192            Plugin instance or None if not available\n   157\u2192        \"\"\"\n   158\u2192        # Normalize language\n   159\u2192        language = language.lower().replace('-', '_')\n   160\u2192        \n   161\u2192        # Check if already loaded\n   162\u2192        if language in self._by_lang:\n   163\u2192            return self._by_lang[language]\n   164\u2192        \n   165\u2192        # If not using factory or already tried to load, return None\n   166\u2192        if not self._use_factory or language in self._loaded_languages:\n   167\u2192            return None\n   168\u2192        \n   169\u2192        # Try to load the plugin\n   170\u2192        try:\n   171\u2192            logger.info(f\"Lazy loading plugin for {language}\")\n   172\u2192            plugin = PluginFactory.create_plugin(\n   173\u2192                language,\n   174\u2192                sqlite_store=self._sqlite_store,\n   175\u2192                enable_semantic=self._semantic_enabled\n   176\u2192            )\n   177\u2192            \n   178\u2192            # Add to collections\n   179\u2192            self._plugins.append(plugin)\n   180\u2192            self._by_lang[language] = plugin\n   181\u2192            self._loaded_languages.add(language)\n   182\u2192            self._operation_stats['plugins_loaded'] += 1\n   183\u2192            \n   184\u2192            # Register with router if needed\n   185\u2192            if self._enable_advanced:\n   186\u2192                capabilities = self._detect_plugin_capabilities(plugin)\n   187\u2192                self._router.register_plugin(plugin, capabilities)\n   188\u2192            \n   189\u2192            logger.info(f\"Successfully loaded {language} plugin\")\n   190\u2192            return plugin\n   191\u2192            \n   192\u2192        except ValueError as e:\n   193\u2192            logger.warning(f\"No plugin available for {language}: {e}\")\n   194\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   195\u2192            return None\n   196\u2192        except Exception as e:\n   197\u2192            logger.error(f\"Error loading plugin for {language}: {e}\")\n   198\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   199\u2192            return None\n   200\u2192    \n   201\u2192    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202\u2192        \"\"\"Ensure a plugin is loaded for the given file.\n   203\u2192        \n   204\u2192        Args:\n   205\u2192            path: File path\n   206\u2192            \n   207\u2192        Returns:\n   208\u2192            Plugin instance or None if not available\n   209\u2192        \"\"\"\n   210\u2192        # Get language from file extension\n   211\u2192        extension = path.suffix.lower()\n   212\u2192        language = get_language_by_extension(extension)\n   213\u2192        \n   214\u2192        if language:\n   215\u2192            return self._ensure_plugin_loaded(language)\n   216\u2192        \n   217\u2192        # Fallback: try all loaded plugins\n   218\u2192        for plugin in self._plugins:\n   219\u2192            if plugin.supports(path):\n   220\u2192                return plugin\n   221\u2192        \n   222\u2192        return None\n   223\u2192    \n   224\u2192    def _register_plugins_with_router(self):\n   225\u2192        \"\"\"Register plugins with the router and assign capabilities.\"\"\"\n   226\u2192        for plugin in self._plugins:\n   227\u2192            # Determine capabilities based on plugin type/language\n   228\u2192            capabilities = self._detect_plugin_capabilities(plugin)\n   229\u2192            self._router.register_plugin(plugin, capabilities)\n   230\u2192    \n   231\u2192    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232\u2192        \"\"\"Detect capabilities for a plugin based on its language and features.\"\"\"\n   233\u2192        capabilities = []\n   234\u2192        lang = getattr(plugin, 'lang', 'unknown')\n   235\u2192        \n   236\u2192        # Base capabilities all plugins have\n   237\u2192        capabilities.append(PluginCapability(\n   238\u2192            'syntax_analysis', '1.0', f'{lang} syntax analysis', \n   239\u2192            priority=70, metadata={'language': lang}\n   240\u2192        ))\n   241\u2192        \n   242\u2192        capabilities.append(PluginCapability(\n   243\u2192            'code_search', '1.0', f'{lang} code search', \n   244\u2192            priority=80, metadata={'language': lang}\n   245\u2192        ))\n   246\u2192        \n   247\u2192        # Check for semantic search capability\n   248\u2192        if hasattr(plugin, '_enable_semantic') and plugin._enable_semantic:\n   249\u2192            capabilities.append(PluginCapability(\n   250\u2192                'semantic_search', '1.0', f'{lang} semantic search', \n   251\u2192                priority=90, metadata={'language': lang}\n   252\u2192            ))\n   253\u2192        \n   254\u2192        # Language-specific capabilities\n   255\u2192        if lang == 'python':\n   256\u2192            capabilities.extend([\n   257\u2192                PluginCapability('refactoring', '1.0', 'Python refactoring support', 75),\n   258\u2192                PluginCapability('type_analysis', '1.0', 'Python type analysis', 85)\n   259\u2192            ])\n   260\u2192        elif lang in ['javascript', 'typescript']:\n   261\u2192            capabilities.extend([\n   262\u2192                PluginCapability('linting', '1.0', 'JavaScript/TypeScript linting', 85),\n   263\u2192                PluginCapability('bundling_analysis', '1.0', 'Module bundling analysis', 70),\n   264\u2192                PluginCapability('framework_support', '1.0', 'Framework-specific support', 75)\n   265\u2192            ])\n   266\u2192        elif lang in ['c', 'cpp']:\n   267\u2192            capabilities.extend([\n   268\u2192                PluginCapability('compilation_analysis', '1.0', 'Compilation analysis', 80),\n   269\u2192                PluginCapability('memory_analysis', '1.0', 'Memory usage analysis', 70),\n   270\u2192                PluginCapability('performance_profiling', '1.0', 'Performance profiling', 75)\n   271\u2192            ])\n   272\u2192        elif lang in ['go', 'rust']:\n   273\u2192            capabilities.extend([\n   274\u2192                PluginCapability('package_analysis', '1.0', f'{lang} package analysis', 80),\n   275\u2192                PluginCapability('concurrency_analysis', '1.0', f'{lang} concurrency analysis', 75)\n   276\u2192            ])\n   277\u2192        elif lang in ['java', 'kotlin', 'scala']:\n   278\u2192            capabilities.extend([\n   279\u2192                PluginCapability('jvm_analysis', '1.0', 'JVM bytecode analysis', 75),\n   280\u2192                PluginCapability('build_tool_integration', '1.0', 'Build tool integration', 70)\n   281\u2192            ])\n   282\u2192        \n   283\u2192        return capabilities\n   284\u2192    \n   285\u2192    @property\n   286\u2192    def plugins(self):\n   287\u2192        \"\"\"Get the dictionary of loaded plugins by language.\"\"\"\n   288\u2192        return self._by_lang\n   289\u2192    \n   290\u2192    @property\n   291\u2192    def supported_languages(self) -> List[str]:\n   292\u2192        \"\"\"Get list of all supported languages (loaded and available).\"\"\"\n   293\u2192        if self._use_factory:\n   294\u2192            return PluginFactory.get_supported_languages()\n   295\u2192        else:\n   296\u2192            return list(self._by_lang.keys())\n   297\u2192    \n   298\u2192    def _match_plugin(self, path: Path) -> IPlugin:\n   299\u2192        \"\"\"Match a plugin for the given file path.\"\"\"\n   300\u2192        # Ensure plugin is loaded if using lazy loading\n   301\u2192        if self._lazy_load and self._use_factory:\n   302\u2192            plugin = self._ensure_plugin_for_file(path)\n   303\u2192            if plugin:\n   304\u2192                return plugin\n   305\u2192        \n   306\u2192        # Use advanced routing if available\n   307\u2192        if self._enable_advanced and self._router:\n   308\u2192            route_result = self._router.get_best_plugin(path)\n   309\u2192            if route_result:\n   310\u2192                return route_result.plugin\n   311\u2192        \n   312\u2192        # Fallback to basic matching\n   313\u2192        for p in self._plugins:\n   314\u2192            if p.supports(path):\n   315\u2192                return p\n   316\u2192        \n   317\u2192        raise RuntimeError(f\"No plugin found for {path}\")\n   318\u2192    \n   319\u2192    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320\u2192        \"\"\"Get all plugins that can handle a file with confidence scores.\"\"\"\n   321\u2192        # Ensure plugin is loaded if using lazy loading\n   322\u2192        if self._lazy_load and self._use_factory:\n   323\u2192            self._ensure_plugin_for_file(path)\n   324\u2192        \n   325\u2192        if self._enable_advanced and self._router:\n   326\u2192            route_results = self._router.route_file(path)\n   327\u2192            return [(result.plugin, result.confidence) for result in route_results]\n   328\u2192        else:\n   329\u2192            # Basic fallback\n   330\u2192            matching_plugins = []\n   331\u2192            for plugin in self._plugins:\n   332\u2192                if plugin.supports(path):\n   333\u2192                    matching_plugins.append((plugin, 1.0))\n   334\u2192            return matching_plugins\n   335\u2192    \n   336\u2192    def lookup(self, symbol: str) -> SymbolDef | None:\n   337\u2192        \"\"\"Look up symbol definition across all plugins.\"\"\"\n   338\u2192        start_time = time.time()\n   339\u2192        \n   340\u2192        try:\n   341\u2192            # For symbol lookup, we may need to search across all languages\n   342\u2192            # Load all plugins if using lazy loading\n   343\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344\u2192                self._load_all_plugins()\n   345\u2192            \n   346\u2192            if self._enable_advanced and self._aggregator:\n   347\u2192                # Use advanced aggregation\n   348\u2192                definitions_by_plugin = {}\n   349\u2192                for plugin in self._plugins:\n   350\u2192                    try:\n   351\u2192                        definition = plugin.getDefinition(symbol)\n   352\u2192                        definitions_by_plugin[plugin] = definition\n   353\u2192                    except Exception as e:\n   354\u2192                        logger.warning(f\"Plugin {plugin.lang} failed to get definition for {symbol}: {e}\")\n   355\u2192                        definitions_by_plugin[plugin] = None\n   356\u2192                \n   357\u2192                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358\u2192                \n   359\u2192                self._operation_stats['lookups'] += 1\n   360\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   361\u2192                \n   362\u2192                return result\n   363\u2192            else:\n   364\u2192                # Fallback to basic lookup\n   365\u2192                for p in self._plugins:\n   366\u2192                    res = p.getDefinition(symbol)\n   367\u2192                    if res:\n   368\u2192                        self._operation_stats['lookups'] += 1\n   369\u2192                        self._operation_stats['total_time'] += time.time() - start_time\n   370\u2192                        return res\n   371\u2192                return None\n   372\u2192                \n   373\u2192        except Exception as e:\n   374\u2192            logger.error(f\"Error in symbol lookup for {symbol}: {e}\", exc_info=True)\n   375\u2192            return None\n   376\u2192    \n   377\u2192    def _is_document_query(self, query: str) -> bool:\n   378\u2192        \"\"\"Check if the query is looking for documentation.\n   379\u2192        \n   380\u2192        Args:\n   381\u2192            query: Search query string\n   382\u2192            \n   383\u2192        Returns:\n   384\u2192            True if this appears to be a documentation query\n   385\u2192        \"\"\"\n   386\u2192        query_lower = query.lower()\n   387\u2192        \n   388\u2192        # Check against document query patterns\n   389\u2192        for pattern in self._compiled_doc_patterns:\n   390\u2192            if pattern.search(query_lower):\n   391\u2192                return True\n   392\u2192        \n   393\u2192        # Check for question words at the beginning\n   394\u2192        question_starters = ['how', 'what', 'where', 'when', 'why', 'can', 'is', 'does', 'should']\n   395\u2192        first_word = query_lower.split()[0] if query_lower.split() else ''\n   396\u2192        if first_word in question_starters:\n   397\u2192            return True\n   398\u2192        \n   399\u2192        return False\n   400\u2192    \n   401\u2192    def _expand_document_query(self, query: str) -> List[str]:\n   402\u2192        \"\"\"Expand a document query with related terms for better search coverage.\n   403\u2192        \n   404\u2192        Args:\n   405\u2192            query: Original search query\n   406\u2192            \n   407\u2192        Returns:\n   408\u2192            List of expanded query variations\n   409\u2192        \"\"\"\n   410\u2192        expanded_queries = [query]  # Always include original\n   411\u2192        query_lower = query.lower()\n   412\u2192        \n   413\u2192        # Common expansions for documentation queries\n   414\u2192        expansions = {\n   415\u2192            'install': ['installation', 'setup', 'getting started', 'requirements'],\n   416\u2192            'config': ['configuration', 'configure', 'settings', 'options', 'parameters'],\n   417\u2192            'api': ['api documentation', 'api reference', 'endpoint', 'method'],\n   418\u2192            'how to': ['tutorial', 'guide', 'example', 'usage'],\n   419\u2192            'example': ['sample', 'snippet', 'demo', 'code example'],\n   420\u2192            'error': ['troubleshoot', 'debug', 'issue', 'problem', 'fix'],\n   421\u2192            'getting started': ['quickstart', 'tutorial', 'introduction', 'setup'],\n   422\u2192            'guide': ['tutorial', 'documentation', 'walkthrough', 'how to'],\n   423\u2192            'usage': ['how to use', 'example', 'api', 'reference']\n   424\u2192        }\n   425\u2192        \n   426\u2192        # Apply expansions\n   427\u2192        for term, related_terms in expansions.items():\n   428\u2192            if term in query_lower:\n   429\u2192                for related in related_terms:\n   430\u2192                    # Replace the term with related term\n   431\u2192                    expanded = query_lower.replace(term, related)\n   432\u2192                    if expanded != query_lower and expanded not in expanded_queries:\n   433\u2192                        expanded_queries.append(expanded)\n   434\u2192                \n   435\u2192                # Also add queries with additional terms\n   436\u2192                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437\u2192                    expanded = f\"{query} {related}\"\n   438\u2192                    if expanded not in expanded_queries:\n   439\u2192                        expanded_queries.append(expanded)\n   440\u2192        \n   441\u2192        # Add file-specific searches for common documentation files\n   442\u2192        if self._is_document_query(query):\n   443\u2192            # Extract the main topic from the query\n   444\u2192            topic_words = []\n   445\u2192            for word in query.lower().split():\n   446\u2192                if word not in ['how', 'to', 'the', 'a', 'an', 'is', 'are', 'what', 'where', 'when']:\n   447\u2192                    topic_words.append(word)\n   448\u2192            \n   449\u2192            if topic_words:\n   450\u2192                topic = ' '.join(topic_words[:2])  # Use first two topic words\n   451\u2192                expanded_queries.extend([\n   452\u2192                    f\"README {topic}\",\n   453\u2192                    f\"{topic} documentation\",\n   454\u2192                    f\"{topic} docs\",\n   455\u2192                    f\"{topic} guide\"\n   456\u2192                ])\n   457\u2192        \n   458\u2192        # Remove duplicates while preserving order\n   459\u2192        seen = set()\n   460\u2192        unique_queries = []\n   461\u2192        for q in expanded_queries:\n   462\u2192            if q not in seen:\n   463\u2192                seen.add(q)\n   464\u2192                unique_queries.append(q)\n   465\u2192        \n   466\u2192        return unique_queries[:10]  # Limit to 10 queries max\n   467\u2192    \n   468\u2192    def _is_documentation_file(self, file_path: str) -> bool:\n   469\u2192        \"\"\"Check if a file path is likely a documentation file.\n   470\u2192        \n   471\u2192        Args:\n   472\u2192            file_path: Path to check\n   473\u2192            \n   474\u2192        Returns:\n   475\u2192            True if this appears to be a documentation file\n   476\u2192        \"\"\"\n   477\u2192        path_lower = file_path.lower()\n   478\u2192        \n   479\u2192        for pattern in self._compiled_file_patterns:\n   480\u2192            if pattern.search(path_lower):\n   481\u2192                return True\n   482\u2192        \n   483\u2192        return False\n   484\u2192    \n   485\u2192    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486\u2192        \"\"\"Adjust ranking to prioritize documentation files for document queries.\n   487\u2192        \n   488\u2192        Args:\n   489\u2192            query: Original search query\n   490\u2192            results: List of aggregated results\n   491\u2192            \n   492\u2192        Returns:\n   493\u2192            Re-ranked results with documentation prioritized\n   494\u2192        \"\"\"\n   495\u2192        if not self._is_document_query(query):\n   496\u2192            return results\n   497\u2192        \n   498\u2192        # Separate documentation and code results\n   499\u2192        doc_results = []\n   500\u2192        code_results = []\n   501\u2192        \n   502\u2192        for result in results:\n   503\u2192            if self._is_documentation_file(result.primary_result.get('file', '')):\n   504\u2192                # Boost documentation files for document queries\n   505\u2192                result.rank_score *= 1.5\n   506\u2192                result.metadata['doc_boost'] = True\n   507\u2192                doc_results.append(result)\n   508\u2192            else:\n   509\u2192                code_results.append(result)\n   510\u2192        \n   511\u2192        # Sort each group by rank score\n   512\u2192        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513\u2192        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514\u2192        \n   515\u2192        # Combine with documentation files first\n   516\u2192        return doc_results + code_results\n   517\u2192    \n   518\u2192    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519\u2192        \"\"\"Search for code and documentation across all plugins.\"\"\"\n   520\u2192        start_time = time.time()\n   521\u2192        \n   522\u2192        try:\n   523\u2192            # For search, we may need to search across all languages\n   524\u2192            # Load all plugins if using lazy loading\n   525\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526\u2192                self._load_all_plugins()\n   527\u2192            \n   528\u2192            # Detect if this is a document query\n   529\u2192            is_doc_query = self._is_document_query(query)\n   530\u2192            \n   531\u2192            # Expand query if it's a document query\n   532\u2192            queries = [query]\n   533\u2192            if is_doc_query:\n   534\u2192                queries = self._expand_document_query(query)\n   535\u2192                logger.info(f\"Expanded document query '{query}' to {len(queries)} variations\")\n   536\u2192                # Force semantic search for natural language queries\n   537\u2192                semantic = True\n   538\u2192            \n   539\u2192            if self._enable_advanced and self._aggregator:\n   540\u2192                # Use advanced aggregation\n   541\u2192                all_results_by_plugin = {}\n   542\u2192                opts = {\"semantic\": semantic, \"limit\": limit * 2 if is_doc_query else limit}\n   543\u2192                \n   544\u2192                # Search with all query variations\n   545\u2192                for search_query in queries:\n   546\u2192                    for plugin in self._plugins:\n   547\u2192                        try:\n   548\u2192                            results = list(plugin.search(search_query, opts))\n   549\u2192                            if results:\n   550\u2192                                if plugin not in all_results_by_plugin:\n   551\u2192                                    all_results_by_plugin[plugin] = []\n   552\u2192                                all_results_by_plugin[plugin].extend(results)\n   553\u2192                        except Exception as e:\n   554\u2192                            logger.warning(f\"Plugin {plugin.lang} failed to search for {search_query}: {e}\")\n   555\u2192                \n   556\u2192                # Deduplicate results per plugin\n   557\u2192                for plugin, results in all_results_by_plugin.items():\n   558\u2192                    seen = set()\n   559\u2192                    unique_results = []\n   560\u2192                    for result in results:\n   561\u2192                        key = f\"{result['file']}:{result['line']}\"\n   562\u2192                        if key not in seen:\n   563\u2192                            seen.add(key)\n   564\u2192                            unique_results.append(result)\n   565\u2192                    all_results_by_plugin[plugin] = unique_results\n   566\u2192                \n   567\u2192                # Configure aggregator for document queries\n   568\u2192                if is_doc_query and self._enable_advanced:\n   569\u2192                    # Adjust ranking criteria for documentation\n   570\u2192                    doc_criteria = RankingCriteria(\n   571\u2192                        relevance_weight=0.5,      # Increase relevance weight\n   572\u2192                        confidence_weight=0.2,     # Reduce confidence weight\n   573\u2192                        frequency_weight=0.2,      # Keep frequency weight\n   574\u2192                        recency_weight=0.1,        # Keep recency weight\n   575\u2192                        prefer_exact_matches=False,  # Natural language doesn't need exact matches\n   576\u2192                        boost_multiple_sources=True,\n   577\u2192                        boost_common_extensions=True\n   578\u2192                    )\n   579\u2192                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580\u2192                \n   581\u2192                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582\u2192                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583\u2192                )\n   584\u2192                \n   585\u2192                # Adjust ranking for document queries\n   586\u2192                if is_doc_query:\n   587\u2192                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588\u2192                \n   589\u2192                # Apply final limit\n   590\u2192                if limit and len(aggregated_results) > limit:\n   591\u2192                    aggregated_results = aggregated_results[:limit]\n   592\u2192                \n   593\u2192                logger.debug(f\"Search aggregation stats: {stats.total_results} total, \"\n   594\u2192                           f\"{stats.unique_results} unique, {stats.plugins_used} plugins used, \"\n   595\u2192                           f\"document_query={is_doc_query}\")\n   596\u2192                \n   597\u2192                self._operation_stats['searches'] += 1\n   598\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   599\u2192                \n   600\u2192                # Yield primary results from aggregated results\n   601\u2192                for aggregated in aggregated_results:\n   602\u2192                    yield aggregated.primary_result\n   603\u2192            else:\n   604\u2192                # Fallback to basic search\n   605\u2192                # Detect if this is a document query\n   606\u2192                is_doc_query = self._is_document_query(query)\n   607\u2192                \n   608\u2192                # Expand query if it's a document query\n   609\u2192                queries = [query]\n   610\u2192                if is_doc_query:\n   611\u2192                    queries = self._expand_document_query(query)\n   612\u2192                    semantic = True  # Force semantic search for natural language\n   613\u2192                \n   614\u2192                opts = {\"semantic\": semantic, \"limit\": limit}\n   615\u2192                all_results = []\n   616\u2192                \n   617\u2192                # Search with all query variations\n   618\u2192                for search_query in queries:\n   619\u2192                    for p in self._plugins:\n   620\u2192                        try:\n   621\u2192                            for result in p.search(search_query, opts):\n   622\u2192                                all_results.append(result)\n   623\u2192                        except Exception as e:\n   624\u2192                            logger.warning(f\"Plugin {p.lang} failed to search for {search_query}: {e}\")\n   625\u2192                \n   626\u2192                # Deduplicate results\n   627\u2192                seen = set()\n   628\u2192                unique_results = []\n   629\u2192                for result in all_results:\n   630\u2192                    key = f\"{result['file']}:{result['line']}\"\n   631\u2192                    if key not in seen:\n   632\u2192                        seen.add(key)\n   633\u2192                        unique_results.append(result)\n   634\u2192                \n   635\u2192                # Sort by score if available\n   636\u2192                unique_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   637\u2192                \n   638\u2192                # Prioritize documentation files for document queries\n   639\u2192                if is_doc_query:\n   640\u2192                    doc_results = []\n   641\u2192                    code_results = []\n   642\u2192                    for result in unique_results:\n   643\u2192                        if self._is_documentation_file(result.get('file', '')):\n   644\u2192                            doc_results.append(result)\n   645\u2192                        else:\n   646\u2192                            code_results.append(result)\n   647\u2192                    unique_results = doc_results + code_results\n   648\u2192                \n   649\u2192                # Apply limit\n   650\u2192                count = 0\n   651\u2192                for result in unique_results:\n   652\u2192                    if limit and count >= limit:\n   653\u2192                        break\n   654\u2192                    yield result\n   655\u2192                    count += 1\n   656\u2192                \n   657\u2192                self._operation_stats['searches'] += 1\n   658\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   659\u2192                \n   660\u2192        except Exception as e:\n   661\u2192            logger.error(f\"Error in search for {query}: {e}\", exc_info=True)\n   662\u2192    \n   663\u2192    def index_file(self, path: Path) -> None:\n   664\u2192        \"\"\"Index a single file if it has changed.\"\"\"\n   665\u2192        try:\n   666\u2192            # Ensure path is absolute to avoid relative/absolute path issues\n   667\u2192            path = path.resolve()\n   668\u2192            \n   669\u2192            # Find the appropriate plugin\n   670\u2192            plugin = self._match_plugin(path)\n   671\u2192            \n   672\u2192            # Read file content\n   673\u2192            try:\n   674\u2192                content = path.read_text(encoding='utf-8')\n   675\u2192            except UnicodeDecodeError:\n   676\u2192                # Try with different encodings\n   677\u2192                try:\n   678\u2192                    content = path.read_text(encoding='latin-1')\n   679\u2192                except Exception as e:\n   680\u2192                    logger.error(f\"Failed to read {path}: {e}\")\n   681\u2192                    return\n   682\u2192            \n   683\u2192            # Check if we need to re-index (simplified for now)\n   684\u2192            # TODO: Implement proper caching logic\n   685\u2192            \n   686\u2192            # Index the file\n   687\u2192            start_time = time.time()\n   688\u2192            logger.info(f\"Indexing {path} with {plugin.lang} plugin\")\n   689\u2192            shard = plugin.indexFile(path, content)\n   690\u2192            \n   691\u2192            # Record performance if advanced features enabled\n   692\u2192            if self._enable_advanced and self._router:\n   693\u2192                execution_time = time.time() - start_time\n   694\u2192                self._router.record_performance(plugin, execution_time)\n   695\u2192            \n   696\u2192            self._operation_stats['indexings'] += 1\n   697\u2192            self._operation_stats['total_time'] += time.time() - start_time\n   698\u2192            \n   699\u2192            logger.info(f\"Successfully indexed {path}: {len(shard.get('symbols', []))} symbols found\")\n   700\u2192            \n   701\u2192        except RuntimeError as e:\n   702\u2192            # No plugin found for this file type\n   703\u2192            logger.debug(f\"No plugin for {path}: {e}\")\n   704\u2192        except Exception as e:\n   705\u2192            logger.error(f\"Error indexing {path}: {e}\", exc_info=True)\n   706\u2192    \n   707\u2192    def get_statistics(self) -> dict:\n   708\u2192        \"\"\"Get comprehensive statistics across all plugins and components.\"\"\"\n   709\u2192        stats = {\n   710\u2192            \"total_plugins\": len(self._plugins),\n   711\u2192            \"loaded_languages\": sorted(list(self._loaded_languages)),\n   712\u2192            \"supported_languages\": len(self.supported_languages),\n   713\u2192            \"operations\": self._operation_stats.copy()\n   714\u2192        }\n   715\u2192        \n   716\u2192        # Add language breakdown\n   717\u2192        stats[\"by_language\"] = {}\n   718\u2192        for lang, plugin in self._by_lang.items():\n   719\u2192            plugin_info = {\n   720\u2192                \"loaded\": True,\n   721\u2192                \"class\": plugin.__class__.__name__\n   722\u2192            }\n   723\u2192            if hasattr(plugin, 'get_indexed_count'):\n   724\u2192                plugin_info[\"indexed_files\"] = plugin.get_indexed_count()\n   725\u2192            stats[\"by_language\"][lang] = plugin_info\n   726\u2192        \n   727\u2192        return stats\n   728\u2192    \n   729\u2192    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730\u2192        \"\"\"\n   731\u2192        Index all files in a directory, respecting ignore patterns.\n   732\u2192        \n   733\u2192        Args:\n   734\u2192            directory: Directory to index\n   735\u2192            recursive: Whether to index subdirectories\n   736\u2192            \n   737\u2192        Returns:\n   738\u2192            Statistics about indexed files\n   739\u2192        \"\"\"\n   740\u2192        logger.info(f\"Indexing directory: {directory} (recursive={recursive})\")\n   741\u2192        \n   742\u2192        # Note: We don't use ignore patterns during indexing\n   743\u2192        # ALL files are indexed for local search capability\n   744\u2192        # Filtering happens only during export/sharing\n   745\u2192        \n   746\u2192        # Get all supported extensions\n   747\u2192        supported_extensions = get_all_extensions()\n   748\u2192        \n   749\u2192        stats = {\n   750\u2192            \"total_files\": 0,\n   751\u2192            \"indexed_files\": 0,\n   752\u2192            \"ignored_files\": 0,\n   753\u2192            \"failed_files\": 0,\n   754\u2192            \"by_language\": {}\n   755\u2192        }\n   756\u2192        \n   757\u2192        # Walk directory\n   758\u2192        if recursive:\n   759\u2192            file_iterator = directory.rglob(\"*\")\n   760\u2192        else:\n   761\u2192            file_iterator = directory.glob(\"*\")\n   762\u2192            \n   763\u2192        for path in file_iterator:\n   764\u2192            if not path.is_file():\n   765\u2192                continue\n   766\u2192                \n   767\u2192            stats[\"total_files\"] += 1\n   768\u2192            \n   769\u2192            # NOTE: We index ALL files locally, including gitignored ones\n   770\u2192            # Filtering happens only during export/sharing\n   771\u2192            # This allows local search of .env, secrets, etc.\n   772\u2192                \n   773\u2192            # Try to find a plugin that supports this file\n   774\u2192            # This allows us to index ALL files, including .env, .key, etc.\n   775\u2192            try:\n   776\u2192                # First try to match by extension\n   777\u2192                if path.suffix in supported_extensions:\n   778\u2192                    self.index_file(path)\n   779\u2192                    stats[\"indexed_files\"] += 1\n   780\u2192                # For files without recognized extensions, try each plugin's supports() method\n   781\u2192                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782\u2192                else:\n   783\u2192                    matched = False\n   784\u2192                    for plugin in self._plugins:\n   785\u2192                        if plugin.supports(path):\n   786\u2192                            self.index_file(path)\n   787\u2192                            stats[\"indexed_files\"] += 1\n   788\u2192                            matched = True\n   789\u2192                            break\n   790\u2192                    \n   791\u2192                    # If no plugin matched but we want to index everything,\n   792\u2192                    # we could add a fallback here to index as plaintext\n   793\u2192                    # For now, we'll skip unmatched files\n   794\u2192                    if not matched:\n   795\u2192                        logger.debug(f\"No plugin found for {path}\")\n   796\u2192                \n   797\u2192                # Track by language\n   798\u2192                language = get_language_by_extension(path.suffix)\n   799\u2192                if language:\n   800\u2192                    stats[\"by_language\"][language] = stats[\"by_language\"].get(language, 0) + 1\n   801\u2192                    \n   802\u2192            except Exception as e:\n   803\u2192                logger.error(f\"Failed to index {path}: {e}\")\n   804\u2192                stats[\"failed_files\"] += 1\n   805\u2192                \n   806\u2192        logger.info(f\"Directory indexing complete: {stats['indexed_files']} indexed, \"\n   807\u2192                   f\"{stats['ignored_files']} ignored, {stats['failed_files']} failed\")\n   808\u2192        \n   809\u2192        return stats\n   810\u2192    \n   811\u2192    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812\u2192        \"\"\"Search specifically across documentation files.\n   813\u2192        \n   814\u2192        Args:\n   815\u2192            topic: Topic to search for (e.g., \"installation\", \"configuration\")\n   816\u2192            doc_types: Optional list of document types to search (e.g., [\"readme\", \"guide\", \"api\"])\n   817\u2192            limit: Maximum number of results\n   818\u2192            \n   819\u2192        Returns:\n   820\u2192            Search results from documentation files\n   821\u2192        \"\"\"\n   822\u2192        # Default document types if not specified\n   823\u2192        if doc_types is None:\n   824\u2192            doc_types = [\"readme\", \"documentation\", \"guide\", \"tutorial\", \"api\", \"changelog\", \"contributing\"]\n   825\u2192        \n   826\u2192        # Build search queries for different document types\n   827\u2192        queries = []\n   828\u2192        for doc_type in doc_types:\n   829\u2192            queries.extend([\n   830\u2192                f\"{doc_type} {topic}\",\n   831\u2192                f\"{topic} {doc_type}\",\n   832\u2192                f\"{topic} in {doc_type}\"\n   833\u2192            ])\n   834\u2192        \n   835\u2192        # Also search for the topic in common doc filenames\n   836\u2192        queries.extend([\n   837\u2192            f\"README {topic}\",\n   838\u2192            f\"CONTRIBUTING {topic}\",\n   839\u2192            f\"docs {topic}\",\n   840\u2192            f\"documentation {topic}\"\n   841\u2192        ])\n   842\u2192        \n   843\u2192        # Deduplicate queries\n   844\u2192        queries = list(dict.fromkeys(queries))\n   845\u2192        \n   846\u2192        logger.info(f\"Cross-document search for '{topic}' with {len(queries)} query variations\")\n   847\u2192        \n   848\u2192        # Use the enhanced search with document-specific handling\n   849\u2192        all_results = []\n   850\u2192        seen = set()\n   851\u2192        \n   852\u2192        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853\u2192            for result in self.search(query, semantic=True, limit=limit):\n   854\u2192                # Only include documentation files\n   855\u2192                if self._is_documentation_file(result.get('file', '')):\n   856\u2192                    key = f\"{result['file']}:{result['line']}\"\n   857\u2192                    if key not in seen:\n   858\u2192                        seen.add(key)\n   859\u2192                        all_results.append(result)\n   860\u2192        \n   861\u2192        # Sort by relevance (score) and return top results\n   862\u2192        all_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   863\u2192        \n   864\u2192        count = 0\n   865\u2192        for result in all_results:\n   866\u2192            if count >= limit:\n   867\u2192                break\n   868\u2192            yield result\n   869\u2192            count += 1\n   870\u2192    \n   871\u2192    def health_check(self) -> Dict[str, Any]:\n   872\u2192        \"\"\"Perform a health check on all components.\"\"\"\n   873\u2192        health = {\n   874\u2192            'status': 'healthy',\n   875\u2192            'components': {\n   876\u2192                'dispatcher': {\n   877\u2192                    'status': 'healthy',\n   878\u2192                    'plugins_loaded': len(self._plugins),\n   879\u2192                    'languages_supported': len(self.supported_languages),\n   880\u2192                    'factory_enabled': self._use_factory,\n   881\u2192                    'lazy_loading': self._lazy_load\n   882\u2192                }\n   883\u2192            },\n   884\u2192            'plugins': {},\n   885\u2192            'errors': []\n   886\u2192        }\n   887\u2192        \n   888\u2192        # Check plugin health\n   889\u2192        for lang, plugin in self._by_lang.items():\n   890\u2192            try:\n   891\u2192                plugin_health = {\n   892\u2192                    'status': 'healthy',\n   893\u2192                    'class': plugin.__class__.__name__,\n   894\u2192                    'semantic_enabled': getattr(plugin, '_enable_semantic', False)\n   895\u2192                }\n   896\u2192                if hasattr(plugin, 'get_indexed_count'):\n   897\u2192                    plugin_health['indexed_files'] = plugin.get_indexed_count()\n   898\u2192            except Exception as e:\n   899\u2192                plugin_health = {\n   900\u2192                    'status': 'error',\n   901\u2192                    'error': str(e)\n   902\u2192                }\n   903\u2192                health['errors'].append(f\"Plugin {lang}: {str(e)}\")\n   904\u2192            \n   905\u2192            health['plugins'][lang] = plugin_health\n   906\u2192        \n   907\u2192        # Determine overall health\n   908\u2192        if len(health['errors']) > 0:\n   909\u2192            health['status'] = 'degraded' if len(health['errors']) < 3 else 'unhealthy'\n   910\u2192        \n   911\u2192        return health\n   912\u2192    \n   913\u2192    def remove_file(self, path: Union[Path, str]) -> None:\n   914\u2192        \"\"\"Remove a file from all indexes.\n   915\u2192        \n   916\u2192        Args:\n   917\u2192            path: File path to remove\n   918\u2192        \"\"\"\n   919\u2192        path = Path(path).resolve()\n   920\u2192        logger.info(f\"Removing file from index: {path}\")\n   921\u2192        \n   922\u2192        try:\n   923\u2192            # Remove from SQLite if available\n   924\u2192            if self._sqlite_store:\n   925\u2192                from ..core.path_resolver import PathResolver\n   926\u2192                path_resolver = PathResolver()\n   927\u2192                try:\n   928\u2192                    relative_path = path_resolver.normalize_path(path)\n   929\u2192                    # Get repository ID - for now assume 1\n   930\u2192                    # TODO: Properly detect repository\n   931\u2192                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932\u2192                except Exception as e:\n   933\u2192                    logger.error(f\"Error removing from SQLite: {e}\")\n   934\u2192            \n   935\u2192            # Remove from semantic index if available\n   936\u2192            try:\n   937\u2192                plugin = self._match_plugin(path)\n   938\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   939\u2192                    plugin._indexer.remove_file(path)\n   940\u2192                    logger.info(f\"Removed from semantic index: {path}\")\n   941\u2192            except Exception as e:\n   942\u2192                logger.warning(f\"Error removing from semantic index: {e}\")\n   943\u2192            \n   944\u2192            # Update statistics\n   945\u2192            self._operation_stats['deletions'] = self._operation_stats.get('deletions', 0) + 1\n   946\u2192            \n   947\u2192        except Exception as e:\n   948\u2192            logger.error(f\"Error removing file {path}: {e}\", exc_info=True)\n   949\u2192    \n   950\u2192    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951\u2192        \"\"\"Move a file in all indexes.\n   952\u2192        \n   953\u2192        Args:\n   954\u2192            old_path: Original file path\n   955\u2192            new_path: New file path\n   956\u2192            content_hash: Optional content hash to verify unchanged content\n   957\u2192        \"\"\"\n   958\u2192        old_path = Path(old_path).resolve()\n   959\u2192        new_path = Path(new_path).resolve()\n   960\u2192        logger.info(f\"Moving file in index: {old_path} -> {new_path}\")\n   961\u2192        \n   962\u2192        try:\n   963\u2192            # Move in SQLite if available\n   964\u2192            if self._sqlite_store:\n   965\u2192                from ..core.path_resolver import PathResolver\n   966\u2192                path_resolver = PathResolver()\n   967\u2192                try:\n   968\u2192                    old_relative = path_resolver.normalize_path(old_path)\n   969\u2192                    new_relative = path_resolver.normalize_path(new_path)\n   970\u2192                    # Get repository ID - for now assume 1\n   971\u2192                    # TODO: Properly detect repository\n   972\u2192                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973\u2192                except Exception as e:\n   974\u2192                    logger.error(f\"Error moving in SQLite: {e}\")\n   975\u2192            \n   976\u2192            # Move in semantic index if available\n   977\u2192            try:\n   978\u2192                plugin = self._match_plugin(new_path)\n   979\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   980\u2192                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981\u2192                    logger.info(f\"Moved in semantic index: {old_path} -> {new_path}\")\n   982\u2192            except Exception as e:\n   983\u2192                logger.warning(f\"Error moving in semantic index: {e}\")\n   984\u2192            \n   985\u2192            # Update statistics\n   986\u2192            self._operation_stats['moves'] = self._operation_stats.get('moves', 0) + 1\n   987\u2192            \n   988\u2192        except Exception as e:\n   989\u2192            logger.error(f\"Error moving file {old_path} -> {new_path}: {e}\", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 124,
    "output_tokens_total": 124,
    "cache_read_tokens": 11005,
    "cache_creation_tokens": 13343,
    "tokens_per_result": 124,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "f86b4d58-381b-4fe7-9d8b-38906cd581f9",
    "test_date": "2025-06-26T07:33:14.213Z",
    "query_id": "transcript_f86b4d58-381b-4fe7-9d8b-38906cd581f9",
    "query_text": [
      {
        "tool_use_id": "toolu_01HjgKNWmNXxZh1nFpM6MCSV",
        "type": "tool_result",
        "content": "     1\u2192#!/usr/bin/env python3\n     2\u2192\"\"\"Test the enhanced dispatcher's document query functionality.\"\"\"\n     3\u2192\n     4\u2192import logging\n     5\u2192from pathlib import Path\n     6\u2192from mcp_server.dispatcher.dispatcher_enhanced import EnhancedDispatcher\n     7\u2192from mcp_server.storage.sqlite_store import SQLiteStore\n     8\u2192\n     9\u2192# Setup logging\n    10\u2192logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    11\u2192logger = logging.getLogger(__name__)\n    12\u2192\n    13\u2192\n    14\u2192def test_document_query_detection():\n    15\u2192    \"\"\"Test document query detection.\"\"\"\n    16\u2192    dispatcher = EnhancedDispatcher(enable_advanced_features=True)\n    17\u2192    \n    18\u2192    # Test queries that should be detected as document queries\n    19\u2192    doc_queries = [\n    20\u2192        \"How to install the package\",\n    21\u2192        \"getting started with Python\",\n    22\u2192        \"API documentation for search\",\n    23\u2192        \"configuration options\",\n    24\u2192        \"What is the architecture\",\n    25\u2192        \"troubleshooting errors\",\n    26\u2192        \"best practices for coding\",\n    27\u2192        \"installation guide\",\n    28\u2192        \"where can I find examples\"\n    29\u2192    ]\n    30\u2192    \n    31\u2192    # Test queries that should NOT be document queries\n    32\u2192    code_queries = [\n    33\u2192        \"def search\",\n    34\u2192        \"class EnhancedDispatcher\",\n    35\u2192        \"import logging\",\n    36\u2192        \"search_function\",\n    37\u2192        \"dispatcher.search\"\n    38\u2192    ]\n    39\u2192    \n    40\u2192    print(\"Testing Document Query Detection:\")\n    41\u2192    print(\"=\" * 50)\n    42\u2192    \n    43\u2192    print(\"\\nDocument Queries (should return True):\")\n    44\u2192    for query in doc_queries:\n    45\u2192        is_doc = dispatcher._is_document_query(query)\n    46\u2192        print(f\"  '{query}' -> {is_doc}\")\n    47\u2192        assert is_doc, f\"Expected '{query}' to be detected as document query\"\n    48\u2192    \n    49\u2192    print(\"\\nCode Queries (should return False):\")\n    50\u2192    for query in code_queries:\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1175,
    "output_tokens_total": 1175,
    "cache_read_tokens": 24348,
    "cache_creation_tokens": 786,
    "tokens_per_result": 1175,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "f86b4d58-381b-4fe7-9d8b-38906cd581f9",
    "test_date": "2025-06-26T07:33:39.675Z",
    "query_id": "transcript_f86b4d58-381b-4fe7-9d8b-38906cd581f9",
    "query_text": [
      {
        "tool_use_id": "toolu_01YKtKp5iH7i555oHcdft66C",
        "type": "tool_result",
        "content": [
          {
            "type": "text",
            "text": "Based on my analysis of the codebase, I found the EnhancedDispatcher class definition. Here are the details:\n\n## File Path\n**File:** `/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher_enhanced.py`\n\n## Class Definition Location\n**Lines:** 24-989 (the class spans from line 24 to the end of the file at line 989)\n\n## Main Methods with Signatures\n\nHere are the main methods of the EnhancedDispatcher class with their signatures:\n\n### Constructor\n- **`__init__(self, plugins: Optional[List[IPlugin]] = None, sqlite_store: Optional[SQLiteStore] = None, enable_advanced_features: bool = True, use_plugin_factory: bool = True, lazy_load: bool = True, semantic_search_enabled: bool = True)`** (lines 63-127)\n\n### Plugin Management Methods\n- **`_load_all_plugins(self)`** (lines 129-147) - Load all available plugins using PluginFactory\n- **`_ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]`** (lines 149-199) - Ensure a plugin for the given language is loaded\n- **`_ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]`** (lines 201-222) - Ensure a plugin is loaded for the given file\n- **`_register_plugins_with_router(self)`** (lines 224-229) - Register plugins with the router\n- **`_detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]`** (lines 231-283) - Detect capabilities for a plugin\n\n### Core Search and Lookup Methods\n- **`lookup(self, symbol: str) -> SymbolDef | None`** (lines 336-375) - Look up symbol definition across all plugins\n- **`search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]`** (lines 518-661) - Search for code and documentation across all plugins\n- **`search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]`** (lines 811-869) - Search specifically across documentation files\n\n### File Operations\n- **`index_file(self, path: Path) -> None`** (lines 663-705) - Index a single file if it has changed\n- **`index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]`** (lines 729-809) - Index all files in a directory\n- **`remove_file(self, path: Union[Path, str]) -> None`** (lines 913-948) - Remove a file from all indexes\n- **`move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None`** (lines 950-989) - Move a file in all indexes\n\n### Plugin Routing and Matching\n- **`_match_plugin(self, path: Path) -> IPlugin`** (lines 298-317) - Match a plugin for the given file path\n- **`get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]`** (lines 319-334) - Get all plugins that can handle a file with confidence scores\n\n### Document Query Processing\n- **`_is_document_query(self, query: str) -> bool`** (lines 377-399) - Check if the query is looking for documentation\n- **`_expand_document_query(self, query: str) -> List[str]`** (lines 401-466) - Expand a document query with related terms\n- **`_is_documentation_file(self, file_path: str) -> bool`** (lines 468-483) - Check if a file path is likely a documentation file\n- **`_adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]`** (lines 485-516) - Adjust ranking to prioritize documentation files\n\n### Properties and Utilities\n- **`plugins` (property)** (lines 286-288) - Get the dictionary of loaded plugins by language\n- **`supported_languages` (property)** (lines 291-296) - Get list of all supported languages\n- **`get_statistics(self) -> dict`** (lines 707-727) - Get comprehensive statistics across all plugins\n- **`health_check(self) -> Dict[str, Any]`** (lines 871-911) - Perform a health check on all components\n\nThe EnhancedDispatcher class is a comprehensive dispatcher that provides advanced plugin management, document-aware search capabilities, and extensive file indexing functionality with support for lazy loading, semantic search, and result aggregation."
          }
        ]
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 6,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 6,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13920,
    "cache_creation_tokens": 2609,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "3bea4874-d851-464f-90ea-6e0b51bf9716",
    "test_date": "2025-06-26T07:31:32.662Z",
    "query_id": "transcript_3bea4874-d851-464f-90ea-6e0b51bf9716",
    "query_text": "Locate the SQLiteStore class and explain its search method",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 126,
    "output_tokens_total": 126,
    "cache_read_tokens": 13172,
    "cache_creation_tokens": 746,
    "tokens_per_result": 126,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "41333a35-f5b8-4f90-875a-e853cef49417",
    "test_date": "2025-06-26T07:36:47.071Z",
    "query_id": "transcript_41333a35-f5b8-4f90-875a-e853cef49417",
    "query_text": "Find all functions that use asyncio",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 202,
    "output_tokens_total": 202,
    "cache_read_tokens": 13913,
    "cache_creation_tokens": 0,
    "tokens_per_result": 202,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "41333a35-f5b8-4f90-875a-e853cef49417",
    "test_date": "2025-06-26T07:36:52.766Z",
    "query_id": "transcript_41333a35-f5b8-4f90-875a-e853cef49417",
    "query_text": "Search the codebase for all functions that use asyncio. Look for:\n1. Functions that import asyncio\n2. Functions that use async/await syntax\n3. Functions that call asyncio methods like asyncio.run(), asyncio.gather(), asyncio.create_task(), etc.\n4. Functions that use asyncio.sleep(), asyncio.wait(), etc.\n\nPlease provide a comprehensive list of all functions that use asyncio, including:\n- File path and line number for each function\n- Function name\n- Brief description of how asyncio is being used\n\nReturn the results in a clear, organized format.",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 101,
    "output_tokens_total": 101,
    "cache_read_tokens": 9986,
    "cache_creation_tokens": 867,
    "tokens_per_result": 101,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "41333a35-f5b8-4f90-875a-e853cef49417",
    "test_date": "2025-06-26T07:36:56.627Z",
    "query_id": "transcript_41333a35-f5b8-4f90-875a-e853cef49417",
    "query_text": [
      {
        "tool_use_id": "toolu_015ZpnHSYVmftdP9EQxJ72RE",
        "type": "tool_result",
        "content": "Found 132 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/conftest.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_cross_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_developer_workflows.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_repository_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_semantic_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_dart_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_js_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_python_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_utils.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_data/markdown/tutorial.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_plugins_direct.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_metadata_extractor.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_multi_language_indexing_with_ignore.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_natural_language_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_readme_parsing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_section_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/swift_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/security/auth_manager.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/security/security_middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_test_prompts.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_test_results_template.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/populate_bm25_quick.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/swift_demo.swift\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_languages_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_plugins_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_specialized_plugins.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_data/markdown/api_docs.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/api_gateway_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/cache_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/dispatcher_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/indexing_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/metrics_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/security_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/shared_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/storage_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/type_analyzer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/dart_plugin/AGENTS.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/js_plugin/AGENTS.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/AGENTS.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_runner.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp-index-kit/install.sh\n(Results are truncated. Consider using a more specific path or pattern.)"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 223,
    "output_tokens_total": 223,
    "cache_read_tokens": 10853,
    "cache_creation_tokens": 3457,
    "tokens_per_result": 223,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "41333a35-f5b8-4f90-875a-e853cef49417",
    "test_date": "2025-06-26T07:37:04.637Z",
    "query_id": "transcript_41333a35-f5b8-4f90-875a-e853cef49417",
    "query_text": [
      {
        "tool_use_id": "toolu_01BkuvhxXCgkT6UfrRGvksK5",
        "type": "tool_result",
        "content": "Found 93 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_python_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_data/markdown/tutorial.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_section_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/security/auth_manager.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/security/security_middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_test_prompts.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_test_results_template.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_data/markdown/api_docs.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/api_gateway_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/cache_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/dispatcher_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/indexing_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/metrics_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/security_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/shared_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/storage_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/AGENTS.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_runner.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/docs/development/TESTING-GUIDE.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/celery_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/fastapi_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/grpc_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/jwt_authentication_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/MCP.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/prometheus_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/pydantic_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/tree_sitter_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/watchdog.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/specialized_plugins_architecture.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/docs/DEPLOYMENT-GUIDE.md"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 1920,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 1920,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 315,
    "output_tokens_total": 315,
    "cache_read_tokens": 14310,
    "cache_creation_tokens": 5638,
    "tokens_per_result": 315,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "41333a35-f5b8-4f90-875a-e853cef49417",
    "test_date": "2025-06-26T07:37:12.789Z",
    "query_id": "transcript_41333a35-f5b8-4f90-875a-e853cef49417",
    "query_text": [
      {
        "tool_use_id": "toolu_01XKQUXYgVqEq7jx4Nq5Ltic",
        "type": "tool_result",
        "content": "     1\u2192from fastapi import FastAPI, HTTPException, Depends, status, Response\n     2\u2192from fastapi.responses import JSONResponse, PlainTextResponse\n     3\u2192from typing import Optional, Dict, Any, List\n     4\u2192from pathlib import Path\n     5\u2192import logging\n     6\u2192import os\n     7\u2192import time\n     8\u2192import threading\n     9\u2192from .dispatcher import Dispatcher\n    10\u2192from .plugin_base import SymbolDef, SearchResult\n    11\u2192from .storage.sqlite_store import SQLiteStore\n    12\u2192from .watcher import FileWatcher\n    13\u2192from .core.logging import setup_logging\n    14\u2192from .utils.index_discovery import IndexDiscovery\n    15\u2192from .plugin_system import PluginManager, PluginSystemConfig\n    16\u2192from .indexer.bm25_indexer import BM25Indexer\n    17\u2192from .indexer.hybrid_search import HybridSearch, HybridSearchConfig\n    18\u2192from .utils.fuzzy_indexer import FuzzyIndexer\n    19\u2192from .security import (\n    20\u2192    SecurityConfig, AuthManager, SecurityMiddlewareStack,\n    21\u2192    AuthCredentials, User, UserRole, Permission,\n    22\u2192    get_current_user, get_current_active_user, require_permission, require_role\n    23\u2192)\n    24\u2192from .metrics import (\n    25\u2192    get_metrics_collector, get_health_checker, HealthStatus\n    26\u2192)\n    27\u2192from .metrics.middleware import setup_metrics_middleware, get_business_metrics\n    28\u2192from .metrics.prometheus_exporter import get_prometheus_exporter\n    29\u2192from .cache import (\n    30\u2192    CacheManagerFactory, CacheConfig, CacheBackendType, \n    31\u2192    QueryResultCache, QueryCacheConfig, QueryType\n    32\u2192)\n    33\u2192\n    34\u2192# Set up logging\n    35\u2192setup_logging(log_level=\"INFO\")\n    36\u2192logger = logging.getLogger(__name__)\n    37\u2192\n    38\u2192app = FastAPI(title=\"MCP Server\", description=\"Code Index MCP Server with Security, Metrics, and Health Checks\")\n    39\u2192dispatcher: Dispatcher | None = None\n    40\u2192sqlite_store: SQLiteStore | None = None\n    41\u2192file_watcher: FileWatcher | None = None\n    42\u2192plugin_manager: PluginManager | None = None\n    43\u2192plugin_loader = None  # Dynamic plugin loader\n    44\u2192auth_manager: AuthManager | None = None\n    45\u2192security_config: SecurityConfig | None = None\n    46\u2192cache_manager = None\n    47\u2192query_cache: QueryResultCache | None = None\n    48\u2192bm25_indexer: BM25Indexer | None = None\n    49\u2192hybrid_search: HybridSearch | None = None\n    50\u2192fuzzy_indexer: FuzzyIndexer | None = None\n    51\u2192\n    52\u2192# Initialize metrics and health checking\n    53\u2192metrics_collector = get_metrics_collector()\n    54\u2192health_checker = get_health_checker()\n    55\u2192business_metrics = get_business_metrics()\n    56\u2192\n    57\u2192# Setup metrics middleware\n    58\u2192setup_metrics_middleware(app, enable_detailed_metrics=True)\n    59\u2192\n    60\u2192@app.on_event(\"startup\")\n    61\u2192async def startup_event():\n    62\u2192    \"\"\"Initialize the dispatcher and register plugins on startup.\"\"\"\n    63\u2192    global dispatcher, sqlite_store, file_watcher, plugin_manager, plugin_loader, auth_manager, security_config, cache_manager, query_cache, bm25_indexer, hybrid_search, fuzzy_indexer\n    64\u2192    \n    65\u2192    try:\n    66\u2192        # Initialize security configuration\n    67\u2192        logger.info(\"Initializing security configuration...\")\n    68\u2192        security_config = SecurityConfig(\n    69\u2192            jwt_secret_key=os.getenv(\"JWT_SECRET_KEY\", \"your-super-secret-jwt-key-change-in-production-min-32-chars\"),\n    70\u2192            jwt_algorithm=\"HS256\",\n    71\u2192            access_token_expire_minutes=int(os.getenv(\"ACCESS_TOKEN_EXPIRE_MINUTES\", \"30\")),\n    72\u2192            refresh_token_expire_days=int(os.getenv(\"REFRESH_TOKEN_EXPIRE_DAYS\", \"7\")),\n    73\u2192            password_min_length=int(os.getenv(\"PASSWORD_MIN_LENGTH\", \"8\")),\n    74\u2192            max_login_attempts=int(os.getenv(\"MAX_LOGIN_ATTEMPTS\", \"5\")),\n    75\u2192            lockout_duration_minutes=int(os.getenv(\"LOCKOUT_DURATION_MINUTES\", \"15\")),\n    76\u2192            rate_limit_requests=int(os.getenv(\"RATE_LIMIT_REQUESTS\", \"100\")),\n    77\u2192            rate_limit_window_minutes=int(os.getenv(\"RATE_LIMIT_WINDOW_MINUTES\", \"1\")),\n    78\u2192            cors_origins=os.getenv(\"CORS_ORIGINS\", \"*\").split(\",\"),\n    79\u2192            cors_methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\"],\n    80\u2192            cors_headers=[\"*\"]\n    81\u2192        )\n    82\u2192        \n    83\u2192        # Initialize authentication manager\n    84\u2192        logger.info(\"Initializing authentication manager...\")\n    85\u2192        auth_manager = AuthManager(security_config)\n    86\u2192        \n    87\u2192        # Create default admin user if it doesn't exist\n    88\u2192        admin_user = await auth_manager.get_user_by_username(\"admin\")\n    89\u2192        if not admin_user:\n    90\u2192            admin_password = os.getenv(\"DEFAULT_ADMIN_PASSWORD\", \"admin123!\")\n    91\u2192            logger.info(\"Creating default admin user...\")\n    92\u2192            await auth_manager.create_user(\n    93\u2192                username=\"admin\",\n    94\u2192                password=admin_password,\n    95\u2192                email=os.getenv(\"DEFAULT_ADMIN_EMAIL\", \"admin@localhost\"),\n    96\u2192                role=UserRole.ADMIN\n    97\u2192            )\n    98\u2192            logger.info(\"Default admin user created\")\n    99\u2192        \n   100\u2192        # Set up security middleware\n   101\u2192        logger.info(\"Setting up security middleware...\")\n   102\u2192        security_middleware = SecurityMiddlewareStack(app, security_config, auth_manager)\n   103\u2192        security_middleware.setup_middleware()\n   104\u2192        logger.info(\"Security middleware configured successfully\")\n   105\u2192        \n   106\u2192        # Initialize cache system\n   107\u2192        logger.info(\"Initializing cache system...\")\n   108\u2192        cache_backend_type = os.getenv(\"CACHE_BACKEND\", \"memory\").lower()\n   109\u2192        redis_url = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n   110\u2192        \n   111\u2192        if cache_backend_type == \"redis\":\n   112\u2192            try:\n   113\u2192                cache_manager = CacheManagerFactory.create_redis_cache(\n   114\u2192                    redis_url=redis_url,\n   115\u2192                    default_ttl=int(os.getenv(\"CACHE_DEFAULT_TTL\", \"3600\"))\n   116\u2192                )\n   117\u2192                logger.info(\"Using Redis cache backend\")\n   118\u2192            except Exception as e:\n   119\u2192                logger.warning(f\"Failed to initialize Redis cache, falling back to memory: {e}\")\n   120\u2192                cache_manager = CacheManagerFactory.create_memory_cache()\n   121\u2192        elif cache_backend_type == \"hybrid\":\n   122\u2192            try:\n   123\u2192                cache_manager = CacheManagerFactory.create_hybrid_cache(\n   124\u2192                    redis_url=redis_url,\n   125\u2192                    max_entries=int(os.getenv(\"CACHE_MAX_ENTRIES\", \"1000\")),\n   126\u2192                    max_memory_mb=int(os.getenv(\"CACHE_MAX_MEMORY_MB\", \"100\")),\n   127\u2192                    default_ttl=int(os.getenv(\"CACHE_DEFAULT_TTL\", \"3600\"))\n   128\u2192                )\n   129\u2192                logger.info(\"Using hybrid cache backend\")\n   130\u2192            except Exception as e:\n   131\u2192                logger.warning(f\"Failed to initialize hybrid cache, falling back to memory: {e}\")\n   132\u2192                cache_manager = CacheManagerFactory.create_memory_cache()\n   133\u2192        else:\n   134\u2192            cache_manager = CacheManagerFactory.create_memory_cache(\n   135\u2192                max_entries=int(os.getenv(\"CACHE_MAX_ENTRIES\", \"1000\")),\n   136\u2192                max_memory_mb=int(os.getenv(\"CACHE_MAX_MEMORY_MB\", \"100\")),\n   137\u2192                default_ttl=int(os.getenv(\"CACHE_DEFAULT_TTL\", \"3600\"))\n   138\u2192            )\n   139\u2192            logger.info(\"Using memory cache backend\")\n   140\u2192        \n   141\u2192        await cache_manager.initialize()\n   142\u2192        \n   143\u2192        # Initialize query result cache\n   144\u2192        query_cache_config = QueryCacheConfig(\n   145\u2192            enabled=os.getenv(\"QUERY_CACHE_ENABLED\", \"true\").lower() == \"true\",\n   146\u2192            default_ttl=int(os.getenv(\"QUERY_CACHE_DEFAULT_TTL\", \"300\")),\n   147\u2192            symbol_lookup_ttl=int(os.getenv(\"QUERY_CACHE_SYMBOL_TTL\", \"1800\")),\n   148\u2192            search_ttl=int(os.getenv(\"QUERY_CACHE_SEARCH_TTL\", \"600\")),\n   149\u2192            semantic_search_ttl=int(os.getenv(\"QUERY_CACHE_SEMANTIC_TTL\", \"3600\"))\n   150\u2192        )\n   151\u2192        query_cache = QueryResultCache(cache_manager, query_cache_config)\n   152\u2192        logger.info(\"Query result cache initialized successfully\")\n   153\u2192        # Check for portable index first\n   154\u2192        workspace_root = Path(\".\")\n   155\u2192        discovery = IndexDiscovery(workspace_root)\n   156\u2192        \n   157\u2192        if discovery.is_index_enabled():\n   158\u2192            logger.info(\"MCP portable index detected\")\n   159\u2192            \n   160\u2192            # Try to use existing index\n   161\u2192            index_path = discovery.get_local_index_path()\n   162\u2192            \n   163\u2192            if not index_path and discovery.should_download_index():\n   164\u2192                logger.info(\"Attempting to download index from GitHub artifacts...\")\n   165\u2192                if discovery.download_latest_index():\n   166\u2192                    index_path = discovery.get_local_index_path()\n   167\u2192                    logger.info(\"Successfully downloaded index from artifacts\")\n   168\u2192                else:\n   169\u2192                    logger.info(\"Could not download index, will use default\")\n   170\u2192            \n   171\u2192            if index_path:\n   172\u2192                logger.info(f\"Using portable index: {index_path}\")\n   173\u2192                sqlite_store = SQLiteStore(str(index_path))\n   174\u2192                \n   175\u2192                # Log index info\n   176\u2192                info = discovery.get_index_info()\n   177\u2192                if info['metadata']:\n   178\u2192                    meta = info['metadata']\n   179\u2192                    logger.info(f\"Index created: {meta.get('created_at', 'unknown')}\")\n   180\u2192                    logger.info(f\"Index commit: {meta.get('commit', 'unknown')[:8]}\")\n   181\u2192            else:\n   182\u2192                logger.info(\"No portable index found, using default\")\n   183\u2192                sqlite_store = SQLiteStore(\"code_index.db\")\n   184\u2192        else:\n   185\u2192            # Initialize SQLite store with default\n   186\u2192            logger.info(\"Initializing SQLite store with default path...\")\n   187\u2192            sqlite_store = SQLiteStore(\"code_index.db\")\n   188\u2192        \n   189\u2192        logger.info(\"SQLite store initialized successfully\")\n   190\u2192        \n   191\u2192        # Initialize plugin system with dynamic discovery\n   192\u2192        logger.info(\"Initializing plugin system with dynamic discovery...\")\n   193\u2192        from .plugin_system.discovery import get_plugin_discovery\n   194\u2192        from .plugin_system.loader import get_plugin_loader\n   195\u2192        \n   196\u2192        # Discover all available plugins\n   197\u2192        plugin_discovery = get_plugin_discovery()\n   198\u2192        discovered = plugin_discovery.discover_plugins()\n   199\u2192        logger.info(f\"Discovered {len(discovered)} plugins: {list(discovered.keys())}\")\n   200\u2192        \n   201\u2192        # Initialize plugin loader\n   202\u2192        plugin_loader = get_plugin_loader()\n   203\u2192        \n   204\u2192        # Load plugins based on configuration or all discovered plugins\n   205\u2192        config_path = Path(\"plugins.yaml\")\n   206\u2192        if config_path.exists():\n   207\u2192            # Load specific plugins from config\n   208\u2192            import yaml\n   209\u2192            with open(config_path, 'r') as f:\n   210\u2192                plugin_config = yaml.safe_load(f)\n   211\u2192            \n   212\u2192            enabled_languages = plugin_config.get('enabled_languages', list(discovered.keys()))\n   213\u2192            logger.info(f\"Loading plugins for languages: {enabled_languages}\")\n   214\u2192        else:\n   215\u2192            # Load all discovered plugins\n   216\u2192            enabled_languages = list(discovered.keys())\n   217\u2192            logger.info(\"No plugins.yaml found, loading all discovered plugins\")\n   218\u2192        \n   219\u2192        # Load plugins\n   220\u2192        plugin_instances = []\n   221\u2192        for language in enabled_languages:\n   222\u2192            try:\n   223\u2192                plugin = plugin_loader.load_plugin(language)\n   224\u2192                if plugin:\n   225\u2192                    plugin_instances.append(plugin)\n   226\u2192                    logger.info(f\"Successfully loaded plugin for {language}\")\n   227\u2192            except Exception as e:\n   228\u2192                logger.error(f\"Failed to load plugin for {language}: {e}\")\n   229\u2192        \n   230\u2192        logger.info(f\"Loaded {len(plugin_instances)} plugins\")\n   231\u2192        \n   232\u2192        # Create plugin manager for compatibility\n   233\u2192        plugin_manager = PluginManager(sqlite_store=sqlite_store)\n   234\u2192        # Register loaded plugins with manager\n   235\u2192        for plugin in plugin_instances:\n   236\u2192            if hasattr(plugin, 'get_language'):\n   237\u2192                lang = plugin.get_language()\n   238\u2192                plugin_manager._plugins[lang] = plugin\n   239\u2192        \n   240\u2192        logger.info(f\"Loaded {len(plugin_instances)} active plugins\")\n   241\u2192        \n   242\u2192        # Create a new Dispatcher instance with the loaded plugins\n   243\u2192        logger.info(\"Creating dispatcher...\")\n   244\u2192        dispatcher = Dispatcher(plugin_instances)\n   245\u2192        logger.info(f\"Dispatcher created with {len(plugin_instances)} plugins\")\n   246\u2192        \n   247\u2192        # Initialize BM25 indexer\n   248\u2192        logger.info(\"Initializing BM25 indexer...\")\n   249\u2192        bm25_indexer = BM25Indexer(sqlite_store)\n   250\u2192        logger.info(\"BM25 indexer initialized successfully\")\n   251\u2192        \n   252\u2192        # Initialize Fuzzy indexer\n   253\u2192        logger.info(\"Initializing Fuzzy indexer...\")\n   254\u2192        fuzzy_indexer = FuzzyIndexer(sqlite_store)\n   255\u2192        logger.info(\"Fuzzy indexer initialized successfully\")\n   256\u2192        \n   257\u2192        # Check if semantic indexer is available\n   258\u2192        semantic_indexer = None\n   259\u2192        try:\n   260\u2192            from .utils.semantic_indexer import SemanticIndexer\n   261\u2192            semantic_indexer = SemanticIndexer(sqlite_store)\n   262\u2192            logger.info(\"Semantic indexer initialized successfully\")\n   263\u2192        except ImportError:\n   264\u2192            logger.warning(\"Semantic indexer not available (missing dependencies)\")\n   265\u2192        \n   266\u2192        # Initialize Hybrid Search\n   267\u2192        logger.info(\"Initializing Hybrid Search...\")\n   268\u2192        hybrid_config = HybridSearchConfig(\n   269\u2192            bm25_weight=float(os.getenv(\"HYBRID_BM25_WEIGHT\", \"0.5\")),\n   270\u2192            semantic_weight=float(os.getenv(\"HYBRID_SEMANTIC_WEIGHT\", \"0.3\")),\n   271\u2192            fuzzy_weight=float(os.getenv(\"HYBRID_FUZZY_WEIGHT\", \"0.2\")),\n   272\u2192            enable_bm25=True,\n   273\u2192            enable_semantic=semantic_indexer is not None,\n   274\u2192            enable_fuzzy=True,\n   275\u2192            parallel_execution=True,\n   276\u2192            cache_results=True\n   277\u2192        )\n   278\u2192        hybrid_search = HybridSearch(\n   279\u2192            storage=sqlite_store,\n   280\u2192            bm25_indexer=bm25_indexer,\n   281\u2192            semantic_indexer=semantic_indexer,\n   282\u2192            fuzzy_indexer=fuzzy_indexer,\n   283\u2192            config=hybrid_config\n   284\u2192        )\n   285\u2192        logger.info(f\"Hybrid Search initialized (BM25: {hybrid_config.enable_bm25}, Semantic: {hybrid_config.enable_semantic}, Fuzzy: {hybrid_config.enable_fuzzy})\")\n   286\u2192        \n   287\u2192        # Initialize file watcher with dispatcher and query cache\n   288\u2192        logger.info(\"Starting file watcher...\")\n   289\u2192        file_watcher = FileWatcher(Path(\".\"), dispatcher, query_cache)\n   290\u2192        file_watcher.start()\n   291\u2192        logger.info(\"File watcher started for current directory with cache invalidation\")\n   292\u2192        \n   293\u2192        # Store in app.state for potential future use\n   294\u2192        app.state.dispatcher = dispatcher\n   295\u2192        app.state.sqlite_store = sqlite_store\n   296\u2192        app.state.file_watcher = file_watcher\n   297\u2192        app.state.plugin_manager = plugin_manager\n   298\u2192        app.state.auth_manager = auth_manager\n   299\u2192        app.state.security_config = security_config\n   300\u2192        app.state.cache_manager = cache_manager\n   301\u2192        app.state.query_cache = query_cache\n   302\u2192        app.state.metrics_collector = metrics_collector\n   303\u2192        app.state.health_checker = health_checker\n   304\u2192        app.state.business_metrics = business_metrics\n   305\u2192        app.state.bm25_indexer = bm25_indexer\n   306\u2192        app.state.hybrid_search = hybrid_search\n   307\u2192        app.state.fuzzy_indexer = fuzzy_indexer\n   308\u2192        \n   309\u2192        # Update status to include search capabilities\n   310\u2192        search_capabilities = []\n   311\u2192        if bm25_indexer:\n   312\u2192            search_capabilities.append(\"bm25\")\n   313\u2192        if fuzzy_indexer:\n   314\u2192            search_capabilities.append(\"fuzzy\") \n   315\u2192        if semantic_indexer:\n   316\u2192            search_capabilities.append(\"semantic\")\n   317\u2192        if hybrid_search:\n   318\u2192            search_capabilities.append(\"hybrid\")\n   319\u2192        \n   320\u2192        logger.info(f\"Search capabilities: {', '.join(search_capabilities)}\")\n   321\u2192        \n   322\u2192        # Register health checks for system components\n   323\u2192        logger.info(\"Registering component health checks...\")\n   324\u2192        health_checker.register_health_check(\n   325\u2192            \"database\", \n   326\u2192            health_checker.create_database_health_check(\"code_index.db\")\n   327\u2192        )\n   328\u2192        health_checker.register_health_check(\n   329\u2192            \"plugins\",\n   330\u2192            health_checker.create_plugin_health_check(plugin_manager)\n   331\u2192        )\n   332\u2192        \n   333\u2192        # Update system metrics\n   334\u2192        business_metrics.update_system_metrics(\n   335\u2192            active_plugins=len(plugin_instances),\n   336\u2192            indexed_files=0,  # Will be updated as files are indexed\n   337\u2192            database_size=0,  # Will be updated periodically\n   338\u2192            memory_usage=0    # Will be updated by middleware\n   339\u2192        )\n   340\u2192        \n   341\u2192        # Log loaded plugins with detailed status\n   342\u2192        plugin_status = plugin_manager.get_detailed_plugin_status()\n   343\u2192        for name, status in plugin_status.items():\n   344\u2192            basic_info = status['basic_info']\n   345\u2192            runtime_info = status['runtime_info']\n   346\u2192            logger.info(f\"Plugin '{name}': {runtime_info['state']} (v{basic_info['version']}, language: {basic_info['language']}, enabled: {runtime_info['enabled']})\")\n   347\u2192            if runtime_info.get('error'):\n   348\u2192                logger.warning(f\"Plugin '{name}' has error: {runtime_info['error']}\")\n   349\u2192        \n   350\u2192        logger.info(\"MCP Server initialized successfully with dynamic plugin system, SQLite persistence, and file watcher\")\n   351\u2192    except Exception as e:\n   352\u2192        logger.error(f\"Failed to initialize MCP Server: {e}\", exc_info=True)\n   353\u2192        raise\n   354\u2192\n   355\u2192@app.on_event(\"shutdown\")\n   356\u2192async def shutdown_event():\n   357\u2192    \"\"\"Clean up resources on shutdown.\"\"\"\n   358\u2192    global file_watcher, plugin_manager, cache_manager\n   359\u2192    \n   360\u2192    if file_watcher:\n   361\u2192        try:\n   362\u2192            file_watcher.stop()\n   363\u2192            logger.info(\"File watcher stopped successfully\")\n   364\u2192        except Exception as e:\n   365\u2192            logger.error(f\"Error stopping file watcher: {e}\", exc_info=True)\n   366\u2192    \n   367\u2192    if plugin_manager:\n   368\u2192        try:\n   369\u2192            shutdown_result = plugin_manager.shutdown_safe()\n   370\u2192            if shutdown_result.success:\n   371\u2192                logger.info(\"Plugin manager shutdown successfully\")\n   372\u2192            else:\n   373\u2192                logger.error(f\"Plugin manager shutdown failed: {shutdown_result.error.message}\")\n   374\u2192                logger.error(f\"Shutdown error details: {shutdown_result.error.details}\")\n   375\u2192        except Exception as e:\n   376\u2192            logger.error(f\"Error shutting down plugin manager: {e}\", exc_info=True)\n   377\u2192    \n   378\u2192    if cache_manager:\n   379\u2192        try:\n   380\u2192            await cache_manager.shutdown()\n   381\u2192            logger.info(\"Cache manager shutdown successfully\")\n   382\u2192        except Exception as e:\n   383\u2192            logger.error(f\"Error shutting down cache manager: {e}\", exc_info=True)\n   384\u2192\n   385\u2192# Authentication endpoints\n   386\u2192\n   387\u2192@app.post(\"/api/v1/auth/login\")\n   388\u2192async def login(credentials: AuthCredentials) -> Dict[str, Any]:\n   389\u2192    \"\"\"User login endpoint.\"\"\"\n   390\u2192    if auth_manager is None:\n   391\u2192        raise HTTPException(503, \"Authentication service not ready\")\n   392\u2192    \n   393\u2192    try:\n   394\u2192        user = await auth_manager.authenticate_user(credentials)\n   395\u2192        if not user:\n   396\u2192            raise HTTPException(401, \"Invalid credentials\")\n   397\u2192        \n   398\u2192        access_token = await auth_manager.create_access_token(user)\n   399\u2192        refresh_token = await auth_manager.create_refresh_token(user)\n   400\u2192        \n   401\u2192        return {\n   402\u2192            \"access_token\": access_token,\n   403\u2192            \"refresh_token\": refresh_token,\n   404\u2192            \"token_type\": \"bearer\",\n   405\u2192            \"expires_in\": security_config.access_token_expire_minutes * 60,\n   406\u2192            \"user\": {\n   407\u2192                \"id\": user.id,\n   408\u2192                \"username\": user.username,\n   409\u2192                \"role\": user.role.value,\n   410\u2192                \"permissions\": [p.value for p in user.permissions]\n   411\u2192            }\n   412\u2192        }\n   413\u2192    except Exception as e:\n   414\u2192        logger.error(f\"Login failed for user '{credentials.username}': {e}\")\n   415\u2192        raise HTTPException(401, \"Authentication failed\")\n   416\u2192\n   417\u2192@app.post(\"/api/v1/auth/refresh\")\n   418\u2192async def refresh_token(refresh_token: str) -> Dict[str, Any]:\n   419\u2192    \"\"\"Refresh access token.\"\"\"\n   420\u2192    if auth_manager is None:\n   421\u2192        raise HTTPException(503, \"Authentication service not ready\")\n   422\u2192    \n   423\u2192    try:\n   424\u2192        new_access_token = await auth_manager.refresh_access_token(refresh_token)\n   425\u2192        if not new_access_token:\n   426\u2192            raise HTTPException(401, \"Invalid refresh token\")\n   427\u2192        \n   428\u2192        return {\n   429\u2192            \"access_token\": new_access_token,\n   430\u2192            \"token_type\": \"bearer\",\n   431\u2192            \"expires_in\": security_config.access_token_expire_minutes * 60\n   432\u2192        }\n   433\u2192    except Exception as e:\n   434\u2192        logger.error(f\"Token refresh failed: {e}\")\n   435\u2192        raise HTTPException(401, \"Token refresh failed\")\n   436\u2192\n   437\u2192@app.post(\"/api/v1/auth/logout\")\n   438\u2192async def logout(\n   439\u2192    refresh_token: Optional[str] = None,\n   440\u2192    current_user: User = Depends(get_current_active_user)\n   441\u2192) -> Dict[str, str]:\n   442\u2192    \"\"\"User logout endpoint.\"\"\"\n   443\u2192    if auth_manager is None:\n   444\u2192        raise HTTPException(503, \"Authentication service not ready\")\n   445\u2192    \n   446\u2192    try:\n   447\u2192        if refresh_token:\n   448\u2192            await auth_manager.revoke_refresh_token(refresh_token)\n   449\u2192        \n   450\u2192        await auth_manager._log_security_event(\n   451\u2192            \"user_logout\",\n   452\u2192            user_id=current_user.id,\n   453\u2192            username=current_user.username\n   454\u2192        )\n   455\u2192        \n   456\u2192        return {\"message\": \"Successfully logged out\"}\n   457\u2192    except Exception as e:\n   458\u2192        logger.error(f\"Logout failed: {e}\")\n   459\u2192        raise HTTPException(500, \"Logout failed\")\n   460\u2192\n   461\u2192@app.get(\"/api/v1/auth/me\")\n   462\u2192async def get_current_user_info(current_user: User = Depends(get_current_active_user)) -> Dict[str, Any]:\n   463\u2192    \"\"\"Get current user information.\"\"\"\n   464\u2192    return {\n   465\u2192        \"id\": current_user.id,\n   466\u2192        \"username\": current_user.username,\n   467\u2192        \"email\": current_user.email,\n   468\u2192        \"role\": current_user.role.value,\n   469\u2192        \"permissions\": [p.value for p in current_user.permissions],\n   470\u2192        \"is_active\": current_user.is_active,\n   471\u2192        \"created_at\": current_user.created_at.isoformat(),\n   472\u2192        \"last_login\": current_user.last_login.isoformat() if current_user.last_login else None\n   473\u2192    }\n   474\u2192\n   475\u2192@app.post(\"/api/v1/auth/register\")\n   476\u2192async def register(\n   477\u2192    credentials: AuthCredentials,\n   478\u2192    email: Optional[str] = None,\n   479\u2192    admin_user: User = Depends(require_role(UserRole.ADMIN))\n   480\u2192) -> Dict[str, Any]:\n   481\u2192    \"\"\"Register new user (admin only).\"\"\"\n   482\u2192    if auth_manager is None:\n   483\u2192        raise HTTPException(503, \"Authentication service not ready\")\n   484\u2192    \n   485\u2192    try:\n   486\u2192        user = await auth_manager.create_user(\n   487\u2192            username=credentials.username,\n   488\u2192            password=credentials.password,\n   489\u2192            email=email,\n   490\u2192            role=UserRole.USER\n   491\u2192        )\n   492\u2192        \n   493\u2192        return {\n   494\u2192            \"message\": \"User created successfully\",\n   495\u2192            \"user\": {\n   496\u2192                \"id\": user.id,\n   497\u2192                \"username\": user.username,\n   498\u2192                \"email\": user.email,\n   499\u2192                \"role\": user.role.value\n   500\u2192            }\n   501\u2192        }\n   502\u2192    except Exception as e:\n   503\u2192        logger.error(f\"User registration failed: {e}\")\n   504\u2192        raise HTTPException(400, str(e))\n   505\u2192\n   506\u2192# Security management endpoints\n   507\u2192\n   508\u2192@app.get(\"/api/v1/security/events\")\n   509\u2192async def get_security_events(\n   510\u2192    limit: int = 100,\n   511\u2192    admin_user: User = Depends(require_role(UserRole.ADMIN))\n   512\u2192) -> Dict[str, Any]:\n   513\u2192    \"\"\"Get security events (admin only).\"\"\"\n   514\u2192    if auth_manager is None:\n   515\u2192        raise HTTPException(503, \"Authentication service not ready\")\n   516\u2192    \n   517\u2192    try:\n   518\u2192        events = await auth_manager.get_security_events(limit)\n   519\u2192        return {\n   520\u2192            \"events\": [\n   521\u2192                {\n   522\u2192                    \"id\": event.id,\n   523\u2192                    \"event_type\": event.event_type,\n   524\u2192                    \"user_id\": event.user_id,\n   525\u2192                    \"username\": event.username,\n   526\u2192                    \"ip_address\": event.ip_address,\n   527\u2192                    \"timestamp\": event.timestamp.isoformat(),\n   528\u2192                    \"details\": event.details,\n   529\u2192                    \"severity\": event.severity\n   530\u2192                }\n   531\u2192                for event in events\n   532\u2192            ]\n   533\u2192        }\n   534\u2192    except Exception as e:\n   535\u2192        logger.error(f\"Failed to get security events: {e}\")\n   536\u2192        raise HTTPException(500, \"Failed to retrieve security events\")\n   537\u2192\n   538\u2192# Health check endpoints (public)\n   539\u2192@app.get(\"/health\")\n   540\u2192async def health_check() -> Dict[str, Any]:\n   541\u2192    \"\"\"Basic health check endpoint.\"\"\"\n   542\u2192    return {\"status\": \"healthy\", \"service\": \"mcp-server\", \"timestamp\": time.time()}\n   543\u2192\n   544\u2192@app.get(\"/health/detailed\")\n   545\u2192async def detailed_health_check() -> Dict[str, Any]:\n   546\u2192    \"\"\"Detailed health check endpoint.\"\"\"\n   547\u2192    try:\n   548\u2192        overall_health = await health_checker.get_overall_health()\n   549\u2192        component_results = await health_checker.check_all_components()\n   550\u2192        \n   551\u2192        return {\n   552\u2192            \"status\": overall_health.status.value,\n   553\u2192            \"message\": overall_health.message,\n   554\u2192            \"timestamp\": overall_health.timestamp,\n   555\u2192            \"details\": overall_health.details,\n   556\u2192            \"components\": [\n   557\u2192                {\n   558\u2192                    \"component\": result.component,\n   559\u2192                    \"status\": result.status.value,\n   560\u2192                    \"message\": result.message,\n   561\u2192                    \"details\": result.details\n   562\u2192                }\n   563\u2192                for result in component_results\n   564\u2192            ]\n   565\u2192        }\n   566\u2192    except Exception as e:\n   567\u2192        logger.error(f\"Health check failed: {e}\", exc_info=True)\n   568\u2192        return {\n   569\u2192            \"status\": \"unhealthy\",\n   570\u2192            \"message\": f\"Health check failed: {str(e)}\",\n   571\u2192            \"timestamp\": time.time()\n   572\u2192        }\n   573\u2192\n   574\u2192@app.get(\"/health/{component}\")\n   575\u2192async def component_health_check(component: str) -> Dict[str, Any]:\n   576\u2192    \"\"\"Health check for a specific component.\"\"\"\n   577\u2192    try:\n   578\u2192        result = await health_checker.check_component(component)\n   579\u2192        return {\n   580\u2192            \"component\": result.component,\n   581\u2192            \"status\": result.status.value,\n   582\u2192            \"message\": result.message,\n   583\u2192            \"timestamp\": result.timestamp,\n   584\u2192            \"details\": result.details\n   585\u2192        }\n   586\u2192    except Exception as e:\n   587\u2192        logger.error(f\"Component health check failed for {component}: {e}\", exc_info=True)\n   588\u2192        raise HTTPException(500, f\"Health check failed: {str(e)}\")\n   589\u2192\n   590\u2192# Metrics endpoints\n   591\u2192@app.get(\"/metrics\", response_class=PlainTextResponse)\n   592\u2192def get_metrics() -> str:\n   593\u2192    \"\"\"Prometheus metrics endpoint.\"\"\"\n   594\u2192    try:\n   595\u2192        return metrics_collector.get_metrics()\n   596\u2192    except Exception as e:\n   597\u2192        logger.error(f\"Failed to get metrics: {e}\", exc_info=True)\n   598\u2192        raise HTTPException(500, f\"Failed to get metrics: {str(e)}\")\n   599\u2192\n   600\u2192@app.get(\"/metrics\")\n   601\u2192def get_prometheus_metrics() -> Response:\n   602\u2192    \"\"\"Prometheus metrics endpoint.\"\"\"\n   603\u2192    try:\n   604\u2192        prometheus_exporter = get_prometheus_exporter()\n   605\u2192        \n   606\u2192        # Update build info\n   607\u2192        prometheus_exporter.set_build_info(\n   608\u2192            version=\"1.0.0\",\n   609\u2192            commit=os.getenv(\"GIT_COMMIT\", \"unknown\"),\n   610\u2192            build_time=os.getenv(\"BUILD_TIME\", \"unknown\")\n   611\u2192        )\n   612\u2192        \n   613\u2192        # Update system metrics\n   614\u2192        import psutil\n   615\u2192        process = psutil.Process()\n   616\u2192        memory_info = process.memory_info()\n   617\u2192        prometheus_exporter.set_memory_usage(memory_info.rss, memory_info.vms)\n   618\u2192        prometheus_exporter.set_cpu_usage(process.cpu_percent())\n   619\u2192        prometheus_exporter.set_active_threads(threading.active_count())\n   620\u2192        \n   621\u2192        # Update plugin metrics\n   622\u2192        if plugin_loader:\n   623\u2192            stats = plugin_loader.get_statistics()\n   624\u2192            for lang, plugin in plugin_loader.get_active_plugins().items():\n   625\u2192                prometheus_exporter.plugin_status.labels(\n   626\u2192                    plugin=plugin.__class__.__name__,\n   627\u2192                    language=lang\n   628\u2192                ).set(1)\n   629\u2192        \n   630\u2192        # Update file watcher metrics\n   631\u2192        if file_watcher:\n   632\u2192            # This would need to be implemented in FileWatcher\n   633\u2192            # prometheus_exporter.set_files_watched(file_watcher.get_watched_count())\n   634\u2192            pass\n   635\u2192        \n   636\u2192        # Generate metrics\n   637\u2192        metrics = prometheus_exporter.generate_metrics()\n   638\u2192        return Response(\n   639\u2192            content=metrics,\n   640\u2192            media_type=prometheus_exporter.get_content_type()\n   641\u2192        )\n   642\u2192    except Exception as e:\n   643\u2192        logger.error(f\"Failed to generate Prometheus metrics: {e}\", exc_info=True)\n   644\u2192        raise HTTPException(500, f\"Failed to generate metrics: {str(e)}\")\n   645\u2192\n   646\u2192@app.get(\"/metrics/json\")\n   647\u2192def get_metrics_json(current_user: User = Depends(require_permission(Permission.READ))) -> Dict[str, Any]:\n   648\u2192    \"\"\"JSON metrics endpoint for programmatic access.\"\"\"\n   649\u2192    try:\n   650\u2192        families = metrics_collector.get_metric_families()\n   651\u2192        stats = metrics_collector.get_stats()\n   652\u2192        \n   653\u2192        return {\n   654\u2192            \"timestamp\": time.time(),\n   655\u2192            \"collector_stats\": stats,\n   656\u2192            \"metric_families\": families\n   657\u2192        }\n   658\u2192    except Exception as e:\n   659\u2192        logger.error(f\"Failed to get JSON metrics: {e}\", exc_info=True)\n   660\u2192        raise HTTPException(500, f\"Failed to get metrics: {str(e)}\")\n   661\u2192\n   662\u2192@app.get(\"/symbol\", response_model=SymbolDef | None)\n   663\u2192async def symbol(\n   664\u2192    symbol: str,\n   665\u2192    current_user: User = Depends(require_permission(Permission.READ))\n   666\u2192):\n   667\u2192    if dispatcher is None:\n   668\u2192        logger.error(\"Symbol lookup attempted but dispatcher not ready\")\n   669\u2192        raise HTTPException(503, \"Dispatcher not ready\")\n   670\u2192    \n   671\u2192    start_time = time.time()\n   672\u2192    try:\n   673\u2192        logger.debug(f\"Looking up symbol: {symbol} for user: {current_user.username}\")\n   674\u2192        \n   675\u2192        # Try cache first if query cache is available\n   676\u2192        cached_result = None\n   677\u2192        if query_cache and query_cache.config.enabled:\n   678\u2192            cached_result = await query_cache.get_cached_result(\n   679\u2192                QueryType.SYMBOL_LOOKUP,\n   680\u2192                symbol=symbol\n   681\u2192            )\n   682\u2192        \n   683\u2192        if cached_result is not None:\n   684\u2192            logger.debug(f\"Found cached symbol: {symbol}\")\n   685\u2192            duration = time.time() - start_time\n   686\u2192            business_metrics.record_search_performed(\n   687\u2192                query=symbol,\n   688\u2192                semantic=False,\n   689\u2192                results_count=1,\n   690\u2192                duration=duration\n   691\u2192            )\n   692\u2192            return cached_result\n   693\u2192        \n   694\u2192        # Record symbol lookup metrics\n   695\u2192        with metrics_collector.time_function(\"symbol_lookup\"):\n   696\u2192            result = dispatcher.lookup(symbol)\n   697\u2192        \n   698\u2192        # Cache the result if available\n   699\u2192        if query_cache and query_cache.config.enabled and result:\n   700\u2192            await query_cache.cache_result(\n   701\u2192                QueryType.SYMBOL_LOOKUP,\n   702\u2192                result,\n   703\u2192                symbol=symbol\n   704\u2192            )\n   705\u2192        \n   706\u2192        # Record business metrics\n   707\u2192        duration = time.time() - start_time\n   708\u2192        business_metrics.record_search_performed(\n   709\u2192            query=symbol,\n   710\u2192            semantic=False,\n   711\u2192            results_count=1 if result else 0,\n   712\u2192            duration=duration\n   713\u2192        )\n   714\u2192        \n   715\u2192        if result:\n   716\u2192            logger.debug(f\"Found symbol: {symbol}\")\n   717\u2192        else:\n   718\u2192            logger.debug(f\"Symbol not found: {symbol}\")\n   719\u2192        return result\n   720\u2192    except Exception as e:\n   721\u2192        duration = time.time() - start_time\n   722\u2192        business_metrics.record_search_performed(\n   723\u2192            query=symbol,\n   724\u2192            semantic=False,\n   725\u2192            results_count=0,\n   726\u2192            duration=duration\n   727\u2192        )\n   728\u2192        logger.error(f\"Error looking up symbol '{symbol}': {e}\", exc_info=True)\n   729\u2192        raise HTTPException(500, f\"Internal error during symbol lookup: {str(e)}\")\n   730\u2192\n   731\u2192@app.get(\"/search\", response_model=list[SearchResult])\n   732\u2192async def search(\n   733\u2192    q: str,\n   734\u2192    semantic: bool = False,\n   735\u2192    limit: int = 20,\n   736\u2192    mode: str = \"auto\",  # \"auto\", \"hybrid\", \"bm25\", \"semantic\", \"fuzzy\", \"classic\"\n   737\u2192    language: Optional[str] = None,\n   738\u2192    file_filter: Optional[str] = None,\n   739\u2192    current_user: User = Depends(require_permission(Permission.READ))\n   740\u2192):\n   741\u2192    \"\"\"Search with support for multiple modes including hybrid search.\n   742\u2192    \n   743\u2192    Args:\n   744\u2192        q: Search query\n   745\u2192        semantic: Whether to use semantic search (for backward compatibility)\n   746\u2192        limit: Maximum number of results\n   747\u2192        mode: Search mode - \"auto\" (default), \"hybrid\", \"bm25\", \"semantic\", \"fuzzy\", or \"classic\"\n   748\u2192        language: Filter by programming language\n   749\u2192        file_filter: Filter by file path pattern\n   750\u2192        current_user: Authenticated user\n   751\u2192    \"\"\"\n   752\u2192    if dispatcher is None and mode == \"classic\":\n   753\u2192        logger.error(\"Search attempted but dispatcher not ready\")\n   754\u2192        raise HTTPException(503, \"Dispatcher not ready\")\n   755\u2192    \n   756\u2192    start_time = time.time()\n   757\u2192    try:\n   758\u2192        # Determine effective search mode\n   759\u2192        effective_mode = mode\n   760\u2192        if mode == \"auto\":\n   761\u2192            # Auto mode: use hybrid if available, otherwise fall back\n   762\u2192            if hybrid_search is not None:\n   763\u2192                effective_mode = \"hybrid\"\n   764\u2192            elif semantic and hasattr(dispatcher, 'search'):\n   765\u2192                effective_mode = \"classic\"\n   766\u2192            else:\n   767\u2192                effective_mode = \"bm25\" if bm25_indexer else \"classic\"\n   768\u2192        \n   769\u2192        logger.debug(f\"Searching for: '{q}' (mode={effective_mode}, limit={limit}, language={language}) for user: {current_user.username}\")\n   770\u2192        \n   771\u2192        # Build filters\n   772\u2192        filters = {}\n   773\u2192        if language:\n   774\u2192            filters['language'] = language\n   775\u2192        if file_filter:\n   776\u2192            filters['file_filter'] = file_filter\n   777\u2192        \n   778\u2192        # Try cache first if query cache is available\n   779\u2192        cache_key_parts = [q, effective_mode, str(limit)]\n   780\u2192        if filters:\n   781\u2192            cache_key_parts.extend([f\"{k}:{v}\" for k, v in sorted(filters.items())])\n   782\u2192        \n   783\u2192        cached_results = None\n   784\u2192        if query_cache and query_cache.config.enabled:\n   785\u2192            query_type = QueryType.SEMANTIC_SEARCH if effective_mode == \"semantic\" else QueryType.SEARCH\n   786\u2192            cached_results = await query_cache.get_cached_result(\n   787\u2192                query_type,\n   788\u2192                q=q,\n   789\u2192                semantic=(effective_mode == \"semantic\"),\n   790\u2192                limit=limit\n   791\u2192            )\n   792\u2192        \n   793\u2192        if cached_results is not None:\n   794\u2192            logger.debug(f\"Found cached search results for: '{q}' ({len(cached_results)} results)\")\n   795\u2192            duration = time.time() - start_time\n   796\u2192            business_metrics.record_search_performed(\n   797\u2192                query=q,\n   798\u2192                semantic=(effective_mode == \"semantic\"),\n   799\u2192                results_count=len(cached_results),\n   800\u2192                duration=duration\n   801\u2192            )\n   802\u2192            return cached_results\n   803\u2192        \n   804\u2192        # Perform search based on mode\n   805\u2192        results = []\n   806\u2192        \n   807\u2192        if effective_mode == \"hybrid\" and hybrid_search:\n   808\u2192            # Use hybrid search\n   809\u2192            with metrics_collector.time_function(\"search\", labels={\"mode\": \"hybrid\"}):\n   810\u2192                hybrid_results = await hybrid_search.search(\n   811\u2192                    query=q,\n   812\u2192                    filters=filters,\n   813\u2192                    limit=limit\n   814\u2192                )\n   815\u2192                # Convert to SearchResult format\n   816\u2192                for r in hybrid_results:\n   817\u2192                    results.append(SearchResult(\n   818\u2192                        file_path=r['filepath'],\n   819\u2192                        snippet=r['snippet'],\n   820\u2192                        score=r['score']\n   821\u2192                    ))\n   822\u2192        \n   823\u2192        elif effective_mode == \"bm25\" and bm25_indexer:\n   824\u2192            # Direct BM25 search\n   825\u2192            with metrics_collector.time_function(\"search\", labels={\"mode\": \"bm25\"}):\n   826\u2192                bm25_results = bm25_indexer.search(q, limit=limit, **filters)\n   827\u2192                for r in bm25_results:\n   828\u2192                    results.append(SearchResult(\n   829\u2192                        file_path=r['filepath'],\n   830\u2192                        snippet=r.get('snippet', ''),\n   831\u2192                        score=r['score']\n   832\u2192                    ))\n   833\u2192        \n   834\u2192        elif effective_mode == \"fuzzy\" and fuzzy_indexer:\n   835\u2192            # Direct fuzzy search\n   836\u2192            with metrics_collector.time_function(\"search\", labels={\"mode\": \"fuzzy\"}):\n   837\u2192                fuzzy_results = fuzzy_indexer.search_fuzzy(q, max_results=limit)\n   838\u2192                for r in fuzzy_results:\n   839\u2192                    results.append(SearchResult(\n   840\u2192                        file_path=r.get('file_path', ''),\n   841\u2192                        snippet=r.get('context', ''),\n   842\u2192                        score=r.get('score', 0.0)\n   843\u2192                    ))\n   844\u2192        \n   845\u2192        elif effective_mode == \"semantic\":\n   846\u2192            # Use classic dispatcher with semantic=True\n   847\u2192            if dispatcher:\n   848\u2192                with metrics_collector.time_function(\"search\", labels={\"mode\": \"semantic\"}):\n   849\u2192                    results = list(dispatcher.search(q, semantic=True, limit=limit))\n   850\u2192            else:\n   851\u2192                raise HTTPException(\n   852\u2192                    503, \n   853\u2192                    detail={\n   854\u2192                        \"error\": \"Semantic search not available\",\n   855\u2192                        \"reason\": \"Missing Voyage AI API key configuration\",\n   856\u2192                        \"setup\": {\n   857\u2192                            \"method_1_mcp_json\": [\n   858\u2192                                \"Configure in .mcp.json (recommended for Claude Code):\",\n   859\u2192                                \"{\",\n   860\u2192                                '  \"mcpServers\": {',\n   861\u2192                                '    \"code-index-mcp\": {',\n   862\u2192                                '      \"command\": \"uvicorn\",',\n   863\u2192                                '      \"args\": [\"mcp_server.gateway:app\"],',\n   864\u2192                                '      \"env\": {',\n   865\u2192                                '        \"VOYAGE_AI_API_KEY\": \"your-key-here\",',\n   866\u2192                                '        \"SEMANTIC_SEARCH_ENABLED\": \"true\"',\n   867\u2192                                '      }',\n   868\u2192                                '    }',\n   869\u2192                                '  }',\n   870\u2192                                '}'\n   871\u2192                            ],\n   872\u2192                            \"method_2_cli\": [\n   873\u2192                                \"Or use Claude Code CLI:\",\n   874\u2192                                \"claude mcp add code-index-mcp -e VOYAGE_AI_API_KEY=your_key -e SEMANTIC_SEARCH_ENABLED=true -- uvicorn mcp_server.gateway:app\"\n   875\u2192                            ],\n   876\u2192                            \"method_3_env\": [\n   877\u2192                                \"Or set environment variables:\",\n   878\u2192                                \"export VOYAGE_AI_API_KEY=your_key\",\n   879\u2192                                \"export SEMANTIC_SEARCH_ENABLED=true\"\n   880\u2192                            ],\n   881\u2192                            \"method_4_dotenv\": [\n   882\u2192                                \"Or add to .env file:\",\n   883\u2192                                \"VOYAGE_AI_API_KEY=your_key\",\n   884\u2192                                \"SEMANTIC_SEARCH_ENABLED=true\"\n   885\u2192                            ],\n   886\u2192                            \"get_api_key\": \"Get your API key from: https://www.voyageai.com/\",\n   887\u2192                            \"alternative\": \"Use mode='hybrid' or mode='bm25' for keyword-based search\"\n   888\u2192                        }\n   889\u2192                    }\n   890\u2192                )\n   891\u2192        \n   892\u2192        else:\n   893\u2192            # Classic search through dispatcher\n   894\u2192            if dispatcher:\n   895\u2192                with metrics_collector.time_function(\"search\", labels={\"mode\": \"classic\"}):\n   896\u2192                    results = list(dispatcher.search(q, semantic=False, limit=limit))\n   897\u2192            else:\n   898\u2192                raise HTTPException(503, \"Classic search not available\")\n   899\u2192        \n   900\u2192        # Cache the results if available\n   901\u2192        if query_cache and query_cache.config.enabled and results:\n   902\u2192            query_type = QueryType.SEMANTIC_SEARCH if effective_mode == \"semantic\" else QueryType.SEARCH\n   903\u2192            await query_cache.cache_result(\n   904\u2192                query_type,\n   905\u2192                results,\n   906\u2192                q=q,\n   907\u2192                semantic=(effective_mode == \"semantic\"),\n   908\u2192                limit=limit\n   909\u2192            )\n   910\u2192        \n   911\u2192        # Record business metrics\n   912\u2192        duration = time.time() - start_time\n   913\u2192        business_metrics.record_search_performed(\n   914\u2192            query=q,\n   915\u2192            semantic=(effective_mode == \"semantic\"),\n   916\u2192            results_count=len(results),\n   917\u2192            duration=duration\n   918\u2192        )\n   919\u2192        \n   920\u2192        logger.debug(f\"Search returned {len(results)} results using {effective_mode} mode\")\n   921\u2192        return results\n   922\u2192    except Exception as e:\n   923\u2192        duration = time.time() - start_time\n   924\u2192        business_metrics.record_search_performed(\n   925\u2192            query=q,\n   926\u2192            semantic=semantic,\n   927\u2192            results_count=0,\n   928\u2192            duration=duration\n   929\u2192        )\n   930\u2192        logger.error(f\"Error during search for '{q}': {e}\", exc_info=True)\n   931\u2192        raise HTTPException(500, f\"Internal error during search: {str(e)}\")\n   932\u2192\n   933\u2192@app.get(\"/search/capabilities\")\n   934\u2192async def get_search_capabilities() -> Dict[str, Any]:\n   935\u2192    \"\"\"Get available search capabilities and configuration guidance.\"\"\"\n   936\u2192    voyage_key = os.environ.get('VOYAGE_API_KEY') or os.environ.get('VOYAGE_AI_API_KEY')\n   937\u2192    semantic_enabled = os.environ.get('SEMANTIC_SEARCH_ENABLED', 'false').lower() == 'true'\n   938\u2192    \n   939\u2192    return {\n   940\u2192        \"available_modes\": {\n   941\u2192            \"bm25\": bm25_indexer is not None,\n   942\u2192            \"fuzzy\": fuzzy_indexer is not None,\n   943\u2192            \"semantic\": semantic_indexer is not None,\n   944\u2192            \"hybrid\": hybrid_search is not None,\n   945\u2192            \"classic\": dispatcher is not None\n   946\u2192        },\n   947\u2192        \"semantic_config\": {\n   948\u2192            \"enabled\": semantic_indexer is not None,\n   949\u2192            \"api_key_configured\": bool(voyage_key),\n   950\u2192            \"semantic_enabled_flag\": semantic_enabled,\n   951\u2192            \"status\": \"ready\" if semantic_indexer else \"not_configured\"\n   952\u2192        },\n   953\u2192        \"configuration_guide\": {\n   954\u2192            \"mcp_json_example\": {\n   955\u2192                \"description\": \"Add to .mcp.json for Claude Code (recommended)\",\n   956\u2192                \"config\": {\n   957\u2192                    \"mcpServers\": {\n   958\u2192                        \"code-index-mcp\": {\n   959\u2192                            \"command\": \"uvicorn\",\n   960\u2192                            \"args\": [\"mcp_server.gateway:app\"],\n   961\u2192                            \"env\": {\n   962\u2192                                \"VOYAGE_AI_API_KEY\": \"your-key-here\",\n   963\u2192                                \"SEMANTIC_SEARCH_ENABLED\": \"true\"\n   964\u2192                            }\n   965\u2192                        }\n   966\u2192                    }\n   967\u2192                }\n   968\u2192            },\n   969\u2192            \"cli_command\": \"claude mcp add code-index-mcp -e VOYAGE_AI_API_KEY=key -e SEMANTIC_SEARCH_ENABLED=true -- uvicorn mcp_server.gateway:app\",\n   970\u2192            \"env_file\": \"Add to .env: VOYAGE_AI_API_KEY=key and SEMANTIC_SEARCH_ENABLED=true\",\n   971\u2192            \"get_api_key\": \"https://www.voyageai.com/\"\n   972\u2192        }\n   973\u2192    }\n   974\u2192\n   975\u2192@app.get(\"/status\")\n   976\u2192async def status(current_user: User = Depends(require_permission(Permission.READ))) -> Dict[str, Any]:\n   977\u2192    \"\"\"Returns server status including plugin information and statistics.\"\"\"\n   978\u2192    if dispatcher is None:\n   979\u2192        return {\n   980\u2192            \"status\": \"error\",\n   981\u2192            \"plugins\": 0,\n   982\u2192            \"indexed_files\": {\"total\": 0, \"by_language\": {}},\n   983\u2192            \"version\": \"0.1.0\",\n   984\u2192            \"message\": \"Dispatcher not initialized\"\n   985\u2192        }\n   986\u2192    \n   987\u2192    try:\n   988\u2192        # Try cache first if query cache is available\n   989\u2192        cached_status = None\n   990\u2192        if query_cache and query_cache.config.enabled:\n   991\u2192            cached_status = await query_cache.get_cached_result(\n   992\u2192                QueryType.PROJECT_STATUS\n   993\u2192            )\n   994\u2192        \n   995\u2192        if cached_status is not None:\n   996\u2192            return cached_status\n   997\u2192        \n   998\u2192        # Get plugin count\n   999\u2192        plugin_count = len(dispatcher._plugins) if hasattr(dispatcher, '_plugins') else 0\n  1000\u2192        \n  1001\u2192        # Get indexed files statistics\n  1002\u2192        indexed_stats = {\"total\": 0, \"by_language\": {}}\n  1003\u2192        if hasattr(dispatcher, 'get_statistics'):\n  1004\u2192            indexed_stats = dispatcher.get_statistics()\n  1005\u2192        elif hasattr(dispatcher, '_plugins'):\n  1006\u2192            # Calculate basic statistics from plugins\n  1007\u2192            for plugin in dispatcher._plugins:\n  1008\u2192                if hasattr(plugin, 'get_indexed_count'):\n  1009\u2192                    count = plugin.get_indexed_count()\n  1010\u2192                    indexed_stats[\"total\"] += count\n  1011\u2192                    lang = getattr(plugin, 'language', getattr(plugin, 'lang', 'unknown'))\n  1012\u2192                    indexed_stats[\"by_language\"][lang] = count\n  1013\u2192        \n  1014\u2192        # Add database statistics if available\n  1015\u2192        db_stats = {}\n  1016\u2192        if sqlite_store:\n  1017\u2192            db_stats = sqlite_store.get_statistics()\n  1018\u2192        \n  1019\u2192        # Add cache statistics if available\n  1020\u2192        cache_stats = {}\n  1021\u2192        if cache_manager:\n  1022\u2192            try:\n  1023\u2192                cache_metrics = await cache_manager.get_metrics()\n  1024\u2192                cache_stats = {\n  1025\u2192                    \"hit_rate\": cache_metrics.hit_rate,\n  1026\u2192                    \"entries\": cache_metrics.entries_count,\n  1027\u2192                    \"memory_usage_mb\": cache_metrics.memory_usage_mb\n  1028\u2192                }\n  1029\u2192            except Exception as e:\n  1030\u2192                logger.warning(f\"Failed to get cache stats: {e}\")\n  1031\u2192        \n  1032\u2192        status_data = {\n  1033\u2192            \"status\": \"operational\",\n  1034\u2192            \"plugins\": plugin_count,\n  1035\u2192            \"indexed_files\": indexed_stats,\n  1036\u2192            \"database\": db_stats,\n  1037\u2192            \"cache\": cache_stats,\n  1038\u2192            \"search_capabilities\": [],\n  1039\u2192            \"version\": \"0.1.0\"\n  1040\u2192        }\n  1041\u2192        \n  1042\u2192        # Add search capabilities\n  1043\u2192        if bm25_indexer:\n  1044\u2192            status_data[\"search_capabilities\"].append(\"bm25\")\n  1045\u2192        if fuzzy_indexer:\n  1046\u2192            status_data[\"search_capabilities\"].append(\"fuzzy\") \n  1047\u2192        if hasattr(app.state, 'semantic_indexer') and app.state.semantic_indexer:\n  1048\u2192            status_data[\"search_capabilities\"].append(\"semantic\")\n  1049\u2192        if hybrid_search:\n  1050\u2192            status_data[\"search_capabilities\"].append(\"hybrid\")\n  1051\u2192        \n  1052\u2192        # Cache the status\n  1053\u2192        if query_cache and query_cache.config.enabled:\n  1054\u2192            await query_cache.cache_result(\n  1055\u2192                QueryType.PROJECT_STATUS,\n  1056\u2192                status_data\n  1057\u2192            )\n  1058\u2192        \n  1059\u2192        return status_data\n  1060\u2192    except Exception as e:\n  1061\u2192        logger.error(f\"Error getting server status: {e}\", exc_info=True)\n  1062\u2192        return {\n  1063\u2192            \"status\": \"error\",\n  1064\u2192            \"plugins\": 0,\n  1065\u2192            \"indexed_files\": {\"total\": 0, \"by_language\": {}},\n  1066\u2192            \"version\": \"0.1.0\",\n  1067\u2192            \"message\": str(e)\n  1068\u2192        }\n  1069\u2192\n  1070\u2192@app.get(\"/plugins\")\n  1071\u2192def plugins(current_user: User = Depends(require_permission(Permission.READ))) -> List[Dict[str, Any]]:\n  1072\u2192    \"\"\"Returns list of loaded plugins with their information.\"\"\"\n  1073\u2192    if plugin_manager is None:\n  1074\u2192        logger.error(\"Plugin list requested but plugin manager not ready\")\n  1075\u2192        raise HTTPException(503, \"Plugin manager not ready\")\n  1076\u2192    \n  1077\u2192    try:\n  1078\u2192        plugin_list = []\n  1079\u2192        plugin_infos = plugin_manager._registry.list_plugins()\n  1080\u2192        plugin_status = plugin_manager.get_plugin_status()\n  1081\u2192        \n  1082\u2192        for info in plugin_infos:\n  1083\u2192            status = plugin_status.get(info.name, {})\n  1084\u2192            plugin_data = {\n  1085\u2192                \"name\": info.name,\n  1086\u2192                \"version\": info.version,\n  1087\u2192                \"description\": info.description,\n  1088\u2192                \"author\": info.author,\n  1089\u2192                \"language\": info.language,\n  1090\u2192                \"file_extensions\": info.file_extensions,\n  1091\u2192                \"state\": status.get('state', 'unknown'),\n  1092\u2192                \"enabled\": status.get('enabled', False)\n  1093\u2192            }\n  1094\u2192            plugin_list.append(plugin_data)\n  1095\u2192        \n  1096\u2192        logger.debug(f\"Returning {len(plugin_list)} plugins\")\n  1097\u2192        return plugin_list\n  1098\u2192    except Exception as e:\n  1099\u2192        logger.error(f\"Error getting plugin list: {e}\", exc_info=True)\n  1100\u2192        raise HTTPException(500, f\"Internal error getting plugins: {str(e)}\")\n  1101\u2192\n  1102\u2192@app.post(\"/reindex\")\n  1103\u2192async def reindex(\n  1104\u2192    path: Optional[str] = None,\n  1105\u2192    current_user: User = Depends(require_permission(Permission.EXECUTE))\n  1106\u2192) -> Dict[str, str]:\n  1107\u2192    \"\"\"Triggers manual reindexing of files.\n  1108\u2192    \n  1109\u2192    Args:\n  1110\u2192        path: Optional specific directory path to reindex. If not provided,\n  1111\u2192              reindexes all configured paths.\n  1112\u2192    \n  1113\u2192    Returns:\n  1114\u2192        Task status information.\n  1115\u2192    \"\"\"\n  1116\u2192    if dispatcher is None:\n  1117\u2192        logger.error(\"Reindex requested but dispatcher not ready\")\n  1118\u2192        raise HTTPException(503, \"Dispatcher not ready\")\n  1119\u2192    \n  1120\u2192    try:\n  1121\u2192        logger.info(f\"Manual reindex requested for path: {path or 'all'}\")\n  1122\u2192        # Since dispatcher has index_file method, we can use it for reindexing\n  1123\u2192        if path:\n  1124\u2192            # Reindex specific path\n  1125\u2192            target_path = Path(path)\n  1126\u2192            if not target_path.exists():\n  1127\u2192                raise HTTPException(404, f\"Path not found: {path}\")\n  1128\u2192            \n  1129\u2192            indexed_count = 0\n  1130\u2192            if target_path.is_file():\n  1131\u2192                # Single file\n  1132\u2192                dispatcher.index_file(target_path)\n  1133\u2192                indexed_count = 1\n  1134\u2192            else:\n  1135\u2192                # Directory - find all supported files\n  1136\u2192                for file_path in target_path.rglob(\"*\"):\n  1137\u2192                    if file_path.is_file():\n  1138\u2192                        try:\n  1139\u2192                            # Check if any plugin supports this file\n  1140\u2192                            for plugin in dispatcher._plugins:\n  1141\u2192                                if plugin.supports(file_path):\n  1142\u2192                                    dispatcher.index_file(file_path)\n  1143\u2192                                    indexed_count += 1\n  1144\u2192                                    break\n  1145\u2192                        except Exception as e:\n  1146\u2192                            # Log but continue with other files\n  1147\u2192                            logger.warning(f\"Failed to index {file_path}: {e}\")\n  1148\u2192            \n  1149\u2192            logger.info(f\"Successfully reindexed {indexed_count} files in {path}\")\n  1150\u2192            return {\n  1151\u2192                \"status\": \"completed\",\n  1152\u2192                \"message\": f\"Reindexed {indexed_count} files in {path}\"\n  1153\u2192            }\n  1154\u2192        else:\n  1155\u2192            # Reindex all supported files\n  1156\u2192            indexed_count = 0\n  1157\u2192            indexed_by_type = {}\n  1158\u2192            \n  1159\u2192            # Find all files and check if any plugin supports them\n  1160\u2192            for file_path in Path(\".\").rglob(\"*\"):\n  1161\u2192                if file_path.is_file():\n  1162\u2192                    try:\n  1163\u2192                        # Check if any plugin supports this file\n  1164\u2192                        for plugin in dispatcher._plugins:\n  1165\u2192                            if plugin.supports(file_path):\n  1166\u2192                                dispatcher.index_file(file_path)\n  1167\u2192                                indexed_count += 1\n  1168\u2192                                \n  1169\u2192                                # Track by file type\n  1170\u2192                                suffix = file_path.suffix.lower()\n  1171\u2192                                indexed_by_type[suffix] = indexed_by_type.get(suffix, 0) + 1\n  1172\u2192                                break\n  1173\u2192                    except Exception as e:\n  1174\u2192                        # Log but continue with other files\n  1175\u2192                        logger.warning(f\"Failed to index {file_path}: {e}\")\n  1176\u2192            \n  1177\u2192            # Build summary message\n  1178\u2192            type_summary = \", \".join([f\"{count} {ext} files\" for ext, count in indexed_by_type.items()])\n  1179\u2192            logger.info(f\"Successfully reindexed {indexed_count} files: {type_summary}\")\n  1180\u2192            return {\n  1181\u2192                \"status\": \"completed\",\n  1182\u2192                \"message\": f\"Reindexed {indexed_count} files ({type_summary})\"\n  1183\u2192            }\n  1184\u2192    except HTTPException:\n  1185\u2192        raise\n  1186\u2192    except Exception as e:\n  1187\u2192        logger.error(f\"Reindexing failed: {e}\", exc_info=True)\n  1188\u2192        raise HTTPException(500, f\"Reindexing failed: {str(e)}\")\n  1189\u2192\n  1190\u2192@app.post(\"/plugins/{plugin_name}/reload\")\n  1191\u2192async def reload_plugin(\n  1192\u2192    plugin_name: str,\n  1193\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1194\u2192) -> Dict[str, str]:\n  1195\u2192    \"\"\"Reload a specific plugin.\n  1196\u2192    \n  1197\u2192    Args:\n  1198\u2192        plugin_name: Name of the plugin to reload\n  1199\u2192        \n  1200\u2192    Returns:\n  1201\u2192        Status message\n  1202\u2192    \"\"\"\n  1203\u2192    if plugin_manager is None:\n  1204\u2192        logger.error(\"Plugin reload requested but plugin manager not ready\")\n  1205\u2192        raise HTTPException(503, \"Plugin manager not ready\")\n  1206\u2192    \n  1207\u2192    try:\n  1208\u2192        plugin_manager.reload_plugin(plugin_name)\n  1209\u2192        return {\n  1210\u2192            \"status\": \"success\",\n  1211\u2192            \"message\": f\"Plugin '{plugin_name}' reloaded successfully\"\n  1212\u2192        }\n  1213\u2192    except Exception as e:\n  1214\u2192        logger.error(f\"Failed to reload plugin '{plugin_name}': {e}\", exc_info=True)\n  1215\u2192        raise HTTPException(500, f\"Failed to reload plugin: {str(e)}\")\n  1216\u2192\n  1217\u2192@app.post(\"/plugins/{plugin_name}/enable\")\n  1218\u2192async def enable_plugin(\n  1219\u2192    plugin_name: str,\n  1220\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1221\u2192) -> Dict[str, str]:\n  1222\u2192    \"\"\"Enable a disabled plugin.\n  1223\u2192    \n  1224\u2192    Args:\n  1225\u2192        plugin_name: Name of the plugin to enable\n  1226\u2192        \n  1227\u2192    Returns:\n  1228\u2192        Status message\n  1229\u2192    \"\"\"\n  1230\u2192    if plugin_manager is None:\n  1231\u2192        logger.error(\"Plugin enable requested but plugin manager not ready\")\n  1232\u2192        raise HTTPException(503, \"Plugin manager not ready\")\n  1233\u2192    \n  1234\u2192    try:\n  1235\u2192        plugin_manager.enable_plugin(plugin_name)\n  1236\u2192        # Recreate dispatcher with updated plugins\n  1237\u2192        active_plugins = plugin_manager.get_active_plugins()\n  1238\u2192        global dispatcher\n  1239\u2192        dispatcher = Dispatcher(list(active_plugins.values()))\n  1240\u2192        \n  1241\u2192        return {\n  1242\u2192            \"status\": \"success\",\n  1243\u2192            \"message\": f\"Plugin '{plugin_name}' enabled successfully\"\n  1244\u2192        }\n  1245\u2192    except Exception as e:\n  1246\u2192        logger.error(f\"Failed to enable plugin '{plugin_name}': {e}\", exc_info=True)\n  1247\u2192        raise HTTPException(500, f\"Failed to enable plugin: {str(e)}\")\n  1248\u2192\n  1249\u2192@app.post(\"/plugins/{plugin_name}/disable\")\n  1250\u2192async def disable_plugin(\n  1251\u2192    plugin_name: str,\n  1252\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1253\u2192) -> Dict[str, str]:\n  1254\u2192    \"\"\"Disable an active plugin.\n  1255\u2192    \n  1256\u2192    Args:\n  1257\u2192        plugin_name: Name of the plugin to disable\n  1258\u2192        \n  1259\u2192    Returns:\n  1260\u2192        Status message\n  1261\u2192    \"\"\"\n  1262\u2192    if plugin_manager is None:\n  1263\u2192        logger.error(\"Plugin disable requested but plugin manager not ready\")\n  1264\u2192        raise HTTPException(503, \"Plugin manager not ready\")\n  1265\u2192    \n  1266\u2192    try:\n  1267\u2192        plugin_manager.disable_plugin(plugin_name)\n  1268\u2192        # Recreate dispatcher with updated plugins\n  1269\u2192        active_plugins = plugin_manager.get_active_plugins()\n  1270\u2192        global dispatcher\n  1271\u2192        dispatcher = Dispatcher(list(active_plugins.values()))\n  1272\u2192        \n  1273\u2192        return {\n  1274\u2192            \"status\": \"success\",\n  1275\u2192            \"message\": f\"Plugin '{plugin_name}' disabled successfully\"\n  1276\u2192        }\n  1277\u2192    except Exception as e:\n  1278\u2192        logger.error(f\"Failed to disable plugin '{plugin_name}': {e}\", exc_info=True)\n  1279\u2192        raise HTTPException(500, f\"Failed to disable plugin: {str(e)}\")\n  1280\u2192\n  1281\u2192# Cache management endpoints\n  1282\u2192\n  1283\u2192@app.get(\"/cache/stats\")\n  1284\u2192async def get_cache_stats(\n  1285\u2192    current_user: User = Depends(require_permission(Permission.READ))\n  1286\u2192) -> Dict[str, Any]:\n  1287\u2192    \"\"\"Get cache statistics and performance metrics.\"\"\"\n  1288\u2192    if not cache_manager:\n  1289\u2192        raise HTTPException(503, \"Cache manager not ready\")\n  1290\u2192    \n  1291\u2192    try:\n  1292\u2192        cache_metrics = await cache_manager.get_metrics()\n  1293\u2192        backend_stats = await cache_manager.get_backend_stats()\n  1294\u2192        \n  1295\u2192        stats = {\n  1296\u2192            \"cache_metrics\": {\n  1297\u2192                \"hits\": cache_metrics.hits,\n  1298\u2192                \"misses\": cache_metrics.misses,\n  1299\u2192                \"sets\": cache_metrics.sets,\n  1300\u2192                \"deletes\": cache_metrics.deletes,\n  1301\u2192                \"hit_rate\": cache_metrics.hit_rate,\n  1302\u2192                \"avg_response_time_ms\": cache_metrics.avg_response_time_ms,\n  1303\u2192                \"entries_count\": cache_metrics.entries_count,\n  1304\u2192                \"memory_usage_mb\": cache_metrics.memory_usage_mb\n  1305\u2192            },\n  1306\u2192            \"backend_stats\": backend_stats\n  1307\u2192        }\n  1308\u2192        \n  1309\u2192        # Add query cache stats if available\n  1310\u2192        if query_cache:\n  1311\u2192            query_stats = await query_cache.get_cache_stats()\n  1312\u2192            stats[\"query_cache\"] = query_stats\n  1313\u2192        \n  1314\u2192        return stats\n  1315\u2192    except Exception as e:\n  1316\u2192        logger.error(f\"Failed to get cache stats: {e}\")\n  1317\u2192        raise HTTPException(500, f\"Failed to get cache statistics: {str(e)}\")\n  1318\u2192\n  1319\u2192@app.post(\"/cache/clear\")\n  1320\u2192async def clear_cache(\n  1321\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1322\u2192) -> Dict[str, Any]:\n  1323\u2192    \"\"\"Clear all cache entries (admin only).\"\"\"\n  1324\u2192    if not cache_manager:\n  1325\u2192        raise HTTPException(503, \"Cache manager not ready\")\n  1326\u2192    \n  1327\u2192    try:\n  1328\u2192        count = await cache_manager.clear()\n  1329\u2192        logger.info(f\"Cache cleared by admin user {current_user.username}: {count} entries\")\n  1330\u2192        \n  1331\u2192        return {\n  1332\u2192            \"status\": \"success\",\n  1333\u2192            \"message\": f\"Cleared {count} cache entries\",\n  1334\u2192            \"cleared_entries\": count\n  1335\u2192        }\n  1336\u2192    except Exception as e:\n  1337\u2192        logger.error(f\"Failed to clear cache: {e}\")\n  1338\u2192        raise HTTPException(500, f\"Failed to clear cache: {str(e)}\")\n  1339\u2192\n  1340\u2192@app.post(\"/cache/invalidate\")\n  1341\u2192async def invalidate_cache_by_tags(\n  1342\u2192    tags: List[str],\n  1343\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1344\u2192) -> Dict[str, Any]:\n  1345\u2192    \"\"\"Invalidate cache entries by tags (admin only).\"\"\"\n  1346\u2192    if not cache_manager:\n  1347\u2192        raise HTTPException(503, \"Cache manager not ready\")\n  1348\u2192    \n  1349\u2192    try:\n  1350\u2192        tag_set = set(tags)\n  1351\u2192        count = await cache_manager.invalidate_by_tags(tag_set)\n  1352\u2192        logger.info(f\"Cache invalidated by admin user {current_user.username}: {count} entries with tags {tags}\")\n  1353\u2192        \n  1354\u2192        return {\n  1355\u2192            \"status\": \"success\",\n  1356\u2192            \"message\": f\"Invalidated {count} cache entries with tags {tags}\",\n  1357\u2192            \"invalidated_entries\": count,\n  1358\u2192            \"tags\": tags\n  1359\u2192        }\n  1360\u2192    except Exception as e:\n  1361\u2192        logger.error(f\"Failed to invalidate cache by tags: {e}\")\n  1362\u2192        raise HTTPException(500, f\"Failed to invalidate cache: {str(e)}\")\n  1363\u2192\n  1364\u2192@app.post(\"/cache/invalidate/files\")\n  1365\u2192async def invalidate_cache_by_files(\n  1366\u2192    file_paths: List[str],\n  1367\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1368\u2192) -> Dict[str, Any]:\n  1369\u2192    \"\"\"Invalidate cache entries that depend on specific files (admin only).\"\"\"\n  1370\u2192    if not query_cache:\n  1371\u2192        raise HTTPException(503, \"Query cache not ready\")\n  1372\u2192    \n  1373\u2192    try:\n  1374\u2192        total_count = 0\n  1375\u2192        for file_path in file_paths:\n  1376\u2192            count = await query_cache.invalidate_file_queries(file_path)\n  1377\u2192            total_count += count\n  1378\u2192        \n  1379\u2192        logger.info(f\"File-based cache invalidation by admin user {current_user.username}: {total_count} entries for {len(file_paths)} files\")\n  1380\u2192        \n  1381\u2192        return {\n  1382\u2192            \"status\": \"success\",\n  1383\u2192            \"message\": f\"Invalidated {total_count} cache entries for {len(file_paths)} files\",\n  1384\u2192            \"invalidated_entries\": total_count,\n  1385\u2192            \"files\": file_paths\n  1386\u2192        }\n  1387\u2192    except Exception as e:\n  1388\u2192        logger.error(f\"Failed to invalidate cache by files: {e}\")\n  1389\u2192        raise HTTPException(500, f\"Failed to invalidate cache by files: {str(e)}\")\n  1390\u2192\n  1391\u2192@app.post(\"/cache/invalidate/semantic\")\n  1392\u2192async def invalidate_semantic_cache(\n  1393\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1394\u2192) -> Dict[str, Any]:\n  1395\u2192    \"\"\"Invalidate all semantic search cache entries (admin only).\"\"\"\n  1396\u2192    if not query_cache:\n  1397\u2192        raise HTTPException(503, \"Query cache not ready\")\n  1398\u2192    \n  1399\u2192    try:\n  1400\u2192        count = await query_cache.invalidate_semantic_queries()\n  1401\u2192        logger.info(f\"Semantic cache invalidated by admin user {current_user.username}: {count} entries\")\n  1402\u2192        \n  1403\u2192        return {\n  1404\u2192            \"status\": \"success\",\n  1405\u2192            \"message\": f\"Invalidated {count} semantic search cache entries\",\n  1406\u2192            \"invalidated_entries\": count\n  1407\u2192        }\n  1408\u2192    except Exception as e:\n  1409\u2192        logger.error(f\"Failed to invalidate semantic cache: {e}\")\n  1410\u2192        raise HTTPException(500, f\"Failed to invalidate semantic cache: {str(e)}\")\n  1411\u2192\n  1412\u2192@app.post(\"/cache/warm\")\n  1413\u2192async def warm_cache(\n  1414\u2192    keys: List[str],\n  1415\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1416\u2192) -> Dict[str, Any]:\n  1417\u2192    \"\"\"Warm cache with predefined keys (admin only).\"\"\"\n  1418\u2192    if not cache_manager:\n  1419\u2192        raise HTTPException(503, \"Cache manager not ready\")\n  1420\u2192    \n  1421\u2192    try:\n  1422\u2192        # Simple factory function for warming - would need more sophisticated logic in production\n  1423\u2192        async def factory(key: str):\n  1424\u2192            # This is a placeholder - in real implementation would depend on key type\n  1425\u2192            return f\"warmed_value_for_{key}\"\n  1426\u2192        \n  1427\u2192        count = await cache_manager.warm_cache(keys, factory)\n  1428\u2192        logger.info(f\"Cache warmed by admin user {current_user.username}: {count} entries\")\n  1429\u2192        \n  1430\u2192        return {\n  1431\u2192            \"status\": \"success\",\n  1432\u2192            \"message\": f\"Warmed {count} cache entries\",\n  1433\u2192            \"warmed_entries\": count,\n  1434\u2192            \"requested_keys\": len(keys)\n  1435\u2192        }\n  1436\u2192    except Exception as e:\n  1437\u2192        logger.error(f\"Failed to warm cache: {e}\")\n  1438\u2192        raise HTTPException(500, f\"Failed to warm cache: {str(e)}\")\n  1439\u2192\n  1440\u2192@app.post(\"/cache/cleanup\")\n  1441\u2192async def cleanup_cache(\n  1442\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1443\u2192) -> Dict[str, Any]:\n  1444\u2192    \"\"\"Manually trigger cache cleanup (admin only).\"\"\"\n  1445\u2192    if not cache_manager:\n  1446\u2192        raise HTTPException(503, \"Cache manager not ready\")\n  1447\u2192    \n  1448\u2192    try:\n  1449\u2192        count = await cache_manager.cleanup()\n  1450\u2192        logger.info(f\"Cache cleanup triggered by admin user {current_user.username}: {count} entries cleaned\")\n  1451\u2192        \n  1452\u2192        return {\n  1453\u2192            \"status\": \"success\",\n  1454\u2192            \"message\": f\"Cleaned up {count} expired cache entries\",\n  1455\u2192            \"cleaned_entries\": count\n  1456\u2192        }\n  1457\u2192    except Exception as e:\n  1458\u2192        logger.error(f\"Failed to cleanup cache: {e}\")\n  1459\u2192        raise HTTPException(500, f\"Failed to cleanup cache: {str(e)}\")\n  1460\u2192\n  1461\u2192# Hybrid Search endpoints\n  1462\u2192\n  1463\u2192@app.get(\"/search/config\")\n  1464\u2192async def get_search_config(\n  1465\u2192    current_user: User = Depends(require_permission(Permission.READ))\n  1466\u2192) -> Dict[str, Any]:\n  1467\u2192    \"\"\"Get current hybrid search configuration.\"\"\"\n  1468\u2192    if not hybrid_search:\n  1469\u2192        raise HTTPException(503, \"Hybrid search not available\")\n  1470\u2192    \n  1471\u2192    config = hybrid_search.config\n  1472\u2192    return {\n  1473\u2192        \"weights\": {\n  1474\u2192            \"bm25\": config.bm25_weight,\n  1475\u2192            \"semantic\": config.semantic_weight,\n  1476\u2192            \"fuzzy\": config.fuzzy_weight\n  1477\u2192        },\n  1478\u2192        \"enabled_methods\": {\n  1479\u2192            \"bm25\": config.enable_bm25,\n  1480\u2192            \"semantic\": config.enable_semantic,\n  1481\u2192            \"fuzzy\": config.enable_fuzzy\n  1482\u2192        },\n  1483\u2192        \"rrf_k\": config.rrf_k,\n  1484\u2192        \"parallel_execution\": config.parallel_execution,\n  1485\u2192        \"cache_results\": config.cache_results,\n  1486\u2192        \"limits\": {\n  1487\u2192            \"individual_limit\": config.individual_limit,\n  1488\u2192            \"final_limit\": config.final_limit\n  1489\u2192        }\n  1490\u2192    }\n  1491\u2192\n  1492\u2192@app.put(\"/search/config/weights\")\n  1493\u2192async def update_search_weights(\n  1494\u2192    bm25: Optional[float] = None,\n  1495\u2192    semantic: Optional[float] = None,\n  1496\u2192    fuzzy: Optional[float] = None,\n  1497\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1498\u2192) -> Dict[str, Any]:\n  1499\u2192    \"\"\"Update hybrid search weights (admin only).\n  1500\u2192    \n  1501\u2192    Weights will be normalized to sum to 1.0.\n  1502\u2192    \"\"\"\n  1503\u2192    if not hybrid_search:\n  1504\u2192        raise HTTPException(503, \"Hybrid search not available\")\n  1505\u2192    \n  1506\u2192    try:\n  1507\u2192        hybrid_search.set_weights(bm25=bm25, semantic=semantic, fuzzy=fuzzy)\n  1508\u2192        \n  1509\u2192        # Get updated config\n  1510\u2192        config = hybrid_search.config\n  1511\u2192        \n  1512\u2192        logger.info(f\"Search weights updated by {current_user.username}: \"\n  1513\u2192                   f\"BM25={config.bm25_weight:.3f}, \"\n  1514\u2192                   f\"Semantic={config.semantic_weight:.3f}, \"\n  1515\u2192                   f\"Fuzzy={config.fuzzy_weight:.3f}\")\n  1516\u2192        \n  1517\u2192        return {\n  1518\u2192            \"status\": \"success\",\n  1519\u2192            \"weights\": {\n  1520\u2192                \"bm25\": config.bm25_weight,\n  1521\u2192                \"semantic\": config.semantic_weight,\n  1522\u2192                \"fuzzy\": config.fuzzy_weight\n  1523\u2192            }\n  1524\u2192        }\n  1525\u2192    except Exception as e:\n  1526\u2192        logger.error(f\"Failed to update search weights: {e}\")\n  1527\u2192        raise HTTPException(500, f\"Failed to update weights: {str(e)}\")\n  1528\u2192\n  1529\u2192@app.put(\"/search/config/methods\")\n  1530\u2192async def toggle_search_methods(\n  1531\u2192    bm25: Optional[bool] = None,\n  1532\u2192    semantic: Optional[bool] = None,\n  1533\u2192    fuzzy: Optional[bool] = None,\n  1534\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1535\u2192) -> Dict[str, Any]:\n  1536\u2192    \"\"\"Enable or disable search methods (admin only).\"\"\"\n  1537\u2192    if not hybrid_search:\n  1538\u2192        raise HTTPException(503, \"Hybrid search not available\")\n  1539\u2192    \n  1540\u2192    try:\n  1541\u2192        hybrid_search.enable_methods(bm25=bm25, semantic=semantic, fuzzy=fuzzy)\n  1542\u2192        \n  1543\u2192        # Get updated config\n  1544\u2192        config = hybrid_search.config\n  1545\u2192        \n  1546\u2192        logger.info(f\"Search methods updated by {current_user.username}: \"\n  1547\u2192                   f\"BM25={config.enable_bm25}, \"\n  1548\u2192                   f\"Semantic={config.enable_semantic}, \"\n  1549\u2192                   f\"Fuzzy={config.enable_fuzzy}\")\n  1550\u2192        \n  1551\u2192        return {\n  1552\u2192            \"status\": \"success\",\n  1553\u2192            \"enabled_methods\": {\n  1554\u2192                \"bm25\": config.enable_bm25,\n  1555\u2192                \"semantic\": config.enable_semantic,\n  1556\u2192                \"fuzzy\": config.enable_fuzzy\n  1557\u2192            }\n  1558\u2192        }\n  1559\u2192    except Exception as e:\n  1560\u2192        logger.error(f\"Failed to update search methods: {e}\")\n  1561\u2192        raise HTTPException(500, f\"Failed to update methods: {str(e)}\")\n  1562\u2192\n  1563\u2192@app.get(\"/search/statistics\")\n  1564\u2192async def get_search_statistics(\n  1565\u2192    current_user: User = Depends(require_permission(Permission.READ))\n  1566\u2192) -> Dict[str, Any]:\n  1567\u2192    \"\"\"Get search statistics and performance metrics.\"\"\"\n  1568\u2192    stats = {}\n  1569\u2192    \n  1570\u2192    # Hybrid search statistics\n  1571\u2192    if hybrid_search:\n  1572\u2192        stats[\"hybrid_search\"] = hybrid_search.get_statistics()\n  1573\u2192    \n  1574\u2192    # BM25 statistics\n  1575\u2192    if bm25_indexer:\n  1576\u2192        stats[\"bm25\"] = bm25_indexer.get_statistics()\n  1577\u2192    \n  1578\u2192    # Fuzzy search statistics\n  1579\u2192    if fuzzy_indexer and hasattr(fuzzy_indexer, 'get_statistics'):\n  1580\u2192        stats[\"fuzzy\"] = fuzzy_indexer.get_statistics()\n  1581\u2192    \n  1582\u2192    # Add general search metrics from business metrics\n  1583\u2192    if business_metrics:\n  1584\u2192        search_metrics = business_metrics.get_search_metrics()\n  1585\u2192        stats[\"general\"] = {\n  1586\u2192            \"total_searches\": search_metrics.get(\"total_searches\", 0),\n  1587\u2192            \"average_response_time_ms\": search_metrics.get(\"avg_response_time\", 0),\n  1588\u2192            \"search_success_rate\": search_metrics.get(\"success_rate\", 0)\n  1589\u2192        }\n  1590\u2192    \n  1591\u2192    return stats\n  1592\u2192\n  1593\u2192@app.post(\"/search/optimize\")\n  1594\u2192async def optimize_search_indexes(\n  1595\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1596\u2192) -> Dict[str, Any]:\n  1597\u2192    \"\"\"Optimize search indexes for better performance (admin only).\"\"\"\n  1598\u2192    results = {}\n  1599\u2192    \n  1600\u2192    try:\n  1601\u2192        # Optimize BM25 indexes\n  1602\u2192        if bm25_indexer:\n  1603\u2192            bm25_indexer.optimize()\n  1604\u2192            results[\"bm25\"] = \"optimized\"\n  1605\u2192            logger.info(\"BM25 indexes optimized\")\n  1606\u2192        \n  1607\u2192        # Optimize FTS5 tables in SQLite\n  1608\u2192        if sqlite_store:\n  1609\u2192            sqlite_store.optimize_fts_tables()\n  1610\u2192            results[\"fts5\"] = \"optimized\"\n  1611\u2192            logger.info(\"FTS5 tables optimized\")\n  1612\u2192        \n  1613\u2192        # Clear hybrid search cache\n  1614\u2192        if hybrid_search:\n  1615\u2192            hybrid_search.clear_cache()\n  1616\u2192            results[\"hybrid_cache\"] = \"cleared\"\n  1617\u2192            logger.info(\"Hybrid search cache cleared\")\n  1618\u2192        \n  1619\u2192        logger.info(f\"Search indexes optimized by {current_user.username}\")\n  1620\u2192        \n  1621\u2192        return {\n  1622\u2192            \"status\": \"success\",\n  1623\u2192            \"message\": \"Search indexes optimized successfully\",\n  1624\u2192            \"results\": results\n  1625\u2192        }\n  1626\u2192    except Exception as e:\n  1627\u2192        logger.error(f\"Failed to optimize search indexes: {e}\")\n  1628\u2192        raise HTTPException(500, f\"Failed to optimize indexes: {str(e)}\")\n  1629\u2192\n  1630\u2192@app.get(\"/search/term/{term}/stats\")\n  1631\u2192async def get_term_statistics(\n  1632\u2192    term: str,\n  1633\u2192    current_user: User = Depends(require_permission(Permission.READ))\n  1634\u2192) -> Dict[str, Any]:\n  1635\u2192    \"\"\"Get statistics for a specific search term.\"\"\"\n  1636\u2192    stats = {}\n  1637\u2192    \n  1638\u2192    try:\n  1639\u2192        # BM25 term statistics\n  1640\u2192        if bm25_indexer:\n  1641\u2192            stats[\"bm25\"] = bm25_indexer.get_term_statistics(term)\n  1642\u2192        \n  1643\u2192        # SQLite FTS5 statistics\n  1644\u2192        if sqlite_store:\n  1645\u2192            stats[\"fts5\"] = sqlite_store.get_bm25_term_statistics(term)\n  1646\u2192        \n  1647\u2192        return {\n  1648\u2192            \"term\": term,\n  1649\u2192            \"statistics\": stats\n  1650\u2192        }\n  1651\u2192    except Exception as e:\n  1652\u2192        logger.error(f\"Failed to get term statistics: {e}\")\n  1653\u2192        raise HTTPException(500, f\"Failed to get term statistics: {str(e)}\")\n  1654\u2192\n  1655\u2192@app.post(\"/search/rebuild\")\n  1656\u2192async def rebuild_search_indexes(\n  1657\u2192    index_type: str = \"all\",  # \"all\", \"bm25\", \"fuzzy\", \"semantic\"\n  1658\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1659\u2192) -> Dict[str, Any]:\n  1660\u2192    \"\"\"Rebuild search indexes (admin only).\"\"\"\n  1661\u2192    if index_type not in [\"all\", \"bm25\", \"fuzzy\", \"semantic\"]:\n  1662\u2192        raise HTTPException(400, \"Invalid index_type. Must be 'all', 'bm25', 'fuzzy', or 'semantic'\")\n  1663\u2192    \n  1664\u2192    results = {}\n  1665\u2192    \n  1666\u2192    try:\n  1667\u2192        if index_type in [\"all\", \"bm25\"] and bm25_indexer:\n  1668\u2192            bm25_indexer.rebuild()\n  1669\u2192            results[\"bm25\"] = \"rebuilt\"\n  1670\u2192            logger.info(\"BM25 index rebuilt\")\n  1671\u2192        \n  1672\u2192        if index_type in [\"all\", \"fuzzy\"] and fuzzy_indexer:\n  1673\u2192            fuzzy_indexer.clear()\n  1674\u2192            # Re-index all files\n  1675\u2192            if sqlite_store:\n  1676\u2192                files = sqlite_store.get_all_files()\n  1677\u2192                for file_info in files:\n  1678\u2192                    try:\n  1679\u2192                        with open(file_info[\"path\"], 'r', encoding='utf-8') as f:\n  1680\u2192                            content = f.read()\n  1681\u2192                        fuzzy_indexer.add_file(file_info[\"path\"], content)\n  1682\u2192                    except Exception as e:\n  1683\u2192                        logger.warning(f\"Failed to re-index {file_info['path']}: {e}\")\n  1684\u2192            results[\"fuzzy\"] = \"rebuilt\"\n  1685\u2192            logger.info(\"Fuzzy index rebuilt\")\n  1686\u2192        \n  1687\u2192        if index_type in [\"all\", \"semantic\"]:\n  1688\u2192            # Semantic index rebuild would go here if available\n  1689\u2192            if hasattr(hybrid_search, 'semantic_indexer') and hybrid_search.semantic_indexer:\n  1690\u2192                results[\"semantic\"] = \"rebuild_not_implemented\"\n  1691\u2192            else:\n  1692\u2192                results[\"semantic\"] = \"not_available\"\n  1693\u2192        \n  1694\u2192        logger.info(f\"Search indexes rebuilt by {current_user.username}: {index_type}\")\n  1695\u2192        \n  1696\u2192        return {\n  1697\u2192            \"status\": \"success\",\n  1698\u2192            \"message\": f\"Search indexes rebuilt successfully\",\n  1699\u2192            \"index_type\": index_type,\n  1700\u2192            \"results\": results\n  1701\u2192        }\n  1702\u2192    except Exception as e:\n  1703\u2192        logger.error(f\"Failed to rebuild search indexes: {e}\")\n  1704\u2192        raise HTTPException(500, f\"Failed to rebuild indexes: {str(e)}\")\n  1705\u2192\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 8706,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 8706,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 247,
    "output_tokens_total": 247,
    "cache_read_tokens": 19948,
    "cache_creation_tokens": 31774,
    "tokens_per_result": 247,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "41333a35-f5b8-4f90-875a-e853cef49417",
    "test_date": "2025-06-26T07:37:20.543Z",
    "query_id": "transcript_41333a35-f5b8-4f90-875a-e853cef49417",
    "query_text": [
      {
        "tool_use_id": "toolu_016nDSL9GZzRYFaZLR7jm7rh",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"\n     2\u2192Hybrid Search implementation combining BM25 and semantic search.\n     3\u2192\n     4\u2192This module implements reciprocal rank fusion (RRF) to combine results from\n     5\u2192multiple search methods, providing better overall search quality.\n     6\u2192\"\"\"\n     7\u2192\n     8\u2192import logging\n     9\u2192import asyncio\n    10\u2192from typing import List, Dict, Any, Optional, Tuple, Set\n    11\u2192from dataclasses import dataclass, field\n    12\u2192from collections import defaultdict\n    13\u2192import numpy as np\n    14\u2192\n    15\u2192from .bm25_indexer import BM25Indexer\n    16\u2192from ..utils.semantic_indexer import SemanticIndexer\n    17\u2192from ..storage.sqlite_store import SQLiteStore\n    18\u2192from .query_optimizer import Query, QueryType\n    19\u2192from .reranker import RerankerFactory, IReranker\n    20\u2192# Import only what we need to avoid circular dependencies\n    21\u2192from ..config.settings import RerankingSettings\n    22\u2192\n    23\u2192logger = logging.getLogger(__name__)\n    24\u2192\n    25\u2192\n    26\u2192@dataclass\n    27\u2192class SearchResult:\n    28\u2192    \"\"\"Individual search result from any search method.\"\"\"\n    29\u2192    doc_id: str\n    30\u2192    filepath: str\n    31\u2192    score: float\n    32\u2192    snippet: str\n    33\u2192    metadata: Dict[str, Any] = field(default_factory=dict)\n    34\u2192    source: str = \"\"  # 'bm25', 'semantic', 'fuzzy', etc.\n    35\u2192    \n    36\u2192    \n    37\u2192@dataclass\n    38\u2192class HybridSearchConfig:\n    39\u2192    \"\"\"Configuration for hybrid search.\"\"\"\n    40\u2192    # Weight configuration\n    41\u2192    bm25_weight: float = 0.5\n    42\u2192    semantic_weight: float = 0.3\n    43\u2192    fuzzy_weight: float = 0.2\n    44\u2192    \n    45\u2192    # RRF parameters\n    46\u2192    rrf_k: int = 60  # Reciprocal Rank Fusion constant\n    47\u2192    \n    48\u2192    # Search parameters\n    49\u2192    enable_bm25: bool = True\n    50\u2192    enable_semantic: bool = True\n    51\u2192    enable_fuzzy: bool = True\n    52\u2192    \n    53\u2192    # Result limits\n    54\u2192    individual_limit: int = 50  # Results per search method\n    55\u2192    final_limit: int = 20  # Final results after fusion\n    56\u2192    \n    57\u2192    # Optimization\n    58\u2192    parallel_execution: bool = True\n    59\u2192    cache_results: bool = True\n    60\u2192    \n    61\u2192    # Minimum scores\n    62\u2192    min_bm25_score: float = -10.0\n    63\u2192    min_semantic_score: float = 0.5\n    64\u2192    min_fuzzy_score: float = 0.3\n    65\u2192\n    66\u2192\n    67\u2192class HybridSearch:\n    68\u2192    \"\"\"\n    69\u2192    Hybrid search combining multiple search methods with reciprocal rank fusion.\n    70\u2192    \n    71\u2192    This class orchestrates multiple search backends (BM25, semantic, fuzzy) and\n    72\u2192    combines their results using configurable fusion strategies.\n    73\u2192    \"\"\"\n    74\u2192    \n    75\u2192    def __init__(\n    76\u2192        self,\n    77\u2192        storage: SQLiteStore,\n    78\u2192        bm25_indexer: Optional[BM25Indexer] = None,\n    79\u2192        semantic_indexer: Optional[SemanticIndexer] = None,\n    80\u2192        fuzzy_indexer: Optional[Any] = None,\n    81\u2192        config: Optional[HybridSearchConfig] = None,\n    82\u2192        reranking_settings: Optional[RerankingSettings] = None\n    83\u2192    ):\n    84\u2192        \"\"\"\n    85\u2192        Initialize hybrid search.\n    86\u2192        \n    87\u2192        Args:\n    88\u2192            storage: SQLite storage backend\n    89\u2192            bm25_indexer: BM25 full-text search indexer\n    90\u2192            semantic_indexer: Semantic/vector search indexer\n    91\u2192            fuzzy_indexer: Fuzzy search indexer\n    92\u2192            config: Hybrid search configuration\n    93\u2192            reranking_settings: Settings for result reranking\n    94\u2192        \"\"\"\n    95\u2192        self.storage = storage\n    96\u2192        self.bm25_indexer = bm25_indexer\n    97\u2192        self.semantic_indexer = semantic_indexer\n    98\u2192        self.fuzzy_indexer = fuzzy_indexer\n    99\u2192        self.config = config or HybridSearchConfig()\n   100\u2192        self.reranking_settings = reranking_settings\n   101\u2192        \n   102\u2192        # Initialize reranker if enabled\n   103\u2192        self.reranker: Optional[IReranker] = None\n   104\u2192        if reranking_settings and reranking_settings.enabled:\n   105\u2192            self._initialize_reranker()\n   106\u2192        \n   107\u2192        # Result cache\n   108\u2192        self._result_cache: Dict[str, List[SearchResult]] = {}\n   109\u2192        \n   110\u2192        # Statistics\n   111\u2192        self._search_stats = defaultdict(int)\n   112\u2192    \n   113\u2192    def _initialize_reranker(self):\n   114\u2192        \"\"\"Initialize the reranker based on settings.\"\"\"\n   115\u2192        try:\n   116\u2192            factory = RerankerFactory()\n   117\u2192            config = {\n   118\u2192                'cohere_api_key': self.reranking_settings.cohere_api_key,\n   119\u2192                'model': self.reranking_settings.cohere_model,\n   120\u2192                'device': self.reranking_settings.cross_encoder_device,\n   121\u2192                'primary_type': self.reranking_settings.hybrid_primary_type,\n   122\u2192                'fallback_type': self.reranking_settings.hybrid_fallback_type,\n   123\u2192                'weight_primary': self.reranking_settings.hybrid_primary_weight,\n   124\u2192                'weight_fallback': self.reranking_settings.hybrid_fallback_weight,\n   125\u2192                'cache_ttl': self.reranking_settings.cache_ttl\n   126\u2192            }\n   127\u2192            \n   128\u2192            self.reranker = factory.create_reranker(\n   129\u2192                self.reranking_settings.reranker_type,\n   130\u2192                config\n   131\u2192            )\n   132\u2192            \n   133\u2192            # Initialize reranker asynchronously will be done on first use\n   134\u2192            logger.info(f\"Initialized {self.reranking_settings.reranker_type} reranker\")\n   135\u2192        except Exception as e:\n   136\u2192            logger.error(f\"Failed to initialize reranker: {e}\")\n   137\u2192            self.reranker = None\n   138\u2192        \n   139\u2192    async def search(\n   140\u2192        self,\n   141\u2192        query: str,\n   142\u2192        query_type: Optional[QueryType] = None,\n   143\u2192        filters: Optional[Dict[str, Any]] = None,\n   144\u2192        limit: Optional[int] = None\n   145\u2192    ) -> List[Dict[str, Any]]:\n   146\u2192        \"\"\"\n   147\u2192        Perform hybrid search combining multiple search methods.\n   148\u2192        \n   149\u2192        Args:\n   150\u2192            query: Search query\n   151\u2192            query_type: Optional query type hint\n   152\u2192            filters: Optional filters (language, file path, etc.)\n   153\u2192            limit: Maximum number of results\n   154\u2192            \n   155\u2192        Returns:\n   156\u2192            List of combined search results\n   157\u2192        \"\"\"\n   158\u2192        limit = limit or self.config.final_limit\n   159\u2192        \n   160\u2192        # Check cache if enabled\n   161\u2192        cache_key = self._get_cache_key(query, filters)\n   162\u2192        if self.config.cache_results and cache_key in self._result_cache:\n   163\u2192            self._search_stats['cache_hits'] += 1\n   164\u2192            cached_results = self._result_cache[cache_key]\n   165\u2192            return self._format_results(cached_results[:limit])\n   166\u2192        \n   167\u2192        # Collect results from different search methods\n   168\u2192        all_results = []\n   169\u2192        \n   170\u2192        if self.config.parallel_execution:\n   171\u2192            # Execute searches in parallel\n   172\u2192            all_results = await self._parallel_search(query, query_type, filters)\n   173\u2192        else:\n   174\u2192            # Execute searches sequentially\n   175\u2192            all_results = await self._sequential_search(query, query_type, filters)\n   176\u2192        \n   177\u2192        # Combine results using reciprocal rank fusion\n   178\u2192        combined_results = self._reciprocal_rank_fusion(all_results)\n   179\u2192        \n   180\u2192        # Apply reranking if enabled\n   181\u2192        if self.reranker and self.reranking_settings and self.reranking_settings.enabled:\n   182\u2192            combined_results = await self._rerank_results(query, combined_results)\n   183\u2192        \n   184\u2192        # Apply post-processing\n   185\u2192        final_results = self._post_process_results(combined_results, limit)\n   186\u2192        \n   187\u2192        # Cache results if enabled\n   188\u2192        if self.config.cache_results:\n   189\u2192            self._result_cache[cache_key] = final_results\n   190\u2192            self._cleanup_cache()\n   191\u2192        \n   192\u2192        # Update statistics\n   193\u2192        self._search_stats['total_searches'] += 1\n   194\u2192        \n   195\u2192        return self._format_results(final_results)\n   196\u2192    \n   197\u2192    async def _parallel_search(\n   198\u2192        self,\n   199\u2192        query: str,\n   200\u2192        query_type: Optional[QueryType],\n   201\u2192        filters: Optional[Dict[str, Any]]\n   202\u2192    ) -> List[List[SearchResult]]:\n   203\u2192        \"\"\"Execute searches in parallel.\"\"\"\n   204\u2192        tasks = []\n   205\u2192        \n   206\u2192        if self.config.enable_bm25 and self.bm25_indexer:\n   207\u2192            tasks.append(self._search_bm25(query, filters))\n   208\u2192            \n   209\u2192        if self.config.enable_semantic and self.semantic_indexer:\n   210\u2192            tasks.append(self._search_semantic(query, filters))\n   211\u2192            \n   212\u2192        if self.config.enable_fuzzy and self.fuzzy_indexer:\n   213\u2192            tasks.append(self._search_fuzzy(query, filters))\n   214\u2192        \n   215\u2192        # Execute all searches in parallel\n   216\u2192        results = await asyncio.gather(*tasks, return_exceptions=True)\n   217\u2192        \n   218\u2192        # Filter out exceptions and empty results\n   219\u2192        valid_results = []\n   220\u2192        for i, result in enumerate(results):\n   221\u2192            if isinstance(result, Exception):\n   222\u2192                logger.error(f\"Search error in method {i}: {result}\")\n   223\u2192            elif result:\n   224\u2192                valid_results.append(result)\n   225\u2192                \n   226\u2192        return valid_results\n   227\u2192    \n   228\u2192    async def _sequential_search(\n   229\u2192        self,\n   230\u2192        query: str,\n   231\u2192        query_type: Optional[QueryType],\n   232\u2192        filters: Optional[Dict[str, Any]]\n   233\u2192    ) -> List[List[SearchResult]]:\n   234\u2192        \"\"\"Execute searches sequentially.\"\"\"\n   235\u2192        all_results = []\n   236\u2192        \n   237\u2192        if self.config.enable_bm25 and self.bm25_indexer:\n   238\u2192            try:\n   239\u2192                bm25_results = await self._search_bm25(query, filters)\n   240\u2192                if bm25_results:\n   241\u2192                    all_results.append(bm25_results)\n   242\u2192            except Exception as e:\n   243\u2192                logger.error(f\"BM25 search error: {e}\")\n   244\u2192                \n   245\u2192        if self.config.enable_semantic and self.semantic_indexer:\n   246\u2192            try:\n   247\u2192                semantic_results = await self._search_semantic(query, filters)\n   248\u2192                if semantic_results:\n   249\u2192                    all_results.append(semantic_results)\n   250\u2192            except Exception as e:\n   251\u2192                logger.error(f\"Semantic search error: {e}\")\n   252\u2192                \n   253\u2192        if self.config.enable_fuzzy and self.fuzzy_indexer:\n   254\u2192            try:\n   255\u2192                fuzzy_results = await self._search_fuzzy(query, filters)\n   256\u2192                if fuzzy_results:\n   257\u2192                    all_results.append(fuzzy_results)\n   258\u2192            except Exception as e:\n   259\u2192                logger.error(f\"Fuzzy search error: {e}\")\n   260\u2192                \n   261\u2192        return all_results\n   262\u2192    \n   263\u2192    async def _search_bm25(self, query: str, filters: Optional[Dict[str, Any]]) -> List[SearchResult]:\n   264\u2192        \"\"\"Perform BM25 search.\"\"\"\n   265\u2192        # Run BM25 search in thread pool since it's synchronous\n   266\u2192        loop = asyncio.get_event_loop()\n   267\u2192        \n   268\u2192        def run_search():\n   269\u2192            kwargs = filters or {}\n   270\u2192            results = self.bm25_indexer.search(query, limit=self.config.individual_limit, **kwargs)\n   271\u2192            \n   272\u2192            search_results = []\n   273\u2192            for r in results:\n   274\u2192                if r['score'] >= self.config.min_bm25_score:\n   275\u2192                    search_results.append(SearchResult(\n   276\u2192                        doc_id=r.get('filepath', ''),\n   277\u2192                        filepath=r.get('filepath', ''),\n   278\u2192                        score=r['score'],\n   279\u2192                        snippet=r.get('snippet', ''),\n   280\u2192                        metadata=r,\n   281\u2192                        source='bm25'\n   282\u2192                    ))\n   283\u2192            return search_results\n   284\u2192        \n   285\u2192        results = await loop.run_in_executor(None, run_search)\n   286\u2192        self._search_stats['bm25_searches'] += 1\n   287\u2192        return results\n   288\u2192    \n   289\u2192    async def _search_semantic(self, query: str, filters: Optional[Dict[str, Any]]) -> List[SearchResult]:\n   290\u2192        \"\"\"Perform semantic search.\"\"\"\n   291\u2192        # Run semantic search\n   292\u2192        loop = asyncio.get_event_loop()\n   293\u2192        \n   294\u2192        def run_search():\n   295\u2192            # Semantic search with filters\n   296\u2192            kwargs = {\n   297\u2192                'k': self.config.individual_limit,\n   298\u2192                'threshold': self.config.min_semantic_score\n   299\u2192            }\n   300\u2192            if filters:\n   301\u2192                kwargs.update(filters)\n   302\u2192                \n   303\u2192            results = self.semantic_indexer.search(query, **kwargs)\n   304\u2192            \n   305\u2192            search_results = []\n   306\u2192            for r in results:\n   307\u2192                search_results.append(SearchResult(\n   308\u2192                    doc_id=r.get('filepath', ''),\n   309\u2192                    filepath=r.get('filepath', ''),\n   310\u2192                    score=r.get('score', 0.0),\n   311\u2192                    snippet=r.get('content', '')[:200],\n   312\u2192                    metadata=r,\n   313\u2192                    source='semantic'\n   314\u2192                ))\n   315\u2192            return search_results\n   316\u2192        \n   317\u2192        results = await loop.run_in_executor(None, run_search)\n   318\u2192        self._search_stats['semantic_searches'] += 1\n   319\u2192        return results\n   320\u2192    \n   321\u2192    async def _search_fuzzy(self, query: str, filters: Optional[Dict[str, Any]]) -> List[SearchResult]:\n   322\u2192        \"\"\"Perform fuzzy search.\"\"\"\n   323\u2192        loop = asyncio.get_event_loop()\n   324\u2192        \n   325\u2192        def run_search():\n   326\u2192            # Fuzzy search\n   327\u2192            results = self.fuzzy_indexer.search_fuzzy(\n   328\u2192                query,\n   329\u2192                max_results=self.config.individual_limit,\n   330\u2192                threshold=self.config.min_fuzzy_score\n   331\u2192            )\n   332\u2192            \n   333\u2192            search_results = []\n   334\u2192            for r in results:\n   335\u2192                if r['score'] >= self.config.min_fuzzy_score:\n   336\u2192                    search_results.append(SearchResult(\n   337\u2192                        doc_id=r.get('file_path', ''),\n   338\u2192                        filepath=r.get('file_path', ''),\n   339\u2192                        score=r['score'],\n   340\u2192                        snippet=r.get('context', ''),\n   341\u2192                        metadata=r,\n   342\u2192                        source='fuzzy'\n   343\u2192                    ))\n   344\u2192            return search_results\n   345\u2192        \n   346\u2192        results = await loop.run_in_executor(None, run_search)\n   347\u2192        self._search_stats['fuzzy_searches'] += 1\n   348\u2192        return results\n   349\u2192    \n   350\u2192    def _reciprocal_rank_fusion(self, result_lists: List[List[SearchResult]]) -> List[SearchResult]:\n   351\u2192        \"\"\"\n   352\u2192        Combine results using Reciprocal Rank Fusion (RRF).\n   353\u2192        \n   354\u2192        RRF score = \u03a3(1 / (k + rank_i)) for each result list\n   355\u2192        where k is a constant (typically 60) and rank_i is the rank in list i\n   356\u2192        \"\"\"\n   357\u2192        # Track scores for each document\n   358\u2192        doc_scores: Dict[str, float] = defaultdict(float)\n   359\u2192        doc_info: Dict[str, SearchResult] = {}\n   360\u2192        \n   361\u2192        # Calculate RRF scores\n   362\u2192        for result_list in result_lists:\n   363\u2192            for rank, result in enumerate(result_list):\n   364\u2192                doc_id = result.doc_id\n   365\u2192                \n   366\u2192                # RRF score for this ranking\n   367\u2192                rrf_score = 1.0 / (self.config.rrf_k + rank + 1)\n   368\u2192                \n   369\u2192                # Apply source-specific weights\n   370\u2192                if result.source == 'bm25':\n   371\u2192                    rrf_score *= self.config.bm25_weight\n   372\u2192                elif result.source == 'semantic':\n   373\u2192                    rrf_score *= self.config.semantic_weight\n   374\u2192                elif result.source == 'fuzzy':\n   375\u2192                    rrf_score *= self.config.fuzzy_weight\n   376\u2192                \n   377\u2192                doc_scores[doc_id] += rrf_score\n   378\u2192                \n   379\u2192                # Keep the result with the best individual score\n   380\u2192                if doc_id not in doc_info or result.score > doc_info[doc_id].score:\n   381\u2192                    doc_info[doc_id] = result\n   382\u2192        \n   383\u2192        # Create combined results\n   384\u2192        combined = []\n   385\u2192        for doc_id, combined_score in doc_scores.items():\n   386\u2192            result = doc_info[doc_id]\n   387\u2192            # Update score to combined RRF score\n   388\u2192            result.score = combined_score\n   389\u2192            combined.append(result)\n   390\u2192        \n   391\u2192        # Sort by combined score\n   392\u2192        combined.sort(key=lambda x: x.score, reverse=True)\n   393\u2192        \n   394\u2192        return combined\n   395\u2192    \n   396\u2192    async def _rerank_results(self, query: str, results: List[SearchResult]) -> List[SearchResult]:\n   397\u2192        \"\"\"Rerank search results using the configured reranker.\"\"\"\n   398\u2192        if not self.reranker or not results:\n   399\u2192            return results\n   400\u2192        \n   401\u2192        try:\n   402\u2192            # Initialize reranker if not already done\n   403\u2192            if not hasattr(self, '_reranker_initialized'):\n   404\u2192                init_result = await self.reranker.initialize({})\n   405\u2192                if not init_result.is_success:\n   406\u2192                    logger.error(f\"Failed to initialize reranker: {init_result.error}\")\n   407\u2192                    return results\n   408\u2192                self._reranker_initialized = True\n   409\u2192            \n   410\u2192            # Import SearchResult from reranker module to avoid circular dependency\n   411\u2192            from .reranker import SearchResult as RerankSearchResult\n   412\u2192            \n   413\u2192            # Convert our SearchResult to reranker's SearchResult format\n   414\u2192            reranker_results = []\n   415\u2192            for r in results:\n   416\u2192                # Create a reranker-compatible search result\n   417\u2192                reranker_result = RerankSearchResult(\n   418\u2192                    file_path=r.filepath,\n   419\u2192                    line=r.metadata.get('line', 1),\n   420\u2192                    column=r.metadata.get('column', 0),\n   421\u2192                    snippet=r.snippet,\n   422\u2192                    match_type=r.source,  # Use source as match type\n   423\u2192                    score=r.score,\n   424\u2192                    context=r.metadata.get('context', '')\n   425\u2192                )\n   426\u2192                reranker_results.append(reranker_result)\n   427\u2192            \n   428\u2192            # Perform reranking\n   429\u2192            top_k = self.reranking_settings.top_k if self.reranking_settings else len(reranker_results)\n   430\u2192            rerank_result = await self.reranker.rerank(\n   431\u2192                query,\n   432\u2192                reranker_results,\n   433\u2192                top_k=top_k\n   434\u2192            )\n   435\u2192            \n   436\u2192            if not rerank_result.is_success:\n   437\u2192                logger.warning(f\"Reranking failed: {rerank_result.error}\")\n   438\u2192                return results\n   439\u2192            \n   440\u2192            # Handle the new RerankResult structure\n   441\u2192            # rerank_result.data is a RerankResult object with 'results' list and 'metadata' dict\n   442\u2192            rerank_data = rerank_result.data\n   443\u2192            if not rerank_data:\n   444\u2192                logger.warning(\"Reranking returned no data\")\n   445\u2192                return results\n   446\u2192                \n   447\u2192            if not hasattr(rerank_data, 'results'):\n   448\u2192                logger.error(\"Unexpected rerank result structure\")\n   449\u2192                return results\n   450\u2192                \n   451\u2192            if not rerank_data.results:\n   452\u2192                logger.warning(\"Reranking returned empty results\")\n   453\u2192                return results\n   454\u2192                \n   455\u2192            # Convert back to our SearchResult format, preserving all original metadata\n   456\u2192            reranked_results = []\n   457\u2192            for rerank_item in rerank_data.results:\n   458\u2192                # Validate original_rank is within bounds\n   459\u2192                if rerank_item.original_rank < 0 or rerank_item.original_rank >= len(results):\n   460\u2192                    logger.warning(f\"Invalid original_rank {rerank_item.original_rank} for {len(results)} results\")\n   461\u2192                    continue\n   462\u2192                    \n   463\u2192                # Get the original result using the original_rank\n   464\u2192                original = results[rerank_item.original_rank]\n   465\u2192                \n   466\u2192                # Create a new SearchResult with updated score but preserved metadata\n   467\u2192                updated_result = SearchResult(\n   468\u2192                    doc_id=original.doc_id,\n   469\u2192                    filepath=original.filepath,\n   470\u2192                    score=rerank_item.rerank_score,  # Use the new rerank score\n   471\u2192                    snippet=original.snippet,\n   472\u2192                    metadata=original.metadata.copy(),  # Preserve all original metadata\n   473\u2192                    source=original.source\n   474\u2192                )\n   475\u2192                \n   476\u2192                # Add reranking metadata\n   477\u2192                updated_result.metadata['original_score'] = original.score\n   478\u2192                updated_result.metadata['rerank_score'] = rerank_item.rerank_score\n   479\u2192                updated_result.metadata['original_rank'] = rerank_item.original_rank\n   480\u2192                updated_result.metadata['new_rank'] = rerank_item.new_rank\n   481\u2192                if rerank_item.explanation:\n   482\u2192                    updated_result.metadata['rerank_explanation'] = rerank_item.explanation\n   483\u2192                \n   484\u2192                reranked_results.append(updated_result)\n   485\u2192            \n   486\u2192            # Sort by new rank to ensure proper ordering\n   487\u2192            reranked_results.sort(key=lambda x: x.metadata.get('new_rank', float('inf')))\n   488\u2192            \n   489\u2192            # Log reranking metadata if available\n   490\u2192            if hasattr(rerank_data, 'metadata') and rerank_data.metadata:\n   491\u2192                logger.debug(f\"Reranked {len(reranked_results)} results using {rerank_data.metadata.get('reranker', 'unknown')} reranker\")\n   492\u2192            else:\n   493\u2192                logger.debug(f\"Reranked {len(reranked_results)} results\")\n   494\u2192            \n   495\u2192            return reranked_results\n   496\u2192            \n   497\u2192        except Exception as e:\n   498\u2192            logger.error(f\"Error during reranking: {e}\", exc_info=True)\n   499\u2192            return results\n   500\u2192    \n   501\u2192    def _post_process_results(self, results: List[SearchResult], limit: int) -> List[SearchResult]:\n   502\u2192        \"\"\"Apply post-processing to results.\"\"\"\n   503\u2192        # Remove duplicates while preserving order\n   504\u2192        seen = set()\n   505\u2192        unique_results = []\n   506\u2192        for result in results:\n   507\u2192            if result.filepath not in seen:\n   508\u2192                seen.add(result.filepath)\n   509\u2192                unique_results.append(result)\n   510\u2192        \n   511\u2192        # Apply limit\n   512\u2192        unique_results = unique_results[:limit]\n   513\u2192        \n   514\u2192        # Enhance snippets if needed\n   515\u2192        for result in unique_results:\n   516\u2192            if not result.snippet and result.filepath:\n   517\u2192                # Try to generate a snippet\n   518\u2192                result.snippet = self._generate_snippet(result.filepath, result.metadata)\n   519\u2192        \n   520\u2192        return unique_results\n   521\u2192    \n   522\u2192    def _generate_snippet(self, filepath: str, metadata: Dict[str, Any]) -> str:\n   523\u2192        \"\"\"Generate a snippet for a result.\"\"\"\n   524\u2192        # This is a placeholder - in practice, you'd read the file\n   525\u2192        # and extract relevant context\n   526\u2192        return f\"File: {filepath}\"\n   527\u2192    \n   528\u2192    def _format_results(self, results: List[SearchResult]) -> List[Dict[str, Any]]:\n   529\u2192        \"\"\"Format results for output, including all metadata.\"\"\"\n   530\u2192        formatted = []\n   531\u2192        for i, result in enumerate(results):\n   532\u2192            # Ensure metadata exists\n   533\u2192            metadata = result.metadata if result.metadata else {}\n   534\u2192            \n   535\u2192            # Build base result with all fields\n   536\u2192            formatted_result = {\n   537\u2192                'rank': i + 1,\n   538\u2192                'filepath': result.filepath,\n   539\u2192                'score': result.score,\n   540\u2192                'snippet': result.snippet,\n   541\u2192                'source': result.source,\n   542\u2192                # Include all metadata fields\n   543\u2192                'line': metadata.get('line', 1),\n   544\u2192                'column': metadata.get('column', 0),\n   545\u2192                'context': metadata.get('context', ''),\n   546\u2192                'match_type': metadata.get('match_type', result.source),\n   547\u2192                # Include language and symbol information if present\n   548\u2192                'language': metadata.get('language', ''),\n   549\u2192                'symbol': metadata.get('symbol', ''),\n   550\u2192                'symbol_type': metadata.get('symbol_type', ''),\n   551\u2192                # Include reranking information if present\n   552\u2192                'original_score': metadata.get('original_score', result.score),\n   553\u2192                'rerank_score': metadata.get('rerank_score'),\n   554\u2192                'original_rank': metadata.get('original_rank'),\n   555\u2192                'new_rank': metadata.get('new_rank'),\n   556\u2192                'rerank_explanation': metadata.get('rerank_explanation'),\n   557\u2192                # Include any additional metadata\n   558\u2192                'metadata': metadata\n   559\u2192            }\n   560\u2192            \n   561\u2192            # Remove None values and empty strings for cleaner output\n   562\u2192            formatted_result = {k: v for k, v in formatted_result.items() \n   563\u2192                              if v is not None and (not isinstance(v, str) or v != '')}\n   564\u2192            \n   565\u2192            formatted.append(formatted_result)\n   566\u2192        return formatted\n   567\u2192    \n   568\u2192    def _get_cache_key(self, query: str, filters: Optional[Dict[str, Any]]) -> str:\n   569\u2192        \"\"\"Generate cache key for a query.\"\"\"\n   570\u2192        import hashlib\n   571\u2192        key_parts = [query]\n   572\u2192        if filters:\n   573\u2192            key_parts.extend(f\"{k}:{v}\" for k, v in sorted(filters.items()))\n   574\u2192        key_string = \"|\".join(key_parts)\n   575\u2192        return hashlib.md5(key_string.encode()).hexdigest()\n   576\u2192    \n   577\u2192    def _cleanup_cache(self):\n   578\u2192        \"\"\"Clean up cache if it gets too large.\"\"\"\n   579\u2192        max_cache_size = 1000\n   580\u2192        if len(self._result_cache) > max_cache_size:\n   581\u2192            # Remove oldest entries (simple FIFO)\n   582\u2192            entries_to_remove = len(self._result_cache) - max_cache_size // 2\n   583\u2192            for key in list(self._result_cache.keys())[:entries_to_remove]:\n   584\u2192                del self._result_cache[key]\n   585\u2192    \n   586\u2192    # Configuration methods\n   587\u2192    \n   588\u2192    def set_weights(self, bm25: float = None, semantic: float = None, fuzzy: float = None):\n   589\u2192        \"\"\"\n   590\u2192        Update search method weights.\n   591\u2192        \n   592\u2192        Args:\n   593\u2192            bm25: Weight for BM25 search (0-1)\n   594\u2192            semantic: Weight for semantic search (0-1)\n   595\u2192            fuzzy: Weight for fuzzy search (0-1)\n   596\u2192        \"\"\"\n   597\u2192        if bm25 is not None:\n   598\u2192            self.config.bm25_weight = max(0, min(1, bm25))\n   599\u2192        if semantic is not None:\n   600\u2192            self.config.semantic_weight = max(0, min(1, semantic))\n   601\u2192        if fuzzy is not None:\n   602\u2192            self.config.fuzzy_weight = max(0, min(1, fuzzy))\n   603\u2192            \n   604\u2192        # Normalize weights\n   605\u2192        total = self.config.bm25_weight + self.config.semantic_weight + self.config.fuzzy_weight\n   606\u2192        if total > 0:\n   607\u2192            self.config.bm25_weight /= total\n   608\u2192            self.config.semantic_weight /= total\n   609\u2192            self.config.fuzzy_weight /= total\n   610\u2192    \n   611\u2192    def enable_methods(self, bm25: bool = None, semantic: bool = None, fuzzy: bool = None):\n   612\u2192        \"\"\"\n   613\u2192        Enable or disable search methods.\n   614\u2192        \n   615\u2192        Args:\n   616\u2192            bm25: Enable/disable BM25 search\n   617\u2192            semantic: Enable/disable semantic search\n   618\u2192            fuzzy: Enable/disable fuzzy search\n   619\u2192        \"\"\"\n   620\u2192        if bm25 is not None:\n   621\u2192            self.config.enable_bm25 = bm25\n   622\u2192        if semantic is not None:\n   623\u2192            self.config.enable_semantic = semantic\n   624\u2192        if fuzzy is not None:\n   625\u2192            self.config.enable_fuzzy = fuzzy\n   626\u2192    \n   627\u2192    def get_statistics(self) -> Dict[str, Any]:\n   628\u2192        \"\"\"Get search statistics.\"\"\"\n   629\u2192        stats = dict(self._search_stats)\n   630\u2192        \n   631\u2192        # Add cache statistics\n   632\u2192        stats['cache_size'] = len(self._result_cache)\n   633\u2192        cache_hit_rate = 0\n   634\u2192        if stats.get('total_searches', 0) > 0:\n   635\u2192            cache_hit_rate = stats.get('cache_hits', 0) / stats['total_searches']\n   636\u2192        stats['cache_hit_rate'] = cache_hit_rate\n   637\u2192        \n   638\u2192        # Add configuration info\n   639\u2192        stats['config'] = {\n   640\u2192            'weights': {\n   641\u2192                'bm25': self.config.bm25_weight,\n   642\u2192                'semantic': self.config.semantic_weight,\n   643\u2192                'fuzzy': self.config.fuzzy_weight\n   644\u2192            },\n   645\u2192            'enabled_methods': {\n   646\u2192                'bm25': self.config.enable_bm25,\n   647\u2192                'semantic': self.config.enable_semantic,\n   648\u2192                'fuzzy': self.config.enable_fuzzy\n   649\u2192            },\n   650\u2192            'rrf_k': self.config.rrf_k\n   651\u2192        }\n   652\u2192        \n   653\u2192        return stats\n   654\u2192    \n   655\u2192    def clear_cache(self):\n   656\u2192        \"\"\"Clear the result cache.\"\"\"\n   657\u2192        self._result_cache.clear()\n   658\u2192        logger.info(\"Hybrid search cache cleared\")\n   659\u2192\n   660\u2192\n   661\u2192class HybridSearchOptimizer:\n   662\u2192    \"\"\"\n   663\u2192    Optimizer for hybrid search parameters based on user feedback and performance.\n   664\u2192    \"\"\"\n   665\u2192    \n   666\u2192    def __init__(self, hybrid_search: HybridSearch):\n   667\u2192        \"\"\"\n   668\u2192        Initialize the optimizer.\n   669\u2192        \n   670\u2192        Args:\n   671\u2192            hybrid_search: HybridSearch instance to optimize\n   672\u2192        \"\"\"\n   673\u2192        self.hybrid_search = hybrid_search\n   674\u2192        self.feedback_history: List[Dict[str, Any]] = []\n   675\u2192        self.performance_history: List[Dict[str, Any]] = []\n   676\u2192        \n   677\u2192    def record_feedback(self, query: str, selected_result: int, results: List[Dict[str, Any]]):\n   678\u2192        \"\"\"\n   679\u2192        Record user feedback on search results.\n   680\u2192        \n   681\u2192        Args:\n   682\u2192            query: The search query\n   683\u2192            selected_result: Index of the result the user selected\n   684\u2192            results: The search results shown to the user\n   685\u2192        \"\"\"\n   686\u2192        feedback = {\n   687\u2192            'query': query,\n   688\u2192            'selected_rank': selected_result + 1,\n   689\u2192            'selected_source': results[selected_result]['source'] if selected_result < len(results) else None,\n   690\u2192            'num_results': len(results),\n   691\u2192            'timestamp': datetime.now()\n   692\u2192        }\n   693\u2192        self.feedback_history.append(feedback)\n   694\u2192        \n   695\u2192        # Optimize weights after collecting enough feedback\n   696\u2192        if len(self.feedback_history) % 10 == 0:\n   697\u2192            self.optimize_weights()\n   698\u2192    \n   699\u2192    def record_performance(self, query: str, search_time_ms: float, num_results: int):\n   700\u2192        \"\"\"\n   701\u2192        Record search performance metrics.\n   702\u2192        \n   703\u2192        Args:\n   704\u2192            query: The search query\n   705\u2192            search_time_ms: Time taken for search in milliseconds\n   706\u2192            num_results: Number of results returned\n   707\u2192        \"\"\"\n   708\u2192        self.performance_history.append({\n   709\u2192            'query': query,\n   710\u2192            'search_time_ms': search_time_ms,\n   711\u2192            'num_results': num_results,\n   712\u2192            'timestamp': datetime.now()\n   713\u2192        })\n   714\u2192    \n   715\u2192    def optimize_weights(self):\n   716\u2192        \"\"\"Optimize search weights based on feedback history.\"\"\"\n   717\u2192        if len(self.feedback_history) < 10:\n   718\u2192            return\n   719\u2192        \n   720\u2192        # Count selections by source\n   721\u2192        source_selections = defaultdict(int)\n   722\u2192        source_ranks = defaultdict(list)\n   723\u2192        \n   724\u2192        for feedback in self.feedback_history[-100:]:  # Last 100 feedbacks\n   725\u2192            source = feedback.get('selected_source')\n   726\u2192            if source:\n   727\u2192                source_selections[source] += 1\n   728\u2192                source_ranks[source].append(feedback['selected_rank'])\n   729\u2192        \n   730\u2192        # Calculate average rank for each source\n   731\u2192        avg_ranks = {}\n   732\u2192        for source, ranks in source_ranks.items():\n   733\u2192            avg_ranks[source] = sum(ranks) / len(ranks) if ranks else float('inf')\n   734\u2192        \n   735\u2192        # Update weights based on selection frequency and average rank\n   736\u2192        total_selections = sum(source_selections.values())\n   737\u2192        if total_selections > 0:\n   738\u2192            # Base weights on selection frequency\n   739\u2192            bm25_weight = source_selections.get('bm25', 0) / total_selections\n   740\u2192            semantic_weight = source_selections.get('semantic', 0) / total_selections\n   741\u2192            fuzzy_weight = source_selections.get('fuzzy', 0) / total_selections\n   742\u2192            \n   743\u2192            # Adjust based on average rank (lower is better)\n   744\u2192            rank_factor = 0.2\n   745\u2192            if 'bm25' in avg_ranks:\n   746\u2192                bm25_weight *= (1 + rank_factor * (1 / avg_ranks['bm25']))\n   747\u2192            if 'semantic' in avg_ranks:\n   748\u2192                semantic_weight *= (1 + rank_factor * (1 / avg_ranks['semantic']))\n   749\u2192            if 'fuzzy' in avg_ranks:\n   750\u2192                fuzzy_weight *= (1 + rank_factor * (1 / avg_ranks['fuzzy']))\n   751\u2192            \n   752\u2192            # Apply new weights\n   753\u2192            self.hybrid_search.set_weights(bm25_weight, semantic_weight, fuzzy_weight)\n   754\u2192            \n   755\u2192            logger.info(f\"Optimized weights - BM25: {bm25_weight:.3f}, \"\n   756\u2192                       f\"Semantic: {semantic_weight:.3f}, Fuzzy: {fuzzy_weight:.3f}\")\n   757\u2192    \n   758\u2192    def get_optimization_report(self) -> Dict[str, Any]:\n   759\u2192        \"\"\"Get a report on optimization performance.\"\"\"\n   760\u2192        if not self.feedback_history:\n   761\u2192            return {'status': 'no_data'}\n   762\u2192        \n   763\u2192        # Analyze feedback\n   764\u2192        source_stats = defaultdict(lambda: {'count': 0, 'avg_rank': 0})\n   765\u2192        for feedback in self.feedback_history:\n   766\u2192            source = feedback.get('selected_source')\n   767\u2192            if source:\n   768\u2192                stats = source_stats[source]\n   769\u2192                stats['count'] += 1\n   770\u2192                stats['avg_rank'] = (\n   771\u2192                    (stats['avg_rank'] * (stats['count'] - 1) + feedback['selected_rank']) \n   772\u2192                    / stats['count']\n   773\u2192                )\n   774\u2192        \n   775\u2192        # Analyze performance\n   776\u2192        avg_search_time = 0\n   777\u2192        if self.performance_history:\n   778\u2192            avg_search_time = sum(p['search_time_ms'] for p in self.performance_history) / len(self.performance_history)\n   779\u2192        \n   780\u2192        return {\n   781\u2192            'feedback_count': len(self.feedback_history),\n   782\u2192            'source_statistics': dict(source_stats),\n   783\u2192            'average_search_time_ms': avg_search_time,\n   784\u2192            'current_weights': {\n   785\u2192                'bm25': self.hybrid_search.config.bm25_weight,\n   786\u2192                'semantic': self.hybrid_search.config.semantic_weight,\n   787\u2192                'fuzzy': self.hybrid_search.config.fuzzy_weight\n   788\u2192            }\n   789\u2192        }\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 5239,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 5239,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 236,
    "output_tokens_total": 236,
    "cache_read_tokens": 51722,
    "cache_creation_tokens": 25622,
    "tokens_per_result": 236,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "41333a35-f5b8-4f90-875a-e853cef49417",
    "test_date": "2025-06-26T07:37:34.020Z",
    "query_id": "transcript_41333a35-f5b8-4f90-875a-e853cef49417",
    "query_text": [
      {
        "type": "tool_result",
        "content": "Claude requested permissions to use Bash, but you haven't granted it yet.",
        "is_error": true,
        "tool_use_id": "toolu_01PsXYECXJKL1mQGBxU1xTBU"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 8,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 157,
    "output_tokens_total": 157,
    "cache_read_tokens": 77344,
    "cache_creation_tokens": 5558,
    "tokens_per_result": 157,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "41333a35-f5b8-4f90-875a-e853cef49417",
    "test_date": "2025-06-26T07:37:41.059Z",
    "query_id": "transcript_41333a35-f5b8-4f90-875a-e853cef49417",
    "query_text": [
      {
        "tool_use_id": "toolu_01Sqex9qf9pdjZKhP4Dn3zEU",
        "type": "tool_result",
        "content": "Found 51 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_indexer.py"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 8,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 210,
    "output_tokens_total": 210,
    "cache_read_tokens": 82902,
    "cache_creation_tokens": 3606,
    "tokens_per_result": 210,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "41333a35-f5b8-4f90-875a-e853cef49417",
    "test_date": "2025-06-26T07:37:50.002Z",
    "query_id": "transcript_41333a35-f5b8-4f90-875a-e853cef49417",
    "query_text": [
      {
        "tool_use_id": "toolu_01MvVEFNoB8xyZZgDPLFXiKk",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"\n     2\u2192Reranking Module for Search Result Optimization\n     3\u2192\n     4\u2192This module provides implementations for reranking search results to improve relevance.\n     5\u2192It supports both Cohere's reranking API and local cross-encoder models as fallback.\n     6\u2192\"\"\"\n     7\u2192\n     8\u2192import os\n     9\u2192import logging\n    10\u2192import asyncio\n    11\u2192from typing import Dict, List, Optional, Any, Tuple\n    12\u2192from dataclasses import dataclass\n    13\u2192import numpy as np\n    14\u2192from abc import ABC, abstractmethod\n    15\u2192\n    16\u2192# Define interfaces inline for now\n    17\u2192from abc import ABC, abstractmethod\n    18\u2192from dataclasses import dataclass as dc\n    19\u2192from typing import Protocol\n    20\u2192\n    21\u2192# Define SearchResult inline\n    22\u2192@dc \n    23\u2192class SearchResult:\n    24\u2192    \"\"\"Search result information\"\"\"\n    25\u2192    file_path: str\n    26\u2192    line: int\n    27\u2192    column: int\n    28\u2192    snippet: str\n    29\u2192    match_type: str  # exact, fuzzy, semantic\n    30\u2192    score: float\n    31\u2192    context: Optional[str] = None\n    32\u2192\n    33\u2192# Define RerankItem to wrap original SearchResult\n    34\u2192@dc\n    35\u2192class RerankItem:\n    36\u2192    \"\"\"A single reranked item that preserves the complete original SearchResult\"\"\"\n    37\u2192    original_result: SearchResult  # Complete original SearchResult\n    38\u2192    rerank_score: float  # The reranking score\n    39\u2192    original_rank: int  # Original position in results\n    40\u2192    new_rank: int  # New position after reranking\n    41\u2192    explanation: Optional[str] = None  # Optional explanation for the ranking\n    42\u2192\n    43\u2192# Define RerankResult with proper structure\n    44\u2192@dc\n    45\u2192class RerankResult:\n    46\u2192    \"\"\"Result from reranking operation\"\"\"\n    47\u2192    results: List[RerankItem]  # List of reranked items\n    48\u2192    metadata: Dict[str, Any]  # Metadata about the reranking operation\n    49\u2192\n    50\u2192# Define IReranker interface\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 8,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 202,
    "output_tokens_total": 202,
    "cache_read_tokens": 86508,
    "cache_creation_tokens": 1446,
    "tokens_per_result": 202,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "41333a35-f5b8-4f90-875a-e853cef49417",
    "test_date": "2025-06-26T07:37:56.950Z",
    "query_id": "transcript_41333a35-f5b8-4f90-875a-e853cef49417",
    "query_text": [
      {
        "tool_use_id": "toolu_0136yzG3KyXDvkvTqDKa2GVN",
        "type": "tool_result",
        "content": "     1\u2192#!/usr/bin/env python3\n     2\u2192\"\"\"Parallel test runner for document processing features.\"\"\"\n     3\u2192\n     4\u2192import asyncio\n     5\u2192import json\n     6\u2192import time\n     7\u2192import traceback\n     8\u2192from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n     9\u2192from dataclasses import dataclass, field\n    10\u2192from datetime import datetime\n    11\u2192from pathlib import Path\n    12\u2192from typing import Dict, List, Optional, Tuple\n    13\u2192import multiprocessing\n    14\u2192import subprocess\n    15\u2192import sys\n    16\u2192import os\n    17\u2192\n    18\u2192# Add project root to path\n    19\u2192sys.path.insert(0, str(Path(__file__).parent))\n    20\u2192\n    21\u2192\n    22\u2192@dataclass\n    23\u2192class TestResult:\n    24\u2192    \"\"\"Test execution result.\"\"\"\n    25\u2192    name: str\n    26\u2192    group: str\n    27\u2192    status: str  # 'passed', 'failed', 'error'\n    28\u2192    duration: float\n    29\u2192    message: str = \"\"\n    30\u2192    errors: List[str] = field(default_factory=list)\n    31\u2192    metrics: Dict[str, float] = field(default_factory=dict)\n    32\u2192\n    33\u2192\n    34\u2192@dataclass\n    35\u2192class TestGroup:\n    36\u2192    \"\"\"Group of related tests to run in parallel.\"\"\"\n    37\u2192    name: str\n    38\u2192    tests: List[str]\n    39\u2192    max_workers: int = 4\n    40\u2192\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 8,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 2839,
    "output_tokens_total": 2839,
    "cache_read_tokens": 87954,
    "cache_creation_tokens": 1244,
    "tokens_per_result": 2839,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "c6133498-4b89-4f96-b12c-b76e3435cbcf",
    "test_date": "2025-06-26T07:31:09.559Z",
    "query_id": "transcript_c6133498-4b89-4f96-b12c-b76e3435cbcf",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13172,
    "cache_creation_tokens": 748,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "c6133498-4b89-4f96-b12c-b76e3435cbcf",
    "test_date": "2025-06-26T07:31:13.313Z",
    "query_id": "transcript_c6133498-4b89-4f96-b12c-b76e3435cbcf",
    "query_text": [
      {
        "tool_use_id": "toolu_01KT4mFAJVK3N8vkjF5ZBwmz",
        "type": "tool_result",
        "content": "Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/level4/path_management.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/path_management_architecture.md"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 26,
    "output_tokens_total": 26,
    "cache_read_tokens": 13920,
    "cache_creation_tokens": 447,
    "tokens_per_result": 26,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "c6133498-4b89-4f96-b12c-b76e3435cbcf",
    "test_date": "2025-06-26T07:31:18.205Z",
    "query_id": "transcript_c6133498-4b89-4f96-b12c-b76e3435cbcf",
    "query_text": [
      {
        "tool_use_id": "toolu_0172221y9RdEtmQniTvUUMQi",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"Enhanced dispatcher with dynamic plugin loading via PluginFactory.\"\"\"\n     2\u2192from pathlib import Path\n     3\u2192from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4\u2192import logging\n     5\u2192import hashlib\n     6\u2192import time\n     7\u2192import re\n     8\u2192from datetime import datetime\n     9\u2192import os\n    10\u2192\n    11\u2192from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12\u2192from ..plugins.plugin_factory import PluginFactory\n    13\u2192from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14\u2192from ..storage.sqlite_store import SQLiteStore\n    15\u2192from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16\u2192from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17\u2192# Note: We've removed ignore pattern checks to allow indexing ALL files\n    18\u2192# Filtering happens only during export via SecureIndexExporter\n    19\u2192# from ..core.ignore_patterns import get_ignore_manager\n    20\u2192\n    21\u2192logger = logging.getLogger(__name__)\n    22\u2192\n    23\u2192\n    24\u2192class EnhancedDispatcher:\n    25\u2192    \"\"\"Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.\"\"\"\n    26\u2192    \n    27\u2192    # Document query patterns - common documentation search terms\n    28\u2192    DOCUMENT_QUERY_PATTERNS = [\n    29\u2192        r'\\b(how\\s+to|howto)\\b',\n    30\u2192        r'\\b(getting\\s+started|get\\s+started)\\b',\n    31\u2192        r'\\b(installation|install|setup)\\b',\n    32\u2192        r'\\b(configuration|configure|config)\\b',\n    33\u2192        r'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b',\n    34\u2192        r'\\b(tutorial|guide|walkthrough)\\b',\n    35\u2192        r'\\b(example|sample|snippet)\\b',\n    36\u2192        r'\\b(readme|documentation|docs)\\b',\n    37\u2192        r'\\b(usage|use\\s+case|using)\\b',\n    38\u2192        r'\\b(reference|manual)\\b',\n    39\u2192        r'\\b(faq|frequently\\s+asked)\\b',\n    40\u2192        r'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b',\n    41\u2192        r'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b',\n    42\u2192        r'\\b(architecture|design|overview)\\b',\n    43\u2192        r'\\b(changelog|release\\s+notes|migration)\\b'\n    44\u2192    ]\n    45\u2192    \n    46\u2192    # Documentation file patterns\n    47\u2192    DOCUMENTATION_FILE_PATTERNS = [\n    48\u2192        r'readme(\\.\\w+)?$',\n    49\u2192        r'changelog(\\.\\w+)?$',\n    50\u2192        r'contributing(\\.\\w+)?$',\n    51\u2192        r'license(\\.\\w+)?$',\n    52\u2192        r'install(\\.\\w+)?$',\n    53\u2192        r'setup(\\.\\w+)?$',\n    54\u2192        r'guide(\\.\\w+)?$',\n    55\u2192        r'tutorial(\\.\\w+)?$',\n    56\u2192        r'\\.md$',\n    57\u2192        r'\\.rst$',\n    58\u2192        r'\\.txt$',\n    59\u2192        r'docs?/',\n    60\u2192        r'documentation/'\n    61\u2192    ]\n    62\u2192    \n    63\u2192    def __init__(self, \n    64\u2192                 plugins: Optional[List[IPlugin]] = None,\n    65\u2192                 sqlite_store: Optional[SQLiteStore] = None,\n    66\u2192                 enable_advanced_features: bool = True,\n    67\u2192                 use_plugin_factory: bool = True,\n    68\u2192                 lazy_load: bool = True,\n    69\u2192                 semantic_search_enabled: bool = True):\n    70\u2192        \"\"\"Initialize the enhanced dispatcher.\n    71\u2192        \n    72\u2192        Args:\n    73\u2192            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74\u2192            sqlite_store: SQLite store for plugin persistence\n    75\u2192            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76\u2192            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77\u2192            lazy_load: Whether to lazy-load plugins on demand\n    78\u2192            semantic_search_enabled: Whether to enable semantic search in plugins\n    79\u2192        \"\"\"\n    80\u2192        self._sqlite_store = sqlite_store\n    81\u2192        self._enable_advanced = enable_advanced_features\n    82\u2192        self._use_factory = use_plugin_factory\n    83\u2192        self._lazy_load = lazy_load\n    84\u2192        self._semantic_enabled = semantic_search_enabled\n    85\u2192        \n    86\u2192        # Plugin storage\n    87\u2192        self._plugins: List[IPlugin] = []\n    88\u2192        self._by_lang: Dict[str, IPlugin] = {}\n    89\u2192        self._loaded_languages: set[str] = set()\n    90\u2192        \n    91\u2192        # Cache for file hashes to avoid re-indexing unchanged files\n    92\u2192        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93\u2192        \n    94\u2192        # Advanced components\n    95\u2192        if self._enable_advanced:\n    96\u2192            self._file_matcher = FileTypeMatcher()\n    97\u2192            self._router = PluginRouter(self._file_matcher)\n    98\u2192            self._aggregator = ResultAggregator()\n    99\u2192        \n   100\u2192        # Performance tracking\n   101\u2192        self._operation_stats = {\n   102\u2192            'searches': 0,\n   103\u2192            'lookups': 0,\n   104\u2192            'indexings': 0,\n   105\u2192            'total_time': 0.0,\n   106\u2192            'plugins_loaded': 0\n   107\u2192        }\n   108\u2192        \n   109\u2192        # Initialize plugins\n   110\u2192        if plugins:\n   111\u2192            # Use provided plugins (backward compatibility)\n   112\u2192            self._plugins = plugins\n   113\u2192            self._by_lang = {p.lang: p for p in plugins}\n   114\u2192            for plugin in plugins:\n   115\u2192                self._loaded_languages.add(getattr(plugin, 'lang', 'unknown'))\n   116\u2192            if self._enable_advanced:\n   117\u2192                self._register_plugins_with_router()\n   118\u2192        elif use_plugin_factory and not lazy_load:\n   119\u2192            # Load all plugins immediately\n   120\u2192            self._load_all_plugins()\n   121\u2192        # If lazy_load is True, plugins will be loaded on demand\n   122\u2192        \n   123\u2192        # Compile document query patterns for performance\n   124\u2192        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125\u2192        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126\u2192        \n   127\u2192        logger.info(f\"Enhanced dispatcher initialized with {len(self._plugins)} plugins\")\n   128\u2192    \n   129\u2192    def _load_all_plugins(self):\n   130\u2192        \"\"\"Load all available plugins using PluginFactory.\"\"\"\n   131\u2192        logger.info(\"Loading all available plugins...\")\n   132\u2192        \n   133\u2192        all_plugins = PluginFactory.create_all_plugins(\n   134\u2192            sqlite_store=self._sqlite_store,\n   135\u2192            enable_semantic=self._semantic_enabled\n   136\u2192        )\n   137\u2192        \n   138\u2192        for lang, plugin in all_plugins.items():\n   139\u2192            self._plugins.append(plugin)\n   140\u2192            self._by_lang[lang] = plugin\n   141\u2192            self._loaded_languages.add(lang)\n   142\u2192            self._operation_stats['plugins_loaded'] += 1\n   143\u2192        \n   144\u2192        if self._enable_advanced:\n   145\u2192            self._register_plugins_with_router()\n   146\u2192        \n   147\u2192        logger.info(f\"Loaded {len(all_plugins)} plugins: {', '.join(sorted(all_plugins.keys()))}\")\n   148\u2192    \n   149\u2192    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150\u2192        \"\"\"Ensure a plugin for the given language is loaded.\n   151\u2192        \n   152\u2192        Args:\n   153\u2192            language: Language code (e.g., 'python', 'go')\n   154\u2192            \n   155\u2192        Returns:\n   156\u2192            Plugin instance or None if not available\n   157\u2192        \"\"\"\n   158\u2192        # Normalize language\n   159\u2192        language = language.lower().replace('-', '_')\n   160\u2192        \n   161\u2192        # Check if already loaded\n   162\u2192        if language in self._by_lang:\n   163\u2192            return self._by_lang[language]\n   164\u2192        \n   165\u2192        # If not using factory or already tried to load, return None\n   166\u2192        if not self._use_factory or language in self._loaded_languages:\n   167\u2192            return None\n   168\u2192        \n   169\u2192        # Try to load the plugin\n   170\u2192        try:\n   171\u2192            logger.info(f\"Lazy loading plugin for {language}\")\n   172\u2192            plugin = PluginFactory.create_plugin(\n   173\u2192                language,\n   174\u2192                sqlite_store=self._sqlite_store,\n   175\u2192                enable_semantic=self._semantic_enabled\n   176\u2192            )\n   177\u2192            \n   178\u2192            # Add to collections\n   179\u2192            self._plugins.append(plugin)\n   180\u2192            self._by_lang[language] = plugin\n   181\u2192            self._loaded_languages.add(language)\n   182\u2192            self._operation_stats['plugins_loaded'] += 1\n   183\u2192            \n   184\u2192            # Register with router if needed\n   185\u2192            if self._enable_advanced:\n   186\u2192                capabilities = self._detect_plugin_capabilities(plugin)\n   187\u2192                self._router.register_plugin(plugin, capabilities)\n   188\u2192            \n   189\u2192            logger.info(f\"Successfully loaded {language} plugin\")\n   190\u2192            return plugin\n   191\u2192            \n   192\u2192        except ValueError as e:\n   193\u2192            logger.warning(f\"No plugin available for {language}: {e}\")\n   194\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   195\u2192            return None\n   196\u2192        except Exception as e:\n   197\u2192            logger.error(f\"Error loading plugin for {language}: {e}\")\n   198\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   199\u2192            return None\n   200\u2192    \n   201\u2192    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202\u2192        \"\"\"Ensure a plugin is loaded for the given file.\n   203\u2192        \n   204\u2192        Args:\n   205\u2192            path: File path\n   206\u2192            \n   207\u2192        Returns:\n   208\u2192            Plugin instance or None if not available\n   209\u2192        \"\"\"\n   210\u2192        # Get language from file extension\n   211\u2192        extension = path.suffix.lower()\n   212\u2192        language = get_language_by_extension(extension)\n   213\u2192        \n   214\u2192        if language:\n   215\u2192            return self._ensure_plugin_loaded(language)\n   216\u2192        \n   217\u2192        # Fallback: try all loaded plugins\n   218\u2192        for plugin in self._plugins:\n   219\u2192            if plugin.supports(path):\n   220\u2192                return plugin\n   221\u2192        \n   222\u2192        return None\n   223\u2192    \n   224\u2192    def _register_plugins_with_router(self):\n   225\u2192        \"\"\"Register plugins with the router and assign capabilities.\"\"\"\n   226\u2192        for plugin in self._plugins:\n   227\u2192            # Determine capabilities based on plugin type/language\n   228\u2192            capabilities = self._detect_plugin_capabilities(plugin)\n   229\u2192            self._router.register_plugin(plugin, capabilities)\n   230\u2192    \n   231\u2192    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232\u2192        \"\"\"Detect capabilities for a plugin based on its language and features.\"\"\"\n   233\u2192        capabilities = []\n   234\u2192        lang = getattr(plugin, 'lang', 'unknown')\n   235\u2192        \n   236\u2192        # Base capabilities all plugins have\n   237\u2192        capabilities.append(PluginCapability(\n   238\u2192            'syntax_analysis', '1.0', f'{lang} syntax analysis', \n   239\u2192            priority=70, metadata={'language': lang}\n   240\u2192        ))\n   241\u2192        \n   242\u2192        capabilities.append(PluginCapability(\n   243\u2192            'code_search', '1.0', f'{lang} code search', \n   244\u2192            priority=80, metadata={'language': lang}\n   245\u2192        ))\n   246\u2192        \n   247\u2192        # Check for semantic search capability\n   248\u2192        if hasattr(plugin, '_enable_semantic') and plugin._enable_semantic:\n   249\u2192            capabilities.append(PluginCapability(\n   250\u2192                'semantic_search', '1.0', f'{lang} semantic search', \n   251\u2192                priority=90, metadata={'language': lang}\n   252\u2192            ))\n   253\u2192        \n   254\u2192        # Language-specific capabilities\n   255\u2192        if lang == 'python':\n   256\u2192            capabilities.extend([\n   257\u2192                PluginCapability('refactoring', '1.0', 'Python refactoring support', 75),\n   258\u2192                PluginCapability('type_analysis', '1.0', 'Python type analysis', 85)\n   259\u2192            ])\n   260\u2192        elif lang in ['javascript', 'typescript']:\n   261\u2192            capabilities.extend([\n   262\u2192                PluginCapability('linting', '1.0', 'JavaScript/TypeScript linting', 85),\n   263\u2192                PluginCapability('bundling_analysis', '1.0', 'Module bundling analysis', 70),\n   264\u2192                PluginCapability('framework_support', '1.0', 'Framework-specific support', 75)\n   265\u2192            ])\n   266\u2192        elif lang in ['c', 'cpp']:\n   267\u2192            capabilities.extend([\n   268\u2192                PluginCapability('compilation_analysis', '1.0', 'Compilation analysis', 80),\n   269\u2192                PluginCapability('memory_analysis', '1.0', 'Memory usage analysis', 70),\n   270\u2192                PluginCapability('performance_profiling', '1.0', 'Performance profiling', 75)\n   271\u2192            ])\n   272\u2192        elif lang in ['go', 'rust']:\n   273\u2192            capabilities.extend([\n   274\u2192                PluginCapability('package_analysis', '1.0', f'{lang} package analysis', 80),\n   275\u2192                PluginCapability('concurrency_analysis', '1.0', f'{lang} concurrency analysis', 75)\n   276\u2192            ])\n   277\u2192        elif lang in ['java', 'kotlin', 'scala']:\n   278\u2192            capabilities.extend([\n   279\u2192                PluginCapability('jvm_analysis', '1.0', 'JVM bytecode analysis', 75),\n   280\u2192                PluginCapability('build_tool_integration', '1.0', 'Build tool integration', 70)\n   281\u2192            ])\n   282\u2192        \n   283\u2192        return capabilities\n   284\u2192    \n   285\u2192    @property\n   286\u2192    def plugins(self):\n   287\u2192        \"\"\"Get the dictionary of loaded plugins by language.\"\"\"\n   288\u2192        return self._by_lang\n   289\u2192    \n   290\u2192    @property\n   291\u2192    def supported_languages(self) -> List[str]:\n   292\u2192        \"\"\"Get list of all supported languages (loaded and available).\"\"\"\n   293\u2192        if self._use_factory:\n   294\u2192            return PluginFactory.get_supported_languages()\n   295\u2192        else:\n   296\u2192            return list(self._by_lang.keys())\n   297\u2192    \n   298\u2192    def _match_plugin(self, path: Path) -> IPlugin:\n   299\u2192        \"\"\"Match a plugin for the given file path.\"\"\"\n   300\u2192        # Ensure plugin is loaded if using lazy loading\n   301\u2192        if self._lazy_load and self._use_factory:\n   302\u2192            plugin = self._ensure_plugin_for_file(path)\n   303\u2192            if plugin:\n   304\u2192                return plugin\n   305\u2192        \n   306\u2192        # Use advanced routing if available\n   307\u2192        if self._enable_advanced and self._router:\n   308\u2192            route_result = self._router.get_best_plugin(path)\n   309\u2192            if route_result:\n   310\u2192                return route_result.plugin\n   311\u2192        \n   312\u2192        # Fallback to basic matching\n   313\u2192        for p in self._plugins:\n   314\u2192            if p.supports(path):\n   315\u2192                return p\n   316\u2192        \n   317\u2192        raise RuntimeError(f\"No plugin found for {path}\")\n   318\u2192    \n   319\u2192    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320\u2192        \"\"\"Get all plugins that can handle a file with confidence scores.\"\"\"\n   321\u2192        # Ensure plugin is loaded if using lazy loading\n   322\u2192        if self._lazy_load and self._use_factory:\n   323\u2192            self._ensure_plugin_for_file(path)\n   324\u2192        \n   325\u2192        if self._enable_advanced and self._router:\n   326\u2192            route_results = self._router.route_file(path)\n   327\u2192            return [(result.plugin, result.confidence) for result in route_results]\n   328\u2192        else:\n   329\u2192            # Basic fallback\n   330\u2192            matching_plugins = []\n   331\u2192            for plugin in self._plugins:\n   332\u2192                if plugin.supports(path):\n   333\u2192                    matching_plugins.append((plugin, 1.0))\n   334\u2192            return matching_plugins\n   335\u2192    \n   336\u2192    def lookup(self, symbol: str) -> SymbolDef | None:\n   337\u2192        \"\"\"Look up symbol definition across all plugins.\"\"\"\n   338\u2192        start_time = time.time()\n   339\u2192        \n   340\u2192        try:\n   341\u2192            # For symbol lookup, we may need to search across all languages\n   342\u2192            # Load all plugins if using lazy loading\n   343\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344\u2192                self._load_all_plugins()\n   345\u2192            \n   346\u2192            if self._enable_advanced and self._aggregator:\n   347\u2192                # Use advanced aggregation\n   348\u2192                definitions_by_plugin = {}\n   349\u2192                for plugin in self._plugins:\n   350\u2192                    try:\n   351\u2192                        definition = plugin.getDefinition(symbol)\n   352\u2192                        definitions_by_plugin[plugin] = definition\n   353\u2192                    except Exception as e:\n   354\u2192                        logger.warning(f\"Plugin {plugin.lang} failed to get definition for {symbol}: {e}\")\n   355\u2192                        definitions_by_plugin[plugin] = None\n   356\u2192                \n   357\u2192                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358\u2192                \n   359\u2192                self._operation_stats['lookups'] += 1\n   360\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   361\u2192                \n   362\u2192                return result\n   363\u2192            else:\n   364\u2192                # Fallback to basic lookup\n   365\u2192                for p in self._plugins:\n   366\u2192                    res = p.getDefinition(symbol)\n   367\u2192                    if res:\n   368\u2192                        self._operation_stats['lookups'] += 1\n   369\u2192                        self._operation_stats['total_time'] += time.time() - start_time\n   370\u2192                        return res\n   371\u2192                return None\n   372\u2192                \n   373\u2192        except Exception as e:\n   374\u2192            logger.error(f\"Error in symbol lookup for {symbol}: {e}\", exc_info=True)\n   375\u2192            return None\n   376\u2192    \n   377\u2192    def _is_document_query(self, query: str) -> bool:\n   378\u2192        \"\"\"Check if the query is looking for documentation.\n   379\u2192        \n   380\u2192        Args:\n   381\u2192            query: Search query string\n   382\u2192            \n   383\u2192        Returns:\n   384\u2192            True if this appears to be a documentation query\n   385\u2192        \"\"\"\n   386\u2192        query_lower = query.lower()\n   387\u2192        \n   388\u2192        # Check against document query patterns\n   389\u2192        for pattern in self._compiled_doc_patterns:\n   390\u2192            if pattern.search(query_lower):\n   391\u2192                return True\n   392\u2192        \n   393\u2192        # Check for question words at the beginning\n   394\u2192        question_starters = ['how', 'what', 'where', 'when', 'why', 'can', 'is', 'does', 'should']\n   395\u2192        first_word = query_lower.split()[0] if query_lower.split() else ''\n   396\u2192        if first_word in question_starters:\n   397\u2192            return True\n   398\u2192        \n   399\u2192        return False\n   400\u2192    \n   401\u2192    def _expand_document_query(self, query: str) -> List[str]:\n   402\u2192        \"\"\"Expand a document query with related terms for better search coverage.\n   403\u2192        \n   404\u2192        Args:\n   405\u2192            query: Original search query\n   406\u2192            \n   407\u2192        Returns:\n   408\u2192            List of expanded query variations\n   409\u2192        \"\"\"\n   410\u2192        expanded_queries = [query]  # Always include original\n   411\u2192        query_lower = query.lower()\n   412\u2192        \n   413\u2192        # Common expansions for documentation queries\n   414\u2192        expansions = {\n   415\u2192            'install': ['installation', 'setup', 'getting started', 'requirements'],\n   416\u2192            'config': ['configuration', 'configure', 'settings', 'options', 'parameters'],\n   417\u2192            'api': ['api documentation', 'api reference', 'endpoint', 'method'],\n   418\u2192            'how to': ['tutorial', 'guide', 'example', 'usage'],\n   419\u2192            'example': ['sample', 'snippet', 'demo', 'code example'],\n   420\u2192            'error': ['troubleshoot', 'debug', 'issue', 'problem', 'fix'],\n   421\u2192            'getting started': ['quickstart', 'tutorial', 'introduction', 'setup'],\n   422\u2192            'guide': ['tutorial', 'documentation', 'walkthrough', 'how to'],\n   423\u2192            'usage': ['how to use', 'example', 'api', 'reference']\n   424\u2192        }\n   425\u2192        \n   426\u2192        # Apply expansions\n   427\u2192        for term, related_terms in expansions.items():\n   428\u2192            if term in query_lower:\n   429\u2192                for related in related_terms:\n   430\u2192                    # Replace the term with related term\n   431\u2192                    expanded = query_lower.replace(term, related)\n   432\u2192                    if expanded != query_lower and expanded not in expanded_queries:\n   433\u2192                        expanded_queries.append(expanded)\n   434\u2192                \n   435\u2192                # Also add queries with additional terms\n   436\u2192                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437\u2192                    expanded = f\"{query} {related}\"\n   438\u2192                    if expanded not in expanded_queries:\n   439\u2192                        expanded_queries.append(expanded)\n   440\u2192        \n   441\u2192        # Add file-specific searches for common documentation files\n   442\u2192        if self._is_document_query(query):\n   443\u2192            # Extract the main topic from the query\n   444\u2192            topic_words = []\n   445\u2192            for word in query.lower().split():\n   446\u2192                if word not in ['how', 'to', 'the', 'a', 'an', 'is', 'are', 'what', 'where', 'when']:\n   447\u2192                    topic_words.append(word)\n   448\u2192            \n   449\u2192            if topic_words:\n   450\u2192                topic = ' '.join(topic_words[:2])  # Use first two topic words\n   451\u2192                expanded_queries.extend([\n   452\u2192                    f\"README {topic}\",\n   453\u2192                    f\"{topic} documentation\",\n   454\u2192                    f\"{topic} docs\",\n   455\u2192                    f\"{topic} guide\"\n   456\u2192                ])\n   457\u2192        \n   458\u2192        # Remove duplicates while preserving order\n   459\u2192        seen = set()\n   460\u2192        unique_queries = []\n   461\u2192        for q in expanded_queries:\n   462\u2192            if q not in seen:\n   463\u2192                seen.add(q)\n   464\u2192                unique_queries.append(q)\n   465\u2192        \n   466\u2192        return unique_queries[:10]  # Limit to 10 queries max\n   467\u2192    \n   468\u2192    def _is_documentation_file(self, file_path: str) -> bool:\n   469\u2192        \"\"\"Check if a file path is likely a documentation file.\n   470\u2192        \n   471\u2192        Args:\n   472\u2192            file_path: Path to check\n   473\u2192            \n   474\u2192        Returns:\n   475\u2192            True if this appears to be a documentation file\n   476\u2192        \"\"\"\n   477\u2192        path_lower = file_path.lower()\n   478\u2192        \n   479\u2192        for pattern in self._compiled_file_patterns:\n   480\u2192            if pattern.search(path_lower):\n   481\u2192                return True\n   482\u2192        \n   483\u2192        return False\n   484\u2192    \n   485\u2192    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486\u2192        \"\"\"Adjust ranking to prioritize documentation files for document queries.\n   487\u2192        \n   488\u2192        Args:\n   489\u2192            query: Original search query\n   490\u2192            results: List of aggregated results\n   491\u2192            \n   492\u2192        Returns:\n   493\u2192            Re-ranked results with documentation prioritized\n   494\u2192        \"\"\"\n   495\u2192        if not self._is_document_query(query):\n   496\u2192            return results\n   497\u2192        \n   498\u2192        # Separate documentation and code results\n   499\u2192        doc_results = []\n   500\u2192        code_results = []\n   501\u2192        \n   502\u2192        for result in results:\n   503\u2192            if self._is_documentation_file(result.primary_result.get('file', '')):\n   504\u2192                # Boost documentation files for document queries\n   505\u2192                result.rank_score *= 1.5\n   506\u2192                result.metadata['doc_boost'] = True\n   507\u2192                doc_results.append(result)\n   508\u2192            else:\n   509\u2192                code_results.append(result)\n   510\u2192        \n   511\u2192        # Sort each group by rank score\n   512\u2192        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513\u2192        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514\u2192        \n   515\u2192        # Combine with documentation files first\n   516\u2192        return doc_results + code_results\n   517\u2192    \n   518\u2192    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519\u2192        \"\"\"Search for code and documentation across all plugins.\"\"\"\n   520\u2192        start_time = time.time()\n   521\u2192        \n   522\u2192        try:\n   523\u2192            # For search, we may need to search across all languages\n   524\u2192            # Load all plugins if using lazy loading\n   525\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526\u2192                self._load_all_plugins()\n   527\u2192            \n   528\u2192            # Detect if this is a document query\n   529\u2192            is_doc_query = self._is_document_query(query)\n   530\u2192            \n   531\u2192            # Expand query if it's a document query\n   532\u2192            queries = [query]\n   533\u2192            if is_doc_query:\n   534\u2192                queries = self._expand_document_query(query)\n   535\u2192                logger.info(f\"Expanded document query '{query}' to {len(queries)} variations\")\n   536\u2192                # Force semantic search for natural language queries\n   537\u2192                semantic = True\n   538\u2192            \n   539\u2192            if self._enable_advanced and self._aggregator:\n   540\u2192                # Use advanced aggregation\n   541\u2192                all_results_by_plugin = {}\n   542\u2192                opts = {\"semantic\": semantic, \"limit\": limit * 2 if is_doc_query else limit}\n   543\u2192                \n   544\u2192                # Search with all query variations\n   545\u2192                for search_query in queries:\n   546\u2192                    for plugin in self._plugins:\n   547\u2192                        try:\n   548\u2192                            results = list(plugin.search(search_query, opts))\n   549\u2192                            if results:\n   550\u2192                                if plugin not in all_results_by_plugin:\n   551\u2192                                    all_results_by_plugin[plugin] = []\n   552\u2192                                all_results_by_plugin[plugin].extend(results)\n   553\u2192                        except Exception as e:\n   554\u2192                            logger.warning(f\"Plugin {plugin.lang} failed to search for {search_query}: {e}\")\n   555\u2192                \n   556\u2192                # Deduplicate results per plugin\n   557\u2192                for plugin, results in all_results_by_plugin.items():\n   558\u2192                    seen = set()\n   559\u2192                    unique_results = []\n   560\u2192                    for result in results:\n   561\u2192                        key = f\"{result['file']}:{result['line']}\"\n   562\u2192                        if key not in seen:\n   563\u2192                            seen.add(key)\n   564\u2192                            unique_results.append(result)\n   565\u2192                    all_results_by_plugin[plugin] = unique_results\n   566\u2192                \n   567\u2192                # Configure aggregator for document queries\n   568\u2192                if is_doc_query and self._enable_advanced:\n   569\u2192                    # Adjust ranking criteria for documentation\n   570\u2192                    doc_criteria = RankingCriteria(\n   571\u2192                        relevance_weight=0.5,      # Increase relevance weight\n   572\u2192                        confidence_weight=0.2,     # Reduce confidence weight\n   573\u2192                        frequency_weight=0.2,      # Keep frequency weight\n   574\u2192                        recency_weight=0.1,        # Keep recency weight\n   575\u2192                        prefer_exact_matches=False,  # Natural language doesn't need exact matches\n   576\u2192                        boost_multiple_sources=True,\n   577\u2192                        boost_common_extensions=True\n   578\u2192                    )\n   579\u2192                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580\u2192                \n   581\u2192                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582\u2192                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583\u2192                )\n   584\u2192                \n   585\u2192                # Adjust ranking for document queries\n   586\u2192                if is_doc_query:\n   587\u2192                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588\u2192                \n   589\u2192                # Apply final limit\n   590\u2192                if limit and len(aggregated_results) > limit:\n   591\u2192                    aggregated_results = aggregated_results[:limit]\n   592\u2192                \n   593\u2192                logger.debug(f\"Search aggregation stats: {stats.total_results} total, \"\n   594\u2192                           f\"{stats.unique_results} unique, {stats.plugins_used} plugins used, \"\n   595\u2192                           f\"document_query={is_doc_query}\")\n   596\u2192                \n   597\u2192                self._operation_stats['searches'] += 1\n   598\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   599\u2192                \n   600\u2192                # Yield primary results from aggregated results\n   601\u2192                for aggregated in aggregated_results:\n   602\u2192                    yield aggregated.primary_result\n   603\u2192            else:\n   604\u2192                # Fallback to basic search\n   605\u2192                # Detect if this is a document query\n   606\u2192                is_doc_query = self._is_document_query(query)\n   607\u2192                \n   608\u2192                # Expand query if it's a document query\n   609\u2192                queries = [query]\n   610\u2192                if is_doc_query:\n   611\u2192                    queries = self._expand_document_query(query)\n   612\u2192                    semantic = True  # Force semantic search for natural language\n   613\u2192                \n   614\u2192                opts = {\"semantic\": semantic, \"limit\": limit}\n   615\u2192                all_results = []\n   616\u2192                \n   617\u2192                # Search with all query variations\n   618\u2192                for search_query in queries:\n   619\u2192                    for p in self._plugins:\n   620\u2192                        try:\n   621\u2192                            for result in p.search(search_query, opts):\n   622\u2192                                all_results.append(result)\n   623\u2192                        except Exception as e:\n   624\u2192                            logger.warning(f\"Plugin {p.lang} failed to search for {search_query}: {e}\")\n   625\u2192                \n   626\u2192                # Deduplicate results\n   627\u2192                seen = set()\n   628\u2192                unique_results = []\n   629\u2192                for result in all_results:\n   630\u2192                    key = f\"{result['file']}:{result['line']}\"\n   631\u2192                    if key not in seen:\n   632\u2192                        seen.add(key)\n   633\u2192                        unique_results.append(result)\n   634\u2192                \n   635\u2192                # Sort by score if available\n   636\u2192                unique_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   637\u2192                \n   638\u2192                # Prioritize documentation files for document queries\n   639\u2192                if is_doc_query:\n   640\u2192                    doc_results = []\n   641\u2192                    code_results = []\n   642\u2192                    for result in unique_results:\n   643\u2192                        if self._is_documentation_file(result.get('file', '')):\n   644\u2192                            doc_results.append(result)\n   645\u2192                        else:\n   646\u2192                            code_results.append(result)\n   647\u2192                    unique_results = doc_results + code_results\n   648\u2192                \n   649\u2192                # Apply limit\n   650\u2192                count = 0\n   651\u2192                for result in unique_results:\n   652\u2192                    if limit and count >= limit:\n   653\u2192                        break\n   654\u2192                    yield result\n   655\u2192                    count += 1\n   656\u2192                \n   657\u2192                self._operation_stats['searches'] += 1\n   658\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   659\u2192                \n   660\u2192        except Exception as e:\n   661\u2192            logger.error(f\"Error in search for {query}: {e}\", exc_info=True)\n   662\u2192    \n   663\u2192    def index_file(self, path: Path) -> None:\n   664\u2192        \"\"\"Index a single file if it has changed.\"\"\"\n   665\u2192        try:\n   666\u2192            # Ensure path is absolute to avoid relative/absolute path issues\n   667\u2192            path = path.resolve()\n   668\u2192            \n   669\u2192            # Find the appropriate plugin\n   670\u2192            plugin = self._match_plugin(path)\n   671\u2192            \n   672\u2192            # Read file content\n   673\u2192            try:\n   674\u2192                content = path.read_text(encoding='utf-8')\n   675\u2192            except UnicodeDecodeError:\n   676\u2192                # Try with different encodings\n   677\u2192                try:\n   678\u2192                    content = path.read_text(encoding='latin-1')\n   679\u2192                except Exception as e:\n   680\u2192                    logger.error(f\"Failed to read {path}: {e}\")\n   681\u2192                    return\n   682\u2192            \n   683\u2192            # Check if we need to re-index (simplified for now)\n   684\u2192            # TODO: Implement proper caching logic\n   685\u2192            \n   686\u2192            # Index the file\n   687\u2192            start_time = time.time()\n   688\u2192            logger.info(f\"Indexing {path} with {plugin.lang} plugin\")\n   689\u2192            shard = plugin.indexFile(path, content)\n   690\u2192            \n   691\u2192            # Record performance if advanced features enabled\n   692\u2192            if self._enable_advanced and self._router:\n   693\u2192                execution_time = time.time() - start_time\n   694\u2192                self._router.record_performance(plugin, execution_time)\n   695\u2192            \n   696\u2192            self._operation_stats['indexings'] += 1\n   697\u2192            self._operation_stats['total_time'] += time.time() - start_time\n   698\u2192            \n   699\u2192            logger.info(f\"Successfully indexed {path}: {len(shard.get('symbols', []))} symbols found\")\n   700\u2192            \n   701\u2192        except RuntimeError as e:\n   702\u2192            # No plugin found for this file type\n   703\u2192            logger.debug(f\"No plugin for {path}: {e}\")\n   704\u2192        except Exception as e:\n   705\u2192            logger.error(f\"Error indexing {path}: {e}\", exc_info=True)\n   706\u2192    \n   707\u2192    def get_statistics(self) -> dict:\n   708\u2192        \"\"\"Get comprehensive statistics across all plugins and components.\"\"\"\n   709\u2192        stats = {\n   710\u2192            \"total_plugins\": len(self._plugins),\n   711\u2192            \"loaded_languages\": sorted(list(self._loaded_languages)),\n   712\u2192            \"supported_languages\": len(self.supported_languages),\n   713\u2192            \"operations\": self._operation_stats.copy()\n   714\u2192        }\n   715\u2192        \n   716\u2192        # Add language breakdown\n   717\u2192        stats[\"by_language\"] = {}\n   718\u2192        for lang, plugin in self._by_lang.items():\n   719\u2192            plugin_info = {\n   720\u2192                \"loaded\": True,\n   721\u2192                \"class\": plugin.__class__.__name__\n   722\u2192            }\n   723\u2192            if hasattr(plugin, 'get_indexed_count'):\n   724\u2192                plugin_info[\"indexed_files\"] = plugin.get_indexed_count()\n   725\u2192            stats[\"by_language\"][lang] = plugin_info\n   726\u2192        \n   727\u2192        return stats\n   728\u2192    \n   729\u2192    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730\u2192        \"\"\"\n   731\u2192        Index all files in a directory, respecting ignore patterns.\n   732\u2192        \n   733\u2192        Args:\n   734\u2192            directory: Directory to index\n   735\u2192            recursive: Whether to index subdirectories\n   736\u2192            \n   737\u2192        Returns:\n   738\u2192            Statistics about indexed files\n   739\u2192        \"\"\"\n   740\u2192        logger.info(f\"Indexing directory: {directory} (recursive={recursive})\")\n   741\u2192        \n   742\u2192        # Note: We don't use ignore patterns during indexing\n   743\u2192        # ALL files are indexed for local search capability\n   744\u2192        # Filtering happens only during export/sharing\n   745\u2192        \n   746\u2192        # Get all supported extensions\n   747\u2192        supported_extensions = get_all_extensions()\n   748\u2192        \n   749\u2192        stats = {\n   750\u2192            \"total_files\": 0,\n   751\u2192            \"indexed_files\": 0,\n   752\u2192            \"ignored_files\": 0,\n   753\u2192            \"failed_files\": 0,\n   754\u2192            \"by_language\": {}\n   755\u2192        }\n   756\u2192        \n   757\u2192        # Walk directory\n   758\u2192        if recursive:\n   759\u2192            file_iterator = directory.rglob(\"*\")\n   760\u2192        else:\n   761\u2192            file_iterator = directory.glob(\"*\")\n   762\u2192            \n   763\u2192        for path in file_iterator:\n   764\u2192            if not path.is_file():\n   765\u2192                continue\n   766\u2192                \n   767\u2192            stats[\"total_files\"] += 1\n   768\u2192            \n   769\u2192            # NOTE: We index ALL files locally, including gitignored ones\n   770\u2192            # Filtering happens only during export/sharing\n   771\u2192            # This allows local search of .env, secrets, etc.\n   772\u2192                \n   773\u2192            # Try to find a plugin that supports this file\n   774\u2192            # This allows us to index ALL files, including .env, .key, etc.\n   775\u2192            try:\n   776\u2192                # First try to match by extension\n   777\u2192                if path.suffix in supported_extensions:\n   778\u2192                    self.index_file(path)\n   779\u2192                    stats[\"indexed_files\"] += 1\n   780\u2192                # For files without recognized extensions, try each plugin's supports() method\n   781\u2192                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782\u2192                else:\n   783\u2192                    matched = False\n   784\u2192                    for plugin in self._plugins:\n   785\u2192                        if plugin.supports(path):\n   786\u2192                            self.index_file(path)\n   787\u2192                            stats[\"indexed_files\"] += 1\n   788\u2192                            matched = True\n   789\u2192                            break\n   790\u2192                    \n   791\u2192                    # If no plugin matched but we want to index everything,\n   792\u2192                    # we could add a fallback here to index as plaintext\n   793\u2192                    # For now, we'll skip unmatched files\n   794\u2192                    if not matched:\n   795\u2192                        logger.debug(f\"No plugin found for {path}\")\n   796\u2192                \n   797\u2192                # Track by language\n   798\u2192                language = get_language_by_extension(path.suffix)\n   799\u2192                if language:\n   800\u2192                    stats[\"by_language\"][language] = stats[\"by_language\"].get(language, 0) + 1\n   801\u2192                    \n   802\u2192            except Exception as e:\n   803\u2192                logger.error(f\"Failed to index {path}: {e}\")\n   804\u2192                stats[\"failed_files\"] += 1\n   805\u2192                \n   806\u2192        logger.info(f\"Directory indexing complete: {stats['indexed_files']} indexed, \"\n   807\u2192                   f\"{stats['ignored_files']} ignored, {stats['failed_files']} failed\")\n   808\u2192        \n   809\u2192        return stats\n   810\u2192    \n   811\u2192    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812\u2192        \"\"\"Search specifically across documentation files.\n   813\u2192        \n   814\u2192        Args:\n   815\u2192            topic: Topic to search for (e.g., \"installation\", \"configuration\")\n   816\u2192            doc_types: Optional list of document types to search (e.g., [\"readme\", \"guide\", \"api\"])\n   817\u2192            limit: Maximum number of results\n   818\u2192            \n   819\u2192        Returns:\n   820\u2192            Search results from documentation files\n   821\u2192        \"\"\"\n   822\u2192        # Default document types if not specified\n   823\u2192        if doc_types is None:\n   824\u2192            doc_types = [\"readme\", \"documentation\", \"guide\", \"tutorial\", \"api\", \"changelog\", \"contributing\"]\n   825\u2192        \n   826\u2192        # Build search queries for different document types\n   827\u2192        queries = []\n   828\u2192        for doc_type in doc_types:\n   829\u2192            queries.extend([\n   830\u2192                f\"{doc_type} {topic}\",\n   831\u2192                f\"{topic} {doc_type}\",\n   832\u2192                f\"{topic} in {doc_type}\"\n   833\u2192            ])\n   834\u2192        \n   835\u2192        # Also search for the topic in common doc filenames\n   836\u2192        queries.extend([\n   837\u2192            f\"README {topic}\",\n   838\u2192            f\"CONTRIBUTING {topic}\",\n   839\u2192            f\"docs {topic}\",\n   840\u2192            f\"documentation {topic}\"\n   841\u2192        ])\n   842\u2192        \n   843\u2192        # Deduplicate queries\n   844\u2192        queries = list(dict.fromkeys(queries))\n   845\u2192        \n   846\u2192        logger.info(f\"Cross-document search for '{topic}' with {len(queries)} query variations\")\n   847\u2192        \n   848\u2192        # Use the enhanced search with document-specific handling\n   849\u2192        all_results = []\n   850\u2192        seen = set()\n   851\u2192        \n   852\u2192        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853\u2192            for result in self.search(query, semantic=True, limit=limit):\n   854\u2192                # Only include documentation files\n   855\u2192                if self._is_documentation_file(result.get('file', '')):\n   856\u2192                    key = f\"{result['file']}:{result['line']}\"\n   857\u2192                    if key not in seen:\n   858\u2192                        seen.add(key)\n   859\u2192                        all_results.append(result)\n   860\u2192        \n   861\u2192        # Sort by relevance (score) and return top results\n   862\u2192        all_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   863\u2192        \n   864\u2192        count = 0\n   865\u2192        for result in all_results:\n   866\u2192            if count >= limit:\n   867\u2192                break\n   868\u2192            yield result\n   869\u2192            count += 1\n   870\u2192    \n   871\u2192    def health_check(self) -> Dict[str, Any]:\n   872\u2192        \"\"\"Perform a health check on all components.\"\"\"\n   873\u2192        health = {\n   874\u2192            'status': 'healthy',\n   875\u2192            'components': {\n   876\u2192                'dispatcher': {\n   877\u2192                    'status': 'healthy',\n   878\u2192                    'plugins_loaded': len(self._plugins),\n   879\u2192                    'languages_supported': len(self.supported_languages),\n   880\u2192                    'factory_enabled': self._use_factory,\n   881\u2192                    'lazy_loading': self._lazy_load\n   882\u2192                }\n   883\u2192            },\n   884\u2192            'plugins': {},\n   885\u2192            'errors': []\n   886\u2192        }\n   887\u2192        \n   888\u2192        # Check plugin health\n   889\u2192        for lang, plugin in self._by_lang.items():\n   890\u2192            try:\n   891\u2192                plugin_health = {\n   892\u2192                    'status': 'healthy',\n   893\u2192                    'class': plugin.__class__.__name__,\n   894\u2192                    'semantic_enabled': getattr(plugin, '_enable_semantic', False)\n   895\u2192                }\n   896\u2192                if hasattr(plugin, 'get_indexed_count'):\n   897\u2192                    plugin_health['indexed_files'] = plugin.get_indexed_count()\n   898\u2192            except Exception as e:\n   899\u2192                plugin_health = {\n   900\u2192                    'status': 'error',\n   901\u2192                    'error': str(e)\n   902\u2192                }\n   903\u2192                health['errors'].append(f\"Plugin {lang}: {str(e)}\")\n   904\u2192            \n   905\u2192            health['plugins'][lang] = plugin_health\n   906\u2192        \n   907\u2192        # Determine overall health\n   908\u2192        if len(health['errors']) > 0:\n   909\u2192            health['status'] = 'degraded' if len(health['errors']) < 3 else 'unhealthy'\n   910\u2192        \n   911\u2192        return health\n   912\u2192    \n   913\u2192    def remove_file(self, path: Union[Path, str]) -> None:\n   914\u2192        \"\"\"Remove a file from all indexes.\n   915\u2192        \n   916\u2192        Args:\n   917\u2192            path: File path to remove\n   918\u2192        \"\"\"\n   919\u2192        path = Path(path).resolve()\n   920\u2192        logger.info(f\"Removing file from index: {path}\")\n   921\u2192        \n   922\u2192        try:\n   923\u2192            # Remove from SQLite if available\n   924\u2192            if self._sqlite_store:\n   925\u2192                from ..core.path_resolver import PathResolver\n   926\u2192                path_resolver = PathResolver()\n   927\u2192                try:\n   928\u2192                    relative_path = path_resolver.normalize_path(path)\n   929\u2192                    # Get repository ID - for now assume 1\n   930\u2192                    # TODO: Properly detect repository\n   931\u2192                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932\u2192                except Exception as e:\n   933\u2192                    logger.error(f\"Error removing from SQLite: {e}\")\n   934\u2192            \n   935\u2192            # Remove from semantic index if available\n   936\u2192            try:\n   937\u2192                plugin = self._match_plugin(path)\n   938\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   939\u2192                    plugin._indexer.remove_file(path)\n   940\u2192                    logger.info(f\"Removed from semantic index: {path}\")\n   941\u2192            except Exception as e:\n   942\u2192                logger.warning(f\"Error removing from semantic index: {e}\")\n   943\u2192            \n   944\u2192            # Update statistics\n   945\u2192            self._operation_stats['deletions'] = self._operation_stats.get('deletions', 0) + 1\n   946\u2192            \n   947\u2192        except Exception as e:\n   948\u2192            logger.error(f\"Error removing file {path}: {e}\", exc_info=True)\n   949\u2192    \n   950\u2192    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951\u2192        \"\"\"Move a file in all indexes.\n   952\u2192        \n   953\u2192        Args:\n   954\u2192            old_path: Original file path\n   955\u2192            new_path: New file path\n   956\u2192            content_hash: Optional content hash to verify unchanged content\n   957\u2192        \"\"\"\n   958\u2192        old_path = Path(old_path).resolve()\n   959\u2192        new_path = Path(new_path).resolve()\n   960\u2192        logger.info(f\"Moving file in index: {old_path} -> {new_path}\")\n   961\u2192        \n   962\u2192        try:\n   963\u2192            # Move in SQLite if available\n   964\u2192            if self._sqlite_store:\n   965\u2192                from ..core.path_resolver import PathResolver\n   966\u2192                path_resolver = PathResolver()\n   967\u2192                try:\n   968\u2192                    old_relative = path_resolver.normalize_path(old_path)\n   969\u2192                    new_relative = path_resolver.normalize_path(new_path)\n   970\u2192                    # Get repository ID - for now assume 1\n   971\u2192                    # TODO: Properly detect repository\n   972\u2192                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973\u2192                except Exception as e:\n   974\u2192                    logger.error(f\"Error moving in SQLite: {e}\")\n   975\u2192            \n   976\u2192            # Move in semantic index if available\n   977\u2192            try:\n   978\u2192                plugin = self._match_plugin(new_path)\n   979\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   980\u2192                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981\u2192                    logger.info(f\"Moved in semantic index: {old_path} -> {new_path}\")\n   982\u2192            except Exception as e:\n   983\u2192                logger.warning(f\"Error moving in semantic index: {e}\")\n   984\u2192            \n   985\u2192            # Update statistics\n   986\u2192            self._operation_stats['moves'] = self._operation_stats.get('moves', 0) + 1\n   987\u2192            \n   988\u2192        except Exception as e:\n   989\u2192            logger.error(f\"Error moving file {old_path} -> {new_path}: {e}\", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 14367,
    "cache_creation_tokens": 13323,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "1a4d62a8-4e27-4462-80ca-11c6a0771da9",
    "test_date": "2025-06-26T07:39:12.070Z",
    "query_id": "transcript_1a4d62a8-4e27-4462-80ca-11c6a0771da9",
    "query_text": "Find the BM25Indexer class definition",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13916,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "1a4d62a8-4e27-4462-80ca-11c6a0771da9",
    "test_date": "2025-06-26T07:39:14.927Z",
    "query_id": "transcript_1a4d62a8-4e27-4462-80ca-11c6a0771da9",
    "query_text": [
      {
        "tool_use_id": "toolu_014Ni45aGbsjdNWvSS15vpYS",
        "type": "tool_result",
        "content": "Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/bm25_indexer.py"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 24,
    "output_tokens_total": 24,
    "cache_read_tokens": 14142,
    "cache_creation_tokens": 0,
    "tokens_per_result": 24,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "1a4d62a8-4e27-4462-80ca-11c6a0771da9",
    "test_date": "2025-06-26T07:39:20.291Z",
    "query_id": "transcript_1a4d62a8-4e27-4462-80ca-11c6a0771da9",
    "query_text": [
      {
        "tool_use_id": "toolu_012YEXU3G3P6HKmsmox7skdi",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"\n     2\u2192BM25 Indexer using SQLite FTS5 for full-text search.\n     3\u2192\n     4\u2192This module provides BM25-based full-text search indexing using SQLite's FTS5 \n     5\u2192extension, which includes built-in BM25 ranking algorithms.\n     6\u2192\"\"\"\n     7\u2192\n     8\u2192import sqlite3\n     9\u2192import logging\n    10\u2192import hashlib\n    11\u2192import json\n    12\u2192from typing import List, Dict, Any, Optional, Tuple\n    13\u2192from datetime import datetime\n    14\u2192from pathlib import Path\n    15\u2192from contextlib import contextmanager\n    16\u2192\n    17\u2192from ..storage.sqlite_store import SQLiteStore\n    18\u2192# Interface definition inline for now\n    19\u2192from abc import ABC, abstractmethod\n    20\u2192\n    21\u2192logger = logging.getLogger(__name__)\n    22\u2192\n    23\u2192# Define IIndexer interface inline\n    24\u2192class IIndexer(ABC):\n    25\u2192    @abstractmethod\n    26\u2192    def index_documents(self, documents: List[Dict[str, Any]]) -> bool:\n    27\u2192        pass\n    28\u2192    \n    29\u2192    @abstractmethod\n    30\u2192    def search(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:\n    31\u2192        pass\n    32\u2192\n    33\u2192\n    34\u2192class BM25Indexer(IIndexer):\n    35\u2192    \"\"\"\n    36\u2192    BM25-based full-text search indexer using SQLite FTS5.\n    37\u2192    \n    38\u2192    This indexer provides:\n    39\u2192    - Efficient full-text search using inverted indexes\n    40\u2192    - Built-in BM25 ranking algorithm\n    41\u2192    - Support for phrase searches, prefix searches, and boolean operators\n    42\u2192    - Integration with existing SQLite storage\n    43\u2192    \"\"\"\n    44\u2192    \n    45\u2192    def __init__(self, storage: SQLiteStore, table_name: str = \"bm25_content\"):\n    46\u2192        \"\"\"\n    47\u2192        Initialize the BM25 indexer.\n    48\u2192        \n    49\u2192        Args:\n    50\u2192            storage: SQLite storage instance\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 14142,
    "cache_creation_tokens": 727,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "a712ff2a-98c3-41fc-9a1c-0469a20a4330",
    "test_date": "2025-06-26T07:33:48.076Z",
    "query_id": "transcript_a712ff2a-98c3-41fc-9a1c-0469a20a4330",
    "query_text": "Find all functions that use asyncio",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13172,
    "cache_creation_tokens": 741,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "927dc689-c758-442e-be8e-cfd1ea400fd3",
    "test_date": "2025-06-26T07:39:25.623Z",
    "query_id": "transcript_927dc689-c758-442e-be8e-cfd1ea400fd3",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13920,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "927dc689-c758-442e-be8e-cfd1ea400fd3",
    "test_date": "2025-06-26T07:39:28.848Z",
    "query_id": "transcript_927dc689-c758-442e-be8e-cfd1ea400fd3",
    "query_text": [
      {
        "tool_use_id": "toolu_01X2tS5FLZrPy5NEvFYaTCe7",
        "type": "tool_result",
        "content": "Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/level4/path_management.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/path_management_architecture.md"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 32,
    "output_tokens_total": 32,
    "cache_read_tokens": 14367,
    "cache_creation_tokens": 0,
    "tokens_per_result": 32,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "927dc689-c758-442e-be8e-cfd1ea400fd3",
    "test_date": "2025-06-26T07:39:32.545Z",
    "query_id": "transcript_927dc689-c758-442e-be8e-cfd1ea400fd3",
    "query_text": [
      {
        "tool_use_id": "toolu_013j8zvX8FzhHhBx9BfQJuoK",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"Enhanced dispatcher with dynamic plugin loading via PluginFactory.\"\"\"\n     2\u2192from pathlib import Path\n     3\u2192from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4\u2192import logging\n     5\u2192import hashlib\n     6\u2192import time\n     7\u2192import re\n     8\u2192from datetime import datetime\n     9\u2192import os\n    10\u2192\n    11\u2192from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12\u2192from ..plugins.plugin_factory import PluginFactory\n    13\u2192from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14\u2192from ..storage.sqlite_store import SQLiteStore\n    15\u2192from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16\u2192from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17\u2192# Note: We've removed ignore pattern checks to allow indexing ALL files\n    18\u2192# Filtering happens only during export via SecureIndexExporter\n    19\u2192# from ..core.ignore_patterns import get_ignore_manager\n    20\u2192\n    21\u2192logger = logging.getLogger(__name__)\n    22\u2192\n    23\u2192\n    24\u2192class EnhancedDispatcher:\n    25\u2192    \"\"\"Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.\"\"\"\n    26\u2192    \n    27\u2192    # Document query patterns - common documentation search terms\n    28\u2192    DOCUMENT_QUERY_PATTERNS = [\n    29\u2192        r'\\b(how\\s+to|howto)\\b',\n    30\u2192        r'\\b(getting\\s+started|get\\s+started)\\b',\n    31\u2192        r'\\b(installation|install|setup)\\b',\n    32\u2192        r'\\b(configuration|configure|config)\\b',\n    33\u2192        r'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b',\n    34\u2192        r'\\b(tutorial|guide|walkthrough)\\b',\n    35\u2192        r'\\b(example|sample|snippet)\\b',\n    36\u2192        r'\\b(readme|documentation|docs)\\b',\n    37\u2192        r'\\b(usage|use\\s+case|using)\\b',\n    38\u2192        r'\\b(reference|manual)\\b',\n    39\u2192        r'\\b(faq|frequently\\s+asked)\\b',\n    40\u2192        r'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b',\n    41\u2192        r'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b',\n    42\u2192        r'\\b(architecture|design|overview)\\b',\n    43\u2192        r'\\b(changelog|release\\s+notes|migration)\\b'\n    44\u2192    ]\n    45\u2192    \n    46\u2192    # Documentation file patterns\n    47\u2192    DOCUMENTATION_FILE_PATTERNS = [\n    48\u2192        r'readme(\\.\\w+)?$',\n    49\u2192        r'changelog(\\.\\w+)?$',\n    50\u2192        r'contributing(\\.\\w+)?$',\n    51\u2192        r'license(\\.\\w+)?$',\n    52\u2192        r'install(\\.\\w+)?$',\n    53\u2192        r'setup(\\.\\w+)?$',\n    54\u2192        r'guide(\\.\\w+)?$',\n    55\u2192        r'tutorial(\\.\\w+)?$',\n    56\u2192        r'\\.md$',\n    57\u2192        r'\\.rst$',\n    58\u2192        r'\\.txt$',\n    59\u2192        r'docs?/',\n    60\u2192        r'documentation/'\n    61\u2192    ]\n    62\u2192    \n    63\u2192    def __init__(self, \n    64\u2192                 plugins: Optional[List[IPlugin]] = None,\n    65\u2192                 sqlite_store: Optional[SQLiteStore] = None,\n    66\u2192                 enable_advanced_features: bool = True,\n    67\u2192                 use_plugin_factory: bool = True,\n    68\u2192                 lazy_load: bool = True,\n    69\u2192                 semantic_search_enabled: bool = True):\n    70\u2192        \"\"\"Initialize the enhanced dispatcher.\n    71\u2192        \n    72\u2192        Args:\n    73\u2192            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74\u2192            sqlite_store: SQLite store for plugin persistence\n    75\u2192            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76\u2192            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77\u2192            lazy_load: Whether to lazy-load plugins on demand\n    78\u2192            semantic_search_enabled: Whether to enable semantic search in plugins\n    79\u2192        \"\"\"\n    80\u2192        self._sqlite_store = sqlite_store\n    81\u2192        self._enable_advanced = enable_advanced_features\n    82\u2192        self._use_factory = use_plugin_factory\n    83\u2192        self._lazy_load = lazy_load\n    84\u2192        self._semantic_enabled = semantic_search_enabled\n    85\u2192        \n    86\u2192        # Plugin storage\n    87\u2192        self._plugins: List[IPlugin] = []\n    88\u2192        self._by_lang: Dict[str, IPlugin] = {}\n    89\u2192        self._loaded_languages: set[str] = set()\n    90\u2192        \n    91\u2192        # Cache for file hashes to avoid re-indexing unchanged files\n    92\u2192        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93\u2192        \n    94\u2192        # Advanced components\n    95\u2192        if self._enable_advanced:\n    96\u2192            self._file_matcher = FileTypeMatcher()\n    97\u2192            self._router = PluginRouter(self._file_matcher)\n    98\u2192            self._aggregator = ResultAggregator()\n    99\u2192        \n   100\u2192        # Performance tracking\n   101\u2192        self._operation_stats = {\n   102\u2192            'searches': 0,\n   103\u2192            'lookups': 0,\n   104\u2192            'indexings': 0,\n   105\u2192            'total_time': 0.0,\n   106\u2192            'plugins_loaded': 0\n   107\u2192        }\n   108\u2192        \n   109\u2192        # Initialize plugins\n   110\u2192        if plugins:\n   111\u2192            # Use provided plugins (backward compatibility)\n   112\u2192            self._plugins = plugins\n   113\u2192            self._by_lang = {p.lang: p for p in plugins}\n   114\u2192            for plugin in plugins:\n   115\u2192                self._loaded_languages.add(getattr(plugin, 'lang', 'unknown'))\n   116\u2192            if self._enable_advanced:\n   117\u2192                self._register_plugins_with_router()\n   118\u2192        elif use_plugin_factory and not lazy_load:\n   119\u2192            # Load all plugins immediately\n   120\u2192            self._load_all_plugins()\n   121\u2192        # If lazy_load is True, plugins will be loaded on demand\n   122\u2192        \n   123\u2192        # Compile document query patterns for performance\n   124\u2192        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125\u2192        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126\u2192        \n   127\u2192        logger.info(f\"Enhanced dispatcher initialized with {len(self._plugins)} plugins\")\n   128\u2192    \n   129\u2192    def _load_all_plugins(self):\n   130\u2192        \"\"\"Load all available plugins using PluginFactory.\"\"\"\n   131\u2192        logger.info(\"Loading all available plugins...\")\n   132\u2192        \n   133\u2192        all_plugins = PluginFactory.create_all_plugins(\n   134\u2192            sqlite_store=self._sqlite_store,\n   135\u2192            enable_semantic=self._semantic_enabled\n   136\u2192        )\n   137\u2192        \n   138\u2192        for lang, plugin in all_plugins.items():\n   139\u2192            self._plugins.append(plugin)\n   140\u2192            self._by_lang[lang] = plugin\n   141\u2192            self._loaded_languages.add(lang)\n   142\u2192            self._operation_stats['plugins_loaded'] += 1\n   143\u2192        \n   144\u2192        if self._enable_advanced:\n   145\u2192            self._register_plugins_with_router()\n   146\u2192        \n   147\u2192        logger.info(f\"Loaded {len(all_plugins)} plugins: {', '.join(sorted(all_plugins.keys()))}\")\n   148\u2192    \n   149\u2192    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150\u2192        \"\"\"Ensure a plugin for the given language is loaded.\n   151\u2192        \n   152\u2192        Args:\n   153\u2192            language: Language code (e.g., 'python', 'go')\n   154\u2192            \n   155\u2192        Returns:\n   156\u2192            Plugin instance or None if not available\n   157\u2192        \"\"\"\n   158\u2192        # Normalize language\n   159\u2192        language = language.lower().replace('-', '_')\n   160\u2192        \n   161\u2192        # Check if already loaded\n   162\u2192        if language in self._by_lang:\n   163\u2192            return self._by_lang[language]\n   164\u2192        \n   165\u2192        # If not using factory or already tried to load, return None\n   166\u2192        if not self._use_factory or language in self._loaded_languages:\n   167\u2192            return None\n   168\u2192        \n   169\u2192        # Try to load the plugin\n   170\u2192        try:\n   171\u2192            logger.info(f\"Lazy loading plugin for {language}\")\n   172\u2192            plugin = PluginFactory.create_plugin(\n   173\u2192                language,\n   174\u2192                sqlite_store=self._sqlite_store,\n   175\u2192                enable_semantic=self._semantic_enabled\n   176\u2192            )\n   177\u2192            \n   178\u2192            # Add to collections\n   179\u2192            self._plugins.append(plugin)\n   180\u2192            self._by_lang[language] = plugin\n   181\u2192            self._loaded_languages.add(language)\n   182\u2192            self._operation_stats['plugins_loaded'] += 1\n   183\u2192            \n   184\u2192            # Register with router if needed\n   185\u2192            if self._enable_advanced:\n   186\u2192                capabilities = self._detect_plugin_capabilities(plugin)\n   187\u2192                self._router.register_plugin(plugin, capabilities)\n   188\u2192            \n   189\u2192            logger.info(f\"Successfully loaded {language} plugin\")\n   190\u2192            return plugin\n   191\u2192            \n   192\u2192        except ValueError as e:\n   193\u2192            logger.warning(f\"No plugin available for {language}: {e}\")\n   194\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   195\u2192            return None\n   196\u2192        except Exception as e:\n   197\u2192            logger.error(f\"Error loading plugin for {language}: {e}\")\n   198\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   199\u2192            return None\n   200\u2192    \n   201\u2192    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202\u2192        \"\"\"Ensure a plugin is loaded for the given file.\n   203\u2192        \n   204\u2192        Args:\n   205\u2192            path: File path\n   206\u2192            \n   207\u2192        Returns:\n   208\u2192            Plugin instance or None if not available\n   209\u2192        \"\"\"\n   210\u2192        # Get language from file extension\n   211\u2192        extension = path.suffix.lower()\n   212\u2192        language = get_language_by_extension(extension)\n   213\u2192        \n   214\u2192        if language:\n   215\u2192            return self._ensure_plugin_loaded(language)\n   216\u2192        \n   217\u2192        # Fallback: try all loaded plugins\n   218\u2192        for plugin in self._plugins:\n   219\u2192            if plugin.supports(path):\n   220\u2192                return plugin\n   221\u2192        \n   222\u2192        return None\n   223\u2192    \n   224\u2192    def _register_plugins_with_router(self):\n   225\u2192        \"\"\"Register plugins with the router and assign capabilities.\"\"\"\n   226\u2192        for plugin in self._plugins:\n   227\u2192            # Determine capabilities based on plugin type/language\n   228\u2192            capabilities = self._detect_plugin_capabilities(plugin)\n   229\u2192            self._router.register_plugin(plugin, capabilities)\n   230\u2192    \n   231\u2192    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232\u2192        \"\"\"Detect capabilities for a plugin based on its language and features.\"\"\"\n   233\u2192        capabilities = []\n   234\u2192        lang = getattr(plugin, 'lang', 'unknown')\n   235\u2192        \n   236\u2192        # Base capabilities all plugins have\n   237\u2192        capabilities.append(PluginCapability(\n   238\u2192            'syntax_analysis', '1.0', f'{lang} syntax analysis', \n   239\u2192            priority=70, metadata={'language': lang}\n   240\u2192        ))\n   241\u2192        \n   242\u2192        capabilities.append(PluginCapability(\n   243\u2192            'code_search', '1.0', f'{lang} code search', \n   244\u2192            priority=80, metadata={'language': lang}\n   245\u2192        ))\n   246\u2192        \n   247\u2192        # Check for semantic search capability\n   248\u2192        if hasattr(plugin, '_enable_semantic') and plugin._enable_semantic:\n   249\u2192            capabilities.append(PluginCapability(\n   250\u2192                'semantic_search', '1.0', f'{lang} semantic search', \n   251\u2192                priority=90, metadata={'language': lang}\n   252\u2192            ))\n   253\u2192        \n   254\u2192        # Language-specific capabilities\n   255\u2192        if lang == 'python':\n   256\u2192            capabilities.extend([\n   257\u2192                PluginCapability('refactoring', '1.0', 'Python refactoring support', 75),\n   258\u2192                PluginCapability('type_analysis', '1.0', 'Python type analysis', 85)\n   259\u2192            ])\n   260\u2192        elif lang in ['javascript', 'typescript']:\n   261\u2192            capabilities.extend([\n   262\u2192                PluginCapability('linting', '1.0', 'JavaScript/TypeScript linting', 85),\n   263\u2192                PluginCapability('bundling_analysis', '1.0', 'Module bundling analysis', 70),\n   264\u2192                PluginCapability('framework_support', '1.0', 'Framework-specific support', 75)\n   265\u2192            ])\n   266\u2192        elif lang in ['c', 'cpp']:\n   267\u2192            capabilities.extend([\n   268\u2192                PluginCapability('compilation_analysis', '1.0', 'Compilation analysis', 80),\n   269\u2192                PluginCapability('memory_analysis', '1.0', 'Memory usage analysis', 70),\n   270\u2192                PluginCapability('performance_profiling', '1.0', 'Performance profiling', 75)\n   271\u2192            ])\n   272\u2192        elif lang in ['go', 'rust']:\n   273\u2192            capabilities.extend([\n   274\u2192                PluginCapability('package_analysis', '1.0', f'{lang} package analysis', 80),\n   275\u2192                PluginCapability('concurrency_analysis', '1.0', f'{lang} concurrency analysis', 75)\n   276\u2192            ])\n   277\u2192        elif lang in ['java', 'kotlin', 'scala']:\n   278\u2192            capabilities.extend([\n   279\u2192                PluginCapability('jvm_analysis', '1.0', 'JVM bytecode analysis', 75),\n   280\u2192                PluginCapability('build_tool_integration', '1.0', 'Build tool integration', 70)\n   281\u2192            ])\n   282\u2192        \n   283\u2192        return capabilities\n   284\u2192    \n   285\u2192    @property\n   286\u2192    def plugins(self):\n   287\u2192        \"\"\"Get the dictionary of loaded plugins by language.\"\"\"\n   288\u2192        return self._by_lang\n   289\u2192    \n   290\u2192    @property\n   291\u2192    def supported_languages(self) -> List[str]:\n   292\u2192        \"\"\"Get list of all supported languages (loaded and available).\"\"\"\n   293\u2192        if self._use_factory:\n   294\u2192            return PluginFactory.get_supported_languages()\n   295\u2192        else:\n   296\u2192            return list(self._by_lang.keys())\n   297\u2192    \n   298\u2192    def _match_plugin(self, path: Path) -> IPlugin:\n   299\u2192        \"\"\"Match a plugin for the given file path.\"\"\"\n   300\u2192        # Ensure plugin is loaded if using lazy loading\n   301\u2192        if self._lazy_load and self._use_factory:\n   302\u2192            plugin = self._ensure_plugin_for_file(path)\n   303\u2192            if plugin:\n   304\u2192                return plugin\n   305\u2192        \n   306\u2192        # Use advanced routing if available\n   307\u2192        if self._enable_advanced and self._router:\n   308\u2192            route_result = self._router.get_best_plugin(path)\n   309\u2192            if route_result:\n   310\u2192                return route_result.plugin\n   311\u2192        \n   312\u2192        # Fallback to basic matching\n   313\u2192        for p in self._plugins:\n   314\u2192            if p.supports(path):\n   315\u2192                return p\n   316\u2192        \n   317\u2192        raise RuntimeError(f\"No plugin found for {path}\")\n   318\u2192    \n   319\u2192    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320\u2192        \"\"\"Get all plugins that can handle a file with confidence scores.\"\"\"\n   321\u2192        # Ensure plugin is loaded if using lazy loading\n   322\u2192        if self._lazy_load and self._use_factory:\n   323\u2192            self._ensure_plugin_for_file(path)\n   324\u2192        \n   325\u2192        if self._enable_advanced and self._router:\n   326\u2192            route_results = self._router.route_file(path)\n   327\u2192            return [(result.plugin, result.confidence) for result in route_results]\n   328\u2192        else:\n   329\u2192            # Basic fallback\n   330\u2192            matching_plugins = []\n   331\u2192            for plugin in self._plugins:\n   332\u2192                if plugin.supports(path):\n   333\u2192                    matching_plugins.append((plugin, 1.0))\n   334\u2192            return matching_plugins\n   335\u2192    \n   336\u2192    def lookup(self, symbol: str) -> SymbolDef | None:\n   337\u2192        \"\"\"Look up symbol definition across all plugins.\"\"\"\n   338\u2192        start_time = time.time()\n   339\u2192        \n   340\u2192        try:\n   341\u2192            # For symbol lookup, we may need to search across all languages\n   342\u2192            # Load all plugins if using lazy loading\n   343\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344\u2192                self._load_all_plugins()\n   345\u2192            \n   346\u2192            if self._enable_advanced and self._aggregator:\n   347\u2192                # Use advanced aggregation\n   348\u2192                definitions_by_plugin = {}\n   349\u2192                for plugin in self._plugins:\n   350\u2192                    try:\n   351\u2192                        definition = plugin.getDefinition(symbol)\n   352\u2192                        definitions_by_plugin[plugin] = definition\n   353\u2192                    except Exception as e:\n   354\u2192                        logger.warning(f\"Plugin {plugin.lang} failed to get definition for {symbol}: {e}\")\n   355\u2192                        definitions_by_plugin[plugin] = None\n   356\u2192                \n   357\u2192                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358\u2192                \n   359\u2192                self._operation_stats['lookups'] += 1\n   360\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   361\u2192                \n   362\u2192                return result\n   363\u2192            else:\n   364\u2192                # Fallback to basic lookup\n   365\u2192                for p in self._plugins:\n   366\u2192                    res = p.getDefinition(symbol)\n   367\u2192                    if res:\n   368\u2192                        self._operation_stats['lookups'] += 1\n   369\u2192                        self._operation_stats['total_time'] += time.time() - start_time\n   370\u2192                        return res\n   371\u2192                return None\n   372\u2192                \n   373\u2192        except Exception as e:\n   374\u2192            logger.error(f\"Error in symbol lookup for {symbol}: {e}\", exc_info=True)\n   375\u2192            return None\n   376\u2192    \n   377\u2192    def _is_document_query(self, query: str) -> bool:\n   378\u2192        \"\"\"Check if the query is looking for documentation.\n   379\u2192        \n   380\u2192        Args:\n   381\u2192            query: Search query string\n   382\u2192            \n   383\u2192        Returns:\n   384\u2192            True if this appears to be a documentation query\n   385\u2192        \"\"\"\n   386\u2192        query_lower = query.lower()\n   387\u2192        \n   388\u2192        # Check against document query patterns\n   389\u2192        for pattern in self._compiled_doc_patterns:\n   390\u2192            if pattern.search(query_lower):\n   391\u2192                return True\n   392\u2192        \n   393\u2192        # Check for question words at the beginning\n   394\u2192        question_starters = ['how', 'what', 'where', 'when', 'why', 'can', 'is', 'does', 'should']\n   395\u2192        first_word = query_lower.split()[0] if query_lower.split() else ''\n   396\u2192        if first_word in question_starters:\n   397\u2192            return True\n   398\u2192        \n   399\u2192        return False\n   400\u2192    \n   401\u2192    def _expand_document_query(self, query: str) -> List[str]:\n   402\u2192        \"\"\"Expand a document query with related terms for better search coverage.\n   403\u2192        \n   404\u2192        Args:\n   405\u2192            query: Original search query\n   406\u2192            \n   407\u2192        Returns:\n   408\u2192            List of expanded query variations\n   409\u2192        \"\"\"\n   410\u2192        expanded_queries = [query]  # Always include original\n   411\u2192        query_lower = query.lower()\n   412\u2192        \n   413\u2192        # Common expansions for documentation queries\n   414\u2192        expansions = {\n   415\u2192            'install': ['installation', 'setup', 'getting started', 'requirements'],\n   416\u2192            'config': ['configuration', 'configure', 'settings', 'options', 'parameters'],\n   417\u2192            'api': ['api documentation', 'api reference', 'endpoint', 'method'],\n   418\u2192            'how to': ['tutorial', 'guide', 'example', 'usage'],\n   419\u2192            'example': ['sample', 'snippet', 'demo', 'code example'],\n   420\u2192            'error': ['troubleshoot', 'debug', 'issue', 'problem', 'fix'],\n   421\u2192            'getting started': ['quickstart', 'tutorial', 'introduction', 'setup'],\n   422\u2192            'guide': ['tutorial', 'documentation', 'walkthrough', 'how to'],\n   423\u2192            'usage': ['how to use', 'example', 'api', 'reference']\n   424\u2192        }\n   425\u2192        \n   426\u2192        # Apply expansions\n   427\u2192        for term, related_terms in expansions.items():\n   428\u2192            if term in query_lower:\n   429\u2192                for related in related_terms:\n   430\u2192                    # Replace the term with related term\n   431\u2192                    expanded = query_lower.replace(term, related)\n   432\u2192                    if expanded != query_lower and expanded not in expanded_queries:\n   433\u2192                        expanded_queries.append(expanded)\n   434\u2192                \n   435\u2192                # Also add queries with additional terms\n   436\u2192                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437\u2192                    expanded = f\"{query} {related}\"\n   438\u2192                    if expanded not in expanded_queries:\n   439\u2192                        expanded_queries.append(expanded)\n   440\u2192        \n   441\u2192        # Add file-specific searches for common documentation files\n   442\u2192        if self._is_document_query(query):\n   443\u2192            # Extract the main topic from the query\n   444\u2192            topic_words = []\n   445\u2192            for word in query.lower().split():\n   446\u2192                if word not in ['how', 'to', 'the', 'a', 'an', 'is', 'are', 'what', 'where', 'when']:\n   447\u2192                    topic_words.append(word)\n   448\u2192            \n   449\u2192            if topic_words:\n   450\u2192                topic = ' '.join(topic_words[:2])  # Use first two topic words\n   451\u2192                expanded_queries.extend([\n   452\u2192                    f\"README {topic}\",\n   453\u2192                    f\"{topic} documentation\",\n   454\u2192                    f\"{topic} docs\",\n   455\u2192                    f\"{topic} guide\"\n   456\u2192                ])\n   457\u2192        \n   458\u2192        # Remove duplicates while preserving order\n   459\u2192        seen = set()\n   460\u2192        unique_queries = []\n   461\u2192        for q in expanded_queries:\n   462\u2192            if q not in seen:\n   463\u2192                seen.add(q)\n   464\u2192                unique_queries.append(q)\n   465\u2192        \n   466\u2192        return unique_queries[:10]  # Limit to 10 queries max\n   467\u2192    \n   468\u2192    def _is_documentation_file(self, file_path: str) -> bool:\n   469\u2192        \"\"\"Check if a file path is likely a documentation file.\n   470\u2192        \n   471\u2192        Args:\n   472\u2192            file_path: Path to check\n   473\u2192            \n   474\u2192        Returns:\n   475\u2192            True if this appears to be a documentation file\n   476\u2192        \"\"\"\n   477\u2192        path_lower = file_path.lower()\n   478\u2192        \n   479\u2192        for pattern in self._compiled_file_patterns:\n   480\u2192            if pattern.search(path_lower):\n   481\u2192                return True\n   482\u2192        \n   483\u2192        return False\n   484\u2192    \n   485\u2192    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486\u2192        \"\"\"Adjust ranking to prioritize documentation files for document queries.\n   487\u2192        \n   488\u2192        Args:\n   489\u2192            query: Original search query\n   490\u2192            results: List of aggregated results\n   491\u2192            \n   492\u2192        Returns:\n   493\u2192            Re-ranked results with documentation prioritized\n   494\u2192        \"\"\"\n   495\u2192        if not self._is_document_query(query):\n   496\u2192            return results\n   497\u2192        \n   498\u2192        # Separate documentation and code results\n   499\u2192        doc_results = []\n   500\u2192        code_results = []\n   501\u2192        \n   502\u2192        for result in results:\n   503\u2192            if self._is_documentation_file(result.primary_result.get('file', '')):\n   504\u2192                # Boost documentation files for document queries\n   505\u2192                result.rank_score *= 1.5\n   506\u2192                result.metadata['doc_boost'] = True\n   507\u2192                doc_results.append(result)\n   508\u2192            else:\n   509\u2192                code_results.append(result)\n   510\u2192        \n   511\u2192        # Sort each group by rank score\n   512\u2192        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513\u2192        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514\u2192        \n   515\u2192        # Combine with documentation files first\n   516\u2192        return doc_results + code_results\n   517\u2192    \n   518\u2192    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519\u2192        \"\"\"Search for code and documentation across all plugins.\"\"\"\n   520\u2192        start_time = time.time()\n   521\u2192        \n   522\u2192        try:\n   523\u2192            # For search, we may need to search across all languages\n   524\u2192            # Load all plugins if using lazy loading\n   525\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526\u2192                self._load_all_plugins()\n   527\u2192            \n   528\u2192            # Detect if this is a document query\n   529\u2192            is_doc_query = self._is_document_query(query)\n   530\u2192            \n   531\u2192            # Expand query if it's a document query\n   532\u2192            queries = [query]\n   533\u2192            if is_doc_query:\n   534\u2192                queries = self._expand_document_query(query)\n   535\u2192                logger.info(f\"Expanded document query '{query}' to {len(queries)} variations\")\n   536\u2192                # Force semantic search for natural language queries\n   537\u2192                semantic = True\n   538\u2192            \n   539\u2192            if self._enable_advanced and self._aggregator:\n   540\u2192                # Use advanced aggregation\n   541\u2192                all_results_by_plugin = {}\n   542\u2192                opts = {\"semantic\": semantic, \"limit\": limit * 2 if is_doc_query else limit}\n   543\u2192                \n   544\u2192                # Search with all query variations\n   545\u2192                for search_query in queries:\n   546\u2192                    for plugin in self._plugins:\n   547\u2192                        try:\n   548\u2192                            results = list(plugin.search(search_query, opts))\n   549\u2192                            if results:\n   550\u2192                                if plugin not in all_results_by_plugin:\n   551\u2192                                    all_results_by_plugin[plugin] = []\n   552\u2192                                all_results_by_plugin[plugin].extend(results)\n   553\u2192                        except Exception as e:\n   554\u2192                            logger.warning(f\"Plugin {plugin.lang} failed to search for {search_query}: {e}\")\n   555\u2192                \n   556\u2192                # Deduplicate results per plugin\n   557\u2192                for plugin, results in all_results_by_plugin.items():\n   558\u2192                    seen = set()\n   559\u2192                    unique_results = []\n   560\u2192                    for result in results:\n   561\u2192                        key = f\"{result['file']}:{result['line']}\"\n   562\u2192                        if key not in seen:\n   563\u2192                            seen.add(key)\n   564\u2192                            unique_results.append(result)\n   565\u2192                    all_results_by_plugin[plugin] = unique_results\n   566\u2192                \n   567\u2192                # Configure aggregator for document queries\n   568\u2192                if is_doc_query and self._enable_advanced:\n   569\u2192                    # Adjust ranking criteria for documentation\n   570\u2192                    doc_criteria = RankingCriteria(\n   571\u2192                        relevance_weight=0.5,      # Increase relevance weight\n   572\u2192                        confidence_weight=0.2,     # Reduce confidence weight\n   573\u2192                        frequency_weight=0.2,      # Keep frequency weight\n   574\u2192                        recency_weight=0.1,        # Keep recency weight\n   575\u2192                        prefer_exact_matches=False,  # Natural language doesn't need exact matches\n   576\u2192                        boost_multiple_sources=True,\n   577\u2192                        boost_common_extensions=True\n   578\u2192                    )\n   579\u2192                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580\u2192                \n   581\u2192                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582\u2192                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583\u2192                )\n   584\u2192                \n   585\u2192                # Adjust ranking for document queries\n   586\u2192                if is_doc_query:\n   587\u2192                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588\u2192                \n   589\u2192                # Apply final limit\n   590\u2192                if limit and len(aggregated_results) > limit:\n   591\u2192                    aggregated_results = aggregated_results[:limit]\n   592\u2192                \n   593\u2192                logger.debug(f\"Search aggregation stats: {stats.total_results} total, \"\n   594\u2192                           f\"{stats.unique_results} unique, {stats.plugins_used} plugins used, \"\n   595\u2192                           f\"document_query={is_doc_query}\")\n   596\u2192                \n   597\u2192                self._operation_stats['searches'] += 1\n   598\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   599\u2192                \n   600\u2192                # Yield primary results from aggregated results\n   601\u2192                for aggregated in aggregated_results:\n   602\u2192                    yield aggregated.primary_result\n   603\u2192            else:\n   604\u2192                # Fallback to basic search\n   605\u2192                # Detect if this is a document query\n   606\u2192                is_doc_query = self._is_document_query(query)\n   607\u2192                \n   608\u2192                # Expand query if it's a document query\n   609\u2192                queries = [query]\n   610\u2192                if is_doc_query:\n   611\u2192                    queries = self._expand_document_query(query)\n   612\u2192                    semantic = True  # Force semantic search for natural language\n   613\u2192                \n   614\u2192                opts = {\"semantic\": semantic, \"limit\": limit}\n   615\u2192                all_results = []\n   616\u2192                \n   617\u2192                # Search with all query variations\n   618\u2192                for search_query in queries:\n   619\u2192                    for p in self._plugins:\n   620\u2192                        try:\n   621\u2192                            for result in p.search(search_query, opts):\n   622\u2192                                all_results.append(result)\n   623\u2192                        except Exception as e:\n   624\u2192                            logger.warning(f\"Plugin {p.lang} failed to search for {search_query}: {e}\")\n   625\u2192                \n   626\u2192                # Deduplicate results\n   627\u2192                seen = set()\n   628\u2192                unique_results = []\n   629\u2192                for result in all_results:\n   630\u2192                    key = f\"{result['file']}:{result['line']}\"\n   631\u2192                    if key not in seen:\n   632\u2192                        seen.add(key)\n   633\u2192                        unique_results.append(result)\n   634\u2192                \n   635\u2192                # Sort by score if available\n   636\u2192                unique_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   637\u2192                \n   638\u2192                # Prioritize documentation files for document queries\n   639\u2192                if is_doc_query:\n   640\u2192                    doc_results = []\n   641\u2192                    code_results = []\n   642\u2192                    for result in unique_results:\n   643\u2192                        if self._is_documentation_file(result.get('file', '')):\n   644\u2192                            doc_results.append(result)\n   645\u2192                        else:\n   646\u2192                            code_results.append(result)\n   647\u2192                    unique_results = doc_results + code_results\n   648\u2192                \n   649\u2192                # Apply limit\n   650\u2192                count = 0\n   651\u2192                for result in unique_results:\n   652\u2192                    if limit and count >= limit:\n   653\u2192                        break\n   654\u2192                    yield result\n   655\u2192                    count += 1\n   656\u2192                \n   657\u2192                self._operation_stats['searches'] += 1\n   658\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   659\u2192                \n   660\u2192        except Exception as e:\n   661\u2192            logger.error(f\"Error in search for {query}: {e}\", exc_info=True)\n   662\u2192    \n   663\u2192    def index_file(self, path: Path) -> None:\n   664\u2192        \"\"\"Index a single file if it has changed.\"\"\"\n   665\u2192        try:\n   666\u2192            # Ensure path is absolute to avoid relative/absolute path issues\n   667\u2192            path = path.resolve()\n   668\u2192            \n   669\u2192            # Find the appropriate plugin\n   670\u2192            plugin = self._match_plugin(path)\n   671\u2192            \n   672\u2192            # Read file content\n   673\u2192            try:\n   674\u2192                content = path.read_text(encoding='utf-8')\n   675\u2192            except UnicodeDecodeError:\n   676\u2192                # Try with different encodings\n   677\u2192                try:\n   678\u2192                    content = path.read_text(encoding='latin-1')\n   679\u2192                except Exception as e:\n   680\u2192                    logger.error(f\"Failed to read {path}: {e}\")\n   681\u2192                    return\n   682\u2192            \n   683\u2192            # Check if we need to re-index (simplified for now)\n   684\u2192            # TODO: Implement proper caching logic\n   685\u2192            \n   686\u2192            # Index the file\n   687\u2192            start_time = time.time()\n   688\u2192            logger.info(f\"Indexing {path} with {plugin.lang} plugin\")\n   689\u2192            shard = plugin.indexFile(path, content)\n   690\u2192            \n   691\u2192            # Record performance if advanced features enabled\n   692\u2192            if self._enable_advanced and self._router:\n   693\u2192                execution_time = time.time() - start_time\n   694\u2192                self._router.record_performance(plugin, execution_time)\n   695\u2192            \n   696\u2192            self._operation_stats['indexings'] += 1\n   697\u2192            self._operation_stats['total_time'] += time.time() - start_time\n   698\u2192            \n   699\u2192            logger.info(f\"Successfully indexed {path}: {len(shard.get('symbols', []))} symbols found\")\n   700\u2192            \n   701\u2192        except RuntimeError as e:\n   702\u2192            # No plugin found for this file type\n   703\u2192            logger.debug(f\"No plugin for {path}: {e}\")\n   704\u2192        except Exception as e:\n   705\u2192            logger.error(f\"Error indexing {path}: {e}\", exc_info=True)\n   706\u2192    \n   707\u2192    def get_statistics(self) -> dict:\n   708\u2192        \"\"\"Get comprehensive statistics across all plugins and components.\"\"\"\n   709\u2192        stats = {\n   710\u2192            \"total_plugins\": len(self._plugins),\n   711\u2192            \"loaded_languages\": sorted(list(self._loaded_languages)),\n   712\u2192            \"supported_languages\": len(self.supported_languages),\n   713\u2192            \"operations\": self._operation_stats.copy()\n   714\u2192        }\n   715\u2192        \n   716\u2192        # Add language breakdown\n   717\u2192        stats[\"by_language\"] = {}\n   718\u2192        for lang, plugin in self._by_lang.items():\n   719\u2192            plugin_info = {\n   720\u2192                \"loaded\": True,\n   721\u2192                \"class\": plugin.__class__.__name__\n   722\u2192            }\n   723\u2192            if hasattr(plugin, 'get_indexed_count'):\n   724\u2192                plugin_info[\"indexed_files\"] = plugin.get_indexed_count()\n   725\u2192            stats[\"by_language\"][lang] = plugin_info\n   726\u2192        \n   727\u2192        return stats\n   728\u2192    \n   729\u2192    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730\u2192        \"\"\"\n   731\u2192        Index all files in a directory, respecting ignore patterns.\n   732\u2192        \n   733\u2192        Args:\n   734\u2192            directory: Directory to index\n   735\u2192            recursive: Whether to index subdirectories\n   736\u2192            \n   737\u2192        Returns:\n   738\u2192            Statistics about indexed files\n   739\u2192        \"\"\"\n   740\u2192        logger.info(f\"Indexing directory: {directory} (recursive={recursive})\")\n   741\u2192        \n   742\u2192        # Note: We don't use ignore patterns during indexing\n   743\u2192        # ALL files are indexed for local search capability\n   744\u2192        # Filtering happens only during export/sharing\n   745\u2192        \n   746\u2192        # Get all supported extensions\n   747\u2192        supported_extensions = get_all_extensions()\n   748\u2192        \n   749\u2192        stats = {\n   750\u2192            \"total_files\": 0,\n   751\u2192            \"indexed_files\": 0,\n   752\u2192            \"ignored_files\": 0,\n   753\u2192            \"failed_files\": 0,\n   754\u2192            \"by_language\": {}\n   755\u2192        }\n   756\u2192        \n   757\u2192        # Walk directory\n   758\u2192        if recursive:\n   759\u2192            file_iterator = directory.rglob(\"*\")\n   760\u2192        else:\n   761\u2192            file_iterator = directory.glob(\"*\")\n   762\u2192            \n   763\u2192        for path in file_iterator:\n   764\u2192            if not path.is_file():\n   765\u2192                continue\n   766\u2192                \n   767\u2192            stats[\"total_files\"] += 1\n   768\u2192            \n   769\u2192            # NOTE: We index ALL files locally, including gitignored ones\n   770\u2192            # Filtering happens only during export/sharing\n   771\u2192            # This allows local search of .env, secrets, etc.\n   772\u2192                \n   773\u2192            # Try to find a plugin that supports this file\n   774\u2192            # This allows us to index ALL files, including .env, .key, etc.\n   775\u2192            try:\n   776\u2192                # First try to match by extension\n   777\u2192                if path.suffix in supported_extensions:\n   778\u2192                    self.index_file(path)\n   779\u2192                    stats[\"indexed_files\"] += 1\n   780\u2192                # For files without recognized extensions, try each plugin's supports() method\n   781\u2192                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782\u2192                else:\n   783\u2192                    matched = False\n   784\u2192                    for plugin in self._plugins:\n   785\u2192                        if plugin.supports(path):\n   786\u2192                            self.index_file(path)\n   787\u2192                            stats[\"indexed_files\"] += 1\n   788\u2192                            matched = True\n   789\u2192                            break\n   790\u2192                    \n   791\u2192                    # If no plugin matched but we want to index everything,\n   792\u2192                    # we could add a fallback here to index as plaintext\n   793\u2192                    # For now, we'll skip unmatched files\n   794\u2192                    if not matched:\n   795\u2192                        logger.debug(f\"No plugin found for {path}\")\n   796\u2192                \n   797\u2192                # Track by language\n   798\u2192                language = get_language_by_extension(path.suffix)\n   799\u2192                if language:\n   800\u2192                    stats[\"by_language\"][language] = stats[\"by_language\"].get(language, 0) + 1\n   801\u2192                    \n   802\u2192            except Exception as e:\n   803\u2192                logger.error(f\"Failed to index {path}: {e}\")\n   804\u2192                stats[\"failed_files\"] += 1\n   805\u2192                \n   806\u2192        logger.info(f\"Directory indexing complete: {stats['indexed_files']} indexed, \"\n   807\u2192                   f\"{stats['ignored_files']} ignored, {stats['failed_files']} failed\")\n   808\u2192        \n   809\u2192        return stats\n   810\u2192    \n   811\u2192    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812\u2192        \"\"\"Search specifically across documentation files.\n   813\u2192        \n   814\u2192        Args:\n   815\u2192            topic: Topic to search for (e.g., \"installation\", \"configuration\")\n   816\u2192            doc_types: Optional list of document types to search (e.g., [\"readme\", \"guide\", \"api\"])\n   817\u2192            limit: Maximum number of results\n   818\u2192            \n   819\u2192        Returns:\n   820\u2192            Search results from documentation files\n   821\u2192        \"\"\"\n   822\u2192        # Default document types if not specified\n   823\u2192        if doc_types is None:\n   824\u2192            doc_types = [\"readme\", \"documentation\", \"guide\", \"tutorial\", \"api\", \"changelog\", \"contributing\"]\n   825\u2192        \n   826\u2192        # Build search queries for different document types\n   827\u2192        queries = []\n   828\u2192        for doc_type in doc_types:\n   829\u2192            queries.extend([\n   830\u2192                f\"{doc_type} {topic}\",\n   831\u2192                f\"{topic} {doc_type}\",\n   832\u2192                f\"{topic} in {doc_type}\"\n   833\u2192            ])\n   834\u2192        \n   835\u2192        # Also search for the topic in common doc filenames\n   836\u2192        queries.extend([\n   837\u2192            f\"README {topic}\",\n   838\u2192            f\"CONTRIBUTING {topic}\",\n   839\u2192            f\"docs {topic}\",\n   840\u2192            f\"documentation {topic}\"\n   841\u2192        ])\n   842\u2192        \n   843\u2192        # Deduplicate queries\n   844\u2192        queries = list(dict.fromkeys(queries))\n   845\u2192        \n   846\u2192        logger.info(f\"Cross-document search for '{topic}' with {len(queries)} query variations\")\n   847\u2192        \n   848\u2192        # Use the enhanced search with document-specific handling\n   849\u2192        all_results = []\n   850\u2192        seen = set()\n   851\u2192        \n   852\u2192        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853\u2192            for result in self.search(query, semantic=True, limit=limit):\n   854\u2192                # Only include documentation files\n   855\u2192                if self._is_documentation_file(result.get('file', '')):\n   856\u2192                    key = f\"{result['file']}:{result['line']}\"\n   857\u2192                    if key not in seen:\n   858\u2192                        seen.add(key)\n   859\u2192                        all_results.append(result)\n   860\u2192        \n   861\u2192        # Sort by relevance (score) and return top results\n   862\u2192        all_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   863\u2192        \n   864\u2192        count = 0\n   865\u2192        for result in all_results:\n   866\u2192            if count >= limit:\n   867\u2192                break\n   868\u2192            yield result\n   869\u2192            count += 1\n   870\u2192    \n   871\u2192    def health_check(self) -> Dict[str, Any]:\n   872\u2192        \"\"\"Perform a health check on all components.\"\"\"\n   873\u2192        health = {\n   874\u2192            'status': 'healthy',\n   875\u2192            'components': {\n   876\u2192                'dispatcher': {\n   877\u2192                    'status': 'healthy',\n   878\u2192                    'plugins_loaded': len(self._plugins),\n   879\u2192                    'languages_supported': len(self.supported_languages),\n   880\u2192                    'factory_enabled': self._use_factory,\n   881\u2192                    'lazy_loading': self._lazy_load\n   882\u2192                }\n   883\u2192            },\n   884\u2192            'plugins': {},\n   885\u2192            'errors': []\n   886\u2192        }\n   887\u2192        \n   888\u2192        # Check plugin health\n   889\u2192        for lang, plugin in self._by_lang.items():\n   890\u2192            try:\n   891\u2192                plugin_health = {\n   892\u2192                    'status': 'healthy',\n   893\u2192                    'class': plugin.__class__.__name__,\n   894\u2192                    'semantic_enabled': getattr(plugin, '_enable_semantic', False)\n   895\u2192                }\n   896\u2192                if hasattr(plugin, 'get_indexed_count'):\n   897\u2192                    plugin_health['indexed_files'] = plugin.get_indexed_count()\n   898\u2192            except Exception as e:\n   899\u2192                plugin_health = {\n   900\u2192                    'status': 'error',\n   901\u2192                    'error': str(e)\n   902\u2192                }\n   903\u2192                health['errors'].append(f\"Plugin {lang}: {str(e)}\")\n   904\u2192            \n   905\u2192            health['plugins'][lang] = plugin_health\n   906\u2192        \n   907\u2192        # Determine overall health\n   908\u2192        if len(health['errors']) > 0:\n   909\u2192            health['status'] = 'degraded' if len(health['errors']) < 3 else 'unhealthy'\n   910\u2192        \n   911\u2192        return health\n   912\u2192    \n   913\u2192    def remove_file(self, path: Union[Path, str]) -> None:\n   914\u2192        \"\"\"Remove a file from all indexes.\n   915\u2192        \n   916\u2192        Args:\n   917\u2192            path: File path to remove\n   918\u2192        \"\"\"\n   919\u2192        path = Path(path).resolve()\n   920\u2192        logger.info(f\"Removing file from index: {path}\")\n   921\u2192        \n   922\u2192        try:\n   923\u2192            # Remove from SQLite if available\n   924\u2192            if self._sqlite_store:\n   925\u2192                from ..core.path_resolver import PathResolver\n   926\u2192                path_resolver = PathResolver()\n   927\u2192                try:\n   928\u2192                    relative_path = path_resolver.normalize_path(path)\n   929\u2192                    # Get repository ID - for now assume 1\n   930\u2192                    # TODO: Properly detect repository\n   931\u2192                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932\u2192                except Exception as e:\n   933\u2192                    logger.error(f\"Error removing from SQLite: {e}\")\n   934\u2192            \n   935\u2192            # Remove from semantic index if available\n   936\u2192            try:\n   937\u2192                plugin = self._match_plugin(path)\n   938\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   939\u2192                    plugin._indexer.remove_file(path)\n   940\u2192                    logger.info(f\"Removed from semantic index: {path}\")\n   941\u2192            except Exception as e:\n   942\u2192                logger.warning(f\"Error removing from semantic index: {e}\")\n   943\u2192            \n   944\u2192            # Update statistics\n   945\u2192            self._operation_stats['deletions'] = self._operation_stats.get('deletions', 0) + 1\n   946\u2192            \n   947\u2192        except Exception as e:\n   948\u2192            logger.error(f\"Error removing file {path}: {e}\", exc_info=True)\n   949\u2192    \n   950\u2192    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951\u2192        \"\"\"Move a file in all indexes.\n   952\u2192        \n   953\u2192        Args:\n   954\u2192            old_path: Original file path\n   955\u2192            new_path: New file path\n   956\u2192            content_hash: Optional content hash to verify unchanged content\n   957\u2192        \"\"\"\n   958\u2192        old_path = Path(old_path).resolve()\n   959\u2192        new_path = Path(new_path).resolve()\n   960\u2192        logger.info(f\"Moving file in index: {old_path} -> {new_path}\")\n   961\u2192        \n   962\u2192        try:\n   963\u2192            # Move in SQLite if available\n   964\u2192            if self._sqlite_store:\n   965\u2192                from ..core.path_resolver import PathResolver\n   966\u2192                path_resolver = PathResolver()\n   967\u2192                try:\n   968\u2192                    old_relative = path_resolver.normalize_path(old_path)\n   969\u2192                    new_relative = path_resolver.normalize_path(new_path)\n   970\u2192                    # Get repository ID - for now assume 1\n   971\u2192                    # TODO: Properly detect repository\n   972\u2192                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973\u2192                except Exception as e:\n   974\u2192                    logger.error(f\"Error moving in SQLite: {e}\")\n   975\u2192            \n   976\u2192            # Move in semantic index if available\n   977\u2192            try:\n   978\u2192                plugin = self._match_plugin(new_path)\n   979\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   980\u2192                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981\u2192                    logger.info(f\"Moved in semantic index: {old_path} -> {new_path}\")\n   982\u2192            except Exception as e:\n   983\u2192                logger.warning(f\"Error moving in semantic index: {e}\")\n   984\u2192            \n   985\u2192            # Update statistics\n   986\u2192            self._operation_stats['moves'] = self._operation_stats.get('moves', 0) + 1\n   987\u2192            \n   988\u2192        except Exception as e:\n   989\u2192            logger.error(f\"Error moving file {old_path} -> {new_path}: {e}\", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 27690,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "a9f87083-f603-4b60-a151-8cabc58f385f",
    "test_date": "2025-06-26T07:39:47.016Z",
    "query_id": "transcript_a9f87083-f603-4b60-a151-8cabc58f385f",
    "query_text": "Find all functions that use asyncio",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13913,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "a9f87083-f603-4b60-a151-8cabc58f385f",
    "test_date": "2025-06-26T07:39:50.500Z",
    "query_id": "transcript_a9f87083-f603-4b60-a151-8cabc58f385f",
    "query_text": [
      {
        "tool_use_id": "toolu_012WYtGvzXaiAUGSYUpSxZed",
        "type": "tool_result",
        "content": "Found 78 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/conftest.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_developer_workflows.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_repository_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_semantic_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_indexer_advanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_reranking_metadata_preservation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_plugins_direct.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/populate_bm25_quick.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/pytest.ini\n/workspaces/Code-Index-MCP/testing-env/worktree-native/requirements-production.txt\n/workspaces/Code-Index-MCP/testing-env/worktree-native/requirements.txt\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_bm25_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/docs/development/TESTING-GUIDE.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/grpc_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/MCP.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/prometheus_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/watchdog.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/level4/file_watcher.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-native/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/codex-setup.sh\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_reranking_working.py"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13913,
    "cache_creation_tokens": 5223,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "a9f87083-f603-4b60-a151-8cabc58f385f",
    "test_date": "2025-06-26T07:39:58.653Z",
    "query_id": "transcript_a9f87083-f603-4b60-a151-8cabc58f385f",
    "query_text": "Find all functions that use asyncio in the codebase. I need you to:\n1. Search for \"import asyncio\" imports\n2. Search for \"async def\" function definitions\n3. Search for asyncio.run, asyncio.create_task, asyncio.gather, and other asyncio method calls\n4. For each file that contains asyncio usage, read the file and identify the specific function names that use asyncio\n5. Create a comprehensive list showing the file path, line number, and function name for each function that uses asyncio\n\nFocus on the main source code files in mcp_server/ directory, not just test files. Return a detailed list of all asyncio functions with their locations.",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 343,
    "output_tokens_total": 343,
    "cache_read_tokens": 9986,
    "cache_creation_tokens": 886,
    "tokens_per_result": 343,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "a9f87083-f603-4b60-a151-8cabc58f385f",
    "test_date": "2025-06-26T07:40:06.620Z",
    "query_id": "transcript_a9f87083-f603-4b60-a151-8cabc58f385f",
    "query_text": [
      {
        "tool_use_id": "toolu_012G7ePQUKtfvazbgfhxUTc4",
        "type": "tool_result",
        "content": "Found 62 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/conftest.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/performance/test_document_search_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_advanced_indexing.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_performance_scaling.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_redis_caching.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/real_world/test_symbol_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_metrics.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/tests/test_security.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_processing_parallel.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_structure_extraction.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_final_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_cli_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_comprehensive_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_debug.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_multi_language.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_rapid_validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_real.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_actual.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking_demo.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_mcp_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_multi_language_codebase.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_real_world_repos.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_fixed.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_quality.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_semantic_comprehensive.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server_cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/watcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_document_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/run_parallel_tests.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/scripts/validate_production_performance.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_all_fixes.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_comprehensive_reranking.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_contextual_pipeline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/index_with_mcp.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/docs/development/TESTING-GUIDE.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/grpc_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/MCP.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/prometheus_overview.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/ai_docs/watchdog.md\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/level4/file_watcher.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-native/benchmark_reranking_comparison.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_contextual_simple.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/demo_indexer.py"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 8,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 90,
    "output_tokens_total": 90,
    "cache_read_tokens": 10872,
    "cache_creation_tokens": 7901,
    "tokens_per_result": 90,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "a9f87083-f603-4b60-a151-8cabc58f385f",
    "test_date": "2025-06-26T07:40:13.309Z",
    "query_id": "transcript_a9f87083-f603-4b60-a151-8cabc58f385f",
    "query_text": [
      {
        "tool_use_id": "toolu_012m3HDMDdRZbrD3EnF71hyN",
        "type": "tool_result",
        "content": "/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_benchmarks.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/run_baseline.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_runner.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/benchmark_suite.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/__main__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/benchmarks/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_base_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_base.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/gateway.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/generic_treesitter_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/plugin_registry.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/plugin_manager.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/plugin_loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/plugin_discovery.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/models.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/loader.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/discovery.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/config.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/cli.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugin_system/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/prometheus_exporter.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/middleware.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/metrics_collector.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/health_check.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/metrics/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/storage_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/shared_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/security_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/plugin_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/metrics_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/indexing_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/dispatcher_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/cache_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/api_gateway_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/interfaces/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/reranker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/query_optimizer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/index_engine.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/hybrid_search.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/bm25_indexer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/semantic_chunker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/metadata_extractor.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/document_interfaces.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/contextual_embeddings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/chunk_optimizer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/base_document_plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/document_processing/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/result_aggregator.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/plugin_router.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/core/path_resolver.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/core/logging.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/core/ignore_patterns.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/core/errors.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/core/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cli/index_management.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cli/artifact_commands.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cli/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/config/validation.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/config/settings.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/config/environment.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/config/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cache/query_cache.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cache/cache_manager.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cache/backends.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/cache/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/kotlin_plugin/java_interop.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/kotlin_plugin/coroutines_analyzer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/kotlin_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/js_plugin/plugin_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/js_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/js_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/java_plugin/type_analyzer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/java_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/java_plugin/import_resolver.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/java_plugin/build_system.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/java_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/html_css_plugin/plugin_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/html_css_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/html_css_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/go_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/go_plugin/package_analyzer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/go_plugin/module_resolver.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/go_plugin/interface_checker.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/go_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/dart_plugin/plugin_semantic.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/dart_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/dart_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/type_analyzer.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/plugin.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/nuget_integration.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/namespace_resolver.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/csharp_plugin/__init__.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/plugins/cpp_plugin/plugin_semantic.py\n(Results are truncated. Consider using a more specific path or pattern.)"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 391,
    "output_tokens_total": 391,
    "cache_read_tokens": 18773,
    "cache_creation_tokens": 3726,
    "tokens_per_result": 391,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "a9f87083-f603-4b60-a151-8cabc58f385f",
    "test_date": "2025-06-26T07:40:21.997Z",
    "query_id": "transcript_a9f87083-f603-4b60-a151-8cabc58f385f",
    "query_text": [
      {
        "tool_use_id": "toolu_01Gtbau7JDxAuhDZgWePDd1p",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"\n     2\u2192Reranking Module for Search Result Optimization\n     3\u2192\n     4\u2192This module provides implementations for reranking search results to improve relevance.\n     5\u2192It supports both Cohere's reranking API and local cross-encoder models as fallback.\n     6\u2192\"\"\"\n     7\u2192\n     8\u2192import os\n     9\u2192import logging\n    10\u2192import asyncio\n    11\u2192from typing import Dict, List, Optional, Any, Tuple\n    12\u2192from dataclasses import dataclass\n    13\u2192import numpy as np\n    14\u2192from abc import ABC, abstractmethod\n    15\u2192\n    16\u2192# Define interfaces inline for now\n    17\u2192from abc import ABC, abstractmethod\n    18\u2192from dataclasses import dataclass as dc\n    19\u2192from typing import Protocol\n    20\u2192\n    21\u2192# Define SearchResult inline\n    22\u2192@dc \n    23\u2192class SearchResult:\n    24\u2192    \"\"\"Search result information\"\"\"\n    25\u2192    file_path: str\n    26\u2192    line: int\n    27\u2192    column: int\n    28\u2192    snippet: str\n    29\u2192    match_type: str  # exact, fuzzy, semantic\n    30\u2192    score: float\n    31\u2192    context: Optional[str] = None\n    32\u2192\n    33\u2192# Define RerankItem to wrap original SearchResult\n    34\u2192@dc\n    35\u2192class RerankItem:\n    36\u2192    \"\"\"A single reranked item that preserves the complete original SearchResult\"\"\"\n    37\u2192    original_result: SearchResult  # Complete original SearchResult\n    38\u2192    rerank_score: float  # The reranking score\n    39\u2192    original_rank: int  # Original position in results\n    40\u2192    new_rank: int  # New position after reranking\n    41\u2192    explanation: Optional[str] = None  # Optional explanation for the ranking\n    42\u2192\n    43\u2192# Define RerankResult with proper structure\n    44\u2192@dc\n    45\u2192class RerankResult:\n    46\u2192    \"\"\"Result from reranking operation\"\"\"\n    47\u2192    results: List[RerankItem]  # List of reranked items\n    48\u2192    metadata: Dict[str, Any]  # Metadata about the reranking operation\n    49\u2192\n    50\u2192# Define IReranker interface\n    51\u2192class IReranker(ABC):\n    52\u2192    @abstractmethod\n    53\u2192    async def rerank(self, query: str, results: List[SearchResult], top_k: int = 10) -> RerankResult:\n    54\u2192        pass\n    55\u2192\n    56\u2192# Define IRerankerFactory interface  \n    57\u2192class IRerankerFactory(ABC):\n    58\u2192    @abstractmethod\n    59\u2192    def create_reranker(self, config: Any) -> Optional[IReranker]:\n    60\u2192        pass\n    61\u2192\n    62\u2192class MCPError(Exception):\n    63\u2192    \"\"\"Base exception for MCP errors.\"\"\"\n    64\u2192    pass\n    65\u2192\n    66\u2192# Simple Result class for this module\n    67\u2192class Result:\n    68\u2192    def __init__(self, success: bool, data=None, error=None):\n    69\u2192        self.is_success = success\n    70\u2192        self.data = data\n    71\u2192        self.error = error\n    72\u2192    \n    73\u2192    @classmethod\n    74\u2192    def ok(cls, data=None):\n    75\u2192        return cls(True, data)\n    76\u2192    \n    77\u2192    @classmethod\n    78\u2192    def error(cls, error):\n    79\u2192        return cls(False, None, error)\n    80\u2192\n    81\u2192logger = logging.getLogger(__name__)\n    82\u2192\n    83\u2192\n    84\u2192class BaseReranker(IReranker, ABC):\n    85\u2192    \"\"\"Base class for all reranker implementations\"\"\"\n    86\u2192    \n    87\u2192    def __init__(self, config: Dict[str, Any]):\n    88\u2192        self.config = config\n    89\u2192        self._cache: Dict[str, Any] = {}  # Simple in-memory cache\n    90\u2192        self.cache_ttl = config.get('cache_ttl', 3600)  # 1 hour default\n    91\u2192        self.initialized = False\n    92\u2192    \n    93\u2192    async def _get_cache_key(self, query: str, results: List[SearchResult]) -> str:\n    94\u2192        \"\"\"Generate cache key for reranking results\"\"\"\n    95\u2192        # Create a deterministic key based on query and result IDs\n    96\u2192        result_ids = [f\"{r.file_path}:{r.line}\" for r in results[:10]]  # Use top 10 for key\n    97\u2192        return f\"rerank:{self.__class__.__name__}:{hash(query)}:{hash(tuple(result_ids))}\"\n    98\u2192    \n    99\u2192    async def _get_cached_results(\n   100\u2192        self, \n   101\u2192        query: str, \n   102\u2192        results: List[SearchResult]\n   103\u2192    ) -> Optional[List[RerankItem]]:\n   104\u2192        \"\"\"Get cached reranking results if available\"\"\"\n   105\u2192        cache_key = await self._get_cache_key(query, results)\n   106\u2192        \n   107\u2192        if cache_key in self._cache:\n   108\u2192            cached_data = self._cache[cache_key]\n   109\u2192            # Check if cache is still valid (simple time-based check)\n   110\u2192            import time\n   111\u2192            if time.time() - cached_data['timestamp'] < self.cache_ttl:\n   112\u2192                logger.debug(f\"Cache hit for reranking query: {query}\")\n   113\u2192                return cached_data['results']\n   114\u2192            else:\n   115\u2192                # Cache expired\n   116\u2192                del self._cache[cache_key]\n   117\u2192        \n   118\u2192        return None\n   119\u2192    \n   120\u2192    async def _cache_results(\n   121\u2192        self, \n   122\u2192        query: str, \n   123\u2192        results: List[SearchResult], \n   124\u2192        reranked: List[RerankItem]\n   125\u2192    ):\n   126\u2192        \"\"\"Cache reranking results\"\"\"\n   127\u2192        import time\n   128\u2192        cache_key = await self._get_cache_key(query, results)\n   129\u2192        self._cache[cache_key] = {\n   130\u2192            'results': reranked,\n   131\u2192            'timestamp': time.time()\n   132\u2192        }\n   133\u2192        \n   134\u2192        # Simple cache size limit\n   135\u2192        if len(self._cache) > 1000:\n   136\u2192            # Remove oldest entries\n   137\u2192            sorted_keys = sorted(\n   138\u2192                self._cache.keys(), \n   139\u2192                key=lambda k: self._cache[k]['timestamp']\n   140\u2192            )\n   141\u2192            for key in sorted_keys[:100]:  # Remove oldest 100\n   142\u2192                del self._cache[key]\n   143\u2192\n   144\u2192\n   145\u2192class CohereReranker(BaseReranker):\n   146\u2192    \"\"\"Reranker using Cohere's reranking API\"\"\"\n   147\u2192    \n   148\u2192    def __init__(self, config: Dict[str, Any]):\n   149\u2192        super().__init__(config)\n   150\u2192        self.api_key = config.get('cohere_api_key') or os.getenv('COHERE_API_KEY')\n   151\u2192        self.model = config.get('model', 'rerank-english-v2.0')\n   152\u2192        self.client = None\n   153\u2192    \n   154\u2192    async def initialize(self, config: Dict[str, Any]) -> Result:\n   155\u2192        \"\"\"Initialize Cohere client\"\"\"\n   156\u2192        try:\n   157\u2192            if not self.api_key:\n   158\u2192                return Result.error(\"Cohere API key not configured\")\n   159\u2192            \n   160\u2192            # Lazy import to avoid dependency if not used\n   161\u2192            try:\n   162\u2192                import cohere\n   163\u2192                self.client = cohere.Client(self.api_key)\n   164\u2192                self.initialized = True\n   165\u2192                logger.info(f\"Initialized Cohere reranker with model: {self.model}\")\n   166\u2192                return Result.ok(None)\n   167\u2192            except ImportError:\n   168\u2192                return Result.error(\n   169\u2192                    \"Cohere library not installed. Install with: pip install cohere\"\n   170\u2192                )\n   171\u2192        except Exception as e:\n   172\u2192            logger.error(f\"Failed to initialize Cohere reranker: {e}\")\n   173\u2192            return Result.error(f\"Initialization failed: {str(e)}\")\n   174\u2192    \n   175\u2192    async def shutdown(self) -> Result:\n   176\u2192        \"\"\"Shutdown Cohere client\"\"\"\n   177\u2192        self.client = None\n   178\u2192        self.initialized = False\n   179\u2192        return Result.ok(None)\n   180\u2192    \n   181\u2192    async def rerank(\n   182\u2192        self, \n   183\u2192        query: str, \n   184\u2192        results: List[SearchResult], \n   185\u2192        top_k: Optional[int] = None\n   186\u2192    ) -> Result:\n   187\u2192        \"\"\"Rerank results using Cohere API\"\"\"\n   188\u2192        if not self.initialized:\n   189\u2192            return Result.error(\"Cohere reranker not initialized\")\n   190\u2192        \n   191\u2192        # Check cache first\n   192\u2192        cached = await self._get_cached_results(query, results)\n   193\u2192        if cached:\n   194\u2192            rerank_result = RerankResult(\n   195\u2192                results=cached[:top_k] if top_k else cached,\n   196\u2192                metadata={\n   197\u2192                    'reranker': 'cohere',\n   198\u2192                    'model': self.model,\n   199\u2192                    'from_cache': True,\n   200\u2192                    'total_results': len(results),\n   201\u2192                    'returned_results': len(cached[:top_k]) if top_k else len(cached)\n   202\u2192                }\n   203\u2192            )\n   204\u2192            return Result.ok(rerank_result)\n   205\u2192        \n   206\u2192        try:\n   207\u2192            # Prepare documents for reranking\n   208\u2192            documents = []\n   209\u2192            for result in results:\n   210\u2192                # Combine relevant information for reranking\n   211\u2192                doc_text = f\"{result.snippet}\"\n   212\u2192                if result.context:\n   213\u2192                    doc_text = f\"{doc_text} {result.context}\"\n   214\u2192                documents.append(doc_text)\n   215\u2192            \n   216\u2192            # Call Cohere rerank API\n   217\u2192            response = await asyncio.to_thread(\n   218\u2192                self.client.rerank,\n   219\u2192                model=self.model,\n   220\u2192                query=query,\n   221\u2192                documents=documents,\n   222\u2192                top_n=top_k or len(results)\n   223\u2192            )\n   224\u2192            \n   225\u2192            # Build reranked results\n   226\u2192            reranked_items = []\n   227\u2192            for idx, result in enumerate(response.results):\n   228\u2192                original_idx = result.index\n   229\u2192                original_result = results[original_idx]\n   230\u2192                \n   231\u2192                rerank_item = RerankItem(\n   232\u2192                    original_result=original_result,\n   233\u2192                    rerank_score=result.relevance_score,\n   234\u2192                    original_rank=original_idx,\n   235\u2192                    new_rank=idx\n   236\u2192                )\n   237\u2192                reranked_items.append(rerank_item)\n   238\u2192            \n   239\u2192            # Cache results\n   240\u2192            await self._cache_results(query, results, reranked_items)\n   241\u2192            \n   242\u2192            # Create RerankResult with metadata\n   243\u2192            rerank_result = RerankResult(\n   244\u2192                results=reranked_items,\n   245\u2192                metadata={\n   246\u2192                    'reranker': 'cohere',\n   247\u2192                    'model': self.model,\n   248\u2192                    'from_cache': False,\n   249\u2192                    'total_results': len(results),\n   250\u2192                    'returned_results': len(reranked_items)\n   251\u2192                }\n   252\u2192            )\n   253\u2192            \n   254\u2192            return Result.ok(rerank_result)\n   255\u2192            \n   256\u2192        except Exception as e:\n   257\u2192            logger.error(f\"Cohere reranking failed: {e}\")\n   258\u2192            return Result.error(f\"Reranking failed: {str(e)}\")\n   259\u2192    \n   260\u2192    def get_capabilities(self) -> Dict[str, Any]:\n   261\u2192        \"\"\"Get Cohere reranker capabilities\"\"\"\n   262\u2192        return {\n   263\u2192            'name': 'Cohere Reranker',\n   264\u2192            'model': self.model,\n   265\u2192            'supports_multilingual': self.model.startswith('rerank-multilingual'),\n   266\u2192            'max_documents': 1000,\n   267\u2192            'requires_api_key': True,\n   268\u2192            'initialized': self.initialized\n   269\u2192        }\n   270\u2192\n   271\u2192\n   272\u2192class LocalCrossEncoderReranker(BaseReranker):\n   273\u2192    \"\"\"Local reranker using cross-encoder models\"\"\"\n   274\u2192    \n   275\u2192    def __init__(self, config: Dict[str, Any]):\n   276\u2192        super().__init__(config)\n   277\u2192        self.model_name = config.get('model', 'cross-encoder/ms-marco-MiniLM-L-6-v2')\n   278\u2192        self.device = config.get('device', 'cpu')\n   279\u2192        self.model = None\n   280\u2192        self.tokenizer = None\n   281\u2192    \n   282\u2192    async def initialize(self, config: Dict[str, Any]) -> Result:\n   283\u2192        \"\"\"Initialize cross-encoder model\"\"\"\n   284\u2192        try:\n   285\u2192            # Lazy import to avoid dependency if not used\n   286\u2192            try:\n   287\u2192                from sentence_transformers import CrossEncoder\n   288\u2192                \n   289\u2192                logger.info(f\"Loading cross-encoder model: {self.model_name}\")\n   290\u2192                self.model = CrossEncoder(self.model_name, device=self.device)\n   291\u2192                self.initialized = True\n   292\u2192                logger.info(f\"Initialized local cross-encoder reranker on {self.device}\")\n   293\u2192                return Result.ok(None)\n   294\u2192                \n   295\u2192            except ImportError:\n   296\u2192                return Result.error(\n   297\u2192                    \"Sentence-transformers library not installed. \"\n   298\u2192                    \"Install with: pip install sentence-transformers\"\n   299\u2192                )\n   300\u2192        except Exception as e:\n   301\u2192            logger.error(f\"Failed to initialize cross-encoder: {e}\")\n   302\u2192            return Result.error(f\"Initialization failed: {str(e)}\")\n   303\u2192    \n   304\u2192    async def shutdown(self) -> Result:\n   305\u2192        \"\"\"Shutdown cross-encoder model\"\"\"\n   306\u2192        self.model = None\n   307\u2192        self.initialized = False\n   308\u2192        return Result.ok(None)\n   309\u2192    \n   310\u2192    async def rerank(\n   311\u2192        self, \n   312\u2192        query: str, \n   313\u2192        results: List[SearchResult], \n   314\u2192        top_k: Optional[int] = None\n   315\u2192    ) -> Result:\n   316\u2192        \"\"\"Rerank results using cross-encoder model\"\"\"\n   317\u2192        if not self.initialized:\n   318\u2192            return Result.error(\"Cross-encoder reranker not initialized\")\n   319\u2192        \n   320\u2192        # Check cache first\n   321\u2192        cached = await self._get_cached_results(query, results)\n   322\u2192        if cached:\n   323\u2192            rerank_result = RerankResult(\n   324\u2192                results=cached[:top_k] if top_k else cached,\n   325\u2192                metadata={\n   326\u2192                    'reranker': 'cross-encoder',\n   327\u2192                    'model': self.model_name,\n   328\u2192                    'device': self.device,\n   329\u2192                    'from_cache': True,\n   330\u2192                    'total_results': len(results),\n   331\u2192                    'returned_results': len(cached[:top_k]) if top_k else len(cached)\n   332\u2192                }\n   333\u2192            )\n   334\u2192            return Result.ok(rerank_result)\n   335\u2192        \n   336\u2192        try:\n   337\u2192            # Prepare query-document pairs\n   338\u2192            pairs = []\n   339\u2192            for result in results:\n   340\u2192                # Combine relevant information for reranking\n   341\u2192                doc_text = f\"{result.snippet}\"\n   342\u2192                if result.context:\n   343\u2192                    doc_text = f\"{doc_text} {result.context}\"\n   344\u2192                pairs.append([query, doc_text])\n   345\u2192            \n   346\u2192            # Get scores from cross-encoder\n   347\u2192            scores = await asyncio.to_thread(self.model.predict, pairs)\n   348\u2192            \n   349\u2192            # Create indexed scores for sorting\n   350\u2192            indexed_scores = [(score, idx) for idx, score in enumerate(scores)]\n   351\u2192            indexed_scores.sort(reverse=True, key=lambda x: x[0])\n   352\u2192            \n   353\u2192            # Build reranked results\n   354\u2192            reranked_items = []\n   355\u2192            for new_rank, (score, original_idx) in enumerate(indexed_scores):\n   356\u2192                if top_k and new_rank >= top_k:\n   357\u2192                    break\n   358\u2192                \n   359\u2192                rerank_item = RerankItem(\n   360\u2192                    original_result=results[original_idx],\n   361\u2192                    rerank_score=float(score),\n   362\u2192                    original_rank=original_idx,\n   363\u2192                    new_rank=new_rank\n   364\u2192                )\n   365\u2192                reranked_items.append(rerank_item)\n   366\u2192            \n   367\u2192            # Cache results\n   368\u2192            await self._cache_results(query, results, reranked_items)\n   369\u2192            \n   370\u2192            # Create RerankResult with metadata\n   371\u2192            rerank_result = RerankResult(\n   372\u2192                results=reranked_items,\n   373\u2192                metadata={\n   374\u2192                    'reranker': 'cross-encoder',\n   375\u2192                    'model': self.model_name,\n   376\u2192                    'device': self.device,\n   377\u2192                    'from_cache': False,\n   378\u2192                    'total_results': len(results),\n   379\u2192                    'returned_results': len(reranked_items)\n   380\u2192                }\n   381\u2192            )\n   382\u2192            \n   383\u2192            return Result.ok(rerank_result)\n   384\u2192            \n   385\u2192        except Exception as e:\n   386\u2192            logger.error(f\"Cross-encoder reranking failed: {e}\")\n   387\u2192            return Result.error(f\"Reranking failed: {str(e)}\")\n   388\u2192    \n   389\u2192    def get_capabilities(self) -> Dict[str, Any]:\n   390\u2192        \"\"\"Get cross-encoder reranker capabilities\"\"\"\n   391\u2192        return {\n   392\u2192            'name': 'Local Cross-Encoder Reranker',\n   393\u2192            'model': self.model_name,\n   394\u2192            'device': self.device,\n   395\u2192            'supports_multilingual': 'multilingual' in self.model_name.lower(),\n   396\u2192            'max_documents': 10000,  # Limited by memory\n   397\u2192            'requires_api_key': False,\n   398\u2192            'initialized': self.initialized\n   399\u2192        }\n   400\u2192\n   401\u2192\n   402\u2192class TFIDFReranker(BaseReranker):\n   403\u2192    \"\"\"Simple TF-IDF based reranker as lightweight fallback\"\"\"\n   404\u2192    \n   405\u2192    def __init__(self, config: Dict[str, Any]):\n   406\u2192        super().__init__(config)\n   407\u2192        self.vectorizer = None\n   408\u2192    \n   409\u2192    async def initialize(self, config: Dict[str, Any]) -> Result:\n   410\u2192        \"\"\"Initialize TF-IDF vectorizer\"\"\"\n   411\u2192        try:\n   412\u2192            from sklearn.feature_extraction.text import TfidfVectorizer\n   413\u2192            from sklearn.metrics.pairwise import cosine_similarity\n   414\u2192            \n   415\u2192            self.vectorizer = TfidfVectorizer(\n   416\u2192                max_features=config.get('max_features', 5000),\n   417\u2192                ngram_range=(1, 2),\n   418\u2192                stop_words='english'\n   419\u2192            )\n   420\u2192            self.cosine_similarity = cosine_similarity\n   421\u2192            self.initialized = True\n   422\u2192            logger.info(\"Initialized TF-IDF reranker\")\n   423\u2192            return Result.ok(None)\n   424\u2192            \n   425\u2192        except ImportError:\n   426\u2192            return Result.error(\n   427\u2192                \"Scikit-learn not installed. Install with: pip install scikit-learn\"\n   428\u2192            )\n   429\u2192        except Exception as e:\n   430\u2192            logger.error(f\"Failed to initialize TF-IDF reranker: {e}\")\n   431\u2192            return Result.error(f\"Initialization failed: {str(e)}\")\n   432\u2192    \n   433\u2192    async def shutdown(self) -> Result:\n   434\u2192        \"\"\"Shutdown TF-IDF reranker\"\"\"\n   435\u2192        self.vectorizer = None\n   436\u2192        self.initialized = False\n   437\u2192        return Result.ok(None)\n   438\u2192    \n   439\u2192    async def rerank(\n   440\u2192        self, \n   441\u2192        query: str, \n   442\u2192        results: List[SearchResult], \n   443\u2192        top_k: Optional[int] = None\n   444\u2192    ) -> Result:\n   445\u2192        \"\"\"Rerank results using TF-IDF similarity\"\"\"\n   446\u2192        if not self.initialized:\n   447\u2192            return Result.error(\"TF-IDF reranker not initialized\")\n   448\u2192        \n   449\u2192        # Check cache first\n   450\u2192        cached = await self._get_cached_results(query, results)\n   451\u2192        if cached:\n   452\u2192            rerank_result = RerankResult(\n   453\u2192                results=cached[:top_k] if top_k else cached,\n   454\u2192                metadata={\n   455\u2192                    'reranker': 'tfidf',\n   456\u2192                    'max_features': self.config.get('max_features', 5000),\n   457\u2192                    'from_cache': True,\n   458\u2192                    'total_results': len(results),\n   459\u2192                    'returned_results': len(cached[:top_k]) if top_k else len(cached)\n   460\u2192                }\n   461\u2192            )\n   462\u2192            return Result.ok(rerank_result)\n   463\u2192        \n   464\u2192        try:\n   465\u2192            # Prepare documents\n   466\u2192            documents = []\n   467\u2192            for result in results:\n   468\u2192                doc_text = f\"{result.snippet}\"\n   469\u2192                if result.context:\n   470\u2192                    doc_text = f\"{doc_text} {result.context}\"\n   471\u2192                documents.append(doc_text)\n   472\u2192            \n   473\u2192            # Add query to documents for vectorization\n   474\u2192            all_texts = [query] + documents\n   475\u2192            \n   476\u2192            # Vectorize texts\n   477\u2192            tfidf_matrix = await asyncio.to_thread(\n   478\u2192                self.vectorizer.fit_transform, all_texts\n   479\u2192            )\n   480\u2192            \n   481\u2192            # Calculate similarities\n   482\u2192            query_vector = tfidf_matrix[0:1]\n   483\u2192            doc_vectors = tfidf_matrix[1:]\n   484\u2192            similarities = self.cosine_similarity(query_vector, doc_vectors)[0]\n   485\u2192            \n   486\u2192            # Create indexed scores for sorting\n   487\u2192            indexed_scores = [(score, idx) for idx, score in enumerate(similarities)]\n   488\u2192            indexed_scores.sort(reverse=True, key=lambda x: x[0])\n   489\u2192            \n   490\u2192            # Build reranked results\n   491\u2192            reranked_items = []\n   492\u2192            for new_rank, (score, original_idx) in enumerate(indexed_scores):\n   493\u2192                if top_k and new_rank >= top_k:\n   494\u2192                    break\n   495\u2192                \n   496\u2192                rerank_item = RerankItem(\n   497\u2192                    original_result=results[original_idx],\n   498\u2192                    rerank_score=float(score),\n   499\u2192                    original_rank=original_idx,\n   500\u2192                    new_rank=new_rank\n   501\u2192                )\n   502\u2192                reranked_items.append(rerank_item)\n   503\u2192            \n   504\u2192            # Cache results\n   505\u2192            await self._cache_results(query, results, reranked_items)\n   506\u2192            \n   507\u2192            # Create RerankResult with metadata\n   508\u2192            rerank_result = RerankResult(\n   509\u2192                results=reranked_items,\n   510\u2192                metadata={\n   511\u2192                    'reranker': 'tfidf',\n   512\u2192                    'max_features': self.config.get('max_features', 5000),\n   513\u2192                    'from_cache': False,\n   514\u2192                    'total_results': len(results),\n   515\u2192                    'returned_results': len(reranked_items)\n   516\u2192                }\n   517\u2192            )\n   518\u2192            \n   519\u2192            return Result.ok(rerank_result)\n   520\u2192            \n   521\u2192        except Exception as e:\n   522\u2192            logger.error(f\"TF-IDF reranking failed: {e}\")\n   523\u2192            return Result.error(f\"Reranking failed: {str(e)}\")\n   524\u2192    \n   525\u2192    def get_capabilities(self) -> Dict[str, Any]:\n   526\u2192        \"\"\"Get TF-IDF reranker capabilities\"\"\"\n   527\u2192        return {\n   528\u2192            'name': 'TF-IDF Reranker',\n   529\u2192            'algorithm': 'TF-IDF with cosine similarity',\n   530\u2192            'supports_multilingual': False,\n   531\u2192            'max_documents': 100000,\n   532\u2192            'requires_api_key': False,\n   533\u2192            'initialized': self.initialized\n   534\u2192        }\n   535\u2192\n   536\u2192\n   537\u2192class HybridReranker(BaseReranker):\n   538\u2192    \"\"\"Hybrid reranker that combines multiple reranking strategies\"\"\"\n   539\u2192    \n   540\u2192    def __init__(self, config: Dict[str, Any]):\n   541\u2192        super().__init__(config)\n   542\u2192        self.primary_reranker: Optional[IReranker] = None\n   543\u2192        self.fallback_reranker: Optional[IReranker] = None\n   544\u2192        self.weight_primary = config.get('weight_primary', 0.7)\n   545\u2192        self.weight_fallback = config.get('weight_fallback', 0.3)\n   546\u2192    \n   547\u2192    def set_rerankers(\n   548\u2192        self, \n   549\u2192        primary: IReranker, \n   550\u2192        fallback: Optional[IReranker] = None\n   551\u2192    ):\n   552\u2192        \"\"\"Set primary and fallback rerankers\"\"\"\n   553\u2192        self.primary_reranker = primary\n   554\u2192        self.fallback_reranker = fallback\n   555\u2192    \n   556\u2192    async def initialize(self, config: Dict[str, Any]) -> Result:\n   557\u2192        \"\"\"Initialize hybrid reranker\"\"\"\n   558\u2192        if not self.primary_reranker:\n   559\u2192            return Result.error(\"Primary reranker not set\")\n   560\u2192        \n   561\u2192        # Initialize primary reranker\n   562\u2192        primary_result = await self.primary_reranker.initialize(config)\n   563\u2192        if not primary_result.is_success:\n   564\u2192            return primary_result\n   565\u2192        \n   566\u2192        # Initialize fallback if available\n   567\u2192        if self.fallback_reranker:\n   568\u2192            fallback_result = await self.fallback_reranker.initialize(config)\n   569\u2192            if not fallback_result.is_success:\n   570\u2192                logger.warning(f\"Fallback reranker initialization failed: {fallback_result.error}\")\n   571\u2192        \n   572\u2192        self.initialized = True\n   573\u2192        return Result.ok(None)\n   574\u2192    \n   575\u2192    async def shutdown(self) -> Result:\n   576\u2192        \"\"\"Shutdown hybrid reranker\"\"\"\n   577\u2192        if self.primary_reranker:\n   578\u2192            await self.primary_reranker.shutdown()\n   579\u2192        if self.fallback_reranker:\n   580\u2192            await self.fallback_reranker.shutdown()\n   581\u2192        self.initialized = False\n   582\u2192        return Result.ok(None)\n   583\u2192    \n   584\u2192    async def rerank(\n   585\u2192        self, \n   586\u2192        query: str, \n   587\u2192        results: List[SearchResult], \n   588\u2192        top_k: Optional[int] = None\n   589\u2192    ) -> Result:\n   590\u2192        \"\"\"Rerank using hybrid approach\"\"\"\n   591\u2192        if not self.initialized:\n   592\u2192            return Result.error(\"Hybrid reranker not initialized\")\n   593\u2192        \n   594\u2192        # Try primary reranker first\n   595\u2192        primary_result = await self.primary_reranker.rerank(query, results, top_k)\n   596\u2192        \n   597\u2192        if primary_result.is_success:\n   598\u2192            # Update metadata to indicate hybrid reranker was used\n   599\u2192            if isinstance(primary_result.data, RerankResult):\n   600\u2192                primary_result.data.metadata['hybrid'] = True\n   601\u2192                primary_result.data.metadata['primary_succeeded'] = True\n   602\u2192            return primary_result\n   603\u2192        \n   604\u2192        # If primary fails and we have fallback, use it\n   605\u2192        if self.fallback_reranker:\n   606\u2192            logger.warning(\n   607\u2192                f\"Primary reranker failed: {primary_result.error}, using fallback\"\n   608\u2192            )\n   609\u2192            fallback_result = await self.fallback_reranker.rerank(query, results, top_k)\n   610\u2192            if fallback_result.is_success and isinstance(fallback_result.data, RerankResult):\n   611\u2192                fallback_result.data.metadata['hybrid'] = True\n   612\u2192                fallback_result.data.metadata['primary_succeeded'] = False\n   613\u2192                fallback_result.data.metadata['fallback_reason'] = str(primary_result.error)\n   614\u2192            return fallback_result\n   615\u2192        \n   616\u2192        return primary_result\n   617\u2192    \n   618\u2192    def get_capabilities(self) -> Dict[str, Any]:\n   619\u2192        \"\"\"Get hybrid reranker capabilities\"\"\"\n   620\u2192        capabilities = {\n   621\u2192            'name': 'Hybrid Reranker',\n   622\u2192            'primary': self.primary_reranker.get_capabilities() if self.primary_reranker else None,\n   623\u2192            'fallback': self.fallback_reranker.get_capabilities() if self.fallback_reranker else None,\n   624\u2192            'weight_primary': self.weight_primary,\n   625\u2192            'weight_fallback': self.weight_fallback,\n   626\u2192            'initialized': self.initialized\n   627\u2192        }\n   628\u2192        return capabilities\n   629\u2192\n   630\u2192\n   631\u2192class RerankerFactory(IRerankerFactory):\n   632\u2192    \"\"\"Factory for creating reranker instances\"\"\"\n   633\u2192    \n   634\u2192    def __init__(self):\n   635\u2192        self.reranker_types = {\n   636\u2192            'cohere': CohereReranker,\n   637\u2192            'cross-encoder': LocalCrossEncoderReranker,\n   638\u2192            'tfidf': TFIDFReranker,\n   639\u2192            'hybrid': HybridReranker\n   640\u2192        }\n   641\u2192    \n   642\u2192    def create_reranker(self, reranker_type: str, config: Dict[str, Any]) -> IReranker:\n   643\u2192        \"\"\"Create a reranker instance\"\"\"\n   644\u2192        if reranker_type not in self.reranker_types:\n   645\u2192            raise ValueError(f\"Unknown reranker type: {reranker_type}\")\n   646\u2192        \n   647\u2192        reranker_class = self.reranker_types[reranker_type]\n   648\u2192        reranker = reranker_class(config)\n   649\u2192        \n   650\u2192        # Special handling for hybrid reranker\n   651\u2192        if reranker_type == 'hybrid':\n   652\u2192            primary_type = config.get('primary_type', 'cohere')\n   653\u2192            fallback_type = config.get('fallback_type', 'tfidf')\n   654\u2192            \n   655\u2192            primary = self.create_reranker(primary_type, config)\n   656\u2192            fallback = self.create_reranker(fallback_type, config) if fallback_type else None\n   657\u2192            \n   658\u2192            reranker.set_rerankers(primary, fallback)\n   659\u2192        \n   660\u2192        return reranker\n   661\u2192    \n   662\u2192    def get_available_rerankers(self) -> List[str]:\n   663\u2192        \"\"\"Get list of available reranker types\"\"\"\n   664\u2192        return list(self.reranker_types.keys())\n   665\u2192    \n   666\u2192    def register_reranker(self, name: str, reranker_class: type):\n   667\u2192        \"\"\"Register a custom reranker type\"\"\"\n   668\u2192        if not issubclass(reranker_class, IReranker):\n   669\u2192            raise ValueError(f\"{reranker_class} must implement IReranker interface\")\n   670\u2192        self.reranker_types[name] = reranker_class\n   671\u2192\n   672\u2192\n   673\u2192# Default factory instance\n   674\u2192default_reranker_factory = RerankerFactory()\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 8,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 237,
    "output_tokens_total": 237,
    "cache_read_tokens": 22499,
    "cache_creation_tokens": 36743,
    "tokens_per_result": 237,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "a9f87083-f603-4b60-a151-8cabc58f385f",
    "test_date": "2025-06-26T07:40:30.654Z",
    "query_id": "transcript_a9f87083-f603-4b60-a151-8cabc58f385f",
    "query_text": [
      {
        "tool_use_id": "toolu_01BuEsq9qhx1CyeSJedDYa97",
        "type": "tool_result",
        "content": "     1\u2192from fastapi import FastAPI, HTTPException, Depends, status, Response\n     2\u2192from fastapi.responses import JSONResponse, PlainTextResponse\n     3\u2192from typing import Optional, Dict, Any, List\n     4\u2192from pathlib import Path\n     5\u2192import logging\n     6\u2192import os\n     7\u2192import time\n     8\u2192import threading\n     9\u2192from .dispatcher import Dispatcher\n    10\u2192from .plugin_base import SymbolDef, SearchResult\n    11\u2192from .storage.sqlite_store import SQLiteStore\n    12\u2192from .watcher import FileWatcher\n    13\u2192from .core.logging import setup_logging\n    14\u2192from .utils.index_discovery import IndexDiscovery\n    15\u2192from .plugin_system import PluginManager, PluginSystemConfig\n    16\u2192from .indexer.bm25_indexer import BM25Indexer\n    17\u2192from .indexer.hybrid_search import HybridSearch, HybridSearchConfig\n    18\u2192from .utils.fuzzy_indexer import FuzzyIndexer\n    19\u2192from .security import (\n    20\u2192    SecurityConfig, AuthManager, SecurityMiddlewareStack,\n    21\u2192    AuthCredentials, User, UserRole, Permission,\n    22\u2192    get_current_user, get_current_active_user, require_permission, require_role\n    23\u2192)\n    24\u2192from .metrics import (\n    25\u2192    get_metrics_collector, get_health_checker, HealthStatus\n    26\u2192)\n    27\u2192from .metrics.middleware import setup_metrics_middleware, get_business_metrics\n    28\u2192from .metrics.prometheus_exporter import get_prometheus_exporter\n    29\u2192from .cache import (\n    30\u2192    CacheManagerFactory, CacheConfig, CacheBackendType, \n    31\u2192    QueryResultCache, QueryCacheConfig, QueryType\n    32\u2192)\n    33\u2192\n    34\u2192# Set up logging\n    35\u2192setup_logging(log_level=\"INFO\")\n    36\u2192logger = logging.getLogger(__name__)\n    37\u2192\n    38\u2192app = FastAPI(title=\"MCP Server\", description=\"Code Index MCP Server with Security, Metrics, and Health Checks\")\n    39\u2192dispatcher: Dispatcher | None = None\n    40\u2192sqlite_store: SQLiteStore | None = None\n    41\u2192file_watcher: FileWatcher | None = None\n    42\u2192plugin_manager: PluginManager | None = None\n    43\u2192plugin_loader = None  # Dynamic plugin loader\n    44\u2192auth_manager: AuthManager | None = None\n    45\u2192security_config: SecurityConfig | None = None\n    46\u2192cache_manager = None\n    47\u2192query_cache: QueryResultCache | None = None\n    48\u2192bm25_indexer: BM25Indexer | None = None\n    49\u2192hybrid_search: HybridSearch | None = None\n    50\u2192fuzzy_indexer: FuzzyIndexer | None = None\n    51\u2192\n    52\u2192# Initialize metrics and health checking\n    53\u2192metrics_collector = get_metrics_collector()\n    54\u2192health_checker = get_health_checker()\n    55\u2192business_metrics = get_business_metrics()\n    56\u2192\n    57\u2192# Setup metrics middleware\n    58\u2192setup_metrics_middleware(app, enable_detailed_metrics=True)\n    59\u2192\n    60\u2192@app.on_event(\"startup\")\n    61\u2192async def startup_event():\n    62\u2192    \"\"\"Initialize the dispatcher and register plugins on startup.\"\"\"\n    63\u2192    global dispatcher, sqlite_store, file_watcher, plugin_manager, plugin_loader, auth_manager, security_config, cache_manager, query_cache, bm25_indexer, hybrid_search, fuzzy_indexer\n    64\u2192    \n    65\u2192    try:\n    66\u2192        # Initialize security configuration\n    67\u2192        logger.info(\"Initializing security configuration...\")\n    68\u2192        security_config = SecurityConfig(\n    69\u2192            jwt_secret_key=os.getenv(\"JWT_SECRET_KEY\", \"your-super-secret-jwt-key-change-in-production-min-32-chars\"),\n    70\u2192            jwt_algorithm=\"HS256\",\n    71\u2192            access_token_expire_minutes=int(os.getenv(\"ACCESS_TOKEN_EXPIRE_MINUTES\", \"30\")),\n    72\u2192            refresh_token_expire_days=int(os.getenv(\"REFRESH_TOKEN_EXPIRE_DAYS\", \"7\")),\n    73\u2192            password_min_length=int(os.getenv(\"PASSWORD_MIN_LENGTH\", \"8\")),\n    74\u2192            max_login_attempts=int(os.getenv(\"MAX_LOGIN_ATTEMPTS\", \"5\")),\n    75\u2192            lockout_duration_minutes=int(os.getenv(\"LOCKOUT_DURATION_MINUTES\", \"15\")),\n    76\u2192            rate_limit_requests=int(os.getenv(\"RATE_LIMIT_REQUESTS\", \"100\")),\n    77\u2192            rate_limit_window_minutes=int(os.getenv(\"RATE_LIMIT_WINDOW_MINUTES\", \"1\")),\n    78\u2192            cors_origins=os.getenv(\"CORS_ORIGINS\", \"*\").split(\",\"),\n    79\u2192            cors_methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\"],\n    80\u2192            cors_headers=[\"*\"]\n    81\u2192        )\n    82\u2192        \n    83\u2192        # Initialize authentication manager\n    84\u2192        logger.info(\"Initializing authentication manager...\")\n    85\u2192        auth_manager = AuthManager(security_config)\n    86\u2192        \n    87\u2192        # Create default admin user if it doesn't exist\n    88\u2192        admin_user = await auth_manager.get_user_by_username(\"admin\")\n    89\u2192        if not admin_user:\n    90\u2192            admin_password = os.getenv(\"DEFAULT_ADMIN_PASSWORD\", \"admin123!\")\n    91\u2192            logger.info(\"Creating default admin user...\")\n    92\u2192            await auth_manager.create_user(\n    93\u2192                username=\"admin\",\n    94\u2192                password=admin_password,\n    95\u2192                email=os.getenv(\"DEFAULT_ADMIN_EMAIL\", \"admin@localhost\"),\n    96\u2192                role=UserRole.ADMIN\n    97\u2192            )\n    98\u2192            logger.info(\"Default admin user created\")\n    99\u2192        \n   100\u2192        # Set up security middleware\n   101\u2192        logger.info(\"Setting up security middleware...\")\n   102\u2192        security_middleware = SecurityMiddlewareStack(app, security_config, auth_manager)\n   103\u2192        security_middleware.setup_middleware()\n   104\u2192        logger.info(\"Security middleware configured successfully\")\n   105\u2192        \n   106\u2192        # Initialize cache system\n   107\u2192        logger.info(\"Initializing cache system...\")\n   108\u2192        cache_backend_type = os.getenv(\"CACHE_BACKEND\", \"memory\").lower()\n   109\u2192        redis_url = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n   110\u2192        \n   111\u2192        if cache_backend_type == \"redis\":\n   112\u2192            try:\n   113\u2192                cache_manager = CacheManagerFactory.create_redis_cache(\n   114\u2192                    redis_url=redis_url,\n   115\u2192                    default_ttl=int(os.getenv(\"CACHE_DEFAULT_TTL\", \"3600\"))\n   116\u2192                )\n   117\u2192                logger.info(\"Using Redis cache backend\")\n   118\u2192            except Exception as e:\n   119\u2192                logger.warning(f\"Failed to initialize Redis cache, falling back to memory: {e}\")\n   120\u2192                cache_manager = CacheManagerFactory.create_memory_cache()\n   121\u2192        elif cache_backend_type == \"hybrid\":\n   122\u2192            try:\n   123\u2192                cache_manager = CacheManagerFactory.create_hybrid_cache(\n   124\u2192                    redis_url=redis_url,\n   125\u2192                    max_entries=int(os.getenv(\"CACHE_MAX_ENTRIES\", \"1000\")),\n   126\u2192                    max_memory_mb=int(os.getenv(\"CACHE_MAX_MEMORY_MB\", \"100\")),\n   127\u2192                    default_ttl=int(os.getenv(\"CACHE_DEFAULT_TTL\", \"3600\"))\n   128\u2192                )\n   129\u2192                logger.info(\"Using hybrid cache backend\")\n   130\u2192            except Exception as e:\n   131\u2192                logger.warning(f\"Failed to initialize hybrid cache, falling back to memory: {e}\")\n   132\u2192                cache_manager = CacheManagerFactory.create_memory_cache()\n   133\u2192        else:\n   134\u2192            cache_manager = CacheManagerFactory.create_memory_cache(\n   135\u2192                max_entries=int(os.getenv(\"CACHE_MAX_ENTRIES\", \"1000\")),\n   136\u2192                max_memory_mb=int(os.getenv(\"CACHE_MAX_MEMORY_MB\", \"100\")),\n   137\u2192                default_ttl=int(os.getenv(\"CACHE_DEFAULT_TTL\", \"3600\"))\n   138\u2192            )\n   139\u2192            logger.info(\"Using memory cache backend\")\n   140\u2192        \n   141\u2192        await cache_manager.initialize()\n   142\u2192        \n   143\u2192        # Initialize query result cache\n   144\u2192        query_cache_config = QueryCacheConfig(\n   145\u2192            enabled=os.getenv(\"QUERY_CACHE_ENABLED\", \"true\").lower() == \"true\",\n   146\u2192            default_ttl=int(os.getenv(\"QUERY_CACHE_DEFAULT_TTL\", \"300\")),\n   147\u2192            symbol_lookup_ttl=int(os.getenv(\"QUERY_CACHE_SYMBOL_TTL\", \"1800\")),\n   148\u2192            search_ttl=int(os.getenv(\"QUERY_CACHE_SEARCH_TTL\", \"600\")),\n   149\u2192            semantic_search_ttl=int(os.getenv(\"QUERY_CACHE_SEMANTIC_TTL\", \"3600\"))\n   150\u2192        )\n   151\u2192        query_cache = QueryResultCache(cache_manager, query_cache_config)\n   152\u2192        logger.info(\"Query result cache initialized successfully\")\n   153\u2192        # Check for portable index first\n   154\u2192        workspace_root = Path(\".\")\n   155\u2192        discovery = IndexDiscovery(workspace_root)\n   156\u2192        \n   157\u2192        if discovery.is_index_enabled():\n   158\u2192            logger.info(\"MCP portable index detected\")\n   159\u2192            \n   160\u2192            # Try to use existing index\n   161\u2192            index_path = discovery.get_local_index_path()\n   162\u2192            \n   163\u2192            if not index_path and discovery.should_download_index():\n   164\u2192                logger.info(\"Attempting to download index from GitHub artifacts...\")\n   165\u2192                if discovery.download_latest_index():\n   166\u2192                    index_path = discovery.get_local_index_path()\n   167\u2192                    logger.info(\"Successfully downloaded index from artifacts\")\n   168\u2192                else:\n   169\u2192                    logger.info(\"Could not download index, will use default\")\n   170\u2192            \n   171\u2192            if index_path:\n   172\u2192                logger.info(f\"Using portable index: {index_path}\")\n   173\u2192                sqlite_store = SQLiteStore(str(index_path))\n   174\u2192                \n   175\u2192                # Log index info\n   176\u2192                info = discovery.get_index_info()\n   177\u2192                if info['metadata']:\n   178\u2192                    meta = info['metadata']\n   179\u2192                    logger.info(f\"Index created: {meta.get('created_at', 'unknown')}\")\n   180\u2192                    logger.info(f\"Index commit: {meta.get('commit', 'unknown')[:8]}\")\n   181\u2192            else:\n   182\u2192                logger.info(\"No portable index found, using default\")\n   183\u2192                sqlite_store = SQLiteStore(\"code_index.db\")\n   184\u2192        else:\n   185\u2192            # Initialize SQLite store with default\n   186\u2192            logger.info(\"Initializing SQLite store with default path...\")\n   187\u2192            sqlite_store = SQLiteStore(\"code_index.db\")\n   188\u2192        \n   189\u2192        logger.info(\"SQLite store initialized successfully\")\n   190\u2192        \n   191\u2192        # Initialize plugin system with dynamic discovery\n   192\u2192        logger.info(\"Initializing plugin system with dynamic discovery...\")\n   193\u2192        from .plugin_system.discovery import get_plugin_discovery\n   194\u2192        from .plugin_system.loader import get_plugin_loader\n   195\u2192        \n   196\u2192        # Discover all available plugins\n   197\u2192        plugin_discovery = get_plugin_discovery()\n   198\u2192        discovered = plugin_discovery.discover_plugins()\n   199\u2192        logger.info(f\"Discovered {len(discovered)} plugins: {list(discovered.keys())}\")\n   200\u2192        \n   201\u2192        # Initialize plugin loader\n   202\u2192        plugin_loader = get_plugin_loader()\n   203\u2192        \n   204\u2192        # Load plugins based on configuration or all discovered plugins\n   205\u2192        config_path = Path(\"plugins.yaml\")\n   206\u2192        if config_path.exists():\n   207\u2192            # Load specific plugins from config\n   208\u2192            import yaml\n   209\u2192            with open(config_path, 'r') as f:\n   210\u2192                plugin_config = yaml.safe_load(f)\n   211\u2192            \n   212\u2192            enabled_languages = plugin_config.get('enabled_languages', list(discovered.keys()))\n   213\u2192            logger.info(f\"Loading plugins for languages: {enabled_languages}\")\n   214\u2192        else:\n   215\u2192            # Load all discovered plugins\n   216\u2192            enabled_languages = list(discovered.keys())\n   217\u2192            logger.info(\"No plugins.yaml found, loading all discovered plugins\")\n   218\u2192        \n   219\u2192        # Load plugins\n   220\u2192        plugin_instances = []\n   221\u2192        for language in enabled_languages:\n   222\u2192            try:\n   223\u2192                plugin = plugin_loader.load_plugin(language)\n   224\u2192                if plugin:\n   225\u2192                    plugin_instances.append(plugin)\n   226\u2192                    logger.info(f\"Successfully loaded plugin for {language}\")\n   227\u2192            except Exception as e:\n   228\u2192                logger.error(f\"Failed to load plugin for {language}: {e}\")\n   229\u2192        \n   230\u2192        logger.info(f\"Loaded {len(plugin_instances)} plugins\")\n   231\u2192        \n   232\u2192        # Create plugin manager for compatibility\n   233\u2192        plugin_manager = PluginManager(sqlite_store=sqlite_store)\n   234\u2192        # Register loaded plugins with manager\n   235\u2192        for plugin in plugin_instances:\n   236\u2192            if hasattr(plugin, 'get_language'):\n   237\u2192                lang = plugin.get_language()\n   238\u2192                plugin_manager._plugins[lang] = plugin\n   239\u2192        \n   240\u2192        logger.info(f\"Loaded {len(plugin_instances)} active plugins\")\n   241\u2192        \n   242\u2192        # Create a new Dispatcher instance with the loaded plugins\n   243\u2192        logger.info(\"Creating dispatcher...\")\n   244\u2192        dispatcher = Dispatcher(plugin_instances)\n   245\u2192        logger.info(f\"Dispatcher created with {len(plugin_instances)} plugins\")\n   246\u2192        \n   247\u2192        # Initialize BM25 indexer\n   248\u2192        logger.info(\"Initializing BM25 indexer...\")\n   249\u2192        bm25_indexer = BM25Indexer(sqlite_store)\n   250\u2192        logger.info(\"BM25 indexer initialized successfully\")\n   251\u2192        \n   252\u2192        # Initialize Fuzzy indexer\n   253\u2192        logger.info(\"Initializing Fuzzy indexer...\")\n   254\u2192        fuzzy_indexer = FuzzyIndexer(sqlite_store)\n   255\u2192        logger.info(\"Fuzzy indexer initialized successfully\")\n   256\u2192        \n   257\u2192        # Check if semantic indexer is available\n   258\u2192        semantic_indexer = None\n   259\u2192        try:\n   260\u2192            from .utils.semantic_indexer import SemanticIndexer\n   261\u2192            semantic_indexer = SemanticIndexer(sqlite_store)\n   262\u2192            logger.info(\"Semantic indexer initialized successfully\")\n   263\u2192        except ImportError:\n   264\u2192            logger.warning(\"Semantic indexer not available (missing dependencies)\")\n   265\u2192        \n   266\u2192        # Initialize Hybrid Search\n   267\u2192        logger.info(\"Initializing Hybrid Search...\")\n   268\u2192        hybrid_config = HybridSearchConfig(\n   269\u2192            bm25_weight=float(os.getenv(\"HYBRID_BM25_WEIGHT\", \"0.5\")),\n   270\u2192            semantic_weight=float(os.getenv(\"HYBRID_SEMANTIC_WEIGHT\", \"0.3\")),\n   271\u2192            fuzzy_weight=float(os.getenv(\"HYBRID_FUZZY_WEIGHT\", \"0.2\")),\n   272\u2192            enable_bm25=True,\n   273\u2192            enable_semantic=semantic_indexer is not None,\n   274\u2192            enable_fuzzy=True,\n   275\u2192            parallel_execution=True,\n   276\u2192            cache_results=True\n   277\u2192        )\n   278\u2192        hybrid_search = HybridSearch(\n   279\u2192            storage=sqlite_store,\n   280\u2192            bm25_indexer=bm25_indexer,\n   281\u2192            semantic_indexer=semantic_indexer,\n   282\u2192            fuzzy_indexer=fuzzy_indexer,\n   283\u2192            config=hybrid_config\n   284\u2192        )\n   285\u2192        logger.info(f\"Hybrid Search initialized (BM25: {hybrid_config.enable_bm25}, Semantic: {hybrid_config.enable_semantic}, Fuzzy: {hybrid_config.enable_fuzzy})\")\n   286\u2192        \n   287\u2192        # Initialize file watcher with dispatcher and query cache\n   288\u2192        logger.info(\"Starting file watcher...\")\n   289\u2192        file_watcher = FileWatcher(Path(\".\"), dispatcher, query_cache)\n   290\u2192        file_watcher.start()\n   291\u2192        logger.info(\"File watcher started for current directory with cache invalidation\")\n   292\u2192        \n   293\u2192        # Store in app.state for potential future use\n   294\u2192        app.state.dispatcher = dispatcher\n   295\u2192        app.state.sqlite_store = sqlite_store\n   296\u2192        app.state.file_watcher = file_watcher\n   297\u2192        app.state.plugin_manager = plugin_manager\n   298\u2192        app.state.auth_manager = auth_manager\n   299\u2192        app.state.security_config = security_config\n   300\u2192        app.state.cache_manager = cache_manager\n   301\u2192        app.state.query_cache = query_cache\n   302\u2192        app.state.metrics_collector = metrics_collector\n   303\u2192        app.state.health_checker = health_checker\n   304\u2192        app.state.business_metrics = business_metrics\n   305\u2192        app.state.bm25_indexer = bm25_indexer\n   306\u2192        app.state.hybrid_search = hybrid_search\n   307\u2192        app.state.fuzzy_indexer = fuzzy_indexer\n   308\u2192        \n   309\u2192        # Update status to include search capabilities\n   310\u2192        search_capabilities = []\n   311\u2192        if bm25_indexer:\n   312\u2192            search_capabilities.append(\"bm25\")\n   313\u2192        if fuzzy_indexer:\n   314\u2192            search_capabilities.append(\"fuzzy\") \n   315\u2192        if semantic_indexer:\n   316\u2192            search_capabilities.append(\"semantic\")\n   317\u2192        if hybrid_search:\n   318\u2192            search_capabilities.append(\"hybrid\")\n   319\u2192        \n   320\u2192        logger.info(f\"Search capabilities: {', '.join(search_capabilities)}\")\n   321\u2192        \n   322\u2192        # Register health checks for system components\n   323\u2192        logger.info(\"Registering component health checks...\")\n   324\u2192        health_checker.register_health_check(\n   325\u2192            \"database\", \n   326\u2192            health_checker.create_database_health_check(\"code_index.db\")\n   327\u2192        )\n   328\u2192        health_checker.register_health_check(\n   329\u2192            \"plugins\",\n   330\u2192            health_checker.create_plugin_health_check(plugin_manager)\n   331\u2192        )\n   332\u2192        \n   333\u2192        # Update system metrics\n   334\u2192        business_metrics.update_system_metrics(\n   335\u2192            active_plugins=len(plugin_instances),\n   336\u2192            indexed_files=0,  # Will be updated as files are indexed\n   337\u2192            database_size=0,  # Will be updated periodically\n   338\u2192            memory_usage=0    # Will be updated by middleware\n   339\u2192        )\n   340\u2192        \n   341\u2192        # Log loaded plugins with detailed status\n   342\u2192        plugin_status = plugin_manager.get_detailed_plugin_status()\n   343\u2192        for name, status in plugin_status.items():\n   344\u2192            basic_info = status['basic_info']\n   345\u2192            runtime_info = status['runtime_info']\n   346\u2192            logger.info(f\"Plugin '{name}': {runtime_info['state']} (v{basic_info['version']}, language: {basic_info['language']}, enabled: {runtime_info['enabled']})\")\n   347\u2192            if runtime_info.get('error'):\n   348\u2192                logger.warning(f\"Plugin '{name}' has error: {runtime_info['error']}\")\n   349\u2192        \n   350\u2192        logger.info(\"MCP Server initialized successfully with dynamic plugin system, SQLite persistence, and file watcher\")\n   351\u2192    except Exception as e:\n   352\u2192        logger.error(f\"Failed to initialize MCP Server: {e}\", exc_info=True)\n   353\u2192        raise\n   354\u2192\n   355\u2192@app.on_event(\"shutdown\")\n   356\u2192async def shutdown_event():\n   357\u2192    \"\"\"Clean up resources on shutdown.\"\"\"\n   358\u2192    global file_watcher, plugin_manager, cache_manager\n   359\u2192    \n   360\u2192    if file_watcher:\n   361\u2192        try:\n   362\u2192            file_watcher.stop()\n   363\u2192            logger.info(\"File watcher stopped successfully\")\n   364\u2192        except Exception as e:\n   365\u2192            logger.error(f\"Error stopping file watcher: {e}\", exc_info=True)\n   366\u2192    \n   367\u2192    if plugin_manager:\n   368\u2192        try:\n   369\u2192            shutdown_result = plugin_manager.shutdown_safe()\n   370\u2192            if shutdown_result.success:\n   371\u2192                logger.info(\"Plugin manager shutdown successfully\")\n   372\u2192            else:\n   373\u2192                logger.error(f\"Plugin manager shutdown failed: {shutdown_result.error.message}\")\n   374\u2192                logger.error(f\"Shutdown error details: {shutdown_result.error.details}\")\n   375\u2192        except Exception as e:\n   376\u2192            logger.error(f\"Error shutting down plugin manager: {e}\", exc_info=True)\n   377\u2192    \n   378\u2192    if cache_manager:\n   379\u2192        try:\n   380\u2192            await cache_manager.shutdown()\n   381\u2192            logger.info(\"Cache manager shutdown successfully\")\n   382\u2192        except Exception as e:\n   383\u2192            logger.error(f\"Error shutting down cache manager: {e}\", exc_info=True)\n   384\u2192\n   385\u2192# Authentication endpoints\n   386\u2192\n   387\u2192@app.post(\"/api/v1/auth/login\")\n   388\u2192async def login(credentials: AuthCredentials) -> Dict[str, Any]:\n   389\u2192    \"\"\"User login endpoint.\"\"\"\n   390\u2192    if auth_manager is None:\n   391\u2192        raise HTTPException(503, \"Authentication service not ready\")\n   392\u2192    \n   393\u2192    try:\n   394\u2192        user = await auth_manager.authenticate_user(credentials)\n   395\u2192        if not user:\n   396\u2192            raise HTTPException(401, \"Invalid credentials\")\n   397\u2192        \n   398\u2192        access_token = await auth_manager.create_access_token(user)\n   399\u2192        refresh_token = await auth_manager.create_refresh_token(user)\n   400\u2192        \n   401\u2192        return {\n   402\u2192            \"access_token\": access_token,\n   403\u2192            \"refresh_token\": refresh_token,\n   404\u2192            \"token_type\": \"bearer\",\n   405\u2192            \"expires_in\": security_config.access_token_expire_minutes * 60,\n   406\u2192            \"user\": {\n   407\u2192                \"id\": user.id,\n   408\u2192                \"username\": user.username,\n   409\u2192                \"role\": user.role.value,\n   410\u2192                \"permissions\": [p.value for p in user.permissions]\n   411\u2192            }\n   412\u2192        }\n   413\u2192    except Exception as e:\n   414\u2192        logger.error(f\"Login failed for user '{credentials.username}': {e}\")\n   415\u2192        raise HTTPException(401, \"Authentication failed\")\n   416\u2192\n   417\u2192@app.post(\"/api/v1/auth/refresh\")\n   418\u2192async def refresh_token(refresh_token: str) -> Dict[str, Any]:\n   419\u2192    \"\"\"Refresh access token.\"\"\"\n   420\u2192    if auth_manager is None:\n   421\u2192        raise HTTPException(503, \"Authentication service not ready\")\n   422\u2192    \n   423\u2192    try:\n   424\u2192        new_access_token = await auth_manager.refresh_access_token(refresh_token)\n   425\u2192        if not new_access_token:\n   426\u2192            raise HTTPException(401, \"Invalid refresh token\")\n   427\u2192        \n   428\u2192        return {\n   429\u2192            \"access_token\": new_access_token,\n   430\u2192            \"token_type\": \"bearer\",\n   431\u2192            \"expires_in\": security_config.access_token_expire_minutes * 60\n   432\u2192        }\n   433\u2192    except Exception as e:\n   434\u2192        logger.error(f\"Token refresh failed: {e}\")\n   435\u2192        raise HTTPException(401, \"Token refresh failed\")\n   436\u2192\n   437\u2192@app.post(\"/api/v1/auth/logout\")\n   438\u2192async def logout(\n   439\u2192    refresh_token: Optional[str] = None,\n   440\u2192    current_user: User = Depends(get_current_active_user)\n   441\u2192) -> Dict[str, str]:\n   442\u2192    \"\"\"User logout endpoint.\"\"\"\n   443\u2192    if auth_manager is None:\n   444\u2192        raise HTTPException(503, \"Authentication service not ready\")\n   445\u2192    \n   446\u2192    try:\n   447\u2192        if refresh_token:\n   448\u2192            await auth_manager.revoke_refresh_token(refresh_token)\n   449\u2192        \n   450\u2192        await auth_manager._log_security_event(\n   451\u2192            \"user_logout\",\n   452\u2192            user_id=current_user.id,\n   453\u2192            username=current_user.username\n   454\u2192        )\n   455\u2192        \n   456\u2192        return {\"message\": \"Successfully logged out\"}\n   457\u2192    except Exception as e:\n   458\u2192        logger.error(f\"Logout failed: {e}\")\n   459\u2192        raise HTTPException(500, \"Logout failed\")\n   460\u2192\n   461\u2192@app.get(\"/api/v1/auth/me\")\n   462\u2192async def get_current_user_info(current_user: User = Depends(get_current_active_user)) -> Dict[str, Any]:\n   463\u2192    \"\"\"Get current user information.\"\"\"\n   464\u2192    return {\n   465\u2192        \"id\": current_user.id,\n   466\u2192        \"username\": current_user.username,\n   467\u2192        \"email\": current_user.email,\n   468\u2192        \"role\": current_user.role.value,\n   469\u2192        \"permissions\": [p.value for p in current_user.permissions],\n   470\u2192        \"is_active\": current_user.is_active,\n   471\u2192        \"created_at\": current_user.created_at.isoformat(),\n   472\u2192        \"last_login\": current_user.last_login.isoformat() if current_user.last_login else None\n   473\u2192    }\n   474\u2192\n   475\u2192@app.post(\"/api/v1/auth/register\")\n   476\u2192async def register(\n   477\u2192    credentials: AuthCredentials,\n   478\u2192    email: Optional[str] = None,\n   479\u2192    admin_user: User = Depends(require_role(UserRole.ADMIN))\n   480\u2192) -> Dict[str, Any]:\n   481\u2192    \"\"\"Register new user (admin only).\"\"\"\n   482\u2192    if auth_manager is None:\n   483\u2192        raise HTTPException(503, \"Authentication service not ready\")\n   484\u2192    \n   485\u2192    try:\n   486\u2192        user = await auth_manager.create_user(\n   487\u2192            username=credentials.username,\n   488\u2192            password=credentials.password,\n   489\u2192            email=email,\n   490\u2192            role=UserRole.USER\n   491\u2192        )\n   492\u2192        \n   493\u2192        return {\n   494\u2192            \"message\": \"User created successfully\",\n   495\u2192            \"user\": {\n   496\u2192                \"id\": user.id,\n   497\u2192                \"username\": user.username,\n   498\u2192                \"email\": user.email,\n   499\u2192                \"role\": user.role.value\n   500\u2192            }\n   501\u2192        }\n   502\u2192    except Exception as e:\n   503\u2192        logger.error(f\"User registration failed: {e}\")\n   504\u2192        raise HTTPException(400, str(e))\n   505\u2192\n   506\u2192# Security management endpoints\n   507\u2192\n   508\u2192@app.get(\"/api/v1/security/events\")\n   509\u2192async def get_security_events(\n   510\u2192    limit: int = 100,\n   511\u2192    admin_user: User = Depends(require_role(UserRole.ADMIN))\n   512\u2192) -> Dict[str, Any]:\n   513\u2192    \"\"\"Get security events (admin only).\"\"\"\n   514\u2192    if auth_manager is None:\n   515\u2192        raise HTTPException(503, \"Authentication service not ready\")\n   516\u2192    \n   517\u2192    try:\n   518\u2192        events = await auth_manager.get_security_events(limit)\n   519\u2192        return {\n   520\u2192            \"events\": [\n   521\u2192                {\n   522\u2192                    \"id\": event.id,\n   523\u2192                    \"event_type\": event.event_type,\n   524\u2192                    \"user_id\": event.user_id,\n   525\u2192                    \"username\": event.username,\n   526\u2192                    \"ip_address\": event.ip_address,\n   527\u2192                    \"timestamp\": event.timestamp.isoformat(),\n   528\u2192                    \"details\": event.details,\n   529\u2192                    \"severity\": event.severity\n   530\u2192                }\n   531\u2192                for event in events\n   532\u2192            ]\n   533\u2192        }\n   534\u2192    except Exception as e:\n   535\u2192        logger.error(f\"Failed to get security events: {e}\")\n   536\u2192        raise HTTPException(500, \"Failed to retrieve security events\")\n   537\u2192\n   538\u2192# Health check endpoints (public)\n   539\u2192@app.get(\"/health\")\n   540\u2192async def health_check() -> Dict[str, Any]:\n   541\u2192    \"\"\"Basic health check endpoint.\"\"\"\n   542\u2192    return {\"status\": \"healthy\", \"service\": \"mcp-server\", \"timestamp\": time.time()}\n   543\u2192\n   544\u2192@app.get(\"/health/detailed\")\n   545\u2192async def detailed_health_check() -> Dict[str, Any]:\n   546\u2192    \"\"\"Detailed health check endpoint.\"\"\"\n   547\u2192    try:\n   548\u2192        overall_health = await health_checker.get_overall_health()\n   549\u2192        component_results = await health_checker.check_all_components()\n   550\u2192        \n   551\u2192        return {\n   552\u2192            \"status\": overall_health.status.value,\n   553\u2192            \"message\": overall_health.message,\n   554\u2192            \"timestamp\": overall_health.timestamp,\n   555\u2192            \"details\": overall_health.details,\n   556\u2192            \"components\": [\n   557\u2192                {\n   558\u2192                    \"component\": result.component,\n   559\u2192                    \"status\": result.status.value,\n   560\u2192                    \"message\": result.message,\n   561\u2192                    \"details\": result.details\n   562\u2192                }\n   563\u2192                for result in component_results\n   564\u2192            ]\n   565\u2192        }\n   566\u2192    except Exception as e:\n   567\u2192        logger.error(f\"Health check failed: {e}\", exc_info=True)\n   568\u2192        return {\n   569\u2192            \"status\": \"unhealthy\",\n   570\u2192            \"message\": f\"Health check failed: {str(e)}\",\n   571\u2192            \"timestamp\": time.time()\n   572\u2192        }\n   573\u2192\n   574\u2192@app.get(\"/health/{component}\")\n   575\u2192async def component_health_check(component: str) -> Dict[str, Any]:\n   576\u2192    \"\"\"Health check for a specific component.\"\"\"\n   577\u2192    try:\n   578\u2192        result = await health_checker.check_component(component)\n   579\u2192        return {\n   580\u2192            \"component\": result.component,\n   581\u2192            \"status\": result.status.value,\n   582\u2192            \"message\": result.message,\n   583\u2192            \"timestamp\": result.timestamp,\n   584\u2192            \"details\": result.details\n   585\u2192        }\n   586\u2192    except Exception as e:\n   587\u2192        logger.error(f\"Component health check failed for {component}: {e}\", exc_info=True)\n   588\u2192        raise HTTPException(500, f\"Health check failed: {str(e)}\")\n   589\u2192\n   590\u2192# Metrics endpoints\n   591\u2192@app.get(\"/metrics\", response_class=PlainTextResponse)\n   592\u2192def get_metrics() -> str:\n   593\u2192    \"\"\"Prometheus metrics endpoint.\"\"\"\n   594\u2192    try:\n   595\u2192        return metrics_collector.get_metrics()\n   596\u2192    except Exception as e:\n   597\u2192        logger.error(f\"Failed to get metrics: {e}\", exc_info=True)\n   598\u2192        raise HTTPException(500, f\"Failed to get metrics: {str(e)}\")\n   599\u2192\n   600\u2192@app.get(\"/metrics\")\n   601\u2192def get_prometheus_metrics() -> Response:\n   602\u2192    \"\"\"Prometheus metrics endpoint.\"\"\"\n   603\u2192    try:\n   604\u2192        prometheus_exporter = get_prometheus_exporter()\n   605\u2192        \n   606\u2192        # Update build info\n   607\u2192        prometheus_exporter.set_build_info(\n   608\u2192            version=\"1.0.0\",\n   609\u2192            commit=os.getenv(\"GIT_COMMIT\", \"unknown\"),\n   610\u2192            build_time=os.getenv(\"BUILD_TIME\", \"unknown\")\n   611\u2192        )\n   612\u2192        \n   613\u2192        # Update system metrics\n   614\u2192        import psutil\n   615\u2192        process = psutil.Process()\n   616\u2192        memory_info = process.memory_info()\n   617\u2192        prometheus_exporter.set_memory_usage(memory_info.rss, memory_info.vms)\n   618\u2192        prometheus_exporter.set_cpu_usage(process.cpu_percent())\n   619\u2192        prometheus_exporter.set_active_threads(threading.active_count())\n   620\u2192        \n   621\u2192        # Update plugin metrics\n   622\u2192        if plugin_loader:\n   623\u2192            stats = plugin_loader.get_statistics()\n   624\u2192            for lang, plugin in plugin_loader.get_active_plugins().items():\n   625\u2192                prometheus_exporter.plugin_status.labels(\n   626\u2192                    plugin=plugin.__class__.__name__,\n   627\u2192                    language=lang\n   628\u2192                ).set(1)\n   629\u2192        \n   630\u2192        # Update file watcher metrics\n   631\u2192        if file_watcher:\n   632\u2192            # This would need to be implemented in FileWatcher\n   633\u2192            # prometheus_exporter.set_files_watched(file_watcher.get_watched_count())\n   634\u2192            pass\n   635\u2192        \n   636\u2192        # Generate metrics\n   637\u2192        metrics = prometheus_exporter.generate_metrics()\n   638\u2192        return Response(\n   639\u2192            content=metrics,\n   640\u2192            media_type=prometheus_exporter.get_content_type()\n   641\u2192        )\n   642\u2192    except Exception as e:\n   643\u2192        logger.error(f\"Failed to generate Prometheus metrics: {e}\", exc_info=True)\n   644\u2192        raise HTTPException(500, f\"Failed to generate metrics: {str(e)}\")\n   645\u2192\n   646\u2192@app.get(\"/metrics/json\")\n   647\u2192def get_metrics_json(current_user: User = Depends(require_permission(Permission.READ))) -> Dict[str, Any]:\n   648\u2192    \"\"\"JSON metrics endpoint for programmatic access.\"\"\"\n   649\u2192    try:\n   650\u2192        families = metrics_collector.get_metric_families()\n   651\u2192        stats = metrics_collector.get_stats()\n   652\u2192        \n   653\u2192        return {\n   654\u2192            \"timestamp\": time.time(),\n   655\u2192            \"collector_stats\": stats,\n   656\u2192            \"metric_families\": families\n   657\u2192        }\n   658\u2192    except Exception as e:\n   659\u2192        logger.error(f\"Failed to get JSON metrics: {e}\", exc_info=True)\n   660\u2192        raise HTTPException(500, f\"Failed to get metrics: {str(e)}\")\n   661\u2192\n   662\u2192@app.get(\"/symbol\", response_model=SymbolDef | None)\n   663\u2192async def symbol(\n   664\u2192    symbol: str,\n   665\u2192    current_user: User = Depends(require_permission(Permission.READ))\n   666\u2192):\n   667\u2192    if dispatcher is None:\n   668\u2192        logger.error(\"Symbol lookup attempted but dispatcher not ready\")\n   669\u2192        raise HTTPException(503, \"Dispatcher not ready\")\n   670\u2192    \n   671\u2192    start_time = time.time()\n   672\u2192    try:\n   673\u2192        logger.debug(f\"Looking up symbol: {symbol} for user: {current_user.username}\")\n   674\u2192        \n   675\u2192        # Try cache first if query cache is available\n   676\u2192        cached_result = None\n   677\u2192        if query_cache and query_cache.config.enabled:\n   678\u2192            cached_result = await query_cache.get_cached_result(\n   679\u2192                QueryType.SYMBOL_LOOKUP,\n   680\u2192                symbol=symbol\n   681\u2192            )\n   682\u2192        \n   683\u2192        if cached_result is not None:\n   684\u2192            logger.debug(f\"Found cached symbol: {symbol}\")\n   685\u2192            duration = time.time() - start_time\n   686\u2192            business_metrics.record_search_performed(\n   687\u2192                query=symbol,\n   688\u2192                semantic=False,\n   689\u2192                results_count=1,\n   690\u2192                duration=duration\n   691\u2192            )\n   692\u2192            return cached_result\n   693\u2192        \n   694\u2192        # Record symbol lookup metrics\n   695\u2192        with metrics_collector.time_function(\"symbol_lookup\"):\n   696\u2192            result = dispatcher.lookup(symbol)\n   697\u2192        \n   698\u2192        # Cache the result if available\n   699\u2192        if query_cache and query_cache.config.enabled and result:\n   700\u2192            await query_cache.cache_result(\n   701\u2192                QueryType.SYMBOL_LOOKUP,\n   702\u2192                result,\n   703\u2192                symbol=symbol\n   704\u2192            )\n   705\u2192        \n   706\u2192        # Record business metrics\n   707\u2192        duration = time.time() - start_time\n   708\u2192        business_metrics.record_search_performed(\n   709\u2192            query=symbol,\n   710\u2192            semantic=False,\n   711\u2192            results_count=1 if result else 0,\n   712\u2192            duration=duration\n   713\u2192        )\n   714\u2192        \n   715\u2192        if result:\n   716\u2192            logger.debug(f\"Found symbol: {symbol}\")\n   717\u2192        else:\n   718\u2192            logger.debug(f\"Symbol not found: {symbol}\")\n   719\u2192        return result\n   720\u2192    except Exception as e:\n   721\u2192        duration = time.time() - start_time\n   722\u2192        business_metrics.record_search_performed(\n   723\u2192            query=symbol,\n   724\u2192            semantic=False,\n   725\u2192            results_count=0,\n   726\u2192            duration=duration\n   727\u2192        )\n   728\u2192        logger.error(f\"Error looking up symbol '{symbol}': {e}\", exc_info=True)\n   729\u2192        raise HTTPException(500, f\"Internal error during symbol lookup: {str(e)}\")\n   730\u2192\n   731\u2192@app.get(\"/search\", response_model=list[SearchResult])\n   732\u2192async def search(\n   733\u2192    q: str,\n   734\u2192    semantic: bool = False,\n   735\u2192    limit: int = 20,\n   736\u2192    mode: str = \"auto\",  # \"auto\", \"hybrid\", \"bm25\", \"semantic\", \"fuzzy\", \"classic\"\n   737\u2192    language: Optional[str] = None,\n   738\u2192    file_filter: Optional[str] = None,\n   739\u2192    current_user: User = Depends(require_permission(Permission.READ))\n   740\u2192):\n   741\u2192    \"\"\"Search with support for multiple modes including hybrid search.\n   742\u2192    \n   743\u2192    Args:\n   744\u2192        q: Search query\n   745\u2192        semantic: Whether to use semantic search (for backward compatibility)\n   746\u2192        limit: Maximum number of results\n   747\u2192        mode: Search mode - \"auto\" (default), \"hybrid\", \"bm25\", \"semantic\", \"fuzzy\", or \"classic\"\n   748\u2192        language: Filter by programming language\n   749\u2192        file_filter: Filter by file path pattern\n   750\u2192        current_user: Authenticated user\n   751\u2192    \"\"\"\n   752\u2192    if dispatcher is None and mode == \"classic\":\n   753\u2192        logger.error(\"Search attempted but dispatcher not ready\")\n   754\u2192        raise HTTPException(503, \"Dispatcher not ready\")\n   755\u2192    \n   756\u2192    start_time = time.time()\n   757\u2192    try:\n   758\u2192        # Determine effective search mode\n   759\u2192        effective_mode = mode\n   760\u2192        if mode == \"auto\":\n   761\u2192            # Auto mode: use hybrid if available, otherwise fall back\n   762\u2192            if hybrid_search is not None:\n   763\u2192                effective_mode = \"hybrid\"\n   764\u2192            elif semantic and hasattr(dispatcher, 'search'):\n   765\u2192                effective_mode = \"classic\"\n   766\u2192            else:\n   767\u2192                effective_mode = \"bm25\" if bm25_indexer else \"classic\"\n   768\u2192        \n   769\u2192        logger.debug(f\"Searching for: '{q}' (mode={effective_mode}, limit={limit}, language={language}) for user: {current_user.username}\")\n   770\u2192        \n   771\u2192        # Build filters\n   772\u2192        filters = {}\n   773\u2192        if language:\n   774\u2192            filters['language'] = language\n   775\u2192        if file_filter:\n   776\u2192            filters['file_filter'] = file_filter\n   777\u2192        \n   778\u2192        # Try cache first if query cache is available\n   779\u2192        cache_key_parts = [q, effective_mode, str(limit)]\n   780\u2192        if filters:\n   781\u2192            cache_key_parts.extend([f\"{k}:{v}\" for k, v in sorted(filters.items())])\n   782\u2192        \n   783\u2192        cached_results = None\n   784\u2192        if query_cache and query_cache.config.enabled:\n   785\u2192            query_type = QueryType.SEMANTIC_SEARCH if effective_mode == \"semantic\" else QueryType.SEARCH\n   786\u2192            cached_results = await query_cache.get_cached_result(\n   787\u2192                query_type,\n   788\u2192                q=q,\n   789\u2192                semantic=(effective_mode == \"semantic\"),\n   790\u2192                limit=limit\n   791\u2192            )\n   792\u2192        \n   793\u2192        if cached_results is not None:\n   794\u2192            logger.debug(f\"Found cached search results for: '{q}' ({len(cached_results)} results)\")\n   795\u2192            duration = time.time() - start_time\n   796\u2192            business_metrics.record_search_performed(\n   797\u2192                query=q,\n   798\u2192                semantic=(effective_mode == \"semantic\"),\n   799\u2192                results_count=len(cached_results),\n   800\u2192                duration=duration\n   801\u2192            )\n   802\u2192            return cached_results\n   803\u2192        \n   804\u2192        # Perform search based on mode\n   805\u2192        results = []\n   806\u2192        \n   807\u2192        if effective_mode == \"hybrid\" and hybrid_search:\n   808\u2192            # Use hybrid search\n   809\u2192            with metrics_collector.time_function(\"search\", labels={\"mode\": \"hybrid\"}):\n   810\u2192                hybrid_results = await hybrid_search.search(\n   811\u2192                    query=q,\n   812\u2192                    filters=filters,\n   813\u2192                    limit=limit\n   814\u2192                )\n   815\u2192                # Convert to SearchResult format\n   816\u2192                for r in hybrid_results:\n   817\u2192                    results.append(SearchResult(\n   818\u2192                        file_path=r['filepath'],\n   819\u2192                        snippet=r['snippet'],\n   820\u2192                        score=r['score']\n   821\u2192                    ))\n   822\u2192        \n   823\u2192        elif effective_mode == \"bm25\" and bm25_indexer:\n   824\u2192            # Direct BM25 search\n   825\u2192            with metrics_collector.time_function(\"search\", labels={\"mode\": \"bm25\"}):\n   826\u2192                bm25_results = bm25_indexer.search(q, limit=limit, **filters)\n   827\u2192                for r in bm25_results:\n   828\u2192                    results.append(SearchResult(\n   829\u2192                        file_path=r['filepath'],\n   830\u2192                        snippet=r.get('snippet', ''),\n   831\u2192                        score=r['score']\n   832\u2192                    ))\n   833\u2192        \n   834\u2192        elif effective_mode == \"fuzzy\" and fuzzy_indexer:\n   835\u2192            # Direct fuzzy search\n   836\u2192            with metrics_collector.time_function(\"search\", labels={\"mode\": \"fuzzy\"}):\n   837\u2192                fuzzy_results = fuzzy_indexer.search_fuzzy(q, max_results=limit)\n   838\u2192                for r in fuzzy_results:\n   839\u2192                    results.append(SearchResult(\n   840\u2192                        file_path=r.get('file_path', ''),\n   841\u2192                        snippet=r.get('context', ''),\n   842\u2192                        score=r.get('score', 0.0)\n   843\u2192                    ))\n   844\u2192        \n   845\u2192        elif effective_mode == \"semantic\":\n   846\u2192            # Use classic dispatcher with semantic=True\n   847\u2192            if dispatcher:\n   848\u2192                with metrics_collector.time_function(\"search\", labels={\"mode\": \"semantic\"}):\n   849\u2192                    results = list(dispatcher.search(q, semantic=True, limit=limit))\n   850\u2192            else:\n   851\u2192                raise HTTPException(\n   852\u2192                    503, \n   853\u2192                    detail={\n   854\u2192                        \"error\": \"Semantic search not available\",\n   855\u2192                        \"reason\": \"Missing Voyage AI API key configuration\",\n   856\u2192                        \"setup\": {\n   857\u2192                            \"method_1_mcp_json\": [\n   858\u2192                                \"Configure in .mcp.json (recommended for Claude Code):\",\n   859\u2192                                \"{\",\n   860\u2192                                '  \"mcpServers\": {',\n   861\u2192                                '    \"code-index-mcp\": {',\n   862\u2192                                '      \"command\": \"uvicorn\",',\n   863\u2192                                '      \"args\": [\"mcp_server.gateway:app\"],',\n   864\u2192                                '      \"env\": {',\n   865\u2192                                '        \"VOYAGE_AI_API_KEY\": \"your-key-here\",',\n   866\u2192                                '        \"SEMANTIC_SEARCH_ENABLED\": \"true\"',\n   867\u2192                                '      }',\n   868\u2192                                '    }',\n   869\u2192                                '  }',\n   870\u2192                                '}'\n   871\u2192                            ],\n   872\u2192                            \"method_2_cli\": [\n   873\u2192                                \"Or use Claude Code CLI:\",\n   874\u2192                                \"claude mcp add code-index-mcp -e VOYAGE_AI_API_KEY=your_key -e SEMANTIC_SEARCH_ENABLED=true -- uvicorn mcp_server.gateway:app\"\n   875\u2192                            ],\n   876\u2192                            \"method_3_env\": [\n   877\u2192                                \"Or set environment variables:\",\n   878\u2192                                \"export VOYAGE_AI_API_KEY=your_key\",\n   879\u2192                                \"export SEMANTIC_SEARCH_ENABLED=true\"\n   880\u2192                            ],\n   881\u2192                            \"method_4_dotenv\": [\n   882\u2192                                \"Or add to .env file:\",\n   883\u2192                                \"VOYAGE_AI_API_KEY=your_key\",\n   884\u2192                                \"SEMANTIC_SEARCH_ENABLED=true\"\n   885\u2192                            ],\n   886\u2192                            \"get_api_key\": \"Get your API key from: https://www.voyageai.com/\",\n   887\u2192                            \"alternative\": \"Use mode='hybrid' or mode='bm25' for keyword-based search\"\n   888\u2192                        }\n   889\u2192                    }\n   890\u2192                )\n   891\u2192        \n   892\u2192        else:\n   893\u2192            # Classic search through dispatcher\n   894\u2192            if dispatcher:\n   895\u2192                with metrics_collector.time_function(\"search\", labels={\"mode\": \"classic\"}):\n   896\u2192                    results = list(dispatcher.search(q, semantic=False, limit=limit))\n   897\u2192            else:\n   898\u2192                raise HTTPException(503, \"Classic search not available\")\n   899\u2192        \n   900\u2192        # Cache the results if available\n   901\u2192        if query_cache and query_cache.config.enabled and results:\n   902\u2192            query_type = QueryType.SEMANTIC_SEARCH if effective_mode == \"semantic\" else QueryType.SEARCH\n   903\u2192            await query_cache.cache_result(\n   904\u2192                query_type,\n   905\u2192                results,\n   906\u2192                q=q,\n   907\u2192                semantic=(effective_mode == \"semantic\"),\n   908\u2192                limit=limit\n   909\u2192            )\n   910\u2192        \n   911\u2192        # Record business metrics\n   912\u2192        duration = time.time() - start_time\n   913\u2192        business_metrics.record_search_performed(\n   914\u2192            query=q,\n   915\u2192            semantic=(effective_mode == \"semantic\"),\n   916\u2192            results_count=len(results),\n   917\u2192            duration=duration\n   918\u2192        )\n   919\u2192        \n   920\u2192        logger.debug(f\"Search returned {len(results)} results using {effective_mode} mode\")\n   921\u2192        return results\n   922\u2192    except Exception as e:\n   923\u2192        duration = time.time() - start_time\n   924\u2192        business_metrics.record_search_performed(\n   925\u2192            query=q,\n   926\u2192            semantic=semantic,\n   927\u2192            results_count=0,\n   928\u2192            duration=duration\n   929\u2192        )\n   930\u2192        logger.error(f\"Error during search for '{q}': {e}\", exc_info=True)\n   931\u2192        raise HTTPException(500, f\"Internal error during search: {str(e)}\")\n   932\u2192\n   933\u2192@app.get(\"/search/capabilities\")\n   934\u2192async def get_search_capabilities() -> Dict[str, Any]:\n   935\u2192    \"\"\"Get available search capabilities and configuration guidance.\"\"\"\n   936\u2192    voyage_key = os.environ.get('VOYAGE_API_KEY') or os.environ.get('VOYAGE_AI_API_KEY')\n   937\u2192    semantic_enabled = os.environ.get('SEMANTIC_SEARCH_ENABLED', 'false').lower() == 'true'\n   938\u2192    \n   939\u2192    return {\n   940\u2192        \"available_modes\": {\n   941\u2192            \"bm25\": bm25_indexer is not None,\n   942\u2192            \"fuzzy\": fuzzy_indexer is not None,\n   943\u2192            \"semantic\": semantic_indexer is not None,\n   944\u2192            \"hybrid\": hybrid_search is not None,\n   945\u2192            \"classic\": dispatcher is not None\n   946\u2192        },\n   947\u2192        \"semantic_config\": {\n   948\u2192            \"enabled\": semantic_indexer is not None,\n   949\u2192            \"api_key_configured\": bool(voyage_key),\n   950\u2192            \"semantic_enabled_flag\": semantic_enabled,\n   951\u2192            \"status\": \"ready\" if semantic_indexer else \"not_configured\"\n   952\u2192        },\n   953\u2192        \"configuration_guide\": {\n   954\u2192            \"mcp_json_example\": {\n   955\u2192                \"description\": \"Add to .mcp.json for Claude Code (recommended)\",\n   956\u2192                \"config\": {\n   957\u2192                    \"mcpServers\": {\n   958\u2192                        \"code-index-mcp\": {\n   959\u2192                            \"command\": \"uvicorn\",\n   960\u2192                            \"args\": [\"mcp_server.gateway:app\"],\n   961\u2192                            \"env\": {\n   962\u2192                                \"VOYAGE_AI_API_KEY\": \"your-key-here\",\n   963\u2192                                \"SEMANTIC_SEARCH_ENABLED\": \"true\"\n   964\u2192                            }\n   965\u2192                        }\n   966\u2192                    }\n   967\u2192                }\n   968\u2192            },\n   969\u2192            \"cli_command\": \"claude mcp add code-index-mcp -e VOYAGE_AI_API_KEY=key -e SEMANTIC_SEARCH_ENABLED=true -- uvicorn mcp_server.gateway:app\",\n   970\u2192            \"env_file\": \"Add to .env: VOYAGE_AI_API_KEY=key and SEMANTIC_SEARCH_ENABLED=true\",\n   971\u2192            \"get_api_key\": \"https://www.voyageai.com/\"\n   972\u2192        }\n   973\u2192    }\n   974\u2192\n   975\u2192@app.get(\"/status\")\n   976\u2192async def status(current_user: User = Depends(require_permission(Permission.READ))) -> Dict[str, Any]:\n   977\u2192    \"\"\"Returns server status including plugin information and statistics.\"\"\"\n   978\u2192    if dispatcher is None:\n   979\u2192        return {\n   980\u2192            \"status\": \"error\",\n   981\u2192            \"plugins\": 0,\n   982\u2192            \"indexed_files\": {\"total\": 0, \"by_language\": {}},\n   983\u2192            \"version\": \"0.1.0\",\n   984\u2192            \"message\": \"Dispatcher not initialized\"\n   985\u2192        }\n   986\u2192    \n   987\u2192    try:\n   988\u2192        # Try cache first if query cache is available\n   989\u2192        cached_status = None\n   990\u2192        if query_cache and query_cache.config.enabled:\n   991\u2192            cached_status = await query_cache.get_cached_result(\n   992\u2192                QueryType.PROJECT_STATUS\n   993\u2192            )\n   994\u2192        \n   995\u2192        if cached_status is not None:\n   996\u2192            return cached_status\n   997\u2192        \n   998\u2192        # Get plugin count\n   999\u2192        plugin_count = len(dispatcher._plugins) if hasattr(dispatcher, '_plugins') else 0\n  1000\u2192        \n  1001\u2192        # Get indexed files statistics\n  1002\u2192        indexed_stats = {\"total\": 0, \"by_language\": {}}\n  1003\u2192        if hasattr(dispatcher, 'get_statistics'):\n  1004\u2192            indexed_stats = dispatcher.get_statistics()\n  1005\u2192        elif hasattr(dispatcher, '_plugins'):\n  1006\u2192            # Calculate basic statistics from plugins\n  1007\u2192            for plugin in dispatcher._plugins:\n  1008\u2192                if hasattr(plugin, 'get_indexed_count'):\n  1009\u2192                    count = plugin.get_indexed_count()\n  1010\u2192                    indexed_stats[\"total\"] += count\n  1011\u2192                    lang = getattr(plugin, 'language', getattr(plugin, 'lang', 'unknown'))\n  1012\u2192                    indexed_stats[\"by_language\"][lang] = count\n  1013\u2192        \n  1014\u2192        # Add database statistics if available\n  1015\u2192        db_stats = {}\n  1016\u2192        if sqlite_store:\n  1017\u2192            db_stats = sqlite_store.get_statistics()\n  1018\u2192        \n  1019\u2192        # Add cache statistics if available\n  1020\u2192        cache_stats = {}\n  1021\u2192        if cache_manager:\n  1022\u2192            try:\n  1023\u2192                cache_metrics = await cache_manager.get_metrics()\n  1024\u2192                cache_stats = {\n  1025\u2192                    \"hit_rate\": cache_metrics.hit_rate,\n  1026\u2192                    \"entries\": cache_metrics.entries_count,\n  1027\u2192                    \"memory_usage_mb\": cache_metrics.memory_usage_mb\n  1028\u2192                }\n  1029\u2192            except Exception as e:\n  1030\u2192                logger.warning(f\"Failed to get cache stats: {e}\")\n  1031\u2192        \n  1032\u2192        status_data = {\n  1033\u2192            \"status\": \"operational\",\n  1034\u2192            \"plugins\": plugin_count,\n  1035\u2192            \"indexed_files\": indexed_stats,\n  1036\u2192            \"database\": db_stats,\n  1037\u2192            \"cache\": cache_stats,\n  1038\u2192            \"search_capabilities\": [],\n  1039\u2192            \"version\": \"0.1.0\"\n  1040\u2192        }\n  1041\u2192        \n  1042\u2192        # Add search capabilities\n  1043\u2192        if bm25_indexer:\n  1044\u2192            status_data[\"search_capabilities\"].append(\"bm25\")\n  1045\u2192        if fuzzy_indexer:\n  1046\u2192            status_data[\"search_capabilities\"].append(\"fuzzy\") \n  1047\u2192        if hasattr(app.state, 'semantic_indexer') and app.state.semantic_indexer:\n  1048\u2192            status_data[\"search_capabilities\"].append(\"semantic\")\n  1049\u2192        if hybrid_search:\n  1050\u2192            status_data[\"search_capabilities\"].append(\"hybrid\")\n  1051\u2192        \n  1052\u2192        # Cache the status\n  1053\u2192        if query_cache and query_cache.config.enabled:\n  1054\u2192            await query_cache.cache_result(\n  1055\u2192                QueryType.PROJECT_STATUS,\n  1056\u2192                status_data\n  1057\u2192            )\n  1058\u2192        \n  1059\u2192        return status_data\n  1060\u2192    except Exception as e:\n  1061\u2192        logger.error(f\"Error getting server status: {e}\", exc_info=True)\n  1062\u2192        return {\n  1063\u2192            \"status\": \"error\",\n  1064\u2192            \"plugins\": 0,\n  1065\u2192            \"indexed_files\": {\"total\": 0, \"by_language\": {}},\n  1066\u2192            \"version\": \"0.1.0\",\n  1067\u2192            \"message\": str(e)\n  1068\u2192        }\n  1069\u2192\n  1070\u2192@app.get(\"/plugins\")\n  1071\u2192def plugins(current_user: User = Depends(require_permission(Permission.READ))) -> List[Dict[str, Any]]:\n  1072\u2192    \"\"\"Returns list of loaded plugins with their information.\"\"\"\n  1073\u2192    if plugin_manager is None:\n  1074\u2192        logger.error(\"Plugin list requested but plugin manager not ready\")\n  1075\u2192        raise HTTPException(503, \"Plugin manager not ready\")\n  1076\u2192    \n  1077\u2192    try:\n  1078\u2192        plugin_list = []\n  1079\u2192        plugin_infos = plugin_manager._registry.list_plugins()\n  1080\u2192        plugin_status = plugin_manager.get_plugin_status()\n  1081\u2192        \n  1082\u2192        for info in plugin_infos:\n  1083\u2192            status = plugin_status.get(info.name, {})\n  1084\u2192            plugin_data = {\n  1085\u2192                \"name\": info.name,\n  1086\u2192                \"version\": info.version,\n  1087\u2192                \"description\": info.description,\n  1088\u2192                \"author\": info.author,\n  1089\u2192                \"language\": info.language,\n  1090\u2192                \"file_extensions\": info.file_extensions,\n  1091\u2192                \"state\": status.get('state', 'unknown'),\n  1092\u2192                \"enabled\": status.get('enabled', False)\n  1093\u2192            }\n  1094\u2192            plugin_list.append(plugin_data)\n  1095\u2192        \n  1096\u2192        logger.debug(f\"Returning {len(plugin_list)} plugins\")\n  1097\u2192        return plugin_list\n  1098\u2192    except Exception as e:\n  1099\u2192        logger.error(f\"Error getting plugin list: {e}\", exc_info=True)\n  1100\u2192        raise HTTPException(500, f\"Internal error getting plugins: {str(e)}\")\n  1101\u2192\n  1102\u2192@app.post(\"/reindex\")\n  1103\u2192async def reindex(\n  1104\u2192    path: Optional[str] = None,\n  1105\u2192    current_user: User = Depends(require_permission(Permission.EXECUTE))\n  1106\u2192) -> Dict[str, str]:\n  1107\u2192    \"\"\"Triggers manual reindexing of files.\n  1108\u2192    \n  1109\u2192    Args:\n  1110\u2192        path: Optional specific directory path to reindex. If not provided,\n  1111\u2192              reindexes all configured paths.\n  1112\u2192    \n  1113\u2192    Returns:\n  1114\u2192        Task status information.\n  1115\u2192    \"\"\"\n  1116\u2192    if dispatcher is None:\n  1117\u2192        logger.error(\"Reindex requested but dispatcher not ready\")\n  1118\u2192        raise HTTPException(503, \"Dispatcher not ready\")\n  1119\u2192    \n  1120\u2192    try:\n  1121\u2192        logger.info(f\"Manual reindex requested for path: {path or 'all'}\")\n  1122\u2192        # Since dispatcher has index_file method, we can use it for reindexing\n  1123\u2192        if path:\n  1124\u2192            # Reindex specific path\n  1125\u2192            target_path = Path(path)\n  1126\u2192            if not target_path.exists():\n  1127\u2192                raise HTTPException(404, f\"Path not found: {path}\")\n  1128\u2192            \n  1129\u2192            indexed_count = 0\n  1130\u2192            if target_path.is_file():\n  1131\u2192                # Single file\n  1132\u2192                dispatcher.index_file(target_path)\n  1133\u2192                indexed_count = 1\n  1134\u2192            else:\n  1135\u2192                # Directory - find all supported files\n  1136\u2192                for file_path in target_path.rglob(\"*\"):\n  1137\u2192                    if file_path.is_file():\n  1138\u2192                        try:\n  1139\u2192                            # Check if any plugin supports this file\n  1140\u2192                            for plugin in dispatcher._plugins:\n  1141\u2192                                if plugin.supports(file_path):\n  1142\u2192                                    dispatcher.index_file(file_path)\n  1143\u2192                                    indexed_count += 1\n  1144\u2192                                    break\n  1145\u2192                        except Exception as e:\n  1146\u2192                            # Log but continue with other files\n  1147\u2192                            logger.warning(f\"Failed to index {file_path}: {e}\")\n  1148\u2192            \n  1149\u2192            logger.info(f\"Successfully reindexed {indexed_count} files in {path}\")\n  1150\u2192            return {\n  1151\u2192                \"status\": \"completed\",\n  1152\u2192                \"message\": f\"Reindexed {indexed_count} files in {path}\"\n  1153\u2192            }\n  1154\u2192        else:\n  1155\u2192            # Reindex all supported files\n  1156\u2192            indexed_count = 0\n  1157\u2192            indexed_by_type = {}\n  1158\u2192            \n  1159\u2192            # Find all files and check if any plugin supports them\n  1160\u2192            for file_path in Path(\".\").rglob(\"*\"):\n  1161\u2192                if file_path.is_file():\n  1162\u2192                    try:\n  1163\u2192                        # Check if any plugin supports this file\n  1164\u2192                        for plugin in dispatcher._plugins:\n  1165\u2192                            if plugin.supports(file_path):\n  1166\u2192                                dispatcher.index_file(file_path)\n  1167\u2192                                indexed_count += 1\n  1168\u2192                                \n  1169\u2192                                # Track by file type\n  1170\u2192                                suffix = file_path.suffix.lower()\n  1171\u2192                                indexed_by_type[suffix] = indexed_by_type.get(suffix, 0) + 1\n  1172\u2192                                break\n  1173\u2192                    except Exception as e:\n  1174\u2192                        # Log but continue with other files\n  1175\u2192                        logger.warning(f\"Failed to index {file_path}: {e}\")\n  1176\u2192            \n  1177\u2192            # Build summary message\n  1178\u2192            type_summary = \", \".join([f\"{count} {ext} files\" for ext, count in indexed_by_type.items()])\n  1179\u2192            logger.info(f\"Successfully reindexed {indexed_count} files: {type_summary}\")\n  1180\u2192            return {\n  1181\u2192                \"status\": \"completed\",\n  1182\u2192                \"message\": f\"Reindexed {indexed_count} files ({type_summary})\"\n  1183\u2192            }\n  1184\u2192    except HTTPException:\n  1185\u2192        raise\n  1186\u2192    except Exception as e:\n  1187\u2192        logger.error(f\"Reindexing failed: {e}\", exc_info=True)\n  1188\u2192        raise HTTPException(500, f\"Reindexing failed: {str(e)}\")\n  1189\u2192\n  1190\u2192@app.post(\"/plugins/{plugin_name}/reload\")\n  1191\u2192async def reload_plugin(\n  1192\u2192    plugin_name: str,\n  1193\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1194\u2192) -> Dict[str, str]:\n  1195\u2192    \"\"\"Reload a specific plugin.\n  1196\u2192    \n  1197\u2192    Args:\n  1198\u2192        plugin_name: Name of the plugin to reload\n  1199\u2192        \n  1200\u2192    Returns:\n  1201\u2192        Status message\n  1202\u2192    \"\"\"\n  1203\u2192    if plugin_manager is None:\n  1204\u2192        logger.error(\"Plugin reload requested but plugin manager not ready\")\n  1205\u2192        raise HTTPException(503, \"Plugin manager not ready\")\n  1206\u2192    \n  1207\u2192    try:\n  1208\u2192        plugin_manager.reload_plugin(plugin_name)\n  1209\u2192        return {\n  1210\u2192            \"status\": \"success\",\n  1211\u2192            \"message\": f\"Plugin '{plugin_name}' reloaded successfully\"\n  1212\u2192        }\n  1213\u2192    except Exception as e:\n  1214\u2192        logger.error(f\"Failed to reload plugin '{plugin_name}': {e}\", exc_info=True)\n  1215\u2192        raise HTTPException(500, f\"Failed to reload plugin: {str(e)}\")\n  1216\u2192\n  1217\u2192@app.post(\"/plugins/{plugin_name}/enable\")\n  1218\u2192async def enable_plugin(\n  1219\u2192    plugin_name: str,\n  1220\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1221\u2192) -> Dict[str, str]:\n  1222\u2192    \"\"\"Enable a disabled plugin.\n  1223\u2192    \n  1224\u2192    Args:\n  1225\u2192        plugin_name: Name of the plugin to enable\n  1226\u2192        \n  1227\u2192    Returns:\n  1228\u2192        Status message\n  1229\u2192    \"\"\"\n  1230\u2192    if plugin_manager is None:\n  1231\u2192        logger.error(\"Plugin enable requested but plugin manager not ready\")\n  1232\u2192        raise HTTPException(503, \"Plugin manager not ready\")\n  1233\u2192    \n  1234\u2192    try:\n  1235\u2192        plugin_manager.enable_plugin(plugin_name)\n  1236\u2192        # Recreate dispatcher with updated plugins\n  1237\u2192        active_plugins = plugin_manager.get_active_plugins()\n  1238\u2192        global dispatcher\n  1239\u2192        dispatcher = Dispatcher(list(active_plugins.values()))\n  1240\u2192        \n  1241\u2192        return {\n  1242\u2192            \"status\": \"success\",\n  1243\u2192            \"message\": f\"Plugin '{plugin_name}' enabled successfully\"\n  1244\u2192        }\n  1245\u2192    except Exception as e:\n  1246\u2192        logger.error(f\"Failed to enable plugin '{plugin_name}': {e}\", exc_info=True)\n  1247\u2192        raise HTTPException(500, f\"Failed to enable plugin: {str(e)}\")\n  1248\u2192\n  1249\u2192@app.post(\"/plugins/{plugin_name}/disable\")\n  1250\u2192async def disable_plugin(\n  1251\u2192    plugin_name: str,\n  1252\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1253\u2192) -> Dict[str, str]:\n  1254\u2192    \"\"\"Disable an active plugin.\n  1255\u2192    \n  1256\u2192    Args:\n  1257\u2192        plugin_name: Name of the plugin to disable\n  1258\u2192        \n  1259\u2192    Returns:\n  1260\u2192        Status message\n  1261\u2192    \"\"\"\n  1262\u2192    if plugin_manager is None:\n  1263\u2192        logger.error(\"Plugin disable requested but plugin manager not ready\")\n  1264\u2192        raise HTTPException(503, \"Plugin manager not ready\")\n  1265\u2192    \n  1266\u2192    try:\n  1267\u2192        plugin_manager.disable_plugin(plugin_name)\n  1268\u2192        # Recreate dispatcher with updated plugins\n  1269\u2192        active_plugins = plugin_manager.get_active_plugins()\n  1270\u2192        global dispatcher\n  1271\u2192        dispatcher = Dispatcher(list(active_plugins.values()))\n  1272\u2192        \n  1273\u2192        return {\n  1274\u2192            \"status\": \"success\",\n  1275\u2192            \"message\": f\"Plugin '{plugin_name}' disabled successfully\"\n  1276\u2192        }\n  1277\u2192    except Exception as e:\n  1278\u2192        logger.error(f\"Failed to disable plugin '{plugin_name}': {e}\", exc_info=True)\n  1279\u2192        raise HTTPException(500, f\"Failed to disable plugin: {str(e)}\")\n  1280\u2192\n  1281\u2192# Cache management endpoints\n  1282\u2192\n  1283\u2192@app.get(\"/cache/stats\")\n  1284\u2192async def get_cache_stats(\n  1285\u2192    current_user: User = Depends(require_permission(Permission.READ))\n  1286\u2192) -> Dict[str, Any]:\n  1287\u2192    \"\"\"Get cache statistics and performance metrics.\"\"\"\n  1288\u2192    if not cache_manager:\n  1289\u2192        raise HTTPException(503, \"Cache manager not ready\")\n  1290\u2192    \n  1291\u2192    try:\n  1292\u2192        cache_metrics = await cache_manager.get_metrics()\n  1293\u2192        backend_stats = await cache_manager.get_backend_stats()\n  1294\u2192        \n  1295\u2192        stats = {\n  1296\u2192            \"cache_metrics\": {\n  1297\u2192                \"hits\": cache_metrics.hits,\n  1298\u2192                \"misses\": cache_metrics.misses,\n  1299\u2192                \"sets\": cache_metrics.sets,\n  1300\u2192                \"deletes\": cache_metrics.deletes,\n  1301\u2192                \"hit_rate\": cache_metrics.hit_rate,\n  1302\u2192                \"avg_response_time_ms\": cache_metrics.avg_response_time_ms,\n  1303\u2192                \"entries_count\": cache_metrics.entries_count,\n  1304\u2192                \"memory_usage_mb\": cache_metrics.memory_usage_mb\n  1305\u2192            },\n  1306\u2192            \"backend_stats\": backend_stats\n  1307\u2192        }\n  1308\u2192        \n  1309\u2192        # Add query cache stats if available\n  1310\u2192        if query_cache:\n  1311\u2192            query_stats = await query_cache.get_cache_stats()\n  1312\u2192            stats[\"query_cache\"] = query_stats\n  1313\u2192        \n  1314\u2192        return stats\n  1315\u2192    except Exception as e:\n  1316\u2192        logger.error(f\"Failed to get cache stats: {e}\")\n  1317\u2192        raise HTTPException(500, f\"Failed to get cache statistics: {str(e)}\")\n  1318\u2192\n  1319\u2192@app.post(\"/cache/clear\")\n  1320\u2192async def clear_cache(\n  1321\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1322\u2192) -> Dict[str, Any]:\n  1323\u2192    \"\"\"Clear all cache entries (admin only).\"\"\"\n  1324\u2192    if not cache_manager:\n  1325\u2192        raise HTTPException(503, \"Cache manager not ready\")\n  1326\u2192    \n  1327\u2192    try:\n  1328\u2192        count = await cache_manager.clear()\n  1329\u2192        logger.info(f\"Cache cleared by admin user {current_user.username}: {count} entries\")\n  1330\u2192        \n  1331\u2192        return {\n  1332\u2192            \"status\": \"success\",\n  1333\u2192            \"message\": f\"Cleared {count} cache entries\",\n  1334\u2192            \"cleared_entries\": count\n  1335\u2192        }\n  1336\u2192    except Exception as e:\n  1337\u2192        logger.error(f\"Failed to clear cache: {e}\")\n  1338\u2192        raise HTTPException(500, f\"Failed to clear cache: {str(e)}\")\n  1339\u2192\n  1340\u2192@app.post(\"/cache/invalidate\")\n  1341\u2192async def invalidate_cache_by_tags(\n  1342\u2192    tags: List[str],\n  1343\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1344\u2192) -> Dict[str, Any]:\n  1345\u2192    \"\"\"Invalidate cache entries by tags (admin only).\"\"\"\n  1346\u2192    if not cache_manager:\n  1347\u2192        raise HTTPException(503, \"Cache manager not ready\")\n  1348\u2192    \n  1349\u2192    try:\n  1350\u2192        tag_set = set(tags)\n  1351\u2192        count = await cache_manager.invalidate_by_tags(tag_set)\n  1352\u2192        logger.info(f\"Cache invalidated by admin user {current_user.username}: {count} entries with tags {tags}\")\n  1353\u2192        \n  1354\u2192        return {\n  1355\u2192            \"status\": \"success\",\n  1356\u2192            \"message\": f\"Invalidated {count} cache entries with tags {tags}\",\n  1357\u2192            \"invalidated_entries\": count,\n  1358\u2192            \"tags\": tags\n  1359\u2192        }\n  1360\u2192    except Exception as e:\n  1361\u2192        logger.error(f\"Failed to invalidate cache by tags: {e}\")\n  1362\u2192        raise HTTPException(500, f\"Failed to invalidate cache: {str(e)}\")\n  1363\u2192\n  1364\u2192@app.post(\"/cache/invalidate/files\")\n  1365\u2192async def invalidate_cache_by_files(\n  1366\u2192    file_paths: List[str],\n  1367\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1368\u2192) -> Dict[str, Any]:\n  1369\u2192    \"\"\"Invalidate cache entries that depend on specific files (admin only).\"\"\"\n  1370\u2192    if not query_cache:\n  1371\u2192        raise HTTPException(503, \"Query cache not ready\")\n  1372\u2192    \n  1373\u2192    try:\n  1374\u2192        total_count = 0\n  1375\u2192        for file_path in file_paths:\n  1376\u2192            count = await query_cache.invalidate_file_queries(file_path)\n  1377\u2192            total_count += count\n  1378\u2192        \n  1379\u2192        logger.info(f\"File-based cache invalidation by admin user {current_user.username}: {total_count} entries for {len(file_paths)} files\")\n  1380\u2192        \n  1381\u2192        return {\n  1382\u2192            \"status\": \"success\",\n  1383\u2192            \"message\": f\"Invalidated {total_count} cache entries for {len(file_paths)} files\",\n  1384\u2192            \"invalidated_entries\": total_count,\n  1385\u2192            \"files\": file_paths\n  1386\u2192        }\n  1387\u2192    except Exception as e:\n  1388\u2192        logger.error(f\"Failed to invalidate cache by files: {e}\")\n  1389\u2192        raise HTTPException(500, f\"Failed to invalidate cache by files: {str(e)}\")\n  1390\u2192\n  1391\u2192@app.post(\"/cache/invalidate/semantic\")\n  1392\u2192async def invalidate_semantic_cache(\n  1393\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1394\u2192) -> Dict[str, Any]:\n  1395\u2192    \"\"\"Invalidate all semantic search cache entries (admin only).\"\"\"\n  1396\u2192    if not query_cache:\n  1397\u2192        raise HTTPException(503, \"Query cache not ready\")\n  1398\u2192    \n  1399\u2192    try:\n  1400\u2192        count = await query_cache.invalidate_semantic_queries()\n  1401\u2192        logger.info(f\"Semantic cache invalidated by admin user {current_user.username}: {count} entries\")\n  1402\u2192        \n  1403\u2192        return {\n  1404\u2192            \"status\": \"success\",\n  1405\u2192            \"message\": f\"Invalidated {count} semantic search cache entries\",\n  1406\u2192            \"invalidated_entries\": count\n  1407\u2192        }\n  1408\u2192    except Exception as e:\n  1409\u2192        logger.error(f\"Failed to invalidate semantic cache: {e}\")\n  1410\u2192        raise HTTPException(500, f\"Failed to invalidate semantic cache: {str(e)}\")\n  1411\u2192\n  1412\u2192@app.post(\"/cache/warm\")\n  1413\u2192async def warm_cache(\n  1414\u2192    keys: List[str],\n  1415\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1416\u2192) -> Dict[str, Any]:\n  1417\u2192    \"\"\"Warm cache with predefined keys (admin only).\"\"\"\n  1418\u2192    if not cache_manager:\n  1419\u2192        raise HTTPException(503, \"Cache manager not ready\")\n  1420\u2192    \n  1421\u2192    try:\n  1422\u2192        # Simple factory function for warming - would need more sophisticated logic in production\n  1423\u2192        async def factory(key: str):\n  1424\u2192            # This is a placeholder - in real implementation would depend on key type\n  1425\u2192            return f\"warmed_value_for_{key}\"\n  1426\u2192        \n  1427\u2192        count = await cache_manager.warm_cache(keys, factory)\n  1428\u2192        logger.info(f\"Cache warmed by admin user {current_user.username}: {count} entries\")\n  1429\u2192        \n  1430\u2192        return {\n  1431\u2192            \"status\": \"success\",\n  1432\u2192            \"message\": f\"Warmed {count} cache entries\",\n  1433\u2192            \"warmed_entries\": count,\n  1434\u2192            \"requested_keys\": len(keys)\n  1435\u2192        }\n  1436\u2192    except Exception as e:\n  1437\u2192        logger.error(f\"Failed to warm cache: {e}\")\n  1438\u2192        raise HTTPException(500, f\"Failed to warm cache: {str(e)}\")\n  1439\u2192\n  1440\u2192@app.post(\"/cache/cleanup\")\n  1441\u2192async def cleanup_cache(\n  1442\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1443\u2192) -> Dict[str, Any]:\n  1444\u2192    \"\"\"Manually trigger cache cleanup (admin only).\"\"\"\n  1445\u2192    if not cache_manager:\n  1446\u2192        raise HTTPException(503, \"Cache manager not ready\")\n  1447\u2192    \n  1448\u2192    try:\n  1449\u2192        count = await cache_manager.cleanup()\n  1450\u2192        logger.info(f\"Cache cleanup triggered by admin user {current_user.username}: {count} entries cleaned\")\n  1451\u2192        \n  1452\u2192        return {\n  1453\u2192            \"status\": \"success\",\n  1454\u2192            \"message\": f\"Cleaned up {count} expired cache entries\",\n  1455\u2192            \"cleaned_entries\": count\n  1456\u2192        }\n  1457\u2192    except Exception as e:\n  1458\u2192        logger.error(f\"Failed to cleanup cache: {e}\")\n  1459\u2192        raise HTTPException(500, f\"Failed to cleanup cache: {str(e)}\")\n  1460\u2192\n  1461\u2192# Hybrid Search endpoints\n  1462\u2192\n  1463\u2192@app.get(\"/search/config\")\n  1464\u2192async def get_search_config(\n  1465\u2192    current_user: User = Depends(require_permission(Permission.READ))\n  1466\u2192) -> Dict[str, Any]:\n  1467\u2192    \"\"\"Get current hybrid search configuration.\"\"\"\n  1468\u2192    if not hybrid_search:\n  1469\u2192        raise HTTPException(503, \"Hybrid search not available\")\n  1470\u2192    \n  1471\u2192    config = hybrid_search.config\n  1472\u2192    return {\n  1473\u2192        \"weights\": {\n  1474\u2192            \"bm25\": config.bm25_weight,\n  1475\u2192            \"semantic\": config.semantic_weight,\n  1476\u2192            \"fuzzy\": config.fuzzy_weight\n  1477\u2192        },\n  1478\u2192        \"enabled_methods\": {\n  1479\u2192            \"bm25\": config.enable_bm25,\n  1480\u2192            \"semantic\": config.enable_semantic,\n  1481\u2192            \"fuzzy\": config.enable_fuzzy\n  1482\u2192        },\n  1483\u2192        \"rrf_k\": config.rrf_k,\n  1484\u2192        \"parallel_execution\": config.parallel_execution,\n  1485\u2192        \"cache_results\": config.cache_results,\n  1486\u2192        \"limits\": {\n  1487\u2192            \"individual_limit\": config.individual_limit,\n  1488\u2192            \"final_limit\": config.final_limit\n  1489\u2192        }\n  1490\u2192    }\n  1491\u2192\n  1492\u2192@app.put(\"/search/config/weights\")\n  1493\u2192async def update_search_weights(\n  1494\u2192    bm25: Optional[float] = None,\n  1495\u2192    semantic: Optional[float] = None,\n  1496\u2192    fuzzy: Optional[float] = None,\n  1497\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1498\u2192) -> Dict[str, Any]:\n  1499\u2192    \"\"\"Update hybrid search weights (admin only).\n  1500\u2192    \n  1501\u2192    Weights will be normalized to sum to 1.0.\n  1502\u2192    \"\"\"\n  1503\u2192    if not hybrid_search:\n  1504\u2192        raise HTTPException(503, \"Hybrid search not available\")\n  1505\u2192    \n  1506\u2192    try:\n  1507\u2192        hybrid_search.set_weights(bm25=bm25, semantic=semantic, fuzzy=fuzzy)\n  1508\u2192        \n  1509\u2192        # Get updated config\n  1510\u2192        config = hybrid_search.config\n  1511\u2192        \n  1512\u2192        logger.info(f\"Search weights updated by {current_user.username}: \"\n  1513\u2192                   f\"BM25={config.bm25_weight:.3f}, \"\n  1514\u2192                   f\"Semantic={config.semantic_weight:.3f}, \"\n  1515\u2192                   f\"Fuzzy={config.fuzzy_weight:.3f}\")\n  1516\u2192        \n  1517\u2192        return {\n  1518\u2192            \"status\": \"success\",\n  1519\u2192            \"weights\": {\n  1520\u2192                \"bm25\": config.bm25_weight,\n  1521\u2192                \"semantic\": config.semantic_weight,\n  1522\u2192                \"fuzzy\": config.fuzzy_weight\n  1523\u2192            }\n  1524\u2192        }\n  1525\u2192    except Exception as e:\n  1526\u2192        logger.error(f\"Failed to update search weights: {e}\")\n  1527\u2192        raise HTTPException(500, f\"Failed to update weights: {str(e)}\")\n  1528\u2192\n  1529\u2192@app.put(\"/search/config/methods\")\n  1530\u2192async def toggle_search_methods(\n  1531\u2192    bm25: Optional[bool] = None,\n  1532\u2192    semantic: Optional[bool] = None,\n  1533\u2192    fuzzy: Optional[bool] = None,\n  1534\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1535\u2192) -> Dict[str, Any]:\n  1536\u2192    \"\"\"Enable or disable search methods (admin only).\"\"\"\n  1537\u2192    if not hybrid_search:\n  1538\u2192        raise HTTPException(503, \"Hybrid search not available\")\n  1539\u2192    \n  1540\u2192    try:\n  1541\u2192        hybrid_search.enable_methods(bm25=bm25, semantic=semantic, fuzzy=fuzzy)\n  1542\u2192        \n  1543\u2192        # Get updated config\n  1544\u2192        config = hybrid_search.config\n  1545\u2192        \n  1546\u2192        logger.info(f\"Search methods updated by {current_user.username}: \"\n  1547\u2192                   f\"BM25={config.enable_bm25}, \"\n  1548\u2192                   f\"Semantic={config.enable_semantic}, \"\n  1549\u2192                   f\"Fuzzy={config.enable_fuzzy}\")\n  1550\u2192        \n  1551\u2192        return {\n  1552\u2192            \"status\": \"success\",\n  1553\u2192            \"enabled_methods\": {\n  1554\u2192                \"bm25\": config.enable_bm25,\n  1555\u2192                \"semantic\": config.enable_semantic,\n  1556\u2192                \"fuzzy\": config.enable_fuzzy\n  1557\u2192            }\n  1558\u2192        }\n  1559\u2192    except Exception as e:\n  1560\u2192        logger.error(f\"Failed to update search methods: {e}\")\n  1561\u2192        raise HTTPException(500, f\"Failed to update methods: {str(e)}\")\n  1562\u2192\n  1563\u2192@app.get(\"/search/statistics\")\n  1564\u2192async def get_search_statistics(\n  1565\u2192    current_user: User = Depends(require_permission(Permission.READ))\n  1566\u2192) -> Dict[str, Any]:\n  1567\u2192    \"\"\"Get search statistics and performance metrics.\"\"\"\n  1568\u2192    stats = {}\n  1569\u2192    \n  1570\u2192    # Hybrid search statistics\n  1571\u2192    if hybrid_search:\n  1572\u2192        stats[\"hybrid_search\"] = hybrid_search.get_statistics()\n  1573\u2192    \n  1574\u2192    # BM25 statistics\n  1575\u2192    if bm25_indexer:\n  1576\u2192        stats[\"bm25\"] = bm25_indexer.get_statistics()\n  1577\u2192    \n  1578\u2192    # Fuzzy search statistics\n  1579\u2192    if fuzzy_indexer and hasattr(fuzzy_indexer, 'get_statistics'):\n  1580\u2192        stats[\"fuzzy\"] = fuzzy_indexer.get_statistics()\n  1581\u2192    \n  1582\u2192    # Add general search metrics from business metrics\n  1583\u2192    if business_metrics:\n  1584\u2192        search_metrics = business_metrics.get_search_metrics()\n  1585\u2192        stats[\"general\"] = {\n  1586\u2192            \"total_searches\": search_metrics.get(\"total_searches\", 0),\n  1587\u2192            \"average_response_time_ms\": search_metrics.get(\"avg_response_time\", 0),\n  1588\u2192            \"search_success_rate\": search_metrics.get(\"success_rate\", 0)\n  1589\u2192        }\n  1590\u2192    \n  1591\u2192    return stats\n  1592\u2192\n  1593\u2192@app.post(\"/search/optimize\")\n  1594\u2192async def optimize_search_indexes(\n  1595\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1596\u2192) -> Dict[str, Any]:\n  1597\u2192    \"\"\"Optimize search indexes for better performance (admin only).\"\"\"\n  1598\u2192    results = {}\n  1599\u2192    \n  1600\u2192    try:\n  1601\u2192        # Optimize BM25 indexes\n  1602\u2192        if bm25_indexer:\n  1603\u2192            bm25_indexer.optimize()\n  1604\u2192            results[\"bm25\"] = \"optimized\"\n  1605\u2192            logger.info(\"BM25 indexes optimized\")\n  1606\u2192        \n  1607\u2192        # Optimize FTS5 tables in SQLite\n  1608\u2192        if sqlite_store:\n  1609\u2192            sqlite_store.optimize_fts_tables()\n  1610\u2192            results[\"fts5\"] = \"optimized\"\n  1611\u2192            logger.info(\"FTS5 tables optimized\")\n  1612\u2192        \n  1613\u2192        # Clear hybrid search cache\n  1614\u2192        if hybrid_search:\n  1615\u2192            hybrid_search.clear_cache()\n  1616\u2192            results[\"hybrid_cache\"] = \"cleared\"\n  1617\u2192            logger.info(\"Hybrid search cache cleared\")\n  1618\u2192        \n  1619\u2192        logger.info(f\"Search indexes optimized by {current_user.username}\")\n  1620\u2192        \n  1621\u2192        return {\n  1622\u2192            \"status\": \"success\",\n  1623\u2192            \"message\": \"Search indexes optimized successfully\",\n  1624\u2192            \"results\": results\n  1625\u2192        }\n  1626\u2192    except Exception as e:\n  1627\u2192        logger.error(f\"Failed to optimize search indexes: {e}\")\n  1628\u2192        raise HTTPException(500, f\"Failed to optimize indexes: {str(e)}\")\n  1629\u2192\n  1630\u2192@app.get(\"/search/term/{term}/stats\")\n  1631\u2192async def get_term_statistics(\n  1632\u2192    term: str,\n  1633\u2192    current_user: User = Depends(require_permission(Permission.READ))\n  1634\u2192) -> Dict[str, Any]:\n  1635\u2192    \"\"\"Get statistics for a specific search term.\"\"\"\n  1636\u2192    stats = {}\n  1637\u2192    \n  1638\u2192    try:\n  1639\u2192        # BM25 term statistics\n  1640\u2192        if bm25_indexer:\n  1641\u2192            stats[\"bm25\"] = bm25_indexer.get_term_statistics(term)\n  1642\u2192        \n  1643\u2192        # SQLite FTS5 statistics\n  1644\u2192        if sqlite_store:\n  1645\u2192            stats[\"fts5\"] = sqlite_store.get_bm25_term_statistics(term)\n  1646\u2192        \n  1647\u2192        return {\n  1648\u2192            \"term\": term,\n  1649\u2192            \"statistics\": stats\n  1650\u2192        }\n  1651\u2192    except Exception as e:\n  1652\u2192        logger.error(f\"Failed to get term statistics: {e}\")\n  1653\u2192        raise HTTPException(500, f\"Failed to get term statistics: {str(e)}\")\n  1654\u2192\n  1655\u2192@app.post(\"/search/rebuild\")\n  1656\u2192async def rebuild_search_indexes(\n  1657\u2192    index_type: str = \"all\",  # \"all\", \"bm25\", \"fuzzy\", \"semantic\"\n  1658\u2192    current_user: User = Depends(require_role(UserRole.ADMIN))\n  1659\u2192) -> Dict[str, Any]:\n  1660\u2192    \"\"\"Rebuild search indexes (admin only).\"\"\"\n  1661\u2192    if index_type not in [\"all\", \"bm25\", \"fuzzy\", \"semantic\"]:\n  1662\u2192        raise HTTPException(400, \"Invalid index_type. Must be 'all', 'bm25', 'fuzzy', or 'semantic'\")\n  1663\u2192    \n  1664\u2192    results = {}\n  1665\u2192    \n  1666\u2192    try:\n  1667\u2192        if index_type in [\"all\", \"bm25\"] and bm25_indexer:\n  1668\u2192            bm25_indexer.rebuild()\n  1669\u2192            results[\"bm25\"] = \"rebuilt\"\n  1670\u2192            logger.info(\"BM25 index rebuilt\")\n  1671\u2192        \n  1672\u2192        if index_type in [\"all\", \"fuzzy\"] and fuzzy_indexer:\n  1673\u2192            fuzzy_indexer.clear()\n  1674\u2192            # Re-index all files\n  1675\u2192            if sqlite_store:\n  1676\u2192                files = sqlite_store.get_all_files()\n  1677\u2192                for file_info in files:\n  1678\u2192                    try:\n  1679\u2192                        with open(file_info[\"path\"], 'r', encoding='utf-8') as f:\n  1680\u2192                            content = f.read()\n  1681\u2192                        fuzzy_indexer.add_file(file_info[\"path\"], content)\n  1682\u2192                    except Exception as e:\n  1683\u2192                        logger.warning(f\"Failed to re-index {file_info['path']}: {e}\")\n  1684\u2192            results[\"fuzzy\"] = \"rebuilt\"\n  1685\u2192            logger.info(\"Fuzzy index rebuilt\")\n  1686\u2192        \n  1687\u2192        if index_type in [\"all\", \"semantic\"]:\n  1688\u2192            # Semantic index rebuild would go here if available\n  1689\u2192            if hasattr(hybrid_search, 'semantic_indexer') and hybrid_search.semantic_indexer:\n  1690\u2192                results[\"semantic\"] = \"rebuild_not_implemented\"\n  1691\u2192            else:\n  1692\u2192                results[\"semantic\"] = \"not_available\"\n  1693\u2192        \n  1694\u2192        logger.info(f\"Search indexes rebuilt by {current_user.username}: {index_type}\")\n  1695\u2192        \n  1696\u2192        return {\n  1697\u2192            \"status\": \"success\",\n  1698\u2192            \"message\": f\"Search indexes rebuilt successfully\",\n  1699\u2192            \"index_type\": index_type,\n  1700\u2192            \"results\": results\n  1701\u2192        }\n  1702\u2192    except Exception as e:\n  1703\u2192        logger.error(f\"Failed to rebuild search indexes: {e}\")\n  1704\u2192        raise HTTPException(500, f\"Failed to rebuild indexes: {str(e)}\")\n  1705\u2192\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 8,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 237,
    "output_tokens_total": 237,
    "cache_read_tokens": 59242,
    "cache_creation_tokens": 33236,
    "tokens_per_result": 237,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "a9f87083-f603-4b60-a151-8cabc58f385f",
    "test_date": "2025-06-26T07:40:39.873Z",
    "query_id": "transcript_a9f87083-f603-4b60-a151-8cabc58f385f",
    "query_text": [
      {
        "type": "tool_result",
        "content": "Claude requested permissions to use Bash, but you haven't granted it yet.",
        "is_error": true,
        "tool_use_id": "toolu_01YG8snCckKRLCx49m96r8Br"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 8,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 119,
    "output_tokens_total": 119,
    "cache_read_tokens": 92478,
    "cache_creation_tokens": 899,
    "tokens_per_result": 119,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "a9f87083-f603-4b60-a151-8cabc58f385f",
    "test_date": "2025-06-26T07:40:44.416Z",
    "query_id": "transcript_a9f87083-f603-4b60-a151-8cabc58f385f",
    "query_text": [
      {
        "type": "tool_result",
        "content": "Claude requested permissions to use Bash, but you haven't granted it yet.",
        "is_error": true,
        "tool_use_id": "toolu_01ENroNedxpTy7utd5SxohmL"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 8,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 203,
    "output_tokens_total": 203,
    "cache_read_tokens": 93377,
    "cache_creation_tokens": 154,
    "tokens_per_result": 203,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "a9f87083-f603-4b60-a151-8cabc58f385f",
    "test_date": "2025-06-26T07:40:50.439Z",
    "query_id": "transcript_a9f87083-f603-4b60-a151-8cabc58f385f",
    "query_text": [
      {
        "tool_use_id": "toolu_01JGCnbxWBSvMPKDQqxbCfvj",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"\n     2\u2192Shared Interfaces for MCP Server\n     3\u2192\n     4\u2192Cross-cutting interfaces and common types used across all modules.\n     5\u2192These interfaces define the contracts for logging, metrics, configuration,\n     6\u2192caching, events, and other shared concerns.\n     7\u2192\"\"\"\n     8\u2192\n     9\u2192from abc import ABC, abstractmethod\n    10\u2192from typing import Any, Dict, List, Optional, Callable, ContextManager, Awaitable, TypeVar, Generic\n    11\u2192from dataclasses import dataclass\n    12\u2192from datetime import datetime\n    13\u2192from enum import Enum\n    14\u2192\n    15\u2192T = TypeVar('T')\n    16\u2192\n    17\u2192# ========================================\n    18\u2192# Common Enums\n    19\u2192# ========================================\n    20\u2192\n    21\u2192class LogLevel(Enum):\n    22\u2192    \"\"\"Log levels for the logging system\"\"\"\n    23\u2192    DEBUG = \"debug\"\n    24\u2192    INFO = \"info\"\n    25\u2192    WARNING = \"warning\"\n    26\u2192    ERROR = \"error\"\n    27\u2192    CRITICAL = \"critical\"\n    28\u2192\n    29\u2192class IndexStatus(Enum):\n    30\u2192    \"\"\"Status of indexing operations\"\"\"\n    31\u2192    PENDING = \"pending\"\n    32\u2192    IN_PROGRESS = \"in_progress\"\n    33\u2192    COMPLETED = \"completed\"\n    34\u2192    FAILED = \"failed\"\n    35\u2192    CANCELLED = \"cancelled\"\n    36\u2192\n    37\u2192class PluginStatus(Enum):\n    38\u2192    \"\"\"Status of plugin lifecycle\"\"\"\n    39\u2192    UNLOADED = \"unloaded\"\n    40\u2192    LOADING = \"loading\"\n    41\u2192    READY = \"ready\"\n    42\u2192    ERROR = \"error\"\n    43\u2192    DISABLED = \"disabled\"\n    44\u2192\n    45\u2192# ========================================\n    46\u2192# Common Data Types\n    47\u2192# ========================================\n    48\u2192\n    49\u2192@dataclass\n    50\u2192class Event:\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 8,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 2957,
    "output_tokens_total": 2957,
    "cache_read_tokens": 93531,
    "cache_creation_tokens": 2060,
    "tokens_per_result": 2957,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "1ec7365b-990d-4e16-bf1f-f88a8cc6dbc3",
    "test_date": "2025-06-26T07:31:01.648Z",
    "query_id": "transcript_1ec7365b-990d-4e16-bf1f-f88a8cc6dbc3",
    "query_text": "Find the BM25Indexer class definition",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 13916,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "1ec7365b-990d-4e16-bf1f-f88a8cc6dbc3",
    "test_date": "2025-06-26T07:31:05.406Z",
    "query_id": "transcript_1ec7365b-990d-4e16-bf1f-f88a8cc6dbc3",
    "query_text": [
      {
        "tool_use_id": "toolu_016X3dpUCt4W1Sgkub16uyKX",
        "type": "tool_result",
        "content": "Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/bm25_indexer.py"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 2,
    "output_tokens_total": 2,
    "cache_read_tokens": 13916,
    "cache_creation_tokens": 226,
    "tokens_per_result": 2,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "be840494-fb6b-4efd-85e3-87a494a7de4b",
    "test_date": "2025-06-26T07:36:17.724Z",
    "query_id": "transcript_be840494-fb6b-4efd-85e3-87a494a7de4b",
    "query_text": "Find the BM25Indexer class definition",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 58,
    "output_tokens_total": 58,
    "cache_read_tokens": 13916,
    "cache_creation_tokens": 0,
    "tokens_per_result": 58,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "be840494-fb6b-4efd-85e3-87a494a7de4b",
    "test_date": "2025-06-26T07:36:20.463Z",
    "query_id": "transcript_be840494-fb6b-4efd-85e3-87a494a7de4b",
    "query_text": [
      {
        "tool_use_id": "toolu_011t9wysiLJAso7JJFU7rd7i",
        "type": "tool_result",
        "content": "Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/bm25_indexer.py"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 14142,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "21a1dd51-144a-415a-8e34-3b8b29c0aa7a",
    "test_date": "2025-06-26T07:36:25.893Z",
    "query_id": "transcript_21a1dd51-144a-415a-8e34-3b8b29c0aa7a",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 13920,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "21a1dd51-144a-415a-8e34-3b8b29c0aa7a",
    "test_date": "2025-06-26T07:36:29.226Z",
    "query_id": "transcript_21a1dd51-144a-415a-8e34-3b8b29c0aa7a",
    "query_text": [
      {
        "tool_use_id": "toolu_01CPDdcQTg9yPX1aqgw5k5Bf",
        "type": "tool_result",
        "content": "Found 4 files\n/workspaces/Code-Index-MCP/testing-env/worktree-native/test_document_queries.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/dispatcher/dispatcher_enhanced.py\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/level4/path_management.puml\n/workspaces/Code-Index-MCP/testing-env/worktree-native/architecture/path_management_architecture.md"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 27,
    "output_tokens_total": 27,
    "cache_read_tokens": 13920,
    "cache_creation_tokens": 447,
    "tokens_per_result": 27,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "21a1dd51-144a-415a-8e34-3b8b29c0aa7a",
    "test_date": "2025-06-26T07:36:33.312Z",
    "query_id": "transcript_21a1dd51-144a-415a-8e34-3b8b29c0aa7a",
    "query_text": [
      {
        "tool_use_id": "toolu_01MtQhGfGGjTgcyobqwUMrXZ",
        "type": "tool_result",
        "content": "     1\u2192\"\"\"Enhanced dispatcher with dynamic plugin loading via PluginFactory.\"\"\"\n     2\u2192from pathlib import Path\n     3\u2192from typing import Iterable, Dict, List, Optional, Tuple, Any, Union\n     4\u2192import logging\n     5\u2192import hashlib\n     6\u2192import time\n     7\u2192import re\n     8\u2192from datetime import datetime\n     9\u2192import os\n    10\u2192\n    11\u2192from ..plugin_base import IPlugin, SymbolDef, SearchResult, Reference\n    12\u2192from ..plugins.plugin_factory import PluginFactory\n    13\u2192from ..plugins.language_registry import get_language_by_extension, get_all_extensions\n    14\u2192from ..storage.sqlite_store import SQLiteStore\n    15\u2192from .plugin_router import PluginRouter, FileTypeMatcher, PluginCapability\n    16\u2192from .result_aggregator import ResultAggregator, AggregatedResult, AggregationStats, RankingCriteria\n    17\u2192# Note: We've removed ignore pattern checks to allow indexing ALL files\n    18\u2192# Filtering happens only during export via SecureIndexExporter\n    19\u2192# from ..core.ignore_patterns import get_ignore_manager\n    20\u2192\n    21\u2192logger = logging.getLogger(__name__)\n    22\u2192\n    23\u2192\n    24\u2192class EnhancedDispatcher:\n    25\u2192    \"\"\"Enhanced dispatcher with dynamic plugin loading and advanced routing capabilities.\"\"\"\n    26\u2192    \n    27\u2192    # Document query patterns - common documentation search terms\n    28\u2192    DOCUMENT_QUERY_PATTERNS = [\n    29\u2192        r'\\b(how\\s+to|howto)\\b',\n    30\u2192        r'\\b(getting\\s+started|get\\s+started)\\b',\n    31\u2192        r'\\b(installation|install|setup)\\b',\n    32\u2192        r'\\b(configuration|configure|config)\\b',\n    33\u2192        r'\\b(api\\s+doc|api\\s+documentation|api\\s+reference)\\b',\n    34\u2192        r'\\b(tutorial|guide|walkthrough)\\b',\n    35\u2192        r'\\b(example|sample|snippet)\\b',\n    36\u2192        r'\\b(readme|documentation|docs)\\b',\n    37\u2192        r'\\b(usage|use\\s+case|using)\\b',\n    38\u2192        r'\\b(reference|manual)\\b',\n    39\u2192        r'\\b(faq|frequently\\s+asked)\\b',\n    40\u2192        r'\\b(troubleshoot|troubleshooting|debug|debugging|error|errors|issue|issues)\\b',\n    41\u2192        r'\\b(best\\s+practice|best\\s+practices|convention|conventions)\\b',\n    42\u2192        r'\\b(architecture|design|overview)\\b',\n    43\u2192        r'\\b(changelog|release\\s+notes|migration)\\b'\n    44\u2192    ]\n    45\u2192    \n    46\u2192    # Documentation file patterns\n    47\u2192    DOCUMENTATION_FILE_PATTERNS = [\n    48\u2192        r'readme(\\.\\w+)?$',\n    49\u2192        r'changelog(\\.\\w+)?$',\n    50\u2192        r'contributing(\\.\\w+)?$',\n    51\u2192        r'license(\\.\\w+)?$',\n    52\u2192        r'install(\\.\\w+)?$',\n    53\u2192        r'setup(\\.\\w+)?$',\n    54\u2192        r'guide(\\.\\w+)?$',\n    55\u2192        r'tutorial(\\.\\w+)?$',\n    56\u2192        r'\\.md$',\n    57\u2192        r'\\.rst$',\n    58\u2192        r'\\.txt$',\n    59\u2192        r'docs?/',\n    60\u2192        r'documentation/'\n    61\u2192    ]\n    62\u2192    \n    63\u2192    def __init__(self, \n    64\u2192                 plugins: Optional[List[IPlugin]] = None,\n    65\u2192                 sqlite_store: Optional[SQLiteStore] = None,\n    66\u2192                 enable_advanced_features: bool = True,\n    67\u2192                 use_plugin_factory: bool = True,\n    68\u2192                 lazy_load: bool = True,\n    69\u2192                 semantic_search_enabled: bool = True):\n    70\u2192        \"\"\"Initialize the enhanced dispatcher.\n    71\u2192        \n    72\u2192        Args:\n    73\u2192            plugins: Optional list of pre-instantiated plugins (for backward compatibility)\n    74\u2192            sqlite_store: SQLite store for plugin persistence\n    75\u2192            enable_advanced_features: Whether to enable advanced routing and aggregation\n    76\u2192            use_plugin_factory: Whether to use PluginFactory for dynamic loading\n    77\u2192            lazy_load: Whether to lazy-load plugins on demand\n    78\u2192            semantic_search_enabled: Whether to enable semantic search in plugins\n    79\u2192        \"\"\"\n    80\u2192        self._sqlite_store = sqlite_store\n    81\u2192        self._enable_advanced = enable_advanced_features\n    82\u2192        self._use_factory = use_plugin_factory\n    83\u2192        self._lazy_load = lazy_load\n    84\u2192        self._semantic_enabled = semantic_search_enabled\n    85\u2192        \n    86\u2192        # Plugin storage\n    87\u2192        self._plugins: List[IPlugin] = []\n    88\u2192        self._by_lang: Dict[str, IPlugin] = {}\n    89\u2192        self._loaded_languages: set[str] = set()\n    90\u2192        \n    91\u2192        # Cache for file hashes to avoid re-indexing unchanged files\n    92\u2192        self._file_cache = {}  # path -> (mtime, size, content_hash)\n    93\u2192        \n    94\u2192        # Advanced components\n    95\u2192        if self._enable_advanced:\n    96\u2192            self._file_matcher = FileTypeMatcher()\n    97\u2192            self._router = PluginRouter(self._file_matcher)\n    98\u2192            self._aggregator = ResultAggregator()\n    99\u2192        \n   100\u2192        # Performance tracking\n   101\u2192        self._operation_stats = {\n   102\u2192            'searches': 0,\n   103\u2192            'lookups': 0,\n   104\u2192            'indexings': 0,\n   105\u2192            'total_time': 0.0,\n   106\u2192            'plugins_loaded': 0\n   107\u2192        }\n   108\u2192        \n   109\u2192        # Initialize plugins\n   110\u2192        if plugins:\n   111\u2192            # Use provided plugins (backward compatibility)\n   112\u2192            self._plugins = plugins\n   113\u2192            self._by_lang = {p.lang: p for p in plugins}\n   114\u2192            for plugin in plugins:\n   115\u2192                self._loaded_languages.add(getattr(plugin, 'lang', 'unknown'))\n   116\u2192            if self._enable_advanced:\n   117\u2192                self._register_plugins_with_router()\n   118\u2192        elif use_plugin_factory and not lazy_load:\n   119\u2192            # Load all plugins immediately\n   120\u2192            self._load_all_plugins()\n   121\u2192        # If lazy_load is True, plugins will be loaded on demand\n   122\u2192        \n   123\u2192        # Compile document query patterns for performance\n   124\u2192        self._compiled_doc_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENT_QUERY_PATTERNS]\n   125\u2192        self._compiled_file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.DOCUMENTATION_FILE_PATTERNS]\n   126\u2192        \n   127\u2192        logger.info(f\"Enhanced dispatcher initialized with {len(self._plugins)} plugins\")\n   128\u2192    \n   129\u2192    def _load_all_plugins(self):\n   130\u2192        \"\"\"Load all available plugins using PluginFactory.\"\"\"\n   131\u2192        logger.info(\"Loading all available plugins...\")\n   132\u2192        \n   133\u2192        all_plugins = PluginFactory.create_all_plugins(\n   134\u2192            sqlite_store=self._sqlite_store,\n   135\u2192            enable_semantic=self._semantic_enabled\n   136\u2192        )\n   137\u2192        \n   138\u2192        for lang, plugin in all_plugins.items():\n   139\u2192            self._plugins.append(plugin)\n   140\u2192            self._by_lang[lang] = plugin\n   141\u2192            self._loaded_languages.add(lang)\n   142\u2192            self._operation_stats['plugins_loaded'] += 1\n   143\u2192        \n   144\u2192        if self._enable_advanced:\n   145\u2192            self._register_plugins_with_router()\n   146\u2192        \n   147\u2192        logger.info(f\"Loaded {len(all_plugins)} plugins: {', '.join(sorted(all_plugins.keys()))}\")\n   148\u2192    \n   149\u2192    def _ensure_plugin_loaded(self, language: str) -> Optional[IPlugin]:\n   150\u2192        \"\"\"Ensure a plugin for the given language is loaded.\n   151\u2192        \n   152\u2192        Args:\n   153\u2192            language: Language code (e.g., 'python', 'go')\n   154\u2192            \n   155\u2192        Returns:\n   156\u2192            Plugin instance or None if not available\n   157\u2192        \"\"\"\n   158\u2192        # Normalize language\n   159\u2192        language = language.lower().replace('-', '_')\n   160\u2192        \n   161\u2192        # Check if already loaded\n   162\u2192        if language in self._by_lang:\n   163\u2192            return self._by_lang[language]\n   164\u2192        \n   165\u2192        # If not using factory or already tried to load, return None\n   166\u2192        if not self._use_factory or language in self._loaded_languages:\n   167\u2192            return None\n   168\u2192        \n   169\u2192        # Try to load the plugin\n   170\u2192        try:\n   171\u2192            logger.info(f\"Lazy loading plugin for {language}\")\n   172\u2192            plugin = PluginFactory.create_plugin(\n   173\u2192                language,\n   174\u2192                sqlite_store=self._sqlite_store,\n   175\u2192                enable_semantic=self._semantic_enabled\n   176\u2192            )\n   177\u2192            \n   178\u2192            # Add to collections\n   179\u2192            self._plugins.append(plugin)\n   180\u2192            self._by_lang[language] = plugin\n   181\u2192            self._loaded_languages.add(language)\n   182\u2192            self._operation_stats['plugins_loaded'] += 1\n   183\u2192            \n   184\u2192            # Register with router if needed\n   185\u2192            if self._enable_advanced:\n   186\u2192                capabilities = self._detect_plugin_capabilities(plugin)\n   187\u2192                self._router.register_plugin(plugin, capabilities)\n   188\u2192            \n   189\u2192            logger.info(f\"Successfully loaded {language} plugin\")\n   190\u2192            return plugin\n   191\u2192            \n   192\u2192        except ValueError as e:\n   193\u2192            logger.warning(f\"No plugin available for {language}: {e}\")\n   194\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   195\u2192            return None\n   196\u2192        except Exception as e:\n   197\u2192            logger.error(f\"Error loading plugin for {language}: {e}\")\n   198\u2192            self._loaded_languages.add(language)  # Mark as attempted\n   199\u2192            return None\n   200\u2192    \n   201\u2192    def _ensure_plugin_for_file(self, path: Path) -> Optional[IPlugin]:\n   202\u2192        \"\"\"Ensure a plugin is loaded for the given file.\n   203\u2192        \n   204\u2192        Args:\n   205\u2192            path: File path\n   206\u2192            \n   207\u2192        Returns:\n   208\u2192            Plugin instance or None if not available\n   209\u2192        \"\"\"\n   210\u2192        # Get language from file extension\n   211\u2192        extension = path.suffix.lower()\n   212\u2192        language = get_language_by_extension(extension)\n   213\u2192        \n   214\u2192        if language:\n   215\u2192            return self._ensure_plugin_loaded(language)\n   216\u2192        \n   217\u2192        # Fallback: try all loaded plugins\n   218\u2192        for plugin in self._plugins:\n   219\u2192            if plugin.supports(path):\n   220\u2192                return plugin\n   221\u2192        \n   222\u2192        return None\n   223\u2192    \n   224\u2192    def _register_plugins_with_router(self):\n   225\u2192        \"\"\"Register plugins with the router and assign capabilities.\"\"\"\n   226\u2192        for plugin in self._plugins:\n   227\u2192            # Determine capabilities based on plugin type/language\n   228\u2192            capabilities = self._detect_plugin_capabilities(plugin)\n   229\u2192            self._router.register_plugin(plugin, capabilities)\n   230\u2192    \n   231\u2192    def _detect_plugin_capabilities(self, plugin: IPlugin) -> List[PluginCapability]:\n   232\u2192        \"\"\"Detect capabilities for a plugin based on its language and features.\"\"\"\n   233\u2192        capabilities = []\n   234\u2192        lang = getattr(plugin, 'lang', 'unknown')\n   235\u2192        \n   236\u2192        # Base capabilities all plugins have\n   237\u2192        capabilities.append(PluginCapability(\n   238\u2192            'syntax_analysis', '1.0', f'{lang} syntax analysis', \n   239\u2192            priority=70, metadata={'language': lang}\n   240\u2192        ))\n   241\u2192        \n   242\u2192        capabilities.append(PluginCapability(\n   243\u2192            'code_search', '1.0', f'{lang} code search', \n   244\u2192            priority=80, metadata={'language': lang}\n   245\u2192        ))\n   246\u2192        \n   247\u2192        # Check for semantic search capability\n   248\u2192        if hasattr(plugin, '_enable_semantic') and plugin._enable_semantic:\n   249\u2192            capabilities.append(PluginCapability(\n   250\u2192                'semantic_search', '1.0', f'{lang} semantic search', \n   251\u2192                priority=90, metadata={'language': lang}\n   252\u2192            ))\n   253\u2192        \n   254\u2192        # Language-specific capabilities\n   255\u2192        if lang == 'python':\n   256\u2192            capabilities.extend([\n   257\u2192                PluginCapability('refactoring', '1.0', 'Python refactoring support', 75),\n   258\u2192                PluginCapability('type_analysis', '1.0', 'Python type analysis', 85)\n   259\u2192            ])\n   260\u2192        elif lang in ['javascript', 'typescript']:\n   261\u2192            capabilities.extend([\n   262\u2192                PluginCapability('linting', '1.0', 'JavaScript/TypeScript linting', 85),\n   263\u2192                PluginCapability('bundling_analysis', '1.0', 'Module bundling analysis', 70),\n   264\u2192                PluginCapability('framework_support', '1.0', 'Framework-specific support', 75)\n   265\u2192            ])\n   266\u2192        elif lang in ['c', 'cpp']:\n   267\u2192            capabilities.extend([\n   268\u2192                PluginCapability('compilation_analysis', '1.0', 'Compilation analysis', 80),\n   269\u2192                PluginCapability('memory_analysis', '1.0', 'Memory usage analysis', 70),\n   270\u2192                PluginCapability('performance_profiling', '1.0', 'Performance profiling', 75)\n   271\u2192            ])\n   272\u2192        elif lang in ['go', 'rust']:\n   273\u2192            capabilities.extend([\n   274\u2192                PluginCapability('package_analysis', '1.0', f'{lang} package analysis', 80),\n   275\u2192                PluginCapability('concurrency_analysis', '1.0', f'{lang} concurrency analysis', 75)\n   276\u2192            ])\n   277\u2192        elif lang in ['java', 'kotlin', 'scala']:\n   278\u2192            capabilities.extend([\n   279\u2192                PluginCapability('jvm_analysis', '1.0', 'JVM bytecode analysis', 75),\n   280\u2192                PluginCapability('build_tool_integration', '1.0', 'Build tool integration', 70)\n   281\u2192            ])\n   282\u2192        \n   283\u2192        return capabilities\n   284\u2192    \n   285\u2192    @property\n   286\u2192    def plugins(self):\n   287\u2192        \"\"\"Get the dictionary of loaded plugins by language.\"\"\"\n   288\u2192        return self._by_lang\n   289\u2192    \n   290\u2192    @property\n   291\u2192    def supported_languages(self) -> List[str]:\n   292\u2192        \"\"\"Get list of all supported languages (loaded and available).\"\"\"\n   293\u2192        if self._use_factory:\n   294\u2192            return PluginFactory.get_supported_languages()\n   295\u2192        else:\n   296\u2192            return list(self._by_lang.keys())\n   297\u2192    \n   298\u2192    def _match_plugin(self, path: Path) -> IPlugin:\n   299\u2192        \"\"\"Match a plugin for the given file path.\"\"\"\n   300\u2192        # Ensure plugin is loaded if using lazy loading\n   301\u2192        if self._lazy_load and self._use_factory:\n   302\u2192            plugin = self._ensure_plugin_for_file(path)\n   303\u2192            if plugin:\n   304\u2192                return plugin\n   305\u2192        \n   306\u2192        # Use advanced routing if available\n   307\u2192        if self._enable_advanced and self._router:\n   308\u2192            route_result = self._router.get_best_plugin(path)\n   309\u2192            if route_result:\n   310\u2192                return route_result.plugin\n   311\u2192        \n   312\u2192        # Fallback to basic matching\n   313\u2192        for p in self._plugins:\n   314\u2192            if p.supports(path):\n   315\u2192                return p\n   316\u2192        \n   317\u2192        raise RuntimeError(f\"No plugin found for {path}\")\n   318\u2192    \n   319\u2192    def get_plugins_for_file(self, path: Path) -> List[Tuple[IPlugin, float]]:\n   320\u2192        \"\"\"Get all plugins that can handle a file with confidence scores.\"\"\"\n   321\u2192        # Ensure plugin is loaded if using lazy loading\n   322\u2192        if self._lazy_load and self._use_factory:\n   323\u2192            self._ensure_plugin_for_file(path)\n   324\u2192        \n   325\u2192        if self._enable_advanced and self._router:\n   326\u2192            route_results = self._router.route_file(path)\n   327\u2192            return [(result.plugin, result.confidence) for result in route_results]\n   328\u2192        else:\n   329\u2192            # Basic fallback\n   330\u2192            matching_plugins = []\n   331\u2192            for plugin in self._plugins:\n   332\u2192                if plugin.supports(path):\n   333\u2192                    matching_plugins.append((plugin, 1.0))\n   334\u2192            return matching_plugins\n   335\u2192    \n   336\u2192    def lookup(self, symbol: str) -> SymbolDef | None:\n   337\u2192        \"\"\"Look up symbol definition across all plugins.\"\"\"\n   338\u2192        start_time = time.time()\n   339\u2192        \n   340\u2192        try:\n   341\u2192            # For symbol lookup, we may need to search across all languages\n   342\u2192            # Load all plugins if using lazy loading\n   343\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   344\u2192                self._load_all_plugins()\n   345\u2192            \n   346\u2192            if self._enable_advanced and self._aggregator:\n   347\u2192                # Use advanced aggregation\n   348\u2192                definitions_by_plugin = {}\n   349\u2192                for plugin in self._plugins:\n   350\u2192                    try:\n   351\u2192                        definition = plugin.getDefinition(symbol)\n   352\u2192                        definitions_by_plugin[plugin] = definition\n   353\u2192                    except Exception as e:\n   354\u2192                        logger.warning(f\"Plugin {plugin.lang} failed to get definition for {symbol}: {e}\")\n   355\u2192                        definitions_by_plugin[plugin] = None\n   356\u2192                \n   357\u2192                result = self._aggregator.aggregate_symbol_definitions(definitions_by_plugin)\n   358\u2192                \n   359\u2192                self._operation_stats['lookups'] += 1\n   360\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   361\u2192                \n   362\u2192                return result\n   363\u2192            else:\n   364\u2192                # Fallback to basic lookup\n   365\u2192                for p in self._plugins:\n   366\u2192                    res = p.getDefinition(symbol)\n   367\u2192                    if res:\n   368\u2192                        self._operation_stats['lookups'] += 1\n   369\u2192                        self._operation_stats['total_time'] += time.time() - start_time\n   370\u2192                        return res\n   371\u2192                return None\n   372\u2192                \n   373\u2192        except Exception as e:\n   374\u2192            logger.error(f\"Error in symbol lookup for {symbol}: {e}\", exc_info=True)\n   375\u2192            return None\n   376\u2192    \n   377\u2192    def _is_document_query(self, query: str) -> bool:\n   378\u2192        \"\"\"Check if the query is looking for documentation.\n   379\u2192        \n   380\u2192        Args:\n   381\u2192            query: Search query string\n   382\u2192            \n   383\u2192        Returns:\n   384\u2192            True if this appears to be a documentation query\n   385\u2192        \"\"\"\n   386\u2192        query_lower = query.lower()\n   387\u2192        \n   388\u2192        # Check against document query patterns\n   389\u2192        for pattern in self._compiled_doc_patterns:\n   390\u2192            if pattern.search(query_lower):\n   391\u2192                return True\n   392\u2192        \n   393\u2192        # Check for question words at the beginning\n   394\u2192        question_starters = ['how', 'what', 'where', 'when', 'why', 'can', 'is', 'does', 'should']\n   395\u2192        first_word = query_lower.split()[0] if query_lower.split() else ''\n   396\u2192        if first_word in question_starters:\n   397\u2192            return True\n   398\u2192        \n   399\u2192        return False\n   400\u2192    \n   401\u2192    def _expand_document_query(self, query: str) -> List[str]:\n   402\u2192        \"\"\"Expand a document query with related terms for better search coverage.\n   403\u2192        \n   404\u2192        Args:\n   405\u2192            query: Original search query\n   406\u2192            \n   407\u2192        Returns:\n   408\u2192            List of expanded query variations\n   409\u2192        \"\"\"\n   410\u2192        expanded_queries = [query]  # Always include original\n   411\u2192        query_lower = query.lower()\n   412\u2192        \n   413\u2192        # Common expansions for documentation queries\n   414\u2192        expansions = {\n   415\u2192            'install': ['installation', 'setup', 'getting started', 'requirements'],\n   416\u2192            'config': ['configuration', 'configure', 'settings', 'options', 'parameters'],\n   417\u2192            'api': ['api documentation', 'api reference', 'endpoint', 'method'],\n   418\u2192            'how to': ['tutorial', 'guide', 'example', 'usage'],\n   419\u2192            'example': ['sample', 'snippet', 'demo', 'code example'],\n   420\u2192            'error': ['troubleshoot', 'debug', 'issue', 'problem', 'fix'],\n   421\u2192            'getting started': ['quickstart', 'tutorial', 'introduction', 'setup'],\n   422\u2192            'guide': ['tutorial', 'documentation', 'walkthrough', 'how to'],\n   423\u2192            'usage': ['how to use', 'example', 'api', 'reference']\n   424\u2192        }\n   425\u2192        \n   426\u2192        # Apply expansions\n   427\u2192        for term, related_terms in expansions.items():\n   428\u2192            if term in query_lower:\n   429\u2192                for related in related_terms:\n   430\u2192                    # Replace the term with related term\n   431\u2192                    expanded = query_lower.replace(term, related)\n   432\u2192                    if expanded != query_lower and expanded not in expanded_queries:\n   433\u2192                        expanded_queries.append(expanded)\n   434\u2192                \n   435\u2192                # Also add queries with additional terms\n   436\u2192                for related in related_terms[:2]:  # Limit to avoid too many queries\n   437\u2192                    expanded = f\"{query} {related}\"\n   438\u2192                    if expanded not in expanded_queries:\n   439\u2192                        expanded_queries.append(expanded)\n   440\u2192        \n   441\u2192        # Add file-specific searches for common documentation files\n   442\u2192        if self._is_document_query(query):\n   443\u2192            # Extract the main topic from the query\n   444\u2192            topic_words = []\n   445\u2192            for word in query.lower().split():\n   446\u2192                if word not in ['how', 'to', 'the', 'a', 'an', 'is', 'are', 'what', 'where', 'when']:\n   447\u2192                    topic_words.append(word)\n   448\u2192            \n   449\u2192            if topic_words:\n   450\u2192                topic = ' '.join(topic_words[:2])  # Use first two topic words\n   451\u2192                expanded_queries.extend([\n   452\u2192                    f\"README {topic}\",\n   453\u2192                    f\"{topic} documentation\",\n   454\u2192                    f\"{topic} docs\",\n   455\u2192                    f\"{topic} guide\"\n   456\u2192                ])\n   457\u2192        \n   458\u2192        # Remove duplicates while preserving order\n   459\u2192        seen = set()\n   460\u2192        unique_queries = []\n   461\u2192        for q in expanded_queries:\n   462\u2192            if q not in seen:\n   463\u2192                seen.add(q)\n   464\u2192                unique_queries.append(q)\n   465\u2192        \n   466\u2192        return unique_queries[:10]  # Limit to 10 queries max\n   467\u2192    \n   468\u2192    def _is_documentation_file(self, file_path: str) -> bool:\n   469\u2192        \"\"\"Check if a file path is likely a documentation file.\n   470\u2192        \n   471\u2192        Args:\n   472\u2192            file_path: Path to check\n   473\u2192            \n   474\u2192        Returns:\n   475\u2192            True if this appears to be a documentation file\n   476\u2192        \"\"\"\n   477\u2192        path_lower = file_path.lower()\n   478\u2192        \n   479\u2192        for pattern in self._compiled_file_patterns:\n   480\u2192            if pattern.search(path_lower):\n   481\u2192                return True\n   482\u2192        \n   483\u2192        return False\n   484\u2192    \n   485\u2192    def _adjust_ranking_for_documents(self, query: str, results: List[AggregatedResult]) -> List[AggregatedResult]:\n   486\u2192        \"\"\"Adjust ranking to prioritize documentation files for document queries.\n   487\u2192        \n   488\u2192        Args:\n   489\u2192            query: Original search query\n   490\u2192            results: List of aggregated results\n   491\u2192            \n   492\u2192        Returns:\n   493\u2192            Re-ranked results with documentation prioritized\n   494\u2192        \"\"\"\n   495\u2192        if not self._is_document_query(query):\n   496\u2192            return results\n   497\u2192        \n   498\u2192        # Separate documentation and code results\n   499\u2192        doc_results = []\n   500\u2192        code_results = []\n   501\u2192        \n   502\u2192        for result in results:\n   503\u2192            if self._is_documentation_file(result.primary_result.get('file', '')):\n   504\u2192                # Boost documentation files for document queries\n   505\u2192                result.rank_score *= 1.5\n   506\u2192                result.metadata['doc_boost'] = True\n   507\u2192                doc_results.append(result)\n   508\u2192            else:\n   509\u2192                code_results.append(result)\n   510\u2192        \n   511\u2192        # Sort each group by rank score\n   512\u2192        doc_results.sort(key=lambda r: r.rank_score, reverse=True)\n   513\u2192        code_results.sort(key=lambda r: r.rank_score, reverse=True)\n   514\u2192        \n   515\u2192        # Combine with documentation files first\n   516\u2192        return doc_results + code_results\n   517\u2192    \n   518\u2192    def search(self, query: str, semantic=False, limit=20) -> Iterable[SearchResult]:\n   519\u2192        \"\"\"Search for code and documentation across all plugins.\"\"\"\n   520\u2192        start_time = time.time()\n   521\u2192        \n   522\u2192        try:\n   523\u2192            # For search, we may need to search across all languages\n   524\u2192            # Load all plugins if using lazy loading\n   525\u2192            if self._lazy_load and self._use_factory and len(self._plugins) == 0:\n   526\u2192                self._load_all_plugins()\n   527\u2192            \n   528\u2192            # Detect if this is a document query\n   529\u2192            is_doc_query = self._is_document_query(query)\n   530\u2192            \n   531\u2192            # Expand query if it's a document query\n   532\u2192            queries = [query]\n   533\u2192            if is_doc_query:\n   534\u2192                queries = self._expand_document_query(query)\n   535\u2192                logger.info(f\"Expanded document query '{query}' to {len(queries)} variations\")\n   536\u2192                # Force semantic search for natural language queries\n   537\u2192                semantic = True\n   538\u2192            \n   539\u2192            if self._enable_advanced and self._aggregator:\n   540\u2192                # Use advanced aggregation\n   541\u2192                all_results_by_plugin = {}\n   542\u2192                opts = {\"semantic\": semantic, \"limit\": limit * 2 if is_doc_query else limit}\n   543\u2192                \n   544\u2192                # Search with all query variations\n   545\u2192                for search_query in queries:\n   546\u2192                    for plugin in self._plugins:\n   547\u2192                        try:\n   548\u2192                            results = list(plugin.search(search_query, opts))\n   549\u2192                            if results:\n   550\u2192                                if plugin not in all_results_by_plugin:\n   551\u2192                                    all_results_by_plugin[plugin] = []\n   552\u2192                                all_results_by_plugin[plugin].extend(results)\n   553\u2192                        except Exception as e:\n   554\u2192                            logger.warning(f\"Plugin {plugin.lang} failed to search for {search_query}: {e}\")\n   555\u2192                \n   556\u2192                # Deduplicate results per plugin\n   557\u2192                for plugin, results in all_results_by_plugin.items():\n   558\u2192                    seen = set()\n   559\u2192                    unique_results = []\n   560\u2192                    for result in results:\n   561\u2192                        key = f\"{result['file']}:{result['line']}\"\n   562\u2192                        if key not in seen:\n   563\u2192                            seen.add(key)\n   564\u2192                            unique_results.append(result)\n   565\u2192                    all_results_by_plugin[plugin] = unique_results\n   566\u2192                \n   567\u2192                # Configure aggregator for document queries\n   568\u2192                if is_doc_query and self._enable_advanced:\n   569\u2192                    # Adjust ranking criteria for documentation\n   570\u2192                    doc_criteria = RankingCriteria(\n   571\u2192                        relevance_weight=0.5,      # Increase relevance weight\n   572\u2192                        confidence_weight=0.2,     # Reduce confidence weight\n   573\u2192                        frequency_weight=0.2,      # Keep frequency weight\n   574\u2192                        recency_weight=0.1,        # Keep recency weight\n   575\u2192                        prefer_exact_matches=False,  # Natural language doesn't need exact matches\n   576\u2192                        boost_multiple_sources=True,\n   577\u2192                        boost_common_extensions=True\n   578\u2192                    )\n   579\u2192                    self._aggregator.configure(ranking_criteria=doc_criteria)\n   580\u2192                \n   581\u2192                aggregated_results, stats = self._aggregator.aggregate_search_results(\n   582\u2192                    all_results_by_plugin, limit=limit * 2 if is_doc_query else limit\n   583\u2192                )\n   584\u2192                \n   585\u2192                # Adjust ranking for document queries\n   586\u2192                if is_doc_query:\n   587\u2192                    aggregated_results = self._adjust_ranking_for_documents(query, aggregated_results)\n   588\u2192                \n   589\u2192                # Apply final limit\n   590\u2192                if limit and len(aggregated_results) > limit:\n   591\u2192                    aggregated_results = aggregated_results[:limit]\n   592\u2192                \n   593\u2192                logger.debug(f\"Search aggregation stats: {stats.total_results} total, \"\n   594\u2192                           f\"{stats.unique_results} unique, {stats.plugins_used} plugins used, \"\n   595\u2192                           f\"document_query={is_doc_query}\")\n   596\u2192                \n   597\u2192                self._operation_stats['searches'] += 1\n   598\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   599\u2192                \n   600\u2192                # Yield primary results from aggregated results\n   601\u2192                for aggregated in aggregated_results:\n   602\u2192                    yield aggregated.primary_result\n   603\u2192            else:\n   604\u2192                # Fallback to basic search\n   605\u2192                # Detect if this is a document query\n   606\u2192                is_doc_query = self._is_document_query(query)\n   607\u2192                \n   608\u2192                # Expand query if it's a document query\n   609\u2192                queries = [query]\n   610\u2192                if is_doc_query:\n   611\u2192                    queries = self._expand_document_query(query)\n   612\u2192                    semantic = True  # Force semantic search for natural language\n   613\u2192                \n   614\u2192                opts = {\"semantic\": semantic, \"limit\": limit}\n   615\u2192                all_results = []\n   616\u2192                \n   617\u2192                # Search with all query variations\n   618\u2192                for search_query in queries:\n   619\u2192                    for p in self._plugins:\n   620\u2192                        try:\n   621\u2192                            for result in p.search(search_query, opts):\n   622\u2192                                all_results.append(result)\n   623\u2192                        except Exception as e:\n   624\u2192                            logger.warning(f\"Plugin {p.lang} failed to search for {search_query}: {e}\")\n   625\u2192                \n   626\u2192                # Deduplicate results\n   627\u2192                seen = set()\n   628\u2192                unique_results = []\n   629\u2192                for result in all_results:\n   630\u2192                    key = f\"{result['file']}:{result['line']}\"\n   631\u2192                    if key not in seen:\n   632\u2192                        seen.add(key)\n   633\u2192                        unique_results.append(result)\n   634\u2192                \n   635\u2192                # Sort by score if available\n   636\u2192                unique_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   637\u2192                \n   638\u2192                # Prioritize documentation files for document queries\n   639\u2192                if is_doc_query:\n   640\u2192                    doc_results = []\n   641\u2192                    code_results = []\n   642\u2192                    for result in unique_results:\n   643\u2192                        if self._is_documentation_file(result.get('file', '')):\n   644\u2192                            doc_results.append(result)\n   645\u2192                        else:\n   646\u2192                            code_results.append(result)\n   647\u2192                    unique_results = doc_results + code_results\n   648\u2192                \n   649\u2192                # Apply limit\n   650\u2192                count = 0\n   651\u2192                for result in unique_results:\n   652\u2192                    if limit and count >= limit:\n   653\u2192                        break\n   654\u2192                    yield result\n   655\u2192                    count += 1\n   656\u2192                \n   657\u2192                self._operation_stats['searches'] += 1\n   658\u2192                self._operation_stats['total_time'] += time.time() - start_time\n   659\u2192                \n   660\u2192        except Exception as e:\n   661\u2192            logger.error(f\"Error in search for {query}: {e}\", exc_info=True)\n   662\u2192    \n   663\u2192    def index_file(self, path: Path) -> None:\n   664\u2192        \"\"\"Index a single file if it has changed.\"\"\"\n   665\u2192        try:\n   666\u2192            # Ensure path is absolute to avoid relative/absolute path issues\n   667\u2192            path = path.resolve()\n   668\u2192            \n   669\u2192            # Find the appropriate plugin\n   670\u2192            plugin = self._match_plugin(path)\n   671\u2192            \n   672\u2192            # Read file content\n   673\u2192            try:\n   674\u2192                content = path.read_text(encoding='utf-8')\n   675\u2192            except UnicodeDecodeError:\n   676\u2192                # Try with different encodings\n   677\u2192                try:\n   678\u2192                    content = path.read_text(encoding='latin-1')\n   679\u2192                except Exception as e:\n   680\u2192                    logger.error(f\"Failed to read {path}: {e}\")\n   681\u2192                    return\n   682\u2192            \n   683\u2192            # Check if we need to re-index (simplified for now)\n   684\u2192            # TODO: Implement proper caching logic\n   685\u2192            \n   686\u2192            # Index the file\n   687\u2192            start_time = time.time()\n   688\u2192            logger.info(f\"Indexing {path} with {plugin.lang} plugin\")\n   689\u2192            shard = plugin.indexFile(path, content)\n   690\u2192            \n   691\u2192            # Record performance if advanced features enabled\n   692\u2192            if self._enable_advanced and self._router:\n   693\u2192                execution_time = time.time() - start_time\n   694\u2192                self._router.record_performance(plugin, execution_time)\n   695\u2192            \n   696\u2192            self._operation_stats['indexings'] += 1\n   697\u2192            self._operation_stats['total_time'] += time.time() - start_time\n   698\u2192            \n   699\u2192            logger.info(f\"Successfully indexed {path}: {len(shard.get('symbols', []))} symbols found\")\n   700\u2192            \n   701\u2192        except RuntimeError as e:\n   702\u2192            # No plugin found for this file type\n   703\u2192            logger.debug(f\"No plugin for {path}: {e}\")\n   704\u2192        except Exception as e:\n   705\u2192            logger.error(f\"Error indexing {path}: {e}\", exc_info=True)\n   706\u2192    \n   707\u2192    def get_statistics(self) -> dict:\n   708\u2192        \"\"\"Get comprehensive statistics across all plugins and components.\"\"\"\n   709\u2192        stats = {\n   710\u2192            \"total_plugins\": len(self._plugins),\n   711\u2192            \"loaded_languages\": sorted(list(self._loaded_languages)),\n   712\u2192            \"supported_languages\": len(self.supported_languages),\n   713\u2192            \"operations\": self._operation_stats.copy()\n   714\u2192        }\n   715\u2192        \n   716\u2192        # Add language breakdown\n   717\u2192        stats[\"by_language\"] = {}\n   718\u2192        for lang, plugin in self._by_lang.items():\n   719\u2192            plugin_info = {\n   720\u2192                \"loaded\": True,\n   721\u2192                \"class\": plugin.__class__.__name__\n   722\u2192            }\n   723\u2192            if hasattr(plugin, 'get_indexed_count'):\n   724\u2192                plugin_info[\"indexed_files\"] = plugin.get_indexed_count()\n   725\u2192            stats[\"by_language\"][lang] = plugin_info\n   726\u2192        \n   727\u2192        return stats\n   728\u2192    \n   729\u2192    def index_directory(self, directory: Path, recursive: bool = True) -> Dict[str, int]:\n   730\u2192        \"\"\"\n   731\u2192        Index all files in a directory, respecting ignore patterns.\n   732\u2192        \n   733\u2192        Args:\n   734\u2192            directory: Directory to index\n   735\u2192            recursive: Whether to index subdirectories\n   736\u2192            \n   737\u2192        Returns:\n   738\u2192            Statistics about indexed files\n   739\u2192        \"\"\"\n   740\u2192        logger.info(f\"Indexing directory: {directory} (recursive={recursive})\")\n   741\u2192        \n   742\u2192        # Note: We don't use ignore patterns during indexing\n   743\u2192        # ALL files are indexed for local search capability\n   744\u2192        # Filtering happens only during export/sharing\n   745\u2192        \n   746\u2192        # Get all supported extensions\n   747\u2192        supported_extensions = get_all_extensions()\n   748\u2192        \n   749\u2192        stats = {\n   750\u2192            \"total_files\": 0,\n   751\u2192            \"indexed_files\": 0,\n   752\u2192            \"ignored_files\": 0,\n   753\u2192            \"failed_files\": 0,\n   754\u2192            \"by_language\": {}\n   755\u2192        }\n   756\u2192        \n   757\u2192        # Walk directory\n   758\u2192        if recursive:\n   759\u2192            file_iterator = directory.rglob(\"*\")\n   760\u2192        else:\n   761\u2192            file_iterator = directory.glob(\"*\")\n   762\u2192            \n   763\u2192        for path in file_iterator:\n   764\u2192            if not path.is_file():\n   765\u2192                continue\n   766\u2192                \n   767\u2192            stats[\"total_files\"] += 1\n   768\u2192            \n   769\u2192            # NOTE: We index ALL files locally, including gitignored ones\n   770\u2192            # Filtering happens only during export/sharing\n   771\u2192            # This allows local search of .env, secrets, etc.\n   772\u2192                \n   773\u2192            # Try to find a plugin that supports this file\n   774\u2192            # This allows us to index ALL files, including .env, .key, etc.\n   775\u2192            try:\n   776\u2192                # First try to match by extension\n   777\u2192                if path.suffix in supported_extensions:\n   778\u2192                    self.index_file(path)\n   779\u2192                    stats[\"indexed_files\"] += 1\n   780\u2192                # For files without recognized extensions, try each plugin's supports() method\n   781\u2192                # This allows plugins to match by filename patterns (e.g., .env, Dockerfile)\n   782\u2192                else:\n   783\u2192                    matched = False\n   784\u2192                    for plugin in self._plugins:\n   785\u2192                        if plugin.supports(path):\n   786\u2192                            self.index_file(path)\n   787\u2192                            stats[\"indexed_files\"] += 1\n   788\u2192                            matched = True\n   789\u2192                            break\n   790\u2192                    \n   791\u2192                    # If no plugin matched but we want to index everything,\n   792\u2192                    # we could add a fallback here to index as plaintext\n   793\u2192                    # For now, we'll skip unmatched files\n   794\u2192                    if not matched:\n   795\u2192                        logger.debug(f\"No plugin found for {path}\")\n   796\u2192                \n   797\u2192                # Track by language\n   798\u2192                language = get_language_by_extension(path.suffix)\n   799\u2192                if language:\n   800\u2192                    stats[\"by_language\"][language] = stats[\"by_language\"].get(language, 0) + 1\n   801\u2192                    \n   802\u2192            except Exception as e:\n   803\u2192                logger.error(f\"Failed to index {path}: {e}\")\n   804\u2192                stats[\"failed_files\"] += 1\n   805\u2192                \n   806\u2192        logger.info(f\"Directory indexing complete: {stats['indexed_files']} indexed, \"\n   807\u2192                   f\"{stats['ignored_files']} ignored, {stats['failed_files']} failed\")\n   808\u2192        \n   809\u2192        return stats\n   810\u2192    \n   811\u2192    def search_documentation(self, topic: str, doc_types: Optional[List[str]] = None, limit: int = 20) -> Iterable[SearchResult]:\n   812\u2192        \"\"\"Search specifically across documentation files.\n   813\u2192        \n   814\u2192        Args:\n   815\u2192            topic: Topic to search for (e.g., \"installation\", \"configuration\")\n   816\u2192            doc_types: Optional list of document types to search (e.g., [\"readme\", \"guide\", \"api\"])\n   817\u2192            limit: Maximum number of results\n   818\u2192            \n   819\u2192        Returns:\n   820\u2192            Search results from documentation files\n   821\u2192        \"\"\"\n   822\u2192        # Default document types if not specified\n   823\u2192        if doc_types is None:\n   824\u2192            doc_types = [\"readme\", \"documentation\", \"guide\", \"tutorial\", \"api\", \"changelog\", \"contributing\"]\n   825\u2192        \n   826\u2192        # Build search queries for different document types\n   827\u2192        queries = []\n   828\u2192        for doc_type in doc_types:\n   829\u2192            queries.extend([\n   830\u2192                f\"{doc_type} {topic}\",\n   831\u2192                f\"{topic} {doc_type}\",\n   832\u2192                f\"{topic} in {doc_type}\"\n   833\u2192            ])\n   834\u2192        \n   835\u2192        # Also search for the topic in common doc filenames\n   836\u2192        queries.extend([\n   837\u2192            f\"README {topic}\",\n   838\u2192            f\"CONTRIBUTING {topic}\",\n   839\u2192            f\"docs {topic}\",\n   840\u2192            f\"documentation {topic}\"\n   841\u2192        ])\n   842\u2192        \n   843\u2192        # Deduplicate queries\n   844\u2192        queries = list(dict.fromkeys(queries))\n   845\u2192        \n   846\u2192        logger.info(f\"Cross-document search for '{topic}' with {len(queries)} query variations\")\n   847\u2192        \n   848\u2192        # Use the enhanced search with document-specific handling\n   849\u2192        all_results = []\n   850\u2192        seen = set()\n   851\u2192        \n   852\u2192        for query in queries[:10]:  # Limit to 10 queries to avoid too many searches\n   853\u2192            for result in self.search(query, semantic=True, limit=limit):\n   854\u2192                # Only include documentation files\n   855\u2192                if self._is_documentation_file(result.get('file', '')):\n   856\u2192                    key = f\"{result['file']}:{result['line']}\"\n   857\u2192                    if key not in seen:\n   858\u2192                        seen.add(key)\n   859\u2192                        all_results.append(result)\n   860\u2192        \n   861\u2192        # Sort by relevance (score) and return top results\n   862\u2192        all_results.sort(key=lambda r: r.get('score', 0.5) or 0.5, reverse=True)\n   863\u2192        \n   864\u2192        count = 0\n   865\u2192        for result in all_results:\n   866\u2192            if count >= limit:\n   867\u2192                break\n   868\u2192            yield result\n   869\u2192            count += 1\n   870\u2192    \n   871\u2192    def health_check(self) -> Dict[str, Any]:\n   872\u2192        \"\"\"Perform a health check on all components.\"\"\"\n   873\u2192        health = {\n   874\u2192            'status': 'healthy',\n   875\u2192            'components': {\n   876\u2192                'dispatcher': {\n   877\u2192                    'status': 'healthy',\n   878\u2192                    'plugins_loaded': len(self._plugins),\n   879\u2192                    'languages_supported': len(self.supported_languages),\n   880\u2192                    'factory_enabled': self._use_factory,\n   881\u2192                    'lazy_loading': self._lazy_load\n   882\u2192                }\n   883\u2192            },\n   884\u2192            'plugins': {},\n   885\u2192            'errors': []\n   886\u2192        }\n   887\u2192        \n   888\u2192        # Check plugin health\n   889\u2192        for lang, plugin in self._by_lang.items():\n   890\u2192            try:\n   891\u2192                plugin_health = {\n   892\u2192                    'status': 'healthy',\n   893\u2192                    'class': plugin.__class__.__name__,\n   894\u2192                    'semantic_enabled': getattr(plugin, '_enable_semantic', False)\n   895\u2192                }\n   896\u2192                if hasattr(plugin, 'get_indexed_count'):\n   897\u2192                    plugin_health['indexed_files'] = plugin.get_indexed_count()\n   898\u2192            except Exception as e:\n   899\u2192                plugin_health = {\n   900\u2192                    'status': 'error',\n   901\u2192                    'error': str(e)\n   902\u2192                }\n   903\u2192                health['errors'].append(f\"Plugin {lang}: {str(e)}\")\n   904\u2192            \n   905\u2192            health['plugins'][lang] = plugin_health\n   906\u2192        \n   907\u2192        # Determine overall health\n   908\u2192        if len(health['errors']) > 0:\n   909\u2192            health['status'] = 'degraded' if len(health['errors']) < 3 else 'unhealthy'\n   910\u2192        \n   911\u2192        return health\n   912\u2192    \n   913\u2192    def remove_file(self, path: Union[Path, str]) -> None:\n   914\u2192        \"\"\"Remove a file from all indexes.\n   915\u2192        \n   916\u2192        Args:\n   917\u2192            path: File path to remove\n   918\u2192        \"\"\"\n   919\u2192        path = Path(path).resolve()\n   920\u2192        logger.info(f\"Removing file from index: {path}\")\n   921\u2192        \n   922\u2192        try:\n   923\u2192            # Remove from SQLite if available\n   924\u2192            if self._sqlite_store:\n   925\u2192                from ..core.path_resolver import PathResolver\n   926\u2192                path_resolver = PathResolver()\n   927\u2192                try:\n   928\u2192                    relative_path = path_resolver.normalize_path(path)\n   929\u2192                    # Get repository ID - for now assume 1\n   930\u2192                    # TODO: Properly detect repository\n   931\u2192                    self._sqlite_store.remove_file(relative_path, repository_id=1)\n   932\u2192                except Exception as e:\n   933\u2192                    logger.error(f\"Error removing from SQLite: {e}\")\n   934\u2192            \n   935\u2192            # Remove from semantic index if available\n   936\u2192            try:\n   937\u2192                plugin = self._match_plugin(path)\n   938\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   939\u2192                    plugin._indexer.remove_file(path)\n   940\u2192                    logger.info(f\"Removed from semantic index: {path}\")\n   941\u2192            except Exception as e:\n   942\u2192                logger.warning(f\"Error removing from semantic index: {e}\")\n   943\u2192            \n   944\u2192            # Update statistics\n   945\u2192            self._operation_stats['deletions'] = self._operation_stats.get('deletions', 0) + 1\n   946\u2192            \n   947\u2192        except Exception as e:\n   948\u2192            logger.error(f\"Error removing file {path}: {e}\", exc_info=True)\n   949\u2192    \n   950\u2192    def move_file(self, old_path: Union[Path, str], new_path: Union[Path, str], content_hash: Optional[str] = None) -> None:\n   951\u2192        \"\"\"Move a file in all indexes.\n   952\u2192        \n   953\u2192        Args:\n   954\u2192            old_path: Original file path\n   955\u2192            new_path: New file path\n   956\u2192            content_hash: Optional content hash to verify unchanged content\n   957\u2192        \"\"\"\n   958\u2192        old_path = Path(old_path).resolve()\n   959\u2192        new_path = Path(new_path).resolve()\n   960\u2192        logger.info(f\"Moving file in index: {old_path} -> {new_path}\")\n   961\u2192        \n   962\u2192        try:\n   963\u2192            # Move in SQLite if available\n   964\u2192            if self._sqlite_store:\n   965\u2192                from ..core.path_resolver import PathResolver\n   966\u2192                path_resolver = PathResolver()\n   967\u2192                try:\n   968\u2192                    old_relative = path_resolver.normalize_path(old_path)\n   969\u2192                    new_relative = path_resolver.normalize_path(new_path)\n   970\u2192                    # Get repository ID - for now assume 1\n   971\u2192                    # TODO: Properly detect repository\n   972\u2192                    self._sqlite_store.move_file(old_relative, new_relative, repository_id=1, content_hash=content_hash)\n   973\u2192                except Exception as e:\n   974\u2192                    logger.error(f\"Error moving in SQLite: {e}\")\n   975\u2192            \n   976\u2192            # Move in semantic index if available\n   977\u2192            try:\n   978\u2192                plugin = self._match_plugin(new_path)\n   979\u2192                if plugin and hasattr(plugin, '_indexer') and plugin._indexer:\n   980\u2192                    plugin._indexer.move_file(old_path, new_path, content_hash)\n   981\u2192                    logger.info(f\"Moved in semantic index: {old_path} -> {new_path}\")\n   982\u2192            except Exception as e:\n   983\u2192                logger.warning(f\"Error moving in semantic index: {e}\")\n   984\u2192            \n   985\u2192            # Update statistics\n   986\u2192            self._operation_stats['moves'] = self._operation_stats.get('moves', 0) + 1\n   987\u2192            \n   988\u2192        except Exception as e:\n   989\u2192            logger.error(f\"Error moving file {old_path} -> {new_path}: {e}\", exc_info=True)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 14367,
    "cache_creation_tokens": 13323,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "bce49748-dbd5-4f39-8bef-d4ed666a4850",
    "test_date": "2025-06-26T07:32:40.779Z",
    "query_id": "transcript_bce49748-dbd5-4f39-8bef-d4ed666a4850",
    "query_text": "Find the BM25Indexer class definition",
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 4,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 4,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 58,
    "output_tokens_total": 58,
    "cache_read_tokens": 13916,
    "cache_creation_tokens": 0,
    "tokens_per_result": 58,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "bce49748-dbd5-4f39-8bef-d4ed666a4850",
    "test_date": "2025-06-26T07:32:43.685Z",
    "query_id": "transcript_bce49748-dbd5-4f39-8bef-d4ed666a4850",
    "query_text": [
      {
        "tool_use_id": "toolu_01FJfEB7KEmYNe7Pkss7F6iC",
        "type": "tool_result",
        "content": "Found 1 file\n/workspaces/Code-Index-MCP/testing-env/worktree-native/mcp_server/indexer/bm25_indexer.py"
      }
    ],
    "category": "transcript_query",
    "complexity": "unknown",
    "approach": "native",
    "response_time_ms": 0,
    "success": true,
    "input_tokens_user": 7,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 7,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 1,
    "output_tokens_total": 1,
    "cache_read_tokens": 14142,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "transcript_response"
  },
  {
    "session_id": "1750923060",
    "test_date": "2025-06-26T07:31:00.864352",
    "query_id": "symbol_001",
    "query_text": "Find the BM25Indexer class definition",
    "category": "symbol_queries",
    "complexity": "simple",
    "approach": "mcp",
    "response_time_ms": 8027.41,
    "success": true,
    "input_tokens_user": 9,
    "input_tokens_context": 0,
    "input_tokens_tools": 7,
    "input_tokens_files": 15,
    "input_tokens_total": 31,
    "output_tokens_reasoning": 3,
    "output_tokens_tools": 3,
    "output_tokens_code": 5,
    "output_tokens_explanations": 3,
    "output_tokens_total": 7,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 38.75,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923060",
    "test_date": "2025-06-26T07:31:00.864990",
    "query_id": "symbol_001",
    "query_text": "Find the BM25Indexer class definition",
    "category": "symbol_queries",
    "complexity": "simple",
    "approach": "native",
    "response_time_ms": 7132.414,
    "success": true,
    "input_tokens_user": 9,
    "input_tokens_context": 0,
    "input_tokens_tools": 4,
    "input_tokens_files": 9,
    "input_tokens_total": 19,
    "output_tokens_reasoning": 2,
    "output_tokens_tools": 1,
    "output_tokens_code": 3,
    "output_tokens_explanations": 2,
    "output_tokens_total": 4,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 23.75,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923060",
    "test_date": "2025-06-26T07:31:08.895373",
    "query_id": "symbol_002",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "symbol_queries",
    "complexity": "medium",
    "approach": "mcp",
    "response_time_ms": 22183.313,
    "success": true,
    "input_tokens_user": 14,
    "input_tokens_context": 0,
    "input_tokens_tools": 106,
    "input_tokens_files": 213,
    "input_tokens_total": 426,
    "output_tokens_reasoning": 53,
    "output_tokens_tools": 42,
    "output_tokens_code": 71,
    "output_tokens_explanations": 53,
    "output_tokens_total": 106,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 532.5,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923060",
    "test_date": "2025-06-26T07:31:08.895432",
    "query_id": "symbol_002",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "symbol_queries",
    "complexity": "medium",
    "approach": "native",
    "response_time_ms": 23155.100000000002,
    "success": true,
    "input_tokens_user": 14,
    "input_tokens_context": 0,
    "input_tokens_tools": 115,
    "input_tokens_files": 230,
    "input_tokens_total": 461,
    "output_tokens_reasoning": 57,
    "output_tokens_tools": 46,
    "output_tokens_code": 76,
    "output_tokens_explanations": 57,
    "output_tokens_total": 115,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 576.25,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923377",
    "test_date": "2025-06-26T07:36:17.083057",
    "query_id": "symbol_001",
    "query_text": "Find the BM25Indexer class definition",
    "category": "symbol_queries",
    "complexity": "simple",
    "approach": "mcp",
    "response_time_ms": 8195.618,
    "success": true,
    "input_tokens_user": 9,
    "input_tokens_context": 0,
    "input_tokens_tools": 7,
    "input_tokens_files": 15,
    "input_tokens_total": 31,
    "output_tokens_reasoning": 3,
    "output_tokens_tools": 3,
    "output_tokens_code": 5,
    "output_tokens_explanations": 3,
    "output_tokens_total": 7,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 38.75,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923377",
    "test_date": "2025-06-26T07:36:17.083543",
    "query_id": "symbol_001",
    "query_text": "Find the BM25Indexer class definition",
    "category": "symbol_queries",
    "complexity": "simple",
    "approach": "native",
    "response_time_ms": 6191.5019999999995,
    "success": true,
    "input_tokens_user": 9,
    "input_tokens_context": 0,
    "input_tokens_tools": 4,
    "input_tokens_files": 9,
    "input_tokens_total": 19,
    "output_tokens_reasoning": 2,
    "output_tokens_tools": 1,
    "output_tokens_code": 3,
    "output_tokens_explanations": 2,
    "output_tokens_total": 4,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 23.75,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923377",
    "test_date": "2025-06-26T07:36:25.281056",
    "query_id": "symbol_002",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "symbol_queries",
    "complexity": "medium",
    "approach": "mcp",
    "response_time_ms": 21137.219999999998,
    "success": true,
    "input_tokens_user": 14,
    "input_tokens_context": 0,
    "input_tokens_tools": 126,
    "input_tokens_files": 253,
    "input_tokens_total": 507,
    "output_tokens_reasoning": 63,
    "output_tokens_tools": 50,
    "output_tokens_code": 84,
    "output_tokens_explanations": 63,
    "output_tokens_total": 126,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 633.75,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923377",
    "test_date": "2025-06-26T07:36:25.281110",
    "query_id": "symbol_002",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "symbol_queries",
    "complexity": "medium",
    "approach": "native",
    "response_time_ms": 18932.867000000002,
    "success": true,
    "input_tokens_user": 14,
    "input_tokens_context": 0,
    "input_tokens_tools": 98,
    "input_tokens_files": 196,
    "input_tokens_total": 392,
    "output_tokens_reasoning": 49,
    "output_tokens_tools": 39,
    "output_tokens_code": 65,
    "output_tokens_explanations": 49,
    "output_tokens_total": 98,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 490.0,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923377",
    "test_date": "2025-06-26T07:36:46.420898",
    "query_id": "content_001",
    "query_text": "Find all functions that use asyncio",
    "category": "content_queries",
    "complexity": "simple",
    "approach": "mcp",
    "response_time_ms": 120085.959,
    "success": false,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 1,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 0,
    "output_tokens_total": 0,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1.25,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "failure"
  },
  {
    "session_id": "1750923377",
    "test_date": "2025-06-26T07:36:46.420949",
    "query_id": "content_001",
    "query_text": "Find all functions that use asyncio",
    "category": "content_queries",
    "complexity": "simple",
    "approach": "native",
    "response_time_ms": 120086.18,
    "success": false,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 1,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 0,
    "output_tokens_total": 0,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1.25,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "failure"
  },
  {
    "session_id": "1750923551",
    "test_date": "2025-06-26T07:39:11.442461",
    "query_id": "symbol_001",
    "query_text": "Find the BM25Indexer class definition",
    "category": "symbol_queries",
    "complexity": "simple",
    "approach": "mcp",
    "response_time_ms": 6746.713,
    "success": true,
    "input_tokens_user": 9,
    "input_tokens_context": 0,
    "input_tokens_tools": 7,
    "input_tokens_files": 15,
    "input_tokens_total": 31,
    "output_tokens_reasoning": 3,
    "output_tokens_tools": 3,
    "output_tokens_code": 5,
    "output_tokens_explanations": 3,
    "output_tokens_total": 7,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 38.75,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923551",
    "test_date": "2025-06-26T07:39:11.443389",
    "query_id": "symbol_001",
    "query_text": "Find the BM25Indexer class definition",
    "category": "symbol_queries",
    "complexity": "simple",
    "approach": "native",
    "response_time_ms": 13552.759999999998,
    "success": true,
    "input_tokens_user": 9,
    "input_tokens_context": 0,
    "input_tokens_tools": 9,
    "input_tokens_files": 18,
    "input_tokens_total": 37,
    "output_tokens_reasoning": 4,
    "output_tokens_tools": 3,
    "output_tokens_code": 6,
    "output_tokens_explanations": 4,
    "output_tokens_total": 9,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 46.25,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923551",
    "test_date": "2025-06-26T07:39:24.998742",
    "query_id": "symbol_002",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "symbol_queries",
    "complexity": "medium",
    "approach": "mcp",
    "response_time_ms": 20918.44,
    "success": true,
    "input_tokens_user": 14,
    "input_tokens_context": 0,
    "input_tokens_tools": 117,
    "input_tokens_files": 235,
    "input_tokens_total": 471,
    "output_tokens_reasoning": 58,
    "output_tokens_tools": 47,
    "output_tokens_code": 78,
    "output_tokens_explanations": 58,
    "output_tokens_total": 117,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 588.75,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923551",
    "test_date": "2025-06-26T07:39:24.998789",
    "query_id": "symbol_002",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "symbol_queries",
    "complexity": "medium",
    "approach": "native",
    "response_time_ms": 21368.458,
    "success": true,
    "input_tokens_user": 14,
    "input_tokens_context": 0,
    "input_tokens_tools": 122,
    "input_tokens_files": 245,
    "input_tokens_total": 490,
    "output_tokens_reasoning": 61,
    "output_tokens_tools": 49,
    "output_tokens_code": 81,
    "output_tokens_explanations": 61,
    "output_tokens_total": 122,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 612.5,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923551",
    "test_date": "2025-06-26T07:39:46.369653",
    "query_id": "content_001",
    "query_text": "Find all functions that use asyncio",
    "category": "content_queries",
    "complexity": "simple",
    "approach": "mcp",
    "response_time_ms": 120088.21399999999,
    "success": false,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 1,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 0,
    "output_tokens_total": 0,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1.25,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "failure"
  },
  {
    "session_id": "1750923551",
    "test_date": "2025-06-26T07:39:46.369696",
    "query_id": "content_001",
    "query_text": "Find all functions that use asyncio",
    "category": "content_queries",
    "complexity": "simple",
    "approach": "native",
    "response_time_ms": 119996.527,
    "success": false,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 1,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 0,
    "output_tokens_total": 0,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1.25,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "failure"
  },
  {
    "session_id": "1750923160",
    "test_date": "2025-06-26T07:32:40.149536",
    "query_id": "symbol_001",
    "query_text": "Find the BM25Indexer class definition",
    "category": "symbol_queries",
    "complexity": "simple",
    "approach": "mcp",
    "response_time_ms": 11638.653,
    "success": true,
    "input_tokens_user": 9,
    "input_tokens_context": 0,
    "input_tokens_tools": 8,
    "input_tokens_files": 16,
    "input_tokens_total": 32,
    "output_tokens_reasoning": 4,
    "output_tokens_tools": 3,
    "output_tokens_code": 5,
    "output_tokens_explanations": 4,
    "output_tokens_total": 8,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 40.0,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923160",
    "test_date": "2025-06-26T07:32:40.149739",
    "query_id": "symbol_001",
    "query_text": "Find the BM25Indexer class definition",
    "category": "symbol_queries",
    "complexity": "simple",
    "approach": "native",
    "response_time_ms": 6298.263999999999,
    "success": true,
    "input_tokens_user": 9,
    "input_tokens_context": 0,
    "input_tokens_tools": 4,
    "input_tokens_files": 9,
    "input_tokens_total": 19,
    "output_tokens_reasoning": 2,
    "output_tokens_tools": 1,
    "output_tokens_code": 3,
    "output_tokens_explanations": 2,
    "output_tokens_total": 4,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 23.75,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923160",
    "test_date": "2025-06-26T07:32:51.789971",
    "query_id": "symbol_002",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "symbol_queries",
    "complexity": "medium",
    "approach": "mcp",
    "response_time_ms": 23412.188000000002,
    "success": true,
    "input_tokens_user": 14,
    "input_tokens_context": 0,
    "input_tokens_tools": 153,
    "input_tokens_files": 306,
    "input_tokens_total": 613,
    "output_tokens_reasoning": 76,
    "output_tokens_tools": 61,
    "output_tokens_code": 102,
    "output_tokens_explanations": 76,
    "output_tokens_total": 153,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 766.25,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923160",
    "test_date": "2025-06-26T07:32:51.790023",
    "query_id": "symbol_002",
    "query_text": "Find the EnhancedDispatcher class and show its main methods",
    "category": "symbol_queries",
    "complexity": "medium",
    "approach": "native",
    "response_time_ms": 55664.453,
    "success": true,
    "input_tokens_user": 14,
    "input_tokens_context": 0,
    "input_tokens_tools": 50,
    "input_tokens_files": 101,
    "input_tokens_total": 203,
    "output_tokens_reasoning": 25,
    "output_tokens_tools": 20,
    "output_tokens_code": 33,
    "output_tokens_explanations": 25,
    "output_tokens_total": 50,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 253.75,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "success"
  },
  {
    "session_id": "1750923160",
    "test_date": "2025-06-26T07:33:47.457369",
    "query_id": "content_001",
    "query_text": "Find all functions that use asyncio",
    "category": "content_queries",
    "complexity": "simple",
    "approach": "mcp",
    "response_time_ms": 120026.60299999999,
    "success": false,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 1,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 0,
    "output_tokens_total": 0,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1.25,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "failure"
  },
  {
    "session_id": "1750923160",
    "test_date": "2025-06-26T07:33:47.457416",
    "query_id": "content_001",
    "query_text": "Find all functions that use asyncio",
    "category": "content_queries",
    "complexity": "simple",
    "approach": "native",
    "response_time_ms": 120082.291,
    "success": false,
    "input_tokens_user": 8,
    "input_tokens_context": 0,
    "input_tokens_tools": 0,
    "input_tokens_files": 0,
    "input_tokens_total": 1,
    "output_tokens_reasoning": 0,
    "output_tokens_tools": 0,
    "output_tokens_code": 0,
    "output_tokens_explanations": 0,
    "output_tokens_total": 0,
    "cache_read_tokens": 0,
    "cache_creation_tokens": 0,
    "tokens_per_result": 1.25,
    "context_utilization": 0.5,
    "generation_efficiency": 0.25,
    "tool_call_count": 0,
    "tools_used": [],
    "file_reads": [],
    "file_writes": [],
    "accuracy_score": null,
    "result_quality": "failure"
  }
]