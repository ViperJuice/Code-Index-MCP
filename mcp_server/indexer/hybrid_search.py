"""
Hybrid Search implementation combining BM25 and semantic search.

This module implements reciprocal rank fusion (RRF) to combine results from
multiple search methods, providing better overall search quality.
"""

import logging
import asyncio
from typing import List, Dict, Any, Optional, Tuple, Set
from dataclasses import dataclass, field
from collections import defaultdict
import numpy as np

from .bm25_indexer import BM25Indexer
from ..utils.semantic_indexer import SemanticIndexer
from ..storage.sqlite_store import SQLiteStore
from .query_optimizer import Query, QueryType
from .reranker import RerankerFactory, IReranker

# Import only what we need to avoid circular dependencies
from ..config.settings import RerankingSettings

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """Individual search result from any search method."""

    doc_id: str
    filepath: str
    score: float
    snippet: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    source: str = ""  # 'bm25', 'semantic', 'fuzzy', etc.


@dataclass
class HybridSearchConfig:
    """Configuration for hybrid search."""

    # Weight configuration
    bm25_weight: float = 0.5
    semantic_weight: float = 0.3
    fuzzy_weight: float = 0.2

    # RRF parameters
    rrf_k: int = 60  # Reciprocal Rank Fusion constant

    # Search parameters
    enable_bm25: bool = True
    enable_semantic: bool = True
    enable_fuzzy: bool = True

    # Result limits
    individual_limit: int = 50  # Results per search method
    final_limit: int = 20  # Final results after fusion

    # Optimization
    parallel_execution: bool = True
    cache_results: bool = True

    # Minimum scores
    min_bm25_score: float = -10.0
    min_semantic_score: float = 0.5
    min_fuzzy_score: float = 0.3


class HybridSearch:
    """
    Hybrid search combining multiple search methods with reciprocal rank fusion.

    This class orchestrates multiple search backends (BM25, semantic, fuzzy) and
    combines their results using configurable fusion strategies.
    """

    def __init__(
        self,
        storage: SQLiteStore,
        bm25_indexer: Optional[BM25Indexer] = None,
        semantic_indexer: Optional[SemanticIndexer] = None,
        fuzzy_indexer: Optional[Any] = None,
        config: Optional[HybridSearchConfig] = None,
        reranking_settings: Optional[RerankingSettings] = None,
    ):
        """
        Initialize hybrid search.

        Args:
            storage: SQLite storage backend
            bm25_indexer: BM25 full-text search indexer
            semantic_indexer: Semantic/vector search indexer
            fuzzy_indexer: Fuzzy search indexer
            config: Hybrid search configuration
            reranking_settings: Settings for result reranking
        """
        self.storage = storage
        self.bm25_indexer = bm25_indexer
        self.semantic_indexer = semantic_indexer
        self.fuzzy_indexer = fuzzy_indexer
        self.config = config or HybridSearchConfig()
        self.reranking_settings = reranking_settings

        # Initialize reranker if enabled
        self.reranker: Optional[IReranker] = None
        if reranking_settings and reranking_settings.enabled:
            self._initialize_reranker()

        # Result cache
        self._result_cache: Dict[str, List[SearchResult]] = {}

        # Statistics
        self._search_stats = defaultdict(int)

    def _initialize_reranker(self):
        """Initialize the reranker based on settings."""
        try:
            factory = RerankerFactory()
            config = {
                "cohere_api_key": self.reranking_settings.cohere_api_key,
                "model": self.reranking_settings.cohere_model,
                "device": self.reranking_settings.cross_encoder_device,
                "primary_type": self.reranking_settings.hybrid_primary_type,
                "fallback_type": self.reranking_settings.hybrid_fallback_type,
                "weight_primary": self.reranking_settings.hybrid_primary_weight,
                "weight_fallback": self.reranking_settings.hybrid_fallback_weight,
                "cache_ttl": self.reranking_settings.cache_ttl,
            }

            self.reranker = factory.create_reranker(
                self.reranking_settings.reranker_type, config
            )

            # Initialize reranker asynchronously will be done on first use
            logger.info(f"Initialized {self.reranking_settings.reranker_type} reranker")
        except Exception as e:
            logger.error(f"Failed to initialize reranker: {e}")
            self.reranker = None

    async def search(
        self,
        query: str,
        query_type: Optional[QueryType] = None,
        filters: Optional[Dict[str, Any]] = None,
        limit: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        Perform hybrid search combining multiple search methods.

        Args:
            query: Search query
            query_type: Optional query type hint
            filters: Optional filters (language, file path, etc.)
            limit: Maximum number of results

        Returns:
            List of combined search results
        """
        limit = limit or self.config.final_limit

        # Check cache if enabled
        cache_key = self._get_cache_key(query, filters)
        if self.config.cache_results and cache_key in self._result_cache:
            self._search_stats["cache_hits"] += 1
            cached_results = self._result_cache[cache_key]
            return self._format_results(cached_results[:limit])

        # Collect results from different search methods
        all_results = []

        if self.config.parallel_execution:
            # Execute searches in parallel
            all_results = await self._parallel_search(query, query_type, filters)
        else:
            # Execute searches sequentially
            all_results = await self._sequential_search(query, query_type, filters)

        # Combine results using reciprocal rank fusion
        combined_results = self._reciprocal_rank_fusion(all_results)

        # Apply reranking if enabled
        if (
            self.reranker
            and self.reranking_settings
            and self.reranking_settings.enabled
        ):
            combined_results = await self._rerank_results(query, combined_results)

        # Apply post-processing
        final_results = self._post_process_results(combined_results, limit)

        # Cache results if enabled
        if self.config.cache_results:
            self._result_cache[cache_key] = final_results
            self._cleanup_cache()

        # Update statistics
        self._search_stats["total_searches"] += 1

        return self._format_results(final_results)

    async def _parallel_search(
        self,
        query: str,
        query_type: Optional[QueryType],
        filters: Optional[Dict[str, Any]],
    ) -> List[List[SearchResult]]:
        """Execute searches in parallel."""
        tasks = []

        if self.config.enable_bm25 and self.bm25_indexer:
            tasks.append(self._search_bm25(query, filters))

        if self.config.enable_semantic and self.semantic_indexer:
            tasks.append(self._search_semantic(query, filters))

        if self.config.enable_fuzzy and self.fuzzy_indexer:
            tasks.append(self._search_fuzzy(query, filters))

        # Execute all searches in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Filter out exceptions and empty results
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Search error in method {i}: {result}")
            elif result:
                valid_results.append(result)

        return valid_results

    async def _sequential_search(
        self,
        query: str,
        query_type: Optional[QueryType],
        filters: Optional[Dict[str, Any]],
    ) -> List[List[SearchResult]]:
        """Execute searches sequentially."""
        all_results = []

        if self.config.enable_bm25 and self.bm25_indexer:
            try:
                bm25_results = await self._search_bm25(query, filters)
                if bm25_results:
                    all_results.append(bm25_results)
            except Exception as e:
                logger.error(f"BM25 search error: {e}")

        if self.config.enable_semantic and self.semantic_indexer:
            try:
                semantic_results = await self._search_semantic(query, filters)
                if semantic_results:
                    all_results.append(semantic_results)
            except Exception as e:
                logger.error(f"Semantic search error: {e}")

        if self.config.enable_fuzzy and self.fuzzy_indexer:
            try:
                fuzzy_results = await self._search_fuzzy(query, filters)
                if fuzzy_results:
                    all_results.append(fuzzy_results)
            except Exception as e:
                logger.error(f"Fuzzy search error: {e}")

        return all_results

    async def _search_bm25(
        self, query: str, filters: Optional[Dict[str, Any]]
    ) -> List[SearchResult]:
        """Perform BM25 search."""
        # Run BM25 search in thread pool since it's synchronous
        loop = asyncio.get_event_loop()

        def run_search():
            kwargs = filters or {}
            results = self.bm25_indexer.search(
                query, limit=self.config.individual_limit, **kwargs
            )

            search_results = []
            for r in results:
                if r["score"] >= self.config.min_bm25_score:
                    search_results.append(
                        SearchResult(
                            doc_id=r.get("filepath", ""),
                            filepath=r.get("filepath", ""),
                            score=r["score"],
                            snippet=r.get("snippet", ""),
                            metadata=r,
                            source="bm25",
                        )
                    )
            return search_results

        results = await loop.run_in_executor(None, run_search)
        self._search_stats["bm25_searches"] += 1
        return results

    async def _search_semantic(
        self, query: str, filters: Optional[Dict[str, Any]]
    ) -> List[SearchResult]:
        """Perform semantic search."""
        # Run semantic search
        loop = asyncio.get_event_loop()

        def run_search():
            # Semantic search with filters
            kwargs = {
                "k": self.config.individual_limit,
                "threshold": self.config.min_semantic_score,
            }
            if filters:
                kwargs.update(filters)

            results = self.semantic_indexer.search(query, **kwargs)

            search_results = []
            for r in results:
                search_results.append(
                    SearchResult(
                        doc_id=r.get("filepath", ""),
                        filepath=r.get("filepath", ""),
                        score=r.get("score", 0.0),
                        snippet=r.get("content", "")[:200],
                        metadata=r,
                        source="semantic",
                    )
                )
            return search_results

        results = await loop.run_in_executor(None, run_search)
        self._search_stats["semantic_searches"] += 1
        return results

    async def _search_fuzzy(
        self, query: str, filters: Optional[Dict[str, Any]]
    ) -> List[SearchResult]:
        """Perform fuzzy search."""
        loop = asyncio.get_event_loop()

        def run_search():
            # Fuzzy search
            results = self.fuzzy_indexer.search_fuzzy(
                query,
                max_results=self.config.individual_limit,
                threshold=self.config.min_fuzzy_score,
            )

            search_results = []
            for r in results:
                if r["score"] >= self.config.min_fuzzy_score:
                    search_results.append(
                        SearchResult(
                            doc_id=r.get("file_path", ""),
                            filepath=r.get("file_path", ""),
                            score=r["score"],
                            snippet=r.get("context", ""),
                            metadata=r,
                            source="fuzzy",
                        )
                    )
            return search_results

        results = await loop.run_in_executor(None, run_search)
        self._search_stats["fuzzy_searches"] += 1
        return results

    def _reciprocal_rank_fusion(
        self, result_lists: List[List[SearchResult]]
    ) -> List[SearchResult]:
        """
        Combine results using Reciprocal Rank Fusion (RRF).

        RRF score = Σ(1 / (k + rank_i)) for each result list
        where k is a constant (typically 60) and rank_i is the rank in list i
        """
        # Track scores for each document
        doc_scores: Dict[str, float] = defaultdict(float)
        doc_info: Dict[str, SearchResult] = {}

        # Calculate RRF scores
        for result_list in result_lists:
            for rank, result in enumerate(result_list):
                doc_id = result.doc_id

                # RRF score for this ranking
                rrf_score = 1.0 / (self.config.rrf_k + rank + 1)

                # Apply source-specific weights
                if result.source == "bm25":
                    rrf_score *= self.config.bm25_weight
                elif result.source == "semantic":
                    rrf_score *= self.config.semantic_weight
                elif result.source == "fuzzy":
                    rrf_score *= self.config.fuzzy_weight

                doc_scores[doc_id] += rrf_score

                # Keep the result with the best individual score
                if doc_id not in doc_info or result.score > doc_info[doc_id].score:
                    doc_info[doc_id] = result

        # Create combined results
        combined = []
        for doc_id, combined_score in doc_scores.items():
            result = doc_info[doc_id]
            # Update score to combined RRF score
            result.score = combined_score
            combined.append(result)

        # Sort by combined score
        combined.sort(key=lambda x: x.score, reverse=True)

        return combined

    async def _rerank_results(
        self, query: str, results: List[SearchResult]
    ) -> List[SearchResult]:
        """Rerank search results using the configured reranker."""
        if not self.reranker or not results:
            return results

        try:
            # Initialize reranker if not already done
            if not hasattr(self, "_reranker_initialized"):
                init_result = await self.reranker.initialize({})
                if not init_result.is_success:
                    logger.error(f"Failed to initialize reranker: {init_result.error}")
                    return results
                self._reranker_initialized = True

            # Import SearchResult from reranker module to avoid circular dependency
            from .reranker import SearchResult as RerankSearchResult

            # Convert our SearchResult to reranker's SearchResult format
            reranker_results = []
            for r in results:
                # Create a reranker-compatible search result
                reranker_result = RerankSearchResult(
                    file_path=r.filepath,
                    line=r.metadata.get("line", 1),
                    column=r.metadata.get("column", 0),
                    snippet=r.snippet,
                    match_type=r.source,  # Use source as match type
                    score=r.score,
                    context=r.metadata.get("context", ""),
                )
                reranker_results.append(reranker_result)

            # Perform reranking
            top_k = (
                self.reranking_settings.top_k
                if self.reranking_settings
                else len(reranker_results)
            )
            rerank_result = await self.reranker.rerank(
                query, reranker_results, top_k=top_k
            )

            if not rerank_result.is_success:
                logger.warning(f"Reranking failed: {rerank_result.error}")
                return results

            # Handle the new RerankResult structure
            # rerank_result.data is a RerankResult object with 'results' list and 'metadata' dict
            rerank_data = rerank_result.data
            if not rerank_data:
                logger.warning("Reranking returned no data")
                return results

            if not hasattr(rerank_data, "results"):
                logger.error("Unexpected rerank result structure")
                return results

            if not rerank_data.results:
                logger.warning("Reranking returned empty results")
                return results

            # Convert back to our SearchResult format, preserving all original metadata
            reranked_results = []
            for rerank_item in rerank_data.results:
                # Validate original_rank is within bounds
                if rerank_item.original_rank < 0 or rerank_item.original_rank >= len(
                    results
                ):
                    logger.warning(
                        f"Invalid original_rank {rerank_item.original_rank} for {len(results)} results"
                    )
                    continue

                # Get the original result using the original_rank
                original = results[rerank_item.original_rank]

                # Create a new SearchResult with updated score but preserved metadata
                updated_result = SearchResult(
                    doc_id=original.doc_id,
                    filepath=original.filepath,
                    score=rerank_item.rerank_score,  # Use the new rerank score
                    snippet=original.snippet,
                    metadata=original.metadata.copy(),  # Preserve all original metadata
                    source=original.source,
                )

                # Add reranking metadata
                updated_result.metadata["original_score"] = original.score
                updated_result.metadata["rerank_score"] = rerank_item.rerank_score
                updated_result.metadata["original_rank"] = rerank_item.original_rank
                updated_result.metadata["new_rank"] = rerank_item.new_rank
                if rerank_item.explanation:
                    updated_result.metadata["rerank_explanation"] = (
                        rerank_item.explanation
                    )

                reranked_results.append(updated_result)

            # Sort by new rank to ensure proper ordering
            reranked_results.sort(
                key=lambda x: x.metadata.get("new_rank", float("inf"))
            )

            # Log reranking metadata if available
            if hasattr(rerank_data, "metadata") and rerank_data.metadata:
                logger.debug(
                    f"Reranked {len(reranked_results)} results using {rerank_data.metadata.get('reranker', 'unknown')} reranker"
                )
            else:
                logger.debug(f"Reranked {len(reranked_results)} results")

            return reranked_results

        except Exception as e:
            logger.error(f"Error during reranking: {e}", exc_info=True)
            return results

    def _post_process_results(
        self, results: List[SearchResult], limit: int
    ) -> List[SearchResult]:
        """Apply post-processing to results."""
        # Remove duplicates while preserving order
        seen = set()
        unique_results = []
        for result in results:
            if result.filepath not in seen:
                seen.add(result.filepath)
                unique_results.append(result)

        # Apply limit
        unique_results = unique_results[:limit]

        # Enhance snippets if needed
        for result in unique_results:
            if not result.snippet and result.filepath:
                # Try to generate a snippet
                result.snippet = self._generate_snippet(
                    result.filepath, result.metadata
                )

        return unique_results

    def _generate_snippet(self, filepath: str, metadata: Dict[str, Any]) -> str:
        """Generate a snippet for a result."""
        # This is a placeholder - in practice, you'd read the file
        # and extract relevant context
        return f"File: {filepath}"

    def _format_results(self, results: List[SearchResult]) -> List[Dict[str, Any]]:
        """Format results for output, including all metadata."""
        formatted = []
        for i, result in enumerate(results):
            # Ensure metadata exists
            metadata = result.metadata if result.metadata else {}

            # Build base result with all fields
            formatted_result = {
                "rank": i + 1,
                "filepath": result.filepath,
                "score": result.score,
                "snippet": result.snippet,
                "source": result.source,
                # Include all metadata fields
                "line": metadata.get("line", 1),
                "column": metadata.get("column", 0),
                "context": metadata.get("context", ""),
                "match_type": metadata.get("match_type", result.source),
                # Include language and symbol information if present
                "language": metadata.get("language", ""),
                "symbol": metadata.get("symbol", ""),
                "symbol_type": metadata.get("symbol_type", ""),
                # Include reranking information if present
                "original_score": metadata.get("original_score", result.score),
                "rerank_score": metadata.get("rerank_score"),
                "original_rank": metadata.get("original_rank"),
                "new_rank": metadata.get("new_rank"),
                "rerank_explanation": metadata.get("rerank_explanation"),
                # Include any additional metadata
                "metadata": metadata,
            }

            # Remove None values and empty strings for cleaner output
            formatted_result = {
                k: v
                for k, v in formatted_result.items()
                if v is not None and (not isinstance(v, str) or v != "")
            }

            formatted.append(formatted_result)
        return formatted

    def _get_cache_key(self, query: str, filters: Optional[Dict[str, Any]]) -> str:
        """Generate cache key for a query."""
        import hashlib

        key_parts = [query]
        if filters:
            key_parts.extend(f"{k}:{v}" for k, v in sorted(filters.items()))
        key_string = "|".join(key_parts)
        return hashlib.md5(key_string.encode()).hexdigest()

    def _cleanup_cache(self):
        """Clean up cache if it gets too large."""
        max_cache_size = 1000
        if len(self._result_cache) > max_cache_size:
            # Remove oldest entries (simple FIFO)
            entries_to_remove = len(self._result_cache) - max_cache_size // 2
            for key in list(self._result_cache.keys())[:entries_to_remove]:
                del self._result_cache[key]

    # Configuration methods

    def set_weights(
        self, bm25: float = None, semantic: float = None, fuzzy: float = None
    ):
        """
        Update search method weights.

        Args:
            bm25: Weight for BM25 search (0-1)
            semantic: Weight for semantic search (0-1)
            fuzzy: Weight for fuzzy search (0-1)
        """
        if bm25 is not None:
            self.config.bm25_weight = max(0, min(1, bm25))
        if semantic is not None:
            self.config.semantic_weight = max(0, min(1, semantic))
        if fuzzy is not None:
            self.config.fuzzy_weight = max(0, min(1, fuzzy))

        # Normalize weights
        total = (
            self.config.bm25_weight
            + self.config.semantic_weight
            + self.config.fuzzy_weight
        )
        if total > 0:
            self.config.bm25_weight /= total
            self.config.semantic_weight /= total
            self.config.fuzzy_weight /= total

    def enable_methods(
        self, bm25: bool = None, semantic: bool = None, fuzzy: bool = None
    ):
        """
        Enable or disable search methods.

        Args:
            bm25: Enable/disable BM25 search
            semantic: Enable/disable semantic search
            fuzzy: Enable/disable fuzzy search
        """
        if bm25 is not None:
            self.config.enable_bm25 = bm25
        if semantic is not None:
            self.config.enable_semantic = semantic
        if fuzzy is not None:
            self.config.enable_fuzzy = fuzzy

    def get_statistics(self) -> Dict[str, Any]:
        """Get search statistics."""
        stats = dict(self._search_stats)

        # Add cache statistics
        stats["cache_size"] = len(self._result_cache)
        cache_hit_rate = 0
        if stats.get("total_searches", 0) > 0:
            cache_hit_rate = stats.get("cache_hits", 0) / stats["total_searches"]
        stats["cache_hit_rate"] = cache_hit_rate

        # Add configuration info
        stats["config"] = {
            "weights": {
                "bm25": self.config.bm25_weight,
                "semantic": self.config.semantic_weight,
                "fuzzy": self.config.fuzzy_weight,
            },
            "enabled_methods": {
                "bm25": self.config.enable_bm25,
                "semantic": self.config.enable_semantic,
                "fuzzy": self.config.enable_fuzzy,
            },
            "rrf_k": self.config.rrf_k,
        }

        return stats

    def clear_cache(self):
        """Clear the result cache."""
        self._result_cache.clear()
        logger.info("Hybrid search cache cleared")


class HybridSearchOptimizer:
    """
    Optimizer for hybrid search parameters based on user feedback and performance.
    """

    def __init__(self, hybrid_search: HybridSearch):
        """
        Initialize the optimizer.

        Args:
            hybrid_search: HybridSearch instance to optimize
        """
        self.hybrid_search = hybrid_search
        self.feedback_history: List[Dict[str, Any]] = []
        self.performance_history: List[Dict[str, Any]] = []

    def record_feedback(
        self, query: str, selected_result: int, results: List[Dict[str, Any]]
    ):
        """
        Record user feedback on search results.

        Args:
            query: The search query
            selected_result: Index of the result the user selected
            results: The search results shown to the user
        """
        feedback = {
            "query": query,
            "selected_rank": selected_result + 1,
            "selected_source": (
                results[selected_result]["source"]
                if selected_result < len(results)
                else None
            ),
            "num_results": len(results),
            "timestamp": datetime.now(),
        }
        self.feedback_history.append(feedback)

        # Optimize weights after collecting enough feedback
        if len(self.feedback_history) % 10 == 0:
            self.optimize_weights()

    def record_performance(self, query: str, search_time_ms: float, num_results: int):
        """
        Record search performance metrics.

        Args:
            query: The search query
            search_time_ms: Time taken for search in milliseconds
            num_results: Number of results returned
        """
        self.performance_history.append(
            {
                "query": query,
                "search_time_ms": search_time_ms,
                "num_results": num_results,
                "timestamp": datetime.now(),
            }
        )

    def optimize_weights(self):
        """Optimize search weights based on feedback history."""
        if len(self.feedback_history) < 10:
            return

        # Count selections by source
        source_selections = defaultdict(int)
        source_ranks = defaultdict(list)

        for feedback in self.feedback_history[-100:]:  # Last 100 feedbacks
            source = feedback.get("selected_source")
            if source:
                source_selections[source] += 1
                source_ranks[source].append(feedback["selected_rank"])

        # Calculate average rank for each source
        avg_ranks = {}
        for source, ranks in source_ranks.items():
            avg_ranks[source] = sum(ranks) / len(ranks) if ranks else float("inf")

        # Update weights based on selection frequency and average rank
        total_selections = sum(source_selections.values())
        if total_selections > 0:
            # Base weights on selection frequency
            bm25_weight = source_selections.get("bm25", 0) / total_selections
            semantic_weight = source_selections.get("semantic", 0) / total_selections
            fuzzy_weight = source_selections.get("fuzzy", 0) / total_selections

            # Adjust based on average rank (lower is better)
            rank_factor = 0.2
            if "bm25" in avg_ranks:
                bm25_weight *= 1 + rank_factor * (1 / avg_ranks["bm25"])
            if "semantic" in avg_ranks:
                semantic_weight *= 1 + rank_factor * (1 / avg_ranks["semantic"])
            if "fuzzy" in avg_ranks:
                fuzzy_weight *= 1 + rank_factor * (1 / avg_ranks["fuzzy"])

            # Apply new weights
            self.hybrid_search.set_weights(bm25_weight, semantic_weight, fuzzy_weight)

            logger.info(
                f"Optimized weights - BM25: {bm25_weight:.3f}, "
                f"Semantic: {semantic_weight:.3f}, Fuzzy: {fuzzy_weight:.3f}"
            )

    def get_optimization_report(self) -> Dict[str, Any]:
        """Get a report on optimization performance."""
        if not self.feedback_history:
            return {"status": "no_data"}

        # Analyze feedback
        source_stats = defaultdict(lambda: {"count": 0, "avg_rank": 0})
        for feedback in self.feedback_history:
            source = feedback.get("selected_source")
            if source:
                stats = source_stats[source]
                stats["count"] += 1
                stats["avg_rank"] = (
                    stats["avg_rank"] * (stats["count"] - 1) + feedback["selected_rank"]
                ) / stats["count"]

        # Analyze performance
        avg_search_time = 0
        if self.performance_history:
            avg_search_time = sum(
                p["search_time_ms"] for p in self.performance_history
            ) / len(self.performance_history)

        return {
            "feedback_count": len(self.feedback_history),
            "source_statistics": dict(source_stats),
            "average_search_time_ms": avg_search_time,
            "current_weights": {
                "bm25": self.hybrid_search.config.bm25_weight,
                "semantic": self.hybrid_search.config.semantic_weight,
                "fuzzy": self.hybrid_search.config.fuzzy_weight,
            },
        }
